{"2024-03-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13254v2","updated":"2024-03-12T17:59:56Z","published":"2024-02-20T18:59:55Z","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples","summary":"  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n","authors":["Jianrui Zhang","Mu Cai","Tengyang Xie","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2402.13254v2.pdf","comment":"13 pages, 6 figures, 8 tables, Project Page:\n  https://countercurate.github.io/"},{"id":"http://arxiv.org/abs/2403.07872v1","updated":"2024-03-12T17:59:48Z","published":"2024-03-12T17:59:48Z","title":"Rethinking Generative Large Language Model Evaluation for Semantic\n  Comprehension","summary":"  Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.\n","authors":["Fangyun Wei","Xi Chen","Lin Luo"],"pdf_url":"https://arxiv.org/pdf/2403.07872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v1","updated":"2024-03-12T17:55:38Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13165v3","updated":"2024-03-12T17:41:13Z","published":"2024-01-24T00:58:30Z","title":"Misgendering and Assuming Gender in Machine Translation when Working\n  with Low-Resource Languages","summary":"  This chapter focuses on gender-related errors in machine translation (MT) in\nthe context of low-resource languages. We begin by explaining what low-resource\nlanguages are, examining the inseparable social and computational factors that\ncreate such linguistic hierarchies. We demonstrate through a case study of our\nmother tongue Bengali, a global language spoken by almost 300 million people\nbut still classified as low-resource, how gender is assumed and inferred in\ntranslations to and from the high(est)-resource English when no such\ninformation is provided in source texts. We discuss the postcolonial and\nsocietal impacts of such errors leading to linguistic erasure and\nrepresentational harms, and conclude by discussing potential solutions towards\nuplifting languages by providing them more agency in MT conversations.\n","authors":["Sourojit Ghosh","Srishti Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2401.13165v3.pdf","comment":"Upcoming Publication, Gendered Technology in Translation and\n  Interpreting Centering Rights in the Development of Language Technology,\n  Routledge 2024"},{"id":"http://arxiv.org/abs/2403.07825v1","updated":"2024-03-12T17:04:28Z","published":"2024-03-12T17:04:28Z","title":"The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage\n  Brought By Model Editing","summary":"  Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy.However, the editing of these models, crucial for\nrectifying outdated or erroneous information, often leads to a complex issue\nknown as the ripple effect in the hidden space. This effect, while difficult to\ndetect, can significantly impede the efficacy of model editing tasks and\ndeteriorate model performance.This paper addresses this scientific challenge by\nproposing a novel evaluation methodology, Graphical Outlier Relation based\nAssessment(GORA), which quantitatively evaluates the adaptations of the model\nand the subsequent impact of editing. Furthermore, we introduce the Selective\nOutlier Re-Editing Approach(SORA), a model editing method designed to mitigate\nthis ripple effect. Our comprehensive evaluations reveal that the ripple effect\nin the hidden space is a significant issue in all current model editing\nmethods. However, our proposed methods, GORA and SORA, effectively identify and\nalleviate this issue, respectively, contributing to the advancement of LLM\nediting techniques.\n","authors":["Jianchen Wang","Zhouhong Gu","Zhuozhi Xiong","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.07825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v3","updated":"2024-03-12T16:58:53Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.07816v1","updated":"2024-03-12T16:54:58Z","published":"2024-03-12T16:54:58Z","title":"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM","summary":"  We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.\n","authors":["Sainbayar Sukhbaatar","Olga Golovneva","Vasu Sharma","Hu Xu","Xi Victoria Lin","Baptiste Rozière","Jacob Kahn","Daniel Li","Wen-tau Yih","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2403.07816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06602v2","updated":"2024-03-12T16:54:57Z","published":"2023-11-11T16:16:11Z","title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance","summary":"  Answering questions within business and finance requires reasoning,\nprecision, and a wide-breadth of technical knowledge. Together, these\nrequirements make this domain difficult for large language models (LLMs). We\nintroduce BizBench, a benchmark for evaluating models' ability to reason about\nrealistic financial problems. BizBench comprises eight quantitative reasoning\ntasks, focusing on question-answering (QA) over financial data via program\nsynthesis. We include three financially-themed code-generation tasks from newly\ncollected and augmented QA data. Additionally, we isolate the reasoning\ncapabilities required for financial QA: reading comprehension of financial text\nand tables for extracting intermediate values, and understanding financial\nconcepts and formulas needed to calculate complex solutions. Collectively,\nthese tasks evaluate a model's financial background knowledge, ability to parse\nfinancial documents, and capacity to solve problems with code. We conduct an\nin-depth evaluation of open-source and commercial LLMs, comparing and\ncontrasting the behavior of code-focused and language-focused models. We\ndemonstrate that the current bottleneck in performance is due to LLMs' limited\nbusiness and financial understanding, highlighting the value of a challenging\nbenchmark for quantitative reasoning within this domain.\n","authors":["Rik Koncel-Kedziorski","Michael Krumdick","Viet Lai","Varshini Reddy","Charles Lovering","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2311.06602v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.07809v1","updated":"2024-03-12T16:46:54Z","published":"2024-03-12T16:46:54Z","title":"pyvene: A Library for Understanding and Improving PyTorch Models via\n  Interventions","summary":"  Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.\n","authors":["Zhengxuan Wu","Atticus Geiger","Aryaman Arora","Jing Huang","Zheng Wang","Noah D. Goodman","Christopher D. Manning","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2403.07809v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.07805v1","updated":"2024-03-12T16:42:44Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.\ncom/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.18603v3","updated":"2024-03-12T16:35:25Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v3.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.07794v1","updated":"2024-03-12T16:33:30Z","published":"2024-03-12T16:33:30Z","title":"Fine-tuning Large Language Models with Sequential Instructions","summary":"  Large language models (LLMs) struggle to follow a sequence of instructions in\na single query as they may ignore or misinterpret part of it. This impairs\ntheir performance in complex problems whose solution requires multiple\nintermediate steps, such as multilingual (translate then answer) and multimodal\n(caption then answer) tasks. We empirically verify this with open-source LLMs\nas large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential\ninstructions in present-day data, we propose sequential instruction tuning, a\nsimple yet effective strategy to automatically augment instruction tuning data\nand equip LLMs with the ability to execute multiple sequential instructions.\nAfter exploring interleaving instructions in existing datasets, such as Alpaca,\nwith a wide range of intermediate tasks, we find that sequential\ninstruction-tuned models consistently outperform the conventional\ninstruction-tuned baselines in downstream tasks involving reasoning,\nmultilingual, and multimodal abilities. To shed further light on our technique,\nwe analyse how adversarial intermediate texts, unseen tasks, prompt\nverbalization, number of tasks, and prompt length affect SIT. We hope that this\nmethod will open new research avenues on instruction tuning for complex tasks.\n","authors":["Hanxu Hu","Pinzhen Chen","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2403.07794v1.pdf","comment":"11pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.08543v3","updated":"2024-03-12T16:15:19Z","published":"2023-06-14T14:44:03Z","title":"Knowledge Distillation of Large Language Models","summary":"  Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\nhttps://github.com/microsoft/LMOps/tree/main/minillm.\n","authors":["Yuxian Gu","Li Dong","Furu Wei","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2306.08543v3.pdf","comment":"Published as a conference paper in ICLR 2024"},{"id":"http://arxiv.org/abs/2402.12177v4","updated":"2024-03-12T16:04:23Z","published":"2024-02-19T14:33:24Z","title":"Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning","summary":"  Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.\n","authors":["Mingtian Zhang","Shawn Lan","Peter Hayes","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.12177v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07769v1","updated":"2024-03-12T15:56:10Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10691v3","updated":"2024-03-12T15:53:06Z","published":"2023-09-19T15:25:42Z","title":"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback","summary":"  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n","authors":["Xingyao Wang","Zihan Wang","Jiateng Liu","Yangyi Chen","Lifan Yuan","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.10691v3.pdf","comment":"ICLR 2024. Code is available on our project website:\n  https://xingyaoww.github.io/mint-bench"},{"id":"http://arxiv.org/abs/2403.06914v2","updated":"2024-03-12T15:52:14Z","published":"2024-03-11T17:03:04Z","title":"MEND: Meta dEmonstratioN Distillation for Efficient and Effective\n  In-Context Learning","summary":"  Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models\n","authors":["Yichuan Li","Xiyao Ma","Sixing Lu","Kyumin Lee","Xiaohu Liu","Chenlei Guo"],"pdf_url":"https://arxiv.org/pdf/2403.06914v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.11725v2","updated":"2024-03-12T15:48:17Z","published":"2024-01-22T07:07:06Z","title":"Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language\n  Conversion for Language Models","summary":"  Symbols (or more broadly, non-natural language textual representations) such\nas numerical sequences, molecular formulas, and table delimiters widely exist,\nplaying important roles in various tasks such as abstract reasoning, chemical\nproperty prediction, and table question answering. Despite the impressive\nnatural language comprehension capabilities of large language models (LLMs),\ntheir reasoning abilities for symbols remain inadequate, which could attributed\nto the difference between symbol representations and general natural languages.\nWe propose symbol-to-language (S2L), a tuning-free method that enables large\nlanguage models to solve symbol-related problems with information expressed in\nnatural language. Specifically, S2L first converts the symbols involved to\nlanguage-based representations, which can be implemented by prompting LLMs or\nleveraging external tools, then these language-based representations are\nintegrated into the original problem via direct substitution or concatenation,\nserving as useful input information for LLMs. We evaluate the S2L method using\nboth API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight\nsymbol-related tasks, ranging from symbol-only abstract reasoning to sentiment\nanalysis in social media. Experimental results show that S2L consistently leads\nto superior performance. For example, by employing S2L for GPT-4, there can be\naverage significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and\nDyck language, respectively. Codes and data are available at\nhttps://github.com/THUNLP-MT/symbol2language.\n","authors":["Yile Wang","Sijie Cheng","Zixin Sun","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.11725v2.pdf","comment":"ICLR AGI Workshop 2024"},{"id":"http://arxiv.org/abs/2403.07747v1","updated":"2024-03-12T15:32:39Z","published":"2024-03-12T15:32:39Z","title":"FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models","summary":"  To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.\n","authors":["Yan Liu","Renren Jin","Lin Shi","Zheng Yao","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07726v1","updated":"2024-03-12T15:06:22Z","published":"2024-03-12T15:06:22Z","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes","summary":"  This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.\n","authors":["Timothee Mickus","Elaine Zosa","Raúl Vázquez","Teemu Vahtola","Jörg Tiedemann","Vincent Segonne","Alessandro Raganato","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2403.07726v1.pdf","comment":"SemEval 2024 shared task. Pre-review version"},{"id":"http://arxiv.org/abs/2403.07714v1","updated":"2024-03-12T14:57:40Z","published":"2024-03-12T14:57:40Z","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models","summary":"  Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.\n","authors":["Zhicheng Guo","Sijie Cheng","Hao Wang","Shihao Liang","Yujia Qin","Peng Li","Zhiyuan Liu","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09582v5","updated":"2024-03-12T14:55:29Z","published":"2023-02-19T14:21:33Z","title":"Language-Specific Representation of Emotion-Concept Knowledge Causally\n  Supports Emotion Inference","summary":"  Humans no doubt use language to communicate about their emotional\nexperiences, but does language in turn help humans understand emotions, or is\nlanguage just a vehicle of communication? This study used a form of artificial\nintelligence (AI) known as large language models (LLMs) to assess whether\nlanguage-based representations of emotion causally contribute to the AI's\nability to generate inferences about the emotional meaning of novel situations.\nFourteen attributes of human emotion concept representation were found to be\nrepresented by the LLM's distinct artificial neuron populations. By\nmanipulating these attribute-related neurons, we in turn demonstrated the role\nof emotion concept knowledge in generative emotion inference. The\nattribute-specific performance deterioration was related to the importance of\ndifferent attributes in human mental space. Our findings provide a\nproof-in-concept that even a LLM can learn about emotions in the absence of\nsensory-motor representations and highlight the contribution of\nlanguage-derived emotion-concept knowledge for emotion inference.\n","authors":["Ming Li","Yusheng Su","Hsiu-Yuan Huang","Jiali Cheng","Xin Hu","Xinmiao Zhang","Huadong Wang","Yujia Qin","Xiaozhi Wang","Kristen A. Lindquist","Zhiyuan Liu","Dan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.09582v5.pdf","comment":"44 pages, 14 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.07708v1","updated":"2024-03-12T14:51:57Z","published":"2024-03-12T14:51:57Z","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive\n  Rewards","summary":"  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.\n","authors":["Wei Shen","Xiaoying Zhang","Yuanshun Yao","Rui Zheng","Hongyi Guo","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01070v3","updated":"2024-03-12T14:50:30Z","published":"2023-11-02T08:37:30Z","title":"Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech\n  Models via Language-Specific Experts","summary":"  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.\n","authors":["Thomas Palmeira Ferraz","Marcely Zanon Boito","Caroline Brun","Vassilina Nikoulina"],"pdf_url":"https://arxiv.org/pdf/2311.01070v3.pdf","comment":"Accepted to IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.07693v1","updated":"2024-03-12T14:37:03Z","published":"2024-03-12T14:37:03Z","title":"Large, Small or Both: A Novel Data Augmentation Framework Based on\n  Language Models for Debiasing Opinion Summarization","summary":"  As more than 70$\\%$ of reviews in the existing opinion summary data set are\npositive, current opinion summarization approaches are reluctant to generate\nnegative summaries given the input of negative texts. To address such sentiment\nbias, a direct approach without the over-reliance on a specific framework is to\ngenerate additional data based on large language models to balance the\nemotional distribution of the dataset. However, data augmentation based on\nlarge language models faces two disadvantages: 1) the potential issues or\ntoxicity in the augmented data; 2) the expensive costs. Therefore, in this\npaper, we propose a novel data augmentation framework based on both large and\nsmall language models for debiasing opinion summarization. In specific, a small\nsize of synthesized negative reviews is obtained by rewriting the positive text\nvia a large language model. Then, a disentangle reconstruction model is trained\nbased on the generated data. After training, a large amount of synthetic data\ncan be obtained by decoding the new representation obtained from the\ncombination of different sample representations and filtering based on\nconfusion degree and sentiment classification. Experiments have proved that our\nframework can effectively alleviate emotional bias same as using only large\nmodels, but more economically.\n","authors":["Yanyue Zhang","Pengfei Li","Yilong Lai","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.07693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07691v1","updated":"2024-03-12T14:34:08Z","published":"2024-03-12T14:34:08Z","title":"Reference-free Monolithic Preference Optimization with Odds Ratio","summary":"  While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.\nWe release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and\nMistral-ORPO-$\\beta$ (7B).\n","authors":["Jiwoo Hong","Noah Lee","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2403.07691v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.07690v1","updated":"2024-03-12T14:33:53Z","published":"2024-03-12T14:33:53Z","title":"SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted\n  Technical Debt","summary":"  Self-admitted technical debt (SATD) refers to a form of technical debt in\nwhich developers explicitly acknowledge and document the existence of technical\nshortcuts, workarounds, or temporary solutions within the codebase. Over recent\nyears, researchers have manually labeled datasets derived from various software\ndevelopment artifacts: source code comments, messages from the issue tracker\nand pull request sections, and commit messages. These datasets are designed for\ntraining, evaluation, performance validation, and improvement of machine\nlearning and deep learning models to accurately identify SATD instances.\nHowever, class imbalance poses a serious challenge across all the existing\ndatasets, particularly when researchers are interested in categorizing the\nspecific types of SATD. In order to address the scarcity of labeled data for\nSATD \\textit{identification} (i.e., whether an instance is SATD or not) and\n\\textit{categorization} (i.e., which type of SATD is being classified) in\nexisting datasets, we share the \\textit{SATDAUG} dataset, an augmented version\nof existing SATD datasets, including source code comments, issue tracker, pull\nrequests, and commit messages. These augmented datasets have been balanced in\nrelation to the available artifacts and provide a much richer source of labeled\ndata for training machine learning or deep learning models.\n","authors":["Edi Sutoyo","Andrea Capiluppi"],"pdf_url":"https://arxiv.org/pdf/2403.07690v1.pdf","comment":"Accepted to be published at the 21st IEEE/ACM International\n  Conference on Mining Software Repositories (MSR 2024)"},{"id":"http://arxiv.org/abs/2403.07687v1","updated":"2024-03-12T14:27:17Z","published":"2024-03-12T14:27:17Z","title":"Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost","summary":"  Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.\n","authors":["Oana Ignat","Longju Bai","Joan Nwatu","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.07687v1.pdf","comment":"accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2403.07678v1","updated":"2024-03-12T14:12:59Z","published":"2024-03-12T14:12:59Z","title":"MoralBERT: Detecting Moral Values in Social Discourse","summary":"  Morality plays a fundamental role in how we perceive information while\ngreatly influencing our decisions and judgements. Controversial topics,\nincluding vaccination, abortion, racism, and sexuality, often elicit opinions\nand attitudes that are not solely based on evidence but rather reflect moral\nworldviews. Recent advances in natural language processing have demonstrated\nthat moral values can be gauged in human-generated textual content. Here, we\ndesign a range of language representation models fine-tuned to capture exactly\nthe moral nuances in text, called MoralBERT. We leverage annotated moral data\nfrom three distinct sources: Twitter, Reddit, and Facebook user-generated\ncontent covering various socially relevant topics. This approach broadens\nlinguistic diversity and potentially enhances the models' ability to comprehend\nmorality in various contexts. We also explore a domain adaptation technique and\ncompare it to the standard fine-tuned BERT model, using two different\nframeworks for moral prediction: single-label and multi-label. We compare\nin-domain approaches with conventional models relying on lexicon-based\ntechniques, as well as a Machine Learning classifier with Word2Vec\nrepresentation. Our results showed that in-domain prediction models\nsignificantly outperformed traditional models. While the single-label setting\nreaches a higher accuracy than previously achieved for the task when using BERT\npretrained models. Experiments in an out-of-domain setting, instead, suggest\nthat further work is needed for existing domain adaptation techniques to\ngeneralise between different social media platforms, especially for the\nmulti-label task. The investigations and outcomes from this study pave the way\nfor further exploration, enabling a more profound comprehension of moral\nnarratives about controversial social issues.\n","authors":["Vjosa Preniqi","Iacopo Ghinassi","Kyriaki Kalimeri","Charalampos Saitis"],"pdf_url":"https://arxiv.org/pdf/2403.07678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12243v3","updated":"2024-03-12T13:52:13Z","published":"2024-02-19T15:58:15Z","title":"Understanding the Effects of Noise in Text-to-SQL: An Examination of the\n  BIRD-Bench Benchmark","summary":"  Text-to-SQL, which involves translating natural language into Structured\nQuery Language (SQL), is crucial for enabling broad access to structured\ndatabases without expert knowledge. However, designing models for such tasks is\nchallenging due to numerous factors, including the presence of 'noise,' such as\nambiguous questions and syntactical errors. This study provides an in-depth\nanalysis of the distribution and types of noise in the widely used BIRD-Bench\nbenchmark and the impact of noise on models. While BIRD-Bench was created to\nmodel dirty and noisy database values, it was not created to contain noise and\nerrors in the questions and gold queries. We found that noise in questions and\ngold queries are prevalent in the dataset, with varying amounts across domains,\nand with an uneven distribution between noise types. The presence of incorrect\ngold SQL queries, which then generate incorrect gold answers, has a significant\nimpact on the benchmark's reliability. Surprisingly, when evaluating models on\ncorrected SQL queries, zero-shot baselines surpassed the performance of\nstate-of-the-art prompting methods. We conclude that informative noise labels\nand reliable benchmarks are crucial to developing new Text-to-SQL methods that\ncan handle varying types of noise.\n","authors":["Niklas Wretblad","Fredrik Gordh Riseby","Rahul Biswas","Amin Ahmadi","Oskar Holmström"],"pdf_url":"https://arxiv.org/pdf/2402.12243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07652v1","updated":"2024-03-12T13:41:15Z","published":"2024-03-12T13:41:15Z","title":"Harder Tasks Need More Experts: Dynamic Routing in MoE Models","summary":"  In this paper, we introduce a novel dynamic expert selection framework for\nMixture of Experts (MoE) models, aiming to enhance computational efficiency and\nmodel performance by adjusting the number of activated experts based on input\ndifficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,\nwhich activates a predetermined number of experts regardless of the input's\ncomplexity, our method dynamically selects experts based on the confidence\nlevel in expert selection for each input. This allows for a more efficient\nutilization of computational resources, activating more experts for complex\ntasks requiring advanced reasoning and fewer for simpler tasks. Through\nextensive evaluations, our dynamic routing method demonstrates substantial\nimprovements over conventional Top-2 routing across various benchmarks,\nachieving an average improvement of 0.7% with less than 90% activated\nparameters. Further analysis shows our model dispatches more experts to tasks\nrequiring complex reasoning skills, like BBH, confirming its ability to\ndynamically allocate computational resources in alignment with the input's\ncomplexity. Our findings also highlight a variation in the number of experts\nneeded across different layers of the transformer model, offering insights into\nthe potential for designing heterogeneous MoE frameworks. The code and models\nare available at https://github.com/ZhenweiAn/Dynamic_MoE.\n","authors":["Quzhe Huang","Zhenwei An","Nan Zhuang","Mingxu Tao","Chen Zhang","Yang Jin","Kun Xu","Kun Xu","Liwei Chen","Songfang Huang","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03646v2","updated":"2024-03-12T13:38:31Z","published":"2023-10-05T16:21:36Z","title":"TRAM: Bridging Trust Regions and Sharpness Aware Minimization","summary":"  Sharpness-aware minimization (SAM) reports improving domain generalization by\nreducing the loss surface curvature in the parameter space. However,\ngeneralization during fine-tuning is often more dependent on the\ntransferability of representations in the function space. Trust-region methods\n(TR) target this goal by regularizing representation curvature to reduce\ncatastrophic forgetting of pre-trained task-agnostic information while adopting\ntask-specific skills. We consider unifying these strategies for low curvature\nin both parameter space and function space to improve out-of-domain (OOD)\ngeneralization. We propose Trust Region Aware Minimization (TRAM), a SAM\nalgorithm fine-tuning for low parameter sharpness and smooth, informative\nrepresentations preserving pre-trained structure. TRAM uses a trust region\nbound to inform the SAM adversarial neighborhood, introducing an awareness of\nfunction curvature within optimization for flatter minima. We empirically\nvalidate TRAM in vision (cross-dataset adaptation) and text (OOD language\nmodeling, zero-shot cross-lingual transfer) tasks where robust domain transfer\nand representation generality are critical. TRAM outperforms SAM- and TR-based\noptimization across all tasks, notably surpassing competing methods for hard\ntransfer between anticorrelated domains. TRAM establishes a novel standard in\nfine-tuning for domain-generalizable models with minimal additional computation\nover previous sharpness-aware methods.\n","authors":["Tom Sherborne","Naomi Saphra","Pradeep Dasigi","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2310.03646v2.pdf","comment":"Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14\n  tables, 2 figures"},{"id":"http://arxiv.org/abs/2212.07249v3","updated":"2024-03-12T13:30:16Z","published":"2022-12-14T14:34:15Z","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","summary":"  Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n","authors":["Jiashuo Sun","Hang Zhang","Chen Lin","Xiangdong Su","Yeyun Gong","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2212.07249v3.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2309.11911v4","updated":"2024-03-12T12:54:36Z","published":"2023-09-21T09:22:07Z","title":"InstructERC: Reforming Emotion Recognition in Conversation with a\n  Retrieval Multi-task LLMs Framework","summary":"  The field of emotion recognition of conversation (ERC) has been focusing on\nseparating sentence feature encoding and context modeling, lacking exploration\nin generative paradigms based on unified designs. In this study, we propose a\nnovel approach,\n  \\textbf{InstructERC}, to reformulate the ERC task from a discriminative\nframework to a generative framework based on Large Language Models (LLMs).\n  InstructERC makes three significant contributions: (1) it introduces a simple\nyet effective retrieval template module, which helps the model explicitly\nintegrate multi-granularity dialogue supervision information. (2) We introduce\ntwo additional emotion alignment tasks, namely speaker identification and\nemotion prediction tasks, to implicitly model the dialogue role relationships\nand future emotional tendencies in conversations. (3) Pioneeringly, we unify\nemotion labels across benchmarks through the feeling wheel to fit real\napplication scenarios. InstructERC still perform impressively on this unified\ndataset. Our LLM-based plugin framework significantly outperforms all previous\nmodels and achieves comprehensive SOTA on three commonly used ERC datasets.\nExtensive analysis of parameter-efficient and data-scaling experiments provides\nempirical guidance for applying it in practical scenarios. Our code and aligned\nunified dataset (UIME) can be found in the Github link.\\footnote{You can find\nthe offical realization in the Github link:\nhttps://github.com/LIN-SHANG/InstructERC}\n","authors":["Shanglin Lei","Guanting Dong","Xiaoping Wang","Keheng Wang","Sirui Wang"],"pdf_url":"https://arxiv.org/pdf/2309.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19282v5","updated":"2024-03-12T12:27:52Z","published":"2024-02-29T15:49:15Z","title":"WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset","summary":"  This paper presents WanJuan-CC, a safe and high-quality open-sourced English\nwebtext dataset derived from Common Crawl data. The study addresses the\nchallenges of constructing large-scale pre-training datasets for language\nmodels, which require vast amounts of high-quality data. A comprehensive\nprocess was designed to handle Common Crawl data, including extraction,\nheuristic rule filtering, fuzzy deduplication, content safety filtering, and\ndata quality filtering. From approximately 68 billion original English\ndocuments, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of\nhigh-quality data as part of WanJuan-CC. We have open-sourced 100B Tokens from\nthis dataset. The paper also provides statistical information related to data\nquality, enabling users to select appropriate data according to their needs. To\nevaluate the quality and utility of the dataset, we trained 1B-parameter and\n3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results\nshow that WanJuan-CC performs better on validation datasets and downstream\ntasks.\n","authors":["Jiantao Qiu","Haijun Lv","Zhenjiang Jin","Rui Wang","Wenchang Ning","Jia Yu","ChaoBin Zhang","Zhenxiang Li","Pei Chu","Yuan Qu","Jin Shi","Lindong Lu","Runyu Peng","Zhiyuan Zeng","Huanze Tang","Zhikai Lei","Jiawei Hong","Keyu Chen","Zhaoye Fei","Ruiliang Xu","Wei Li","Zhongying Tu","Hang Yan","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2402.19282v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05326v2","updated":"2024-03-12T12:12:36Z","published":"2024-03-08T14:05:36Z","title":"ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\n  Dialogues","summary":"  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.\n","authors":["Yiding Liu","Jingjing Wang","Jiamin Luo","Tao Zeng","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07581v1","updated":"2024-03-12T12:10:18Z","published":"2024-03-12T12:10:18Z","title":"LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced\n  Personality Detection Model","summary":"  Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.\n","authors":["Linmei Hu","Hongyu He","Duokang Wang","Ziwang Zhao","Yingxia Shao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2403.07581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16480v2","updated":"2024-03-12T12:07:39Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text\npairs for visual-language models by recognizing and cleaning pathology reports\nwhich narrate diagnostic slides in TCGA. On the model end, we propose the\nmultiple instance generative model (MI-Gen) which can produce pathology reports\nfor gigapixel WSIs. We benchmark our model on the largest subset of\nTCGA-PathoText. Experimental results show our model can generate pathology\nreports which contain multiple clinical clues. Furthermore, WSI-text prediction\ncan be seen as an approach of visual-language pre-training, which enables our\nmodel to be transferred to downstream diagnostic tasks like carcinoma grading\nand phenotyping. We observe that simple semantic extraction from the pathology\nreports can achieve the best performance (0.838 of F1 score) on BRCA subtyping\nwithout adding extra parameters or tricky fine-tuning. Our collected dataset\nand related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07567v1","updated":"2024-03-12T11:53:27Z","published":"2024-03-12T11:53:27Z","title":"Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource\n  Agglutinative Data-to-Text Generation","summary":"  Most data-to-text datasets are for English, so the difficulties of modelling\ndata-to-text for low-resource languages are largely unexplored. In this paper\nwe tackle data-to-text for isiXhosa, which is low-resource and agglutinative.\nWe introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of\nWebNLG, which presents a new linguistic context that shifts modelling demands\nto subword-driven techniques. We also develop an evaluation framework for T2X\nthat measures how accurately generated text describes the data. This enables\nfuture users of T2X to go beyond surface-level metrics in evaluation. On the\nmodelling side we explore two classes of methods - dedicated data-to-text\nmodels trained from scratch and pretrained language models (PLMs). We propose a\nnew dedicated architecture aimed at agglutinative data-to-text, the Subword\nSegmental Pointer Generator (SSPG). It jointly learns to segment words and copy\nentities, and outperforms existing dedicated models for 2 agglutinative\nlanguages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,\nwhich reveals that standard PLMs come up short. Fine-tuning machine translation\nmodels emerges as the best method overall. These findings underscore the\ndistinct challenge presented by T2X: neither well-established data-to-text\narchitectures nor customary pretrained methodologies prove optimal. We conclude\nwith a qualitative analysis of generation errors and an ablation study.\n","authors":["Francois Meyer","Jan Buys"],"pdf_url":"https://arxiv.org/pdf/2403.07567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07557v1","updated":"2024-03-12T11:41:51Z","published":"2024-03-12T11:41:51Z","title":"SIFiD: Reassess Summary Factual Inconsistency Detection with LLM","summary":"  Ensuring factual consistency between the summary and the original document is\nparamount in summarization tasks. Consequently, considerable effort has been\ndedicated to detecting inconsistencies. With the advent of Large Language\nModels (LLMs), recent studies have begun to leverage their advanced language\nunderstanding capabilities for inconsistency detection. However, early attempts\nhave shown that LLMs underperform traditional models due to their limited\nability to follow instructions and the absence of an effective detection\nmethodology. In this study, we reassess summary inconsistency detection with\nLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\nLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\nDetection with Filtered Document) that identify key sentences within documents\nby either employing natural language inference or measuring semantic similarity\nbetween summaries and documents.\n","authors":["Jiuding Yang","Hui Liu","Weidong Guo","Zhuwei Rao","Yu Xu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2403.07557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07556v1","updated":"2024-03-12T11:40:44Z","published":"2024-03-12T11:40:44Z","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts","summary":"  Although large language models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by the untruthful context\nprovided by users or knowledge argumentation tools, thereby producing\nhallucinations. To alleviate the LLMs from being misled by untruthful\ninformation and take advantage of knowledge argumentation, we propose\nTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\ncontext from the inputs. TACS begins by performing truth detection on the input\ncontext, leveraging the parameterized knowledge within the LLM. Subsequently,\nit constructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results show that TACS can effectively\nfilter information in context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07556v1.pdf","comment":"Code is available at: https://github.com/ictnlp/TACS"},{"id":"http://arxiv.org/abs/2403.07544v1","updated":"2024-03-12T11:32:30Z","published":"2024-03-12T11:32:30Z","title":"MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki","summary":"  NLP in the age of monolithic large language models is approaching its limits\nin terms of size and information that can be handled. The trend goes to\nmodularization, a necessary step into the direction of designing smaller\nsub-networks and components with specialized functionality. In this paper, we\npresent the MAMMOTH toolkit: a framework designed for training massively\nmultilingual modular machine translation systems at scale, initially derived\nfrom OpenNMT-py and then adapted to ensure efficient training across\ncomputation clusters. We showcase its efficiency across clusters of A100 and\nV100 NVIDIA GPUs, and discuss our design philosophy and plans for future\ninformation. The toolkit is publicly available online.\n","authors":["Timothee Mickus","Stig-Arne Grönroos","Joseph Attieh","Michele Boggia","Ona De Gibert","Shaoxiong Ji","Niki Andreas Lopi","Alessandro Raganato","Raúl Vázquez","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2403.07544v1.pdf","comment":"Presented as a demo at EACL 2024"},{"id":"http://arxiv.org/abs/2403.05820v2","updated":"2024-03-12T11:26:07Z","published":"2024-03-09T06:59:47Z","title":"An Audio-textual Diffusion Model For Converting Speech Signals Into\n  Ultrasound Tongue Imaging Data","summary":"  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator\nmovements, such as ultrasound tongue imaging (UTI) data. An issue of existing\nAAI methods is only using the personalized acoustic information to derive the\ngeneral patterns of tongue motions, and thus the quality of generated UTI data\nis limited. To address this issue, this paper proposes an audio-textual\ndiffusion model for the UTI data generation task. In this model, the inherent\nacoustic characteristics of individuals related to the tongue motion details\nare encoded by using wav2vec 2.0, while the ASR transcriptions related to the\nuniversality of tongue motions are encoded by using BERT. UTI data are then\ngenerated by using a diffusion module. Experimental results showed that the\nproposed diffusion model could generate high-quality UTI data with clear tongue\ncontour that is crucial for the linguistic analysis and clinical assessment.\nThe project can be found on the\nwebsite\\footnote{https://yangyudong2020.github.io/wav2uti/\n","authors":["Yudong Yang","Rongfeng Su","Xiaokang Liu","Nan Yan","Lan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.05820v2.pdf","comment":"ICASSP2024 Accept"},{"id":"http://arxiv.org/abs/2403.06412v2","updated":"2024-03-12T10:33:06Z","published":"2024-03-11T03:54:33Z","title":"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean","summary":"  Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.\n","authors":["Eunsu Kim","Juyoung Suk","Philhoon Oh","Haneul Yoo","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.06412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11432v3","updated":"2024-03-12T09:51:35Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v3.pdf","comment":"35 pages, 5 figures, 3 tables, has been accepted by frontiers of\n  computer science (FCS), doi={10.1007/s11704-024-40231-1}"},{"id":"http://arxiv.org/abs/2403.01548v3","updated":"2024-03-12T09:49:28Z","published":"2024-03-03T15:53:41Z","title":"In-Context Sharpness as Alerts: An Inner Representation Perspective for\n  Hallucination Mitigation","summary":"  Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.\n","authors":["Shiqi Chen","Miao Xiong","Junteng Liu","Zhengxuan Wu","Teng Xiao","Siyang Gao","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2403.01548v3.pdf","comment":"code repo is available at:\n  https://github.com/hkust-nlp/Activation_decoding.git"},{"id":"http://arxiv.org/abs/2403.07440v1","updated":"2024-03-12T09:32:25Z","published":"2024-03-12T09:32:25Z","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.31% in the DART and\nWebNLG tasks, respectively.\n","authors":["Yao Liang","Yuwei Wang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13040v5","updated":"2024-03-12T08:52:02Z","published":"2023-05-22T13:47:51Z","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents","summary":"  Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.\n","authors":["Shuzheng Si","Wentao Ma","Haoyu Gao","Yuchuan Wu","Ting-En Lin","Yinpei Dai","Hangyu Li","Rui Yan","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2305.13040v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2211.04118v3","updated":"2024-03-12T08:29:41Z","published":"2022-11-08T09:29:45Z","title":"ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning","summary":"  The prompt has become an effective linguistic tool for utilizing pre-trained\nlanguage models. However, in few-shot scenarios, subtle changes in the prompt\ndesign always make the result widely different, and the prompt learning methods\nalso make it easy to overfit the limited samples. To alleviate this, we explore\nutilizing suitable contrastive samples and multi-degree contrastive learning\nmethods to improve the robustness of the prompt representation. Therefore, the\nproposed Consprompt combined with the prompt encoding network, contrastive\nsampling modules, and contrastive scoring modules, is introduced to realize\ndifferential contrastive learning. Our results exhibit state-of-the-art\nperformance in different few-shot settings, and the ablation experiments also\ncertify the effectiveness of utilizing multi-degree contrastive learning in the\nprompt-based fine-tuning process.\n","authors":["Jinta Weng","Yifan Deng","d Donghao Li","Hao You","Yue Hu","Heyan Huang"],"pdf_url":"https://arxiv.org/pdf/2211.04118v3.pdf","comment":"2 figures"},{"id":"http://arxiv.org/abs/2403.07398v1","updated":"2024-03-12T08:13:52Z","published":"2024-03-12T08:13:52Z","title":"Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs","summary":"  Event commonsense reasoning requires the ability to reason about the\nrelationship between events, as well as infer implicit context underlying that\nrelationship. However, data scarcity makes it challenging for language models\nto learn to generate commonsense inferences for contexts and questions\ninvolving interactions between complex events. To address this demand, we\npresent COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop\nlogical queries (e.g., the joint effect or cause of both event A and B, or the\neffect of the effect of event C) from an existing commonsense knowledge graph\n(CSKG), and verbalizing them using handcrafted rules and large language models\ninto multiple-choice and text generation questions. Our experiments show that\nlanguage models trained on COM2 exhibit significant improvements in complex\nreasoning ability, resulting in enhanced zero-shot performance in both\nin-domain and out-of-domain tasks for question answering and generative\ncommonsense reasoning, without expensive human annotations.\n","authors":["Tianqing Fang","Zeming Chen","Yangqiu Song","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2403.07398v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2403.07384v1","updated":"2024-03-12T07:45:33Z","published":"2024-03-12T07:45:33Z","title":"SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models","summary":"  Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.\n","authors":["Yu Yang","Siddhartha Mishra","Jeffrey N Chiang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2403.07384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01841v2","updated":"2024-03-12T07:34:28Z","published":"2024-03-04T08:38:56Z","title":"Making Pre-trained Language Models Great on Tabular Prediction","summary":"  The transferability of deep neural networks (DNNs) has made significant\nprogress in image and language processing. However, due to the heterogeneity\namong tables, such DNN bonus is still far from being well exploited on tabular\ndata prediction (e.g., regression or classification tasks). Condensing\nknowledge from diverse domains, language models (LMs) possess the capability to\ncomprehend feature names from various tables, potentially serving as versatile\nlearners in transferring knowledge across distinct tables and diverse\nprediction tasks, but their discrete text representation space is inherently\nincompatible with numerical feature values in tables. In this paper, we present\nTP-BERTa, a specifically pre-trained LM for tabular data prediction.\nConcretely, a novel relative magnitude tokenization converts scalar numerical\nfeature values to finely discrete, high-dimensional tokens, and an\nintra-feature attention approach integrates feature values with the\ncorresponding feature names. Comprehensive experiments demonstrate that our\npre-trained TP-BERTa leads the performance among tabular DNNs and is\ncompetitive with Gradient Boosted Decision Tree models in typical tabular data\nregime.\n","authors":["Jiahuan Yan","Bo Zheng","Hongxia Xu","Yiheng Zhu","Danny Z. Chen","Jimeng Sun","Jian Wu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01841v2.pdf","comment":"Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).\n  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be\n  available at https://github.com/jyansir/tp-berta"},{"id":"http://arxiv.org/abs/2403.07379v1","updated":"2024-03-12T07:32:47Z","published":"2024-03-12T07:32:47Z","title":"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The\n  Lengths, Bends, and Dead Ends","summary":"  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich structure of parameters contained within their optimization\ntrajectories. Towards this end, we introduce some natural notions of the\ncomplexity of optimization trajectories, both qualitative and quantitative,\nwhich reveal the inherent nuance and interplay involved between various\noptimization choices, such as momentum, weight decay, and batch size. We use\nthem to provide key hallmarks about the nature of optimization in deep neural\nnetworks: when it goes right, and when it finds itself in a dead end. Further,\nthanks to our trajectory perspective, we uncover an intertwined behaviour of\nmomentum and weight decay that promotes directional exploration, as well as a\ndirectional regularization behaviour of some others. We perform experiments\nover large-scale vision and language settings, including large language models\n(LLMs) with up to 12 billion parameters, to demonstrate the value of our\napproach.\n","authors":["Sidak Pal Singh","Bobby He","Thomas Hofmann","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.07379v1.pdf","comment":"Preprint, 51 pages"},{"id":"http://arxiv.org/abs/2403.07378v1","updated":"2024-03-12T07:31:18Z","published":"2024-03-12T07:31:18Z","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression","summary":"  The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitate LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the remaining model parameters\nafter SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening strategy to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na layer-wise closed-form model parameter update strategy to compensate for\naccuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total\nof 11 datasets and seven models from three different LLM families at four\ndifferent scales. Our results demonstrate the superiority of SVD-LLM over\nstate-of-the-arts, especially at high model compression ratios. The source code\nis available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.\n","authors":["Xin Wang","Yu Zheng","Zhongwei Wan","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07378v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.07376v1","updated":"2024-03-12T07:27:02Z","published":"2024-03-12T07:27:02Z","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning","summary":"  Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.\n","authors":["Bingqian Lin","Yunshuang Nie","Ziming Wei","Jiaqi Chen","Shikui Ma","Jianhua Han","Hang Xu","Xiaojun Chang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2403.07376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09476v3","updated":"2024-03-12T07:00:02Z","published":"2023-07-18T17:56:50Z","title":"Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations","summary":"  Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.\n","authors":["Danny Halawi","Jean-Stanislas Denain","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2307.09476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07350v1","updated":"2024-03-12T06:16:33Z","published":"2024-03-12T06:16:33Z","title":"KEBench: A Benchmark on Knowledge Editing for Large Vision-Language\n  Models","summary":"  Currently, little research has been done on knowledge editing for Large\nVision-Language Models (LVLMs). Editing LVLMs faces the challenge of\neffectively integrating diverse modalities (image and text) while ensuring\ncoherent and contextually relevant modifications. An existing benchmark has\nthree metrics (Reliability, Locality and Generality) to measure knowledge\nediting for LVLMs. However, the benchmark falls short in the quality of\ngenerated images used in evaluation and cannot assess whether models\neffectively utilize edited knowledge in relation to the associated content. We\nadopt different data collection methods to construct a new benchmark,\n$\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive\nevaluation. Leveraging a multimodal knowledge graph, our image data exhibits\nclear directionality towards entities. This directional aspect can be further\nutilized to extract entity-related knowledge and form editing data. We\nconducted experiments of different editing methods on five LVLMs, and\nthoroughly analyze how these methods impact the models. The results reveal\nstrengths and deficiencies of these methods and, hopefully, provide insights\ninto potential avenues for future research.\n","authors":["Han Huang","Haitian Zhong","Qiang Liu","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.07350v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2403.07342v1","updated":"2024-03-12T06:01:04Z","published":"2024-03-12T06:01:04Z","title":"Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive\n  Learning","summary":"  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of\nfine-grained sentiment analysis, aiming to extract structured sentiment\ntriplets from unstructured textual data. Existing approaches to ASTE often\ncomplicate the task with additional structures or external data. In this\nresearch, we propose a novel tagging scheme and employ a contrastive learning\napproach to mitigate these challenges. The proposed approach demonstrates\ncomparable or superior performance in comparison to state-of-the-art\ntechniques, while featuring a more compact design and reduced computational\noverhead. Notably, even in the era of Large Language Models (LLMs), our method\nexhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning\nscenarios. This study also provides valuable insights for the advancement of\nASTE techniques within the paradigm of large language models.\n","authors":["Qiao Sun","Liujia Yang","Minghao Ma","Nanyang Ye","Qinying Gu"],"pdf_url":"https://arxiv.org/pdf/2403.07342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07339v1","updated":"2024-03-12T05:44:27Z","published":"2024-03-12T05:44:27Z","title":"IM-Unpack: Training and Inference with Arbitrarily Low Precision\n  Integers","summary":"  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\ncorresponds to the largest chunk of the compute footprint. Therefore, improving\nits efficiency is an active topic of ongoing research. A popular strategy is\nthe use of low bit-width integers to approximate the original entries in a\nmatrix. This allows efficiency gains, but often requires sophisticated\ntechniques to control the rounding error incurred. In this work, we first\nverify/check that when the low bit-width restriction is removed, for a variety\nof Transformer-based models, whether integers are sufficient for all GEMMs need\n-- for {\\em both} training and inference stages, and can achieve parity with\nfloating point counterparts. No sophisticated techniques are needed. We find\nthat while a large majority of entries in matrices (encountered in such models)\ncan be easily represented by {\\em low} bit-width integers, the existence of a\nfew heavy hitter entries make it difficult to achieve efficiency gains via the\nexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\nsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a\nmatrix with large integer entries into a larger matrix whose entries all lie\nwithin the representable range of arbitrarily low bit-width integers. This\nallows {\\em equivalence} with the original GEMM, i.e., the exact result can be\nobtained using purely low bit-width integer GEMMs. This comes at the cost of\nadditional operations -- we show that for many popular models, this overhead is\nquite small.\n","authors":["Zhanpeng Zeng","Karthikeyan Sankaralingam","Vikas Singh"],"pdf_url":"https://arxiv.org/pdf/2403.07339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03102v3","updated":"2024-03-12T05:33:16Z","published":"2024-03-05T16:43:03Z","title":"\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning","summary":"  Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.\n","authors":["Chuanqi Cheng","Quan Tu","Wei Wu","Shuo Shang","Cunli Mao","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.03102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06259v3","updated":"2024-03-12T05:22:46Z","published":"2023-08-11T17:47:54Z","title":"Self-Alignment with Instruction Backtranslation","summary":"  We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.\n","authors":["Xian Li","Ping Yu","Chunting Zhou","Timo Schick","Omer Levy","Luke Zettlemoyer","Jason Weston","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2308.06259v3.pdf","comment":"ICLR2024 camera ready"},{"id":"http://arxiv.org/abs/2403.07321v1","updated":"2024-03-12T05:15:21Z","published":"2024-03-12T05:15:21Z","title":"GPT-generated Text Detection: Benchmark Dataset and Tensor-based\n  Detection Method","summary":"  As natural language models like ChatGPT become increasingly prevalent in\napplications and services, the need for robust and accurate methods to detect\ntheir output is of paramount importance. In this paper, we present GPT Reddit\nDataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text\ndetection dataset designed to assess the performance of detection models in\nidentifying generated responses from ChatGPT. The dataset consists of a diverse\ncollection of context-prompt pairs based on Reddit, with human-generated and\nChatGPT-generated responses. We provide an analysis of the dataset's\ncharacteristics, including linguistic diversity, context complexity, and\nresponse quality. To showcase the dataset's utility, we benchmark several\ndetection methods on it, demonstrating their efficacy in distinguishing between\nhuman and ChatGPT-generated responses. This dataset serves as a resource for\nevaluating and advancing detection techniques in the context of ChatGPT and\ncontributes to the ongoing efforts to ensure responsible and trustworthy\nAI-driven communication on the internet. Finally, we propose GpTen, a novel\ntensor-based GPT text detection method that is semi-supervised in nature since\nit only has access to human-generated text and performs on par with\nfully-supervised baselines.\n","authors":["Zubair Qazi","William Shiao","Evangelos E. Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2403.07321v1.pdf","comment":"4 pages, 2 figures, published in the WWW 2024 Short Papers Track"},{"id":"http://arxiv.org/abs/2403.07311v1","updated":"2024-03-12T04:47:29Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v1.pdf","comment":"24 pages, 3 figures, submit to ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2401.11624v4","updated":"2024-03-12T04:38:53Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06117v2","updated":"2024-03-12T04:38:27Z","published":"2023-10-09T19:48:55Z","title":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models","summary":"  We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide reasoning, LLMs significantly improve their abilities in following a\ncorrect reasoning path towards the solution. We conduct experiments of\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\nsubstantial performance gains on various challenging reasoning-intensive tasks\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\n","authors":["Huaixiu Steven Zheng","Swaroop Mishra","Xinyun Chen","Heng-Tze Cheng","Ed H. Chi","Quoc V Le","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06117v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2311.09618v3","updated":"2024-03-12T04:14:18Z","published":"2023-11-16T07:01:48Z","title":"Simulating Opinion Dynamics with Networks of LLM-based Agents","summary":"  Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations often over-simplify human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\nproducing accurate information, leading simulated agents to consensus in line\nwith scientific reality. This bias limits their utility for understanding\nresistance to consensus views on issues like climate change. After inducing\nconfirmation bias through prompt engineering, however, we observed opinion\nfragmentation in line with existing agent-based modeling and opinion dynamics\nresearch. These insights highlight the promise and limitations of LLM agents in\nthis domain and suggest a path forward: refining LLMs with real-world discourse\nto better simulate the evolution of human beliefs.\n","authors":["Yun-Shiuan Chuang","Agam Goyal","Nikunj Harlalka","Siddharth Suresh","Robert Hawkins","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2311.09618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07300v1","updated":"2024-03-12T04:04:38Z","published":"2024-03-12T04:04:38Z","title":"Taming Pre-trained LLMs for Generalised Time Series Forecasting via\n  Cross-modal Knowledge Distillation","summary":"  Multivariate time series forecasting has recently gained great success with\nthe rapid growth of deep learning models. However, existing approaches usually\ntrain models from scratch using limited temporal data, preventing their\ngeneralization. Recently, with the surge of the Large Language Models (LLMs),\nseveral works have attempted to introduce LLMs into time series forecasting.\nDespite promising results, these methods directly take time series as the input\nto LLMs, ignoring the inherent modality gap between temporal and text data. In\nthis work, we propose a novel Large Language Models and time series alignment\nframework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time\nseries forecasting challenge. Based on cross-modal knowledge distillation, the\nproposed method exploits both input-agnostic static knowledge and\ninput-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers\nthe forecasting model with favorable performance as well as strong\ngeneralization abilities. Extensive experiments demonstrate the proposed method\nestablishes a new state of the art for both long- and short-term forecasting.\nCode is available at \\url{https://github.com/Hank0626/LLaTA}.\n","authors":["Peiyuan Liu","Hang Guo","Tao Dai","Naiqi Li","Jigang Bao","Xudong Ren","Yong Jiang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2403.07300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16319v2","updated":"2024-03-12T03:43:57Z","published":"2023-09-28T10:24:39Z","title":"Augmenting Transformers with Recursively Composed Multi-grained\n  Representations","summary":"  We present ReCAT, a recursive composition augmented Transformer that is able\nto explicitly model hierarchical syntactic structures of raw texts without\nrelying on gold trees during both learning and inference. Existing research\nalong this line restricts data to follow a hierarchical tree structure and thus\nlacks inter-span communications. To overcome the problem, we propose a novel\ncontextual inside-outside (CIO) layer that learns contextualized\nrepresentations of spans through bottom-up and top-down passes, where a\nbottom-up pass forms representations of high-level spans by composing low-level\nspans, while a top-down pass combines information inside and outside a span. By\nstacking several CIO layers between the embedding layer and the attention\nlayers in Transformer, the ReCAT model can perform both deep intra-span and\ndeep inter-span interactions, and thus generate multi-grained representations\nfully contextualized with other spans. Moreover, the CIO layers can be jointly\npre-trained with Transformers, making ReCAT enjoy scaling ability, strong\nperformance, and interpretability at the same time. We conduct experiments on\nvarious sentence-level and span-level tasks. Evaluation results indicate that\nReCAT can significantly outperform vanilla Transformer models on all span-level\ntasks and baselines that combine recursive networks with Transformers on\nnatural language inference tasks. More interestingly, the hierarchical\nstructures induced by ReCAT exhibit strong consistency with human-annotated\nsyntactic trees, indicating good interpretability brought by the CIO layers.\n","authors":["Xiang Hu","Qingyang Zhu","Kewei Tu","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2309.16319v2.pdf","comment":"ICLR 2024 poster"},{"id":"http://arxiv.org/abs/2403.07283v1","updated":"2024-03-12T03:30:04Z","published":"2024-03-12T03:30:04Z","title":"A Framework for Cost-Effective and Self-Adaptive LLM Shaking and\n  Recovery Mechanism","summary":"  As Large Language Models (LLMs) gain great success in real-world\napplications, an increasing number of users are seeking to develop and deploy\ntheir customized LLMs through cloud services. Nonetheless, in some specific\ndomains, there are still concerns regarding cost and trade-offs between privacy\nissues and accuracy. In this study, we introduce a cost-effective and\nself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With\ncarefully designed horizontal and vertical shaking operators, we can achieve\ncomparable accuracy results with SOTA privacy-preserving LLM schemes using\nCryptography-based or Differential Privacy-based methods. Experiments also show\nthat with the CypherTalk framework, users can achieve reliable accuracy when\nusing optimized shaking operator settings. To our best knowledge, this is the\nfirst work that considers cost, and trade-off between model utility and privacy\nin LLM scenarios.\n","authors":["Zhiyu Chen","Yu Li","Suochao Zhang","Jingbo Zhou","Jiwen Zhou","Chenfu Bao","Dianhai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.07283v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2403.07279v1","updated":"2024-03-12T03:17:59Z","published":"2024-03-12T03:17:59Z","title":"A Survey of Explainable Knowledge Tracing","summary":"  With the long term accumulation of high quality educational data, artificial\nintelligence has shown excellent performance in knowledge tracing. However, due\nto the lack of interpretability and transparency of some algorithms, this\napproach will result in reduced stakeholder trust and a decreased acceptance of\nintelligent decisions. Therefore, algorithms need to achieve high accuracy, and\nusers need to understand the internal operating mechanism and provide reliable\nexplanations for decisions. This paper thoroughly analyzes the interpretability\nof KT algorithms. First, the concepts and common methods of explainable\nartificial intelligence and knowledge tracing are introduced. Next, explainable\nknowledge tracing models are classified into two categories: transparent models\nand black box models. Then, the interpretable methods used are reviewed from\nthree stages: ante hoc interpretable methods, post hoc interpretable methods,\nand other dimensions. It is worth noting that current evaluation methods for\nexplainable knowledge tracing are lacking. Hence, contrast and deletion\nexperiments are conducted to explain the prediction results of the deep\nknowledge tracing model on the ASSISTment2009 by using three XAI methods.\nMoreover, this paper offers some insights into evaluation methods from the\nperspective of educational stakeholders. This paper provides a detailed and\ncomprehensive review of the research on explainable knowledge tracing, aiming\nto offer some basis and inspiration for researchers interested in the\ninterpretability of knowledge tracing.\n","authors":["Yanhong Bai","Jiabao Zhao","Tingjiang Wei","Qing Cai","Liang He"],"pdf_url":"https://arxiv.org/pdf/2403.07279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16421v2","updated":"2024-03-12T03:14:18Z","published":"2023-03-29T03:05:43Z","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of\n  Commonsense Problem in Large Language Models","summary":"  Large language models (LLMs) have made significant progress in NLP. However,\ntheir ability to memorize, represent, and leverage commonsense knowledge has\nbeen a well-known pain point. In this paper, we specifically focus on ChatGPT,\na widely used and easily accessible LLM, and ask the following questions: (1)\nCan ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of\nthe underlying commonsense knowledge for answering a specific question? (3) Is\nChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage\ncommonsense for answering questions? We conduct a series of experiments on 11\ndatasets to evaluate ChatGPT's commonsense abilities, including answering\ncommonsense questions, identifying necessary knowledge, generating knowledge\ndescriptions, and using knowledge descriptions to answer questions again.\nExperimental results show that: (1) ChatGPT can achieve good QA accuracies in\ncommonsense tasks, while still struggling with certain domains of datasets. (2)\nChatGPT is knowledgeable, and can accurately generate most of the commonsense\nknowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an\ninexperienced commonsense problem solver, which cannot precisely identify the\nneeded commonsense for answering a specific question. These findings raise the\nneed to explore improved mechanisms for effectively incorporating commonsense\ninto LLMs like ChatGPT, such as better instruction following and commonsense\nguidance.\n","authors":["Ning Bian","Xianpei Han","Le Sun","Hongyu Lin","Yaojie Lu","Ben He","Shanshan Jiang","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2303.16421v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.07260v1","updated":"2024-03-12T02:37:11Z","published":"2024-03-12T02:37:11Z","title":"CKERC : Joint Large Language Models with Commonsense Knowledge for\n  Emotion Recognition in Conversation","summary":"  Emotion recognition in conversation (ERC) is a task which predicts the\nemotion of an utterance in the context of a conversation. It tightly depends on\ndialogue context, speaker identity information, multiparty dialogue scenario\nand so on. However, the state-of-the-art method (instructERC) solely\nidentifying speaker, and ignores commonsense knowledge(i.e., reaction of the\nlisteners and intention of the speaker, etc.) behind speakers during a\nconversation, which can deeply mine speaker information. To this end, we\npropose a novel joint large language models with commonsense knowledge\nframework for emotion recognition in conversation, namely CKERC.We design\nprompts to generate interlocutors' commonsense based on historical utterances\nwith large language model. And we use the interlocutor commonsense\nidentification task for LLM pre-training to fine-tune speaker implicit clues\ninformation.By solving above challenge, our method achieve state-of-the-art.We\nextensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,\nEmoryNLP, demonstrate our method superiority. Also, we conduct in-depth\nanalysis and further demonstrate the effectiveness of commonsense knowledge in\nERC task in large language model.\n","authors":["Yumeng Fu"],"pdf_url":"https://arxiv.org/pdf/2403.07260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07230v1","updated":"2024-03-12T00:58:19Z","published":"2024-03-12T00:58:19Z","title":"Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences","summary":"  Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique.\n","authors":["Pulkit Pattnaik","Rishabh Maheshwary","Kelechi Ogueji","Vikas Yadav","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2403.07230v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.00770v2","updated":"2024-03-12T00:50:00Z","published":"2023-09-02T00:32:55Z","title":"Bias and Fairness in Large Language Models: A Survey","summary":"  Rapid advancements of large language models (LLMs) have enabled the\nprocessing, understanding, and generation of human-like text, with increasing\nintegration into systems that touch our social sphere. Despite this success,\nthese models can learn, perpetuate, and amplify harmful social biases. In this\npaper, we present a comprehensive survey of bias evaluation and mitigation\ntechniques for LLMs. We first consolidate, formalize, and expand notions of\nsocial bias and fairness in natural language processing, defining distinct\nfacets of harm and introducing several desiderata to operationalize fairness\nfor LLMs. We then unify the literature by proposing three intuitive taxonomies,\ntwo for bias evaluation, namely metrics and datasets, and one for mitigation.\nOur first taxonomy of metrics for bias evaluation disambiguates the\nrelationship between metrics and evaluation datasets, and organizes metrics by\nthe different levels at which they operate in a model: embeddings,\nprobabilities, and generated text. Our second taxonomy of datasets for bias\nevaluation categorizes datasets by their structure as counterfactual inputs or\nprompts, and identifies the targeted harms and social groups; we also release a\nconsolidation of publicly-available datasets for improved access. Our third\ntaxonomy of techniques for bias mitigation classifies methods by their\nintervention during pre-processing, in-training, intra-processing, and\npost-processing, with granular subcategories that elucidate research trends.\nFinally, we identify open problems and challenges for future work. Synthesizing\na wide range of recent research, we aim to provide a clear guide of the\nexisting literature that empowers researchers and practitioners to better\nunderstand and prevent the propagation of bias in LLMs.\n","authors":["Isabel O. Gallegos","Ryan A. Rossi","Joe Barrow","Md Mehrab Tanjim","Sungchul Kim","Franck Dernoncourt","Tong Yu","Ruiyi Zhang","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2309.00770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00746v5","updated":"2024-03-12T00:16:10Z","published":"2024-02-01T16:40:32Z","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction System","summary":"  Artificial intelligence (AI) in healthcare has significantly advanced\nintelligent medical treatment. However, traditional intelligent healthcare is\nlimited by static data and unified standards, preventing full integration with\nindividual situations and other challenges. Hence, a more professional and\ndetailed intelligent healthcare method is needed for development. To this end,\nwe propose an innovative framework named Heath-LLM, which combines large-scale\nfeature extraction and medical knowledge trade-off scoring. Compared to\ntraditional health management methods, our system has three main advantages.\nFirst, our system integrates health reports into a large model to provide\ndetailed task information. Second, professional medical expertise is used to\nadjust the weighted scores of health characteristics. Third, we use a\nsemi-automated feature extraction framework to enhance the analytical power of\nlanguage models and incorporate expert insights to improve the accuracy of\ndisease prediction. We have conducted disease prediction experiments on a large\nnumber of health reports to assess the effectiveness of Health-LLM. The results\nof the experiments indicate that the proposed system surpasses traditional\nmethods and has the potential to revolutionize disease prediction and\npersonalized health management. The code is available at\nhttps://github.com/jmyissb/HealthLLM.\n","authors":["Mingyu Jin","Qinkai Yu","Dong Shu","Chong Zhang","Suiyuan Zhu","Mengnan Du","Yanda Meng","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00746v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08140v1","updated":"2024-03-12T23:59:15Z","published":"2024-03-12T23:59:15Z","title":"BAGEL: Bootstrapping Agents by Guiding Exploration with Language","summary":"  Following natural language instructions by executing actions in digital\nenvironments (e.g. web-browsers and REST APIs) is a challenging task for\nlanguage model (LM) agents. Unfortunately, LM agents often fail to generalize\nto new environments without human demonstrations. This work presents BAGEL, a\nmethod for bootstrapping LM agents without human supervision. BAGEL converts a\nseed set of randomly explored trajectories or synthetic instructions, into\ndemonstrations, via round-trips between two noisy LM components: an LM labeler\nwhich converts a trajectory into a synthetic instruction, and a zero-shot LM\nagent which maps the synthetic instruction into a refined trajectory. By\nperforming these round-trips iteratively, BAGEL quickly converts the initial\ndistribution of trajectories towards those that are well-described by natural\nlanguage. We use BAGEL demonstrations to adapt a zero shot LM agent at test\ntime via in-context learning over retrieved demonstrations, and find\nimprovements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x\nreduction in execution failures.\n","authors":["Shikhar Murty","Christopher Manning","Peter Shaw","Mandar Joshi","Kenton Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08137v1","updated":"2024-03-12T23:47:28Z","published":"2024-03-12T23:47:28Z","title":"From Paper to Card: Transforming Design Implications with Generative AI","summary":"  Communicating design implications is common within the HCI community when\npublishing academic papers, yet these papers are rarely read and used by\ndesigners. One solution is to use design cards as a form of translational\nresource that communicates valuable insights from papers in a more digestible\nand accessible format to assist in design processes. However, creating design\ncards can be time-consuming, and authors may lack the resources/know-how to\nproduce cards. Through an iterative design process, we built a system that\nhelps create design cards from academic papers using an LLM and text-to-image\nmodel. Our evaluation with designers (N=21) and authors of selected papers\n(N=12) revealed that designers perceived the design implications from our\ndesign cards as more inspiring and generative, compared to reading original\npaper texts, and the authors viewed our system as an effective way of\ncommunicating their design implications. We also propose future enhancements\nfor AI-generated design cards.\n","authors":["Donghoon Shin","Lucy Lu Wang","Gary Hsieh"],"pdf_url":"https://arxiv.org/pdf/2403.08137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01614v2","updated":"2024-03-12T23:14:33Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08115v1","updated":"2024-03-12T22:53:32Z","published":"2024-03-12T22:53:32Z","title":"Legally Binding but Unfair? Towards Assessing Fairness of Privacy\n  Policies","summary":"  Privacy policies are expected to inform data subjects about their data\nprotection rights. They should explain the data controller's data management\npractices, and make facts such as retention periods or data transfers to third\nparties transparent. Privacy policies only fulfill their purpose, if they are\ncorrectly perceived, interpreted, understood, and trusted by the data subject.\nAmongst others, this requires that a privacy policy is written in a fair way,\ne.g., it does not use polarizing terms, does not require a certain education,\nor does not assume a particular social background. In this work-in-progress\npaper, we outline our approach to assessing fairness in privacy policies. To\nthis end, we identify from fundamental legal sources and fairness research, how\nthe dimensions informational fairness, representational fairness and\nethics/morality are related to privacy policies. We propose options to\nautomatically assess policies in these fairness dimensions, based on text\nstatistics, linguistic methods and artificial intelligence. Finally, we conduct\ninitial experiments with German privacy policies to provide evidence that our\napproach is applicable. Our experiments indicate that there are indeed issues\nin all three dimensions of fairness. For example, our approach finds out if a\npolicy discriminates against individuals with impaired reading skills or\ncertain demographics, and identifies questionable ethics. This is important, as\nfuture privacy policies may be used in a corpus for legal artificial\nintelligence models.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2403.08115v1.pdf","comment":"Submitted to IWSPA 2024 and under review"},{"id":"http://arxiv.org/abs/2403.08111v1","updated":"2024-03-12T22:36:27Z","published":"2024-03-12T22:36:27Z","title":"AI-Assisted Causal Pathway Diagram for Human-Centered Design","summary":"  This paper explores the integration of causal pathway diagrams (CPD) into\nhuman-centered design (HCD), investigating how these diagrams can enhance the\nearly stages of the design process. A dedicated CPD plugin for the online\ncollaborative whiteboard platform Miro was developed to streamline diagram\ncreation and offer real-time AI-driven guidance. Through a user study with\ndesigners (N=20), we found that CPD's branching and its emphasis on causal\nconnections supported both divergent and convergent processes during design.\nCPD can also facilitate communication among stakeholders. Additionally, we\nfound our plugin significantly reduces designers' cognitive workload and\nincreases their creativity during brainstorming, highlighting the implications\nof AI-assisted tools in supporting creative work and evidence-based designs.\n","authors":["Ruican Zhong","Donghoon Shin","Rosemary Meza","Predrag Klasnja","Lucas Colusso","Gary Hsieh"],"pdf_url":"https://arxiv.org/pdf/2403.08111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08103v1","updated":"2024-03-12T22:23:08Z","published":"2024-03-12T22:23:08Z","title":"Contextual Clarity: Generating Sentences with Transformer Models using\n  Context-Reverso Data","summary":"  In the age of information abundance, the ability to provide users with\ncontextually relevant and concise information is crucial. Keyword in Context\n(KIC) generation is a task that plays a vital role in and generation\napplications, such as search engines, personal assistants, and content\nsummarization. In this paper, we present a novel approach to generating\nunambiguous and brief sentence-contexts for given keywords using the T5\ntransformer model, leveraging data obtained from the Context-Reverso API. The\ncode is available at https://github.com/Rusamus/word2context/tree/main .\n","authors":["Ruslan Musaev"],"pdf_url":"https://arxiv.org/pdf/2403.08103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13405v2","updated":"2024-03-12T22:12:25Z","published":"2024-02-20T22:19:56Z","title":"A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set\n  Expansion and Taxonomy Expansion","summary":"  Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy\nConstruction are three representative tasks that can be used to automatically\npopulate an existing taxonomy with new entities. However, previous approaches\noften address these tasks separately with heterogeneous techniques, lacking a\nunified perspective. To tackle this issue, in this paper, we identify the\ncommon key skills needed for these tasks from the view of taxonomy structures\n-- finding 'siblings' and finding 'parents' -- and propose a unified\ntaxonomy-guided instruction tuning framework to jointly solve the three tasks.\nTo be specific, by leveraging the existing taxonomy as a rich source of entity\nrelationships, we utilize instruction tuning to fine-tune a large language\nmodel to generate parent and sibling entities. Extensive experiments on\nmultiple benchmark datasets demonstrate the effectiveness of TaxoInstruct,\nwhich outperforms task-specific baselines across all three tasks.\n","authors":["Yanzhen Shen","Yu Zhang","Yunyi Zhang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2402.13405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00635v3","updated":"2024-03-12T22:05:53Z","published":"2022-11-01T17:56:57Z","title":"Two-stage LLM Fine-tuning with Less Specialization and More\n  Generalization","summary":"  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n","authors":["Yihan Wang","Si Si","Daliang Li","Michal Lukasik","Felix Yu","Cho-Jui Hsieh","Inderjit S Dhillon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2211.00635v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08081v1","updated":"2024-03-12T21:15:38Z","published":"2024-03-12T21:15:38Z","title":"Mechanics of Next Token Prediction with Self-Attention","summary":"  Transformer-based language models are trained on large datasets to predict\nthe next token given an input sequence. Despite this simple training objective,\nthey have led to revolutionary advances in natural language processing.\nUnderlying this success is the self-attention mechanism. In this work, we ask:\n$\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$\n$\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$\n$\\textit{next-token}$ $\\textit{prediction?}$ We show that training\nself-attention with gradient descent learns an automaton which generates the\nnext token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$\n$\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects\nthe $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with\nthe last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It\nthen creates a convex combination of the high-priority tokens from which the\nnext token can be sampled. Under suitable conditions, we rigorously\ncharacterize these mechanics through a directed graph over tokens extracted\nfrom the training data. We prove that gradient descent implicitly discovers the\nstrongly-connected components (SCC) of this graph and self-attention learns to\nretrieve the tokens that belong to the highest-priority SCC available in the\ncontext window. Our theory relies on decomposing the model weights into a\ndirectional component and a finite component that correspond to hard retrieval\nand soft composition steps respectively. This also formalizes a related\nimplicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that\nthese findings shed light on how self-attention processes sequential data and\npave the path toward demystifying more complex architectures.\n","authors":["Yingcong Li","Yixiao Huang","M. Emrullah Ildiz","Ankit Singh Rawat","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2403.08081v1.pdf","comment":"Accepted to AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.08059v1","updated":"2024-03-12T20:11:38Z","published":"2024-03-12T20:11:38Z","title":"FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation","summary":"  Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.\n","authors":["Benjamin D. Killeen","Liam J. Wang","Han Zhang","Mehran Armand","Russell H. Taylor","Greg Osgood","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2403.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11767v3","updated":"2024-03-12T20:10:05Z","published":"2023-08-15T23:22:37Z","title":"Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm","summary":"  ChatGPT and generative AI tools are becoming the new reality. This work is\nmotivated by the premise that ``ChatGPT content may exhibit a distinctive\nbehavior that can be separated from scientific articles''. In this study, we\ndemonstrate how we tested this premise in two phases and prove its validity.\nSubsequently, we introduce xFakeSci, a novel learning algorithm, that is\ncapable of distinguishing ChatGPT-generated articles from publications produced\nby scientists. The algorithm is trained using network models driven from\nmultiple types of data sources, such as ChatGPT-generated documents achieved by\nmeans of prompt-engineering, and PubMed articles. To mitigate over-fitting\nissues, we incorporate a calibration step that is built upon data-driven\nheuristics, including ratios. We evaluate the algorithm across multiple\ndatasets covering publication periods and diseases (cancer, depression, and\nAlzheimer's). Further, we show how the algorithm is benchmarked against the\nstate-of-the-art (SOTA) algorithms. While the xFakeSci algorithm achieve F1\nscore ranging from 80% - 94%, SOTA algorithms score F1 values between 38% -\n52%. We attribute the noticeable difference to the introduction of calibration\nand a proximity distance heuristic, which we underscore this promising\nperformance. Indeed, the prediction of fake science generated by ChatGPT\npresents a considerable challenge. Nonetheless, the introduction of xFakeSci\nalgorithm is a significant step on the way to combating fake science.\n","authors":["Ahmed Abdeen Hamed","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2308.11767v3.pdf","comment":"14 pages, 6 figures, 6 tables, 5 algorithms"},{"id":"http://arxiv.org/abs/2403.08058v1","updated":"2024-03-12T20:10:04Z","published":"2024-03-12T20:10:04Z","title":"CHAI: Clustered Head Attention for Efficient LLM Inference","summary":"  Large Language Models (LLMs) with hundreds of billions of parameters have\ntransformed the field of machine learning. However, serving these models at\ninference time is both compute and memory intensive, where a single request can\nrequire multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is\none of the key components of LLMs, which can account for over 50% of LLMs\nmemory and compute requirement. We observe that there is a high amount of\nredundancy across heads on which tokens they pay attention to. Based on this\ninsight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a\nhigh amount of correlation for self-attention at runtime, thus reducing both\nmemory and compute. In our experiments, we show that CHAI is able to reduce the\nmemory requirements for storing K,V cache by up to 21.4% and inference time\nlatency by up to 1.73x without any fine-tuning required. CHAI achieves this\nwith a maximum 3.2% deviation in accuracy across 3 different models (i.e.\nOPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.\n","authors":["Saurabh Agarwal","Bilge Acun","Basil Homer","Mostafa Elhoushi","Yejin Lee","Shivaram Venkataraman","Dimitris Papailiopoulos","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08053v1","updated":"2024-03-12T19:57:39Z","published":"2024-03-12T19:57:39Z","title":"Generating Clarification Questions for Disambiguating Contracts","summary":"  Enterprises frequently enter into commercial contracts that can serve as\nvital sources of project-specific requirements. Contractual clauses are\nobligatory, and the requirements derived from contracts can detail the\ndownstream implementation activities that non-legal stakeholders, including\nrequirement analysts, engineers, and delivery personnel, need to conduct.\nHowever, comprehending contracts is cognitively demanding and error-prone for\nsuch stakeholders due to the extensive use of Legalese and the inherent\ncomplexity of contract language. Furthermore, contracts often contain\nambiguously worded clauses to ensure comprehensive coverage. In contrast,\nnon-legal stakeholders require a detailed and unambiguous comprehension of\ncontractual clauses to craft actionable requirements. In this work, we\nintroduce a novel legal NLP task that involves generating clarification\nquestions for contracts. These questions aim to identify contract ambiguities\non a document level, thereby assisting non-legal stakeholders in obtaining the\nnecessary details for eliciting requirements. This task is challenged by three\ncore issues: (1) data availability, (2) the length and unstructured nature of\ncontracts, and (3) the complexity of legal text. To address these issues, we\npropose ConRAP, a retrieval-augmented prompting framework for generating\nclarification questions to disambiguate contractual text. Experiments conducted\non contracts sourced from the publicly available CUAD dataset show that ConRAP\nwith ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the\ngenerated clarification questions are deemed useful by human evaluators.\n","authors":["Anmol Singhal","Chirag Jain","Preethu Rose Anish","Arkajyoti Chakraborty","Smita Ghaisas"],"pdf_url":"https://arxiv.org/pdf/2403.08053v1.pdf","comment":"9 pages, 3 figures, accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08046v1","updated":"2024-03-12T19:40:18Z","published":"2024-03-12T19:40:18Z","title":"Big City Bias: Evaluating the Impact of Metropolitan Size on\n  Computational Job Market Abilities of Language Models","summary":"  Large language models (LLMs) have emerged as a useful technology for job\nmatching, for both candidates and employers. Job matching is often based on a\nparticular geographic location, such as a city or region. However, LLMs have\nknown biases, commonly derived from their training data. In this work, we aim\nto quantify the metropolitan size bias encoded within large language models,\nevaluating zero-shot salary, employer presence, and commute duration\npredictions in 384 of the United States' metropolitan regions. Across all\nbenchmarks, we observe negative correlations between the metropolitan size and\nthe performance of the LLMS, indicating that smaller regions are indeed\nunderrepresented. More concretely, the smallest 10 metropolitan regions show\nupwards of 300% worse benchmark performance than the largest 10.\n","authors":["Charlie Campanella","Rob van der Goot"],"pdf_url":"https://arxiv.org/pdf/2403.08046v1.pdf","comment":"5 pages, 3 figures, 2 tables, NLP4HR Workshop @ EACL 2024"},{"id":"http://arxiv.org/abs/2403.08043v1","updated":"2024-03-12T19:34:54Z","published":"2024-03-12T19:34:54Z","title":"Authorship Style Transfer with Policy Optimization","summary":"  Authorship style transfer aims to rewrite a given text into a specified\ntarget while preserving the original meaning in the source. Existing approaches\nrely on the availability of a large number of target style exemplars for model\ntraining. However, these overlook cases where a limited number of target style\nexamples are available. The development of parameter-efficient transfer\nlearning techniques and policy optimization (PO) approaches suggest lightweight\nPO is a feasible approach to low-resource style transfer. In this work, we\npropose a simple two step tune-and-optimize technique for low-resource textual\nstyle transfer. We apply our technique to authorship transfer as well as a\nlarger-data native language style task and in both cases find it outperforms\nstate-of-the-art baseline models.\n","authors":["Shuai Liu","Shantanu Agarwal","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2403.08043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10659v2","updated":"2024-03-12T19:12:55Z","published":"2024-02-16T13:10:14Z","title":"Network Formation and Dynamics Among Multi-LLMs","summary":"  Social networks shape opinions, behaviors, and information dissemination in\nhuman societies. As large language models (LLMs) increasingly integrate into\nsocial and professional environments, understanding their behavior within the\ncontext of social interactions and networks becomes essential. Our study\nanalyzes LLMs' network formation behavior to examine whether the dynamics of\nmultiple LLMs are similar to or different from human social dynamics. We\nobserve that LLMs exhibit key social network principles, including preferential\nattachment, triadic closure, homophily, community structure, and the\nsmall-world phenomenon, when asked about their preferences in network\nformation. We also investigate LLMs' decision-making based on real-world\nnetworks, revealing that triadic closure and homophily have a stronger\ninfluence than preferential attachment and that LLMs perform well in network\nformation predictions. Overall, our study opens up new possibilities for using\nLLMs in network science research and helps develop socially aware LLMs by\nshedding light on their network formation behaviors and exploring their impacts\non social dynamics.\n","authors":["Marios Papachristou","Yuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2402.10659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08035v1","updated":"2024-03-12T19:12:28Z","published":"2024-03-12T19:12:28Z","title":"Harnessing Artificial Intelligence to Combat Online Hate: Exploring the\n  Challenges and Opportunities of Large Language Models in Hate Speech\n  Detection","summary":"  Large language models (LLMs) excel in many diverse applications beyond\nlanguage generation, e.g., translation, summarization, and sentiment analysis.\nOne intriguing application is in text classification. This becomes pertinent in\nthe realm of identifying hateful or toxic speech -- a domain fraught with\nchallenges and ethical dilemmas. In our study, we have two objectives: firstly,\nto offer a literature review revolving around LLMs as classifiers, emphasizing\ntheir role in detecting and classifying hateful or toxic content. Subsequently,\nwe explore the efficacy of several LLMs in classifying hate speech: identifying\nwhich LLMs excel in this task as well as their underlying attributes and\ntraining. Providing insight into the factors that contribute to an LLM\nproficiency (or lack thereof) in discerning hateful content. By combining a\ncomprehensive literature review with an empirical analysis, our paper strives\nto shed light on the capabilities and constraints of LLMs in the crucial domain\nof hate speech detection.\n","authors":["Tharindu Kumarage","Amrita Bhattacharjee","Joshua Garland"],"pdf_url":"https://arxiv.org/pdf/2403.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14336v3","updated":"2024-03-12T18:54:12Z","published":"2023-05-23T17:58:10Z","title":"Schema-Driven Information Extraction from Heterogeneous Tables","summary":"  In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.\n","authors":["Fan Bai","Junmo Kang","Gabriel Stanovsky","Dayne Freitag","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07950v2","updated":"2024-03-12T18:34:05Z","published":"2024-01-15T20:22:21Z","title":"SciGLM: Training Scientific Language Models with Self-Reflective\n  Instruction Annotation and Tuning","summary":"  Large Language Models (LLMs) have shown promise in assisting scientific\ndiscovery. However, such applications are currently limited by LLMs'\ndeficiencies in understanding intricate scientific concepts, deriving symbolic\nequations, and solving advanced numerical calculations. To bridge these gaps,\nwe introduce SciGLM, a suite of scientific language models able to conduct\ncollege-level scientific reasoning. Central to our approach is a novel\nself-reflective instruction annotation framework to address the data scarcity\nchallenge in the science domain. This framework leverages existing LLMs to\ngenerate step-by-step reasoning for unlabelled scientific questions, followed\nby a process of self-reflective critic-and-revise. Applying this framework, we\ncurated SciInstruct, a diverse and high-quality dataset encompassing physics,\nchemistry, math, and formal proofs. We fine-tuned the ChatGLM family of\nlanguage models with SciInstruct, enhancing their scientific and mathematical\nreasoning capabilities. Remarkably, the SciGLM consistently improves both the\nbase model (ChatGLM3-6B-Base) by 4.87% and larger-scale models (32B) by 2.67%,\nwithout sacrificing the language understanding capabilities of the base model.\nThis makes SciGLM a suitable foundational model to facilitate diverse\nscientific discovery tasks. For the benefit of the wider research community, we\nrelease SciInstruct, and SciGLM, alongside a self-reflective framework and\nfine-tuning code at https://github.com/THUDM/SciGLM.\n","authors":["Dan Zhang","Ziniu Hu","Sining Zhoubian","Zhengxiao Du","Kaiyu Yang","Zihan Wang","Yisong Yue","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2401.07950v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.08011v1","updated":"2024-03-12T18:21:20Z","published":"2024-03-12T18:21:20Z","title":"Gujarati-English Code-Switching Speech Recognition using ensemble\n  prediction of spoken language","summary":"  An important and difficult task in code-switched speech recognition is to\nrecognize the language, as lots of words in two languages can sound similar,\nespecially in some accents. We focus on improving performance of end-to-end\nAutomatic Speech Recognition models by conditioning transformer layers on\nlanguage ID of words and character in the output in an per layer supervised\nmanner. To this end, we propose two methods of introducing language specific\nparameters and explainability in the multi-head attention mechanism, and\nimplement a Temporal Loss that helps maintain continuity in input alignment.\nDespite being unable to reduce WER significantly, our method shows promise in\npredicting the correct language from just spoken data. We introduce\nregularization in the language prediction by dropping LID in the sequence,\nwhich helps align long repeated output sequences.\n","authors":["Yash Sharma","Basil Abraham","Preethi Jyothi"],"pdf_url":"https://arxiv.org/pdf/2403.08011v1.pdf","comment":"Bachelor's thesis, 28 pages, includes appendix"},{"id":"http://arxiv.org/abs/2403.08010v1","updated":"2024-03-12T18:19:47Z","published":"2024-03-12T18:19:47Z","title":"Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological\n  Analysis Based on LLM","summary":"  How can we construct an automated debate judge to evaluate an extensive,\nvibrant, multi-turn debate? This task is challenging, as judging a debate\ninvolves grappling with lengthy texts, intricate argument relationships, and\nmulti-dimensional assessments. At the same time, current research mainly\nfocuses on short dialogues, rarely touching upon the evaluation of an entire\ndebate. In this paper, by leveraging Large Language Models (LLMs), we propose\nDebatrix, which makes the analysis and assessment of multi-turn debates more\naligned with majority preferences. Specifically, Debatrix features a vertical,\niterative chronological analysis and a horizontal, multi-dimensional evaluation\ncollaboration. To align with real-world debate scenarios, we introduced the\nPanelBench benchmark, comparing our system's performance to actual debate\noutcomes. The findings indicate a notable enhancement over directly using LLMs\nfor debate evaluation. Source code and benchmark data are available online at\nhttps://github.com/ljcleo/Debatrix .\n","authors":["Jingcong Liang","Rong Ye","Meng Han","Ruofei Lai","Xinyu Zhang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08004v1","updated":"2024-03-12T18:12:50Z","published":"2024-03-12T18:12:50Z","title":"Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing","summary":"  The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.\n","authors":["Rodrigo Santos","João Silva","António Branco"],"pdf_url":"https://arxiv.org/pdf/2403.08004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08002v1","updated":"2024-03-12T18:12:02Z","published":"2024-03-12T18:12:02Z","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A\n  Case Study in Radiology Imaging","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07974v1","updated":"2024-03-12T17:58:04Z","published":"2024-03-12T17:58:04Z","title":"LiveCodeBench: Holistic and Contamination Free Evaluation of Large\n  Language Models for Code","summary":"  Large Language Models (LLMs) applied to code-related applications have\nemerged as a prominent field, attracting significant interest from both\nacademia and industry. However, as new and improved LLMs are developed,\nexisting evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient\nfor assessing their capabilities. In this work, we propose LiveCodeBench, a\ncomprehensive and contamination-free evaluation of LLMs for code, which\ncontinuously collects new problems over time from contests across three\ncompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our\nbenchmark also focuses on a broader range of code related capabilities, such as\nself-repair, code execution, and test output prediction, beyond just code\ngeneration. Currently, LiveCodeBench hosts four hundred high-quality coding\nproblems that were published between May 2023 and February 2024. We have\nevaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We\npresent empirical findings on contamination, holistic performance comparisons,\npotential overfitting in existing benchmarks as well as individual model\ncomparisons. We will release all prompts and model completions for further\ncommunity analysis, along with a general toolkit for adding new scenarios and\nmodel\n","authors":["Naman Jain","King Han","Alex Gu","Wen-Ding Li","Fanjia Yan","Tianjun Zhang","Sida Wang","Armando Solar-Lezama","Koushik Sen","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2403.07974v1.pdf","comment":"Website - https://livecodebench.github.io/"}]},"2024-03-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.08763v1","updated":"2024-03-13T17:58:57Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08755v1","updated":"2024-03-13T17:53:47Z","published":"2024-03-13T17:53:47Z","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","summary":"  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n","authors":["Feng Cheng","Ziyang Wang","Yi-Lin Sung","Yan-Bo Lin","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2403.08755v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2403.08743v1","updated":"2024-03-13T17:46:28Z","published":"2024-03-13T17:46:28Z","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing\n  Framework","summary":"  Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.08739v1","updated":"2024-03-13T17:42:32Z","published":"2024-03-13T17:42:32Z","title":"The Garden of Forking Paths: Observing Dynamic Parameters Distribution\n  in Large Language Models","summary":"  A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.\n","authors":["Carlo Nicolini","Jacopo Staiano","Bruno Lepri","Raffaele Marino"],"pdf_url":"https://arxiv.org/pdf/2403.08739v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2403.08738v1","updated":"2024-03-13T17:42:03Z","published":"2024-03-13T17:42:03Z","title":"Improving Acoustic Word Embeddings through Correspondence Training of\n  Self-supervised Speech Representations","summary":"  Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.\n","authors":["Amit Meghanani","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2403.08738v1.pdf","comment":"Accepted to EACL 2024 Main Conference, Long paper"},{"id":"http://arxiv.org/abs/2212.12799v2","updated":"2024-03-13T17:41:14Z","published":"2022-12-24T18:35:35Z","title":"A Comprehensive Study of Gender Bias in Chemical Named Entity\n  Recognition Models","summary":"  Chemical named entity recognition (NER) models are used in many downstream\ntasks, from adverse drug reaction identification to pharmacoepidemiology.\nHowever, it is unknown whether these models work the same for everyone.\nPerformance disparities can potentially cause harm rather than the intended\ngood. This paper assesses gender-related performance disparities in chemical\nNER systems. We develop a framework for measuring gender bias in chemical NER\nmodels using synthetic data and a newly annotated corpus of over 92,405 words\nwith self-identified gender information from Reddit. Our evaluation of multiple\nbiomedical NER models reveals evident biases. For instance, synthetic data\nsuggests female-related names are frequently misclassified as chemicals,\nespecially for brand name mentions. Additionally, we observe performance\ndisparities between female- and male-associated data in both datasets. Many\nsystems fail to detect contraceptives such as birth control. Our findings\nemphasize the biases in chemical NER models, urging practitioners to account\nfor these biases in downstream applications.\n","authors":["Xingmeng Zhao","Ali Niazi","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2212.12799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04475v2","updated":"2024-03-13T17:40:04Z","published":"2023-10-06T05:27:28Z","title":"Demystifying Embedding Spaces using Large Language Models","summary":"  Embeddings have become a pivotal means to represent complex, multi-faceted\ninformation about entities, concepts, and relationships in a condensed and\nuseful format. Nevertheless, they often preclude direct interpretation. While\ndownstream tasks make use of these compressed representations, meaningful\ninterpretation usually requires visualization using dimensionality reduction or\nspecialized machine learning interpretability methods. This paper addresses the\nchallenge of making such embeddings more interpretable and broadly useful, by\nemploying Large Language Models (LLMs) to directly interact with embeddings --\ntransforming abstract vectors into understandable narratives. By injecting\nembeddings into LLMs, we enable querying and exploration of complex embedding\ndata. We demonstrate our approach on a variety of diverse tasks, including:\nenhancing concept activation vectors (CAVs), communicating novel embedded\nentities, and decoding user preferences in recommender systems. Our work\ncouples the immense information potential of embeddings with the interpretative\npower of LLMs.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Jihwan Jeong","Lior Shani","Azamat Tulepbergenov","Deepak Ramachandran","Martin Mladenov","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2310.04475v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08737v1","updated":"2024-03-13T17:38:05Z","published":"2024-03-13T17:38:05Z","title":"ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation","summary":"  Existing Machine Learning approaches for local citation recommendation\ndirectly map or translate a query, which is typically a claim or an entity\nmention, to citation-worthy research papers. Within such a formulation, it is\nchallenging to pinpoint why one should cite a specific research paper for a\nparticular query, leading to limited recommendation interpretability. To\nalleviate this, we introduce the evidence-grounded local citation\nrecommendation task, where the target latent space comprises evidence spans for\nrecommending specific papers. Using a distantly-supervised evidence retrieval\nand multi-step re-ranking framework, our proposed system, ILCiteR, recommends\npapers to cite for a query grounded on similar evidence spans extracted from\nthe existing research literature. Unlike past formulations that simply output\nrecommendations, ILCiteR retrieves ranked lists of evidence span and\nrecommended paper pairs. Secondly, previously proposed neural models for\ncitation recommendation require expensive training on massive labeled data,\nideally after every significant update to the pool of candidate papers. In\ncontrast, ILCiteR relies solely on distant supervision from a dynamic evidence\ndatabase and pre-trained Transformer-based Language Models without any model\ntraining. We contribute a novel dataset for the evidence-grounded local\ncitation recommendation task and demonstrate the efficacy of our proposed\nconditional neural rank-ensembling approach for re-ranking evidence spans.\n","authors":["Sayar Ghosh Roy","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2403.08737v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.08981v2","updated":"2024-03-13T17:32:50Z","published":"2023-11-15T14:15:30Z","title":"Speculative Contrastive Decoding","summary":"  Large language models~(LLMs) exhibit exceptional performance in language\ntasks, yet their auto-regressive inference is limited due to high computational\nrequirements and is sub-optimal due to the exposure bias. Inspired by\nspeculative decoding and contrastive decoding, we introduce Speculative\nContrastive Decoding~(SCD), a straightforward yet powerful decoding approach\nthat leverages predictions from smaller language models~(LMs) to achieve both\ndecoding acceleration and quality improvement. Extensive evaluations and\nanalyses on four diverse language tasks demonstrate the effectiveness of SCD,\nshowing that decoding efficiency and quality can compatibly benefit from one\nsmaller LM.\n","authors":["Hongyi Yuan","Keming Lu","Fei Huang","Zheng Yuan","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.08981v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2403.08730v1","updated":"2024-03-13T17:29:45Z","published":"2024-03-13T17:29:45Z","title":"Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization","summary":"  Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.\n","authors":["Renjie Pi","Tianyang Han","Wei Xiong","Jipeng Zhang","Runtao Liu","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08715v1","updated":"2024-03-13T17:17:48Z","published":"2024-03-13T17:17:48Z","title":"SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language\n  Agents","summary":"  Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.\n","authors":["Ruiyi Wang","Haofei Yu","Wenxin Zhang","Zhengyang Qi","Maarten Sap","Graham Neubig","Yonatan Bisk","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07793v4","updated":"2024-03-13T17:10:48Z","published":"2023-10-11T18:27:12Z","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","summary":"  The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.\n","authors":["Ruotong Liao","Xu Jia","Yunpu Ma","Yangzhe Li","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2310.07793v4.pdf","comment":"14 pages, Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.09818v2","updated":"2024-03-13T17:07:02Z","published":"2023-11-16T11:48:17Z","title":"SUQL: Conversational Search over Structured and Unstructured Data with\n  Large Language Models","summary":"  While most conversational agents are grounded on either free-text or\nstructured knowledge, many knowledge corpora consist of hybrid sources. This\npaper presents the first conversational agent that supports the full generality\nof hybrid data access for large knowledge corpora, through a language we\ndeveloped called SUQL (Structured and Unstructured Query Language).\nSpecifically, SUQL extends SQL with free-text primitives (summary and answer),\nso information retrieval can be composed with structured data accesses\narbitrarily in a formal, succinct, precise, and interpretable notation. With\nSUQL, we propose the first semantic parser, an LLM with in-context learning,\nthat can handle hybrid data sources.\n  Our in-context learning-based approach, when applied to the HybridQA dataset,\ncomes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K\ndata samples. More significantly, unlike previous approaches, our technique is\napplicable to large databases and free-text corpora. We introduce a dataset\nconsisting of crowdsourced questions and conversations on Yelp, a large, real\nrestaurant knowledge base with structured and unstructured data. We show that\nour few-shot conversational agent based on SUQL finds an entity satisfying all\nuser requirements 90.3% of the time, compared to 63.4% for a baseline based on\nlinearization.\n","authors":["Shicheng Liu","Jialiang Xu","Wesley Tjangnaka","Sina J. Semnani","Chen Jie Yu","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2311.09818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08694v1","updated":"2024-03-13T16:57:57Z","published":"2024-03-13T16:57:57Z","title":"TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via\n  Reinforcement Learning","summary":"  The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of WizardLM's total), along with enhanced capabilities of LLMs in\ncrafting and comprehending complex instructions compared to strong baselines,\nand substantially improved model privacy protection.\n","authors":["Shangding Gu","Alois Knoll","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2403.08694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08693v1","updated":"2024-03-13T16:56:33Z","published":"2024-03-13T16:56:33Z","title":"Do Language Models Care About Text Quality? Evaluating Web-Crawled\n  Corpora Across 11 Languages","summary":"  Large, curated, web-crawled corpora play a vital role in training language\nmodels (LMs). They form the lion's share of the training data in virtually all\nrecent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,\ndespite this importance, relatively little attention has been given to the\nquality of these corpora. In this paper, we compare four of the currently most\nrelevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across\neleven lower-resourced European languages. Our approach is two-fold: first, we\nperform an intrinsic evaluation by performing a human evaluation of the quality\nof samples taken from different corpora; then, we assess the practical impact\nof the qualitative differences by training specific LMs on each of the corpora\nand evaluating their performance on downstream tasks. We find that there are\nclear differences in quality of the corpora, with MaCoCu and OSCAR obtaining\nthe best results. However, during the extrinsic evaluation, we actually find\nthat the CC100 corpus achieves the highest scores. We conclude that, in our\nexperiments, the quality of the web-crawled corpora does not seem to play a\nsignificant role when training LMs.\n","authors":["Rik van Noord","Taja Kuzman","Peter Rupnik","Nikola Ljubešić","Miquel Esplà-Gomis","Gema Ramírez-Sánchez","Antonio Toral"],"pdf_url":"https://arxiv.org/pdf/2403.08693v1.pdf","comment":"Accepted to LREC-COLING 2024 (long)"},{"id":"http://arxiv.org/abs/2402.18060v3","updated":"2024-03-13T16:44:45Z","published":"2024-02-28T05:44:41Z","title":"Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions","summary":"  LLMs have demonstrated impressive performance in answering medical questions,\nsuch as passing scores on medical licensing examinations. However, medical\nboard exam questions or general clinical questions do not capture the\ncomplexity of realistic clinical cases. Moreover, the lack of reference\nexplanations means we cannot easily evaluate the reasoning of model decisions,\na crucial component of supporting doctors in making complex medical decisions.\nTo address these challenges, we construct two new datasets: JAMA Clinical\nChallenge and Medbullets. JAMA Clinical Challenge consists of questions based\non challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style\nclinical questions. Both datasets are structured as multiple-choice\nquestion-answering tasks, where each question is accompanied by an\nexpert-written explanation. We evaluate four LLMs on the two datasets using\nvarious prompts. Experiments demonstrate that our datasets are harder than\nprevious benchmarks. The inconsistency between automatic and human evaluations\nof model-generated explanations highlights the need to develop new metrics to\nsupport future research on explainable medical QA.\n","authors":["Hanjie Chen","Zhouxiang Fang","Yash Singla","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2402.18060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08688v1","updated":"2024-03-13T16:44:39Z","published":"2024-03-13T16:44:39Z","title":"Token Alignment via Character Matching for Subword Completion","summary":"  Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.\n","authors":["Ben Athiwaratkun","Shiqi Wang","Mingyue Shang","Yuchen Tian","Zijian Wang","Sujan Kumar Gonugondla","Sanjay Krishna Gouda","Rob Kwiatowski","Ramesh Nallapati","Bing Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.08688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14158v2","updated":"2024-03-13T16:38:42Z","published":"2024-02-21T22:41:38Z","title":"TOOLVERIFIER: Generalization to New Tools via Self-Verification","summary":"  Teaching language models to use tools is an important milestone towards\nbuilding general assistants, but remains an open problem. While there has been\nsignificant progress on learning to use specific tools via fine-tuning,\nlanguage models still struggle with learning how to robustly use new tools from\nonly a few demonstrations. In this work we introduce a self-verification method\nwhich distinguishes between close candidates by self-asking contrastive\nquestions during (1) tool selection; and (2) parameter generation. We construct\nsynthetic, high-quality, self-generated data for this goal using Llama-2 70B,\nwhich we intend to release publicly. Extensive experiments on 4 tasks from the\nToolBench benchmark, consisting of 17 unseen tools, demonstrate an average\nimprovement of 22% over few-shot baselines, even in scenarios where the\ndistinctions between candidate tools are finely nuanced.\n","authors":["Dheeraj Mekala","Jason Weston","Jack Lanchantin","Roberta Raileanu","Maria Lomeli","Jingbo Shang","Jane Dwivedi-Yu"],"pdf_url":"https://arxiv.org/pdf/2402.14158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v1","updated":"2024-03-13T16:17:09Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.04780v2","updated":"2024-03-13T15:52:33Z","published":"2024-03-02T09:27:32Z","title":"MuseGraph: Graph-oriented Instruction Tuning of Large Language Models\n  for Generic Graph Mining","summary":"  Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.\n","authors":["Yanchao Tan","Hang Lv","Xinyi Huang","Jiawei Zhang","Shiping Wang","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2403.04780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08607v1","updated":"2024-03-13T15:20:30Z","published":"2024-03-13T15:20:30Z","title":"MedInsight: A Multi-Source Context Augmentation Framework for Generating\n  Patient-Centric Medical Responses using Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.\n","authors":["Subash Neupane","Shaswata Mitra","Sudip Mittal","Noorbakhsh Amiri Golilarz","Shahram Rahimi","Amin Amirlatifi"],"pdf_url":"https://arxiv.org/pdf/2403.08607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08604v1","updated":"2024-03-13T15:13:44Z","published":"2024-03-13T15:13:44Z","title":"DevBench: A Comprehensive Benchmark for Software Development","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their coding capabilities. However, existing benchmarks predominantly\nfocused on simplified or isolated aspects of programming, such as single-file\ncode generation or repository issue debugging, falling short of measuring the\nfull spectrum of challenges raised by real-world programming activities. To\nthis end, we propose DevBench, a comprehensive benchmark that evaluates LLMs\nacross various stages of the software development lifecycle, including software\ndesign, environment setup, implementation, acceptance testing, and unit\ntesting. DevBench features a wide range of programming languages and domains,\nhigh-quality data collection, and carefully designed and verified metrics for\neach task. Empirical studies show that current LLMs, including GPT-4-Turbo,\nfail to solve the challenges presented within DevBench. Analyses reveal that\nmodels struggle with understanding the complex structures in the repository,\nmanaging the compilation process, and grasping advanced programming concepts.\nOur findings offer actionable insights for the future development of LLMs\ntoward real-world programming applications. Our benchmark is available at\nhttps://github.com/open-compass/DevBench\n","authors":["Bowen Li","Wenhan Wu","Ziwei Tang","Lin Shi","John Yang","Jinyang Li","Shunyu Yao","Chen Qian","Binyuan Hui","Qicheng Zhang","Zhiyin Yu","He Du","Ping Yang","Dahua Lin","Chao Peng","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08604v1.pdf","comment":"Our data and code are available at\n  https://github.com/open-compass/DevBench"},{"id":"http://arxiv.org/abs/2307.12375v4","updated":"2024-03-13T15:00:20Z","published":"2023-07-23T16:54:41Z","title":"In-Context Learning Learns Label Relationships but Is Not Conventional\n  Learning","summary":"  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n","authors":["Jannik Kossen","Yarin Gal","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2307.12375v4.pdf","comment":"Accepted for publication at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08593v1","updated":"2024-03-13T14:59:07Z","published":"2024-03-13T14:59:07Z","title":"Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over\n  Structured Environments","summary":"  Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.\n","authors":["Sitao Cheng","Ziyuan Zhuang","Yong Xu","Fangkai Yang","Chaoyun Zhang","Xiaoting Qin","Xiang Huang","Ling Chen","Qingwei Lin","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08593v1.pdf","comment":"17 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2203.11155v5","updated":"2024-03-13T14:46:05Z","published":"2022-03-16T12:23:25Z","title":"A New Quantum CNN Model for Image Classification","summary":"  Quantum density matrix represents all the information of the entire quantum\nsystem, and novel models of meaning employing density matrices naturally model\nlinguistic phenomena such as hyponymy and linguistic ambiguity, among others in\nquantum question answering tasks. Naturally, we argue that the quantum density\nmatrix can enhance the image feature information and the relationship between\nthe features for the classical image classification. Specifically, we (i)\ncombine density matrices and CNN to design a new mechanism; (ii) apply the new\nmechanism to some representative classical image classification tasks. A series\nof experiments show that the application of quantum density matrix in image\nclassification has the generalization and high efficiency on different\ndatasets. The application of quantum density matrix both in classical question\nanswering tasks and classical image classification tasks show more effective\nperformance.\n","authors":["X. Q. Zhao","T. L. Chen"],"pdf_url":"https://arxiv.org/pdf/2203.11155v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08564v1","updated":"2024-03-13T14:19:08Z","published":"2024-03-13T14:19:08Z","title":"Non-discrimination Criteria for Generative Language Models","summary":"  Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.\n","authors":["Sara Sterlie","Nina Weng","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.08564v1.pdf","comment":"14 pages, 5 figures. Submitted to ACM Conference on Fairness,\n  Accountability, and Transparency (ACM FAccT 2024)"},{"id":"http://arxiv.org/abs/2403.07714v2","updated":"2024-03-13T14:08:19Z","published":"2024-03-12T14:57:40Z","title":"StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models","summary":"  Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.\n","authors":["Zhicheng Guo","Sijie Cheng","Hao Wang","Shihao Liang","Yujia Qin","Peng Li","Zhiyuan Liu","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08540v1","updated":"2024-03-13T13:54:00Z","published":"2024-03-13T13:54:00Z","title":"Language models scale reliably with over-training and on downstream\n  tasks","summary":"  Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.\n","authors":["Samir Yitzhak Gadre","Georgios Smyrnis","Vaishaal Shankar","Suchin Gururangan","Mitchell Wortsman","Rulin Shao","Jean Mercat","Alex Fang","Jeffrey Li","Sedrick Keh","Rui Xin","Marianna Nezhurina","Igor Vasiljevic","Jenia Jitsev","Alexandros G. Dimakis","Gabriel Ilharco","Shuran Song","Thomas Kollar","Yair Carmon","Achal Dave","Reinhard Heckel","Niklas Muennighoff","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2403.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08495v1","updated":"2024-03-13T13:04:58Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.01678v4","updated":"2024-03-13T13:02:57Z","published":"2023-12-04T07:01:54Z","title":"Jellyfish: A Large Language Model for Data Preprocessing","summary":"  This paper explores the utilization of LLMs for data preprocessing (DP), a\ncrucial step in the data mining pipeline that transforms raw data into a clean\nformat conducive to easy processing. Whereas the use of LLMs has sparked\ninterest in devising universal solutions to DP, recent initiatives in this\ndomain typically rely on GPT APIs, raising inevitable data breach concerns.\nUnlike these approaches, we consider instruction-tuning local LLMs (7 - 13B\nmodels) as universal DP ask solver. We select a collection of datasets across\nfour representative DP tasks and construct instruction-tuning data using\nserialization and knowledge injection techniques tailored to DP. As such, the\ninstruction-tuned LLMs empower users to manually craft instructions for DP.\nMeanwhile, they can operate on a local, single, and low-priced GPU, ensuring\ndata security and enabling further tuning. Our experiments show that our\ndataset constructed for DP instruction tuning, namely Jellyfish, effectively\nenhances LLMs' DP performances and barely compromises their abilities in NLP\ntasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the\nmodels deliver competitiveness compared to state-of-the-art DP methods and\nstrong generalizability to unseen tasks. The models' performance rivals that of\nGPT series models, and the interpretation offers enhanced reasoning\ncapabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available\nat Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B\nhttps://huggingface.co/NECOUDBFM/Jellyfish-13B\n","authors":["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2312.01678v4.pdf","comment":"a.k.a. \"Jellyfish: Instruction-Tuning Local Large Language Models for\n  Data Preprocessing''"},{"id":"http://arxiv.org/abs/2403.08492v1","updated":"2024-03-13T12:55:43Z","published":"2024-03-13T12:55:43Z","title":"Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking","summary":"  Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.\n","authors":["Ming Dong","Yujing Chen","Miao Zhang","Hao Sun","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2403.08492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08484v1","updated":"2024-03-13T12:50:23Z","published":"2024-03-13T12:50:23Z","title":"Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH\n  Mask based Efficient Fine-tuning","summary":"  In view of the huge number of parameters of Large language models (LLMs) ,\ntuning all parameters is very costly, and accordingly fine-tuning specific\nparameters is more sensible. Most of parameter efficient fine-tuning (PEFT)\nconcentrate on parameter selection strategies, such as additive method,\nselective method and reparametrization-based method. However, there are few\nmethods that consider the impact of data samples on parameter selecting, such\nas Fish Mask based method. Fish Mask randomly choose a part of data samples and\ntreat them equally during parameter selection, which is unable to dynamically\nselect optimal parameters for inconstant data distributions. In this work, we\nadopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline\nI}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline\nD}$ecreasing) algorithm to search the best setting of sample-parameter pair for\nFISH Mask. In each iteration, by searching the set of samples and parameters\nwith larger Fish information, IRD can find better sample-parameter pair in most\nscale. We demonstrate the effectiveness and rationality of proposed strategy by\nconducting experiments on GLUE benchmark. Experimental results show our\nstrategy optimizes the parameter selection and achieves preferable performance.\n","authors":["Ming Dong","Kang Xue","Bolong Zheng","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2403.08484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07805v2","updated":"2024-03-13T12:46:38Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v2.pdf","comment":"8 pages, 4 figures; fixed typos"},{"id":"http://arxiv.org/abs/2305.09144v2","updated":"2024-03-13T12:34:17Z","published":"2023-05-16T03:50:38Z","title":"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism\n  of Language Models","summary":"  Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.\n","authors":["Boxi Cao","Qiaoyu Tang","Hongyu Lin","Shanshan Jiang","Bin Dong","Xianpei Han","Jiawei Chen","Tianshu Wang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2305.09144v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08462v1","updated":"2024-03-13T12:25:47Z","published":"2024-03-13T12:25:47Z","title":"Authorship Verification based on the Likelihood Ratio of Grammar Models","summary":"  Authorship Verification (AV) is the process of analyzing a set of documents\nto determine whether they were written by a specific author. This problem often\narises in forensic scenarios, e.g., in cases where the documents in question\nconstitute evidence for a crime. Existing state-of-the-art AV methods use\ncomputational solutions that are not supported by a plausible scientific\nexplanation for their functioning and that are often difficult for analysts to\ninterpret. To address this, we propose a method relying on calculating a\nquantity we call $\\lambda_G$ (LambdaG): the ratio between the likelihood of a\ndocument given a model of the Grammar for the candidate author and the\nlikelihood of the same document given a model of the Grammar for a reference\npopulation. These Grammar Models are estimated using $n$-gram language models\nthat are trained solely on grammatical features. Despite not needing large\namounts of data for training, LambdaG still outperforms other established AV\nmethods with higher computational complexity, including a fine-tuned Siamese\nTransformer network. Our empirical evaluation based on four baseline methods\napplied to twelve datasets shows that LambdaG leads to better results in terms\nof both accuracy and AUC in eleven cases and in all twelve cases if considering\nonly topic-agnostic methods. The algorithm is also highly robust to important\nvariations in the genre of the reference population in many cross-genre\ncomparisons. In addition to these properties, we demonstrate how LambdaG is\neasier to interpret than the current state-of-the-art. We argue that the\nadvantage of LambdaG over other methods is due to fact that it is compatible\nwith Cognitive Linguistic theories of language processing.\n","authors":["Andrea Nini","Oren Halvani","Lukas Graner","Valerio Gherardi","Shunichi Ishihara"],"pdf_url":"https://arxiv.org/pdf/2403.08462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02684v2","updated":"2024-03-13T12:24:06Z","published":"2023-11-05T15:48:29Z","title":"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE","summary":"  Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/paper_list/Octavius.\n","authors":["Zeren Chen","Ziqin Wang","Zhen Wang","Huayang Liu","Zhenfei Yin","Si Liu","Lu Sheng","Wanli Ouyang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2311.02684v2.pdf","comment":"22 pages, 12 figures. Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2312.05720v3","updated":"2024-03-13T11:19:24Z","published":"2023-12-10T01:19:59Z","title":"Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer\n  Inputs of Language Models in Federated Learning","summary":"  Language models trained via federated learning (FL) demonstrate impressive\ncapabilities in handling complex tasks while protecting user privacy. Recent\nstudies indicate that leveraging gradient information and prior knowledge can\npotentially reveal training samples within FL setting. However, these\ninvestigations have overlooked the potential privacy risks tied to the\nintrinsic architecture of the models. This paper presents a two-stage privacy\nattack strategy that targets the vulnerabilities in the architecture of\ncontemporary language models, significantly enhancing attack performance by\ninitially recovering certain feature directions as additional supervisory\nsignals. Our comparative experiments demonstrate superior attack performance\nacross various datasets and scenarios, highlighting the privacy leakage risk\nassociated with the increasingly complex architectures of language models. We\ncall for the community to recognize and address these potential privacy risks\nin designing large language models.\n","authors":["Jianwei Li","Sheng Liu","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2312.05720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08424v1","updated":"2024-03-13T11:16:43Z","published":"2024-03-13T11:16:43Z","title":"Tastle: Distract Large Language Models for Automatic Jailbreak Attack","summary":"  Large language models (LLMs) have achieved significant advances in recent\ndays. Extensive efforts have been made before the public release of LLMs to\nalign their behaviors with human values. The primary goal of alignment is to\nensure their helpfulness, honesty and harmlessness. However, even meticulously\naligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,\nleading to unintended behaviors. The jailbreak is to intentionally develop a\nmalicious prompt that escapes from the LLM security restrictions to produce\nuncensored detrimental contents. Previous works explore different jailbreak\nmethods for red teaming LLMs, yet they encounter challenges regarding to\neffectiveness and scalability. In this work, we propose Tastle, a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.\n","authors":["Zeguan Xiao","Yan Yang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05657v2","updated":"2024-03-13T10:54:21Z","published":"2023-11-09T00:30:13Z","title":"Agent Lumos: Unified and Modular Training for Open-Source Language\n  Agents","summary":"  Closed-source agents suffer from several issues such as a lack of\naffordability, transparency, and reproducibility, particularly on complex\ninteractive tasks. This motivates the development of open-source alternatives.\nWe introduce LUMOS, one of the first frameworks for training open-source\nLLM-based agents. LUMOS features a learnable, unified, and modular architecture\nwith a planning module that learns high-level subgoal generation, and a\ngrounding module trained to translate these into actions using various tools in\nthe execution module. The design allows for modular upgrades and wider\napplicability to diverse interactive tasks. To foster generalizable agent\nlearning, we collect large-scale, unified, and high-quality training\nannotations derived from diverse ground-truth reasoning rationales across\nvarious complex interactive tasks. On 9 datasets, LUMOS exhibits several key\nadvantages: (1) LUMOS excels multiple larger open-source agents on the held-out\ndatasets (unused for training) for each task type. LUMOS even surpasses GPT\nagents on QA and web tasks; (2) LUMOS outperforms open-source agents produced\nby chain-of-thoughts and unmodularized integrated training; and (3) LUMOS\neffectively generalizes to unseen tasks, outperforming 33B-scale agents and\ndomain-specific agents.\n","authors":["Da Yin","Faeze Brahman","Abhilasha Ravichander","Khyathi Chandu","Kai-Wei Chang","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2311.05657v2.pdf","comment":"Project website: https://allenai.github.io/lumos/"},{"id":"http://arxiv.org/abs/2403.08391v1","updated":"2024-03-13T10:10:07Z","published":"2024-03-13T10:10:07Z","title":"Misinformation is not about Bad Facts: An Analysis of the Production and\n  Consumption of Fringe Content","summary":"  What if misinformation is not an information problem at all? Our findings\nsuggest that online fringe ideologies spread through the use of content that is\nconsensus-based and \"factually correct\". We found that Australian news\npublishers with both moderate and far-right political leanings contain\ncomparable levels of information completeness and quality; and furthermore,\nthat far-right Twitter users often share from moderate sources. However, a\nstark difference emerges when we consider two additional factors: 1) the narrow\ntopic selection of articles by far-right users, suggesting that they cherrypick\nonly news articles that engage with specific topics of their concern, and 2)\nthe difference between moderate and far-right publishers when we examine the\nwriting style of their articles. Furthermore, we can even identify users prone\nto sharing misinformation based on their communication style. These findings\nhave important implications for countering online misinformation, as they\nhighlight the powerful role that users' personal bias towards specific topics,\nand publishers' writing styles, have in amplifying fringe ideologies online.\n","authors":["JooYoung Lee","Emily Booth","Hany Farid","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2403.08391v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.03522v2","updated":"2024-03-13T09:50:40Z","published":"2024-03-06T08:03:05Z","title":"Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis","summary":"  Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.\n","authors":["Tirza Biron","Moshe Barboy","Eran Ben-Artzy","Alona Golubchik","Yanir Marmor","Smadar Szekely","Yaron Winter","David Harel"],"pdf_url":"https://arxiv.org/pdf/2403.03522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16589v2","updated":"2024-03-13T09:45:02Z","published":"2024-01-29T21:44:27Z","title":"ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence\n  Labeling Tasks","summary":"  Prompt-based methods have been successfully applied to multilingual\npretrained language models for zero-shot cross-lingual understanding. However,\nmost previous studies primarily focused on sentence-level classification tasks,\nand only a few considered token-level labeling tasks such as Named Entity\nRecognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose\nToken-Level Prompt Decomposition (ToPro), which facilitates the prompt-based\nmethod for token-level sequence labeling tasks. The ToPro method decomposes an\ninput sentence into single tokens and applies one prompt template to each\ntoken. Our experiments on multilingual NER and POS tagging datasets demonstrate\nthat ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning\nin zero-shot cross-lingual transfer, especially for languages that are\ntypologically different from the source language English. Our method also\nattains state-of-the-art performance when employed with the mT5 model. Besides,\nour exploratory study in multilingual large language models shows that ToPro\nperforms much better than the current in-context learning method. Overall, the\nperformance improvements show that ToPro could potentially serve as a novel and\nsimple benchmarking method for sequence labeling tasks.\n","authors":["Bolei Ma","Ercong Nie","Shuzhou Yuan","Helmut Schmid","Michael Färber","Frauke Kreuter","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2401.16589v2.pdf","comment":"EACL 2024"},{"id":"http://arxiv.org/abs/2403.08377v1","updated":"2024-03-13T09:42:46Z","published":"2024-03-13T09:42:46Z","title":"Learning to Describe for Predicting Zero-shot Drug-Drug Interactions","summary":"  Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of\nconcurrent drug administration, posing a significant challenge in healthcare.\nAs the development of new drugs continues, the potential for unknown adverse\neffects resulting from DDIs becomes a growing concern. Traditional\ncomputational methods for DDI prediction may fail to capture interactions for\nnew drugs due to the lack of knowledge. In this paper, we introduce a new\nproblem setup as zero-shot DDI prediction that deals with the case of new\ndrugs. Leveraging textual information from online databases like DrugBank and\nPubChem, we propose an innovative approach TextDDI with a language model-based\nDDI predictor and a reinforcement learning~(RL)-based information selector,\nenabling the selection of concise and pertinent text for accurate DDI\nprediction on new drugs. Empirical results show the benefits of the proposed\napproach on several settings including zero-shot and few-shot DDI prediction,\nand the selected texts are semantically relevant. Our code and data are\navailable at \\url{https://github.com/zhufq00/DDIs-Prediction}.\n","authors":["Fangqi Zhu","Yongqi Zhang","Lei Chen","Bing Qin","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08375v1","updated":"2024-03-13T09:38:39Z","published":"2024-03-13T09:38:39Z","title":"Translating between SQL Dialects for Cloud Migration","summary":"  Migrations of systems from on-site premises to the cloud has been a\nfundamental endeavor by many industrial institutions. A crucial component of\nsuch cloud migrations is the transition of databases to be hosted online. In\nthis work, we consider the difficulties of this migration for SQL databases.\nWhile SQL is one of the prominent methods for storing database procedures,\nthere are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)\nwhich can complicate migrations when the on-premise SQL dialect differs to the\ndialect hosted on the cloud. Tools exist by common cloud provides such as AWS\nand Azure to aid in translating between dialects in order to mitigate the\nmajority of the difficulties. However, these tools do not successfully\ntranslate $100\\%$ of the code. Consequently, software engineers must manually\nconvert the remainder of the untranslated database. For large organizations,\nthis task quickly becomes intractable and so more innovative solutions are\nrequired. We consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud migrations.\nFurthermore, we introduce potential avenues of research to tackle this\nchallenge that have yielded promising preliminary results.\n","authors":["Ran Zmigrod","Salwa Alamir","Xiaomo Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08370v1","updated":"2024-03-13T09:31:50Z","published":"2024-03-13T09:31:50Z","title":"SMART: Submodular Data Mixture Strategy for Instruction Tuning","summary":"  Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.\n","authors":["H S V N S Kowndinya Renduchintala","Sumit Bhatia","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06935v2","updated":"2024-03-13T09:26:26Z","published":"2024-03-11T17:20:12Z","title":"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs","summary":"  While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision \\& Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, humans show clear production\npreferences for highly context-sensitive expressions, such as the quantifiers\n`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on\nthree categories (nouns, attributes, and quantifiers) where humans show great\nsubjective variability concerning the distribution over plausible labels, using\ndatasets and resources mostly under-explored in previous work. Our results\nreveal mixed evidence on the ability of VLLMs to capture human naming\npreferences, with all models failing in tasks that require high-level reasoning\nsuch as assigning quantifiers.\n","authors":["Alberto Testoni","Juell Sprott","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2403.06935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08358v1","updated":"2024-03-13T09:18:46Z","published":"2024-03-13T09:18:46Z","title":"Log Summarisation for Defect Evolution Analysis","summary":"  Log analysis and monitoring are essential aspects in software maintenance and\nidentifying defects. In particular, the temporal nature and vast size of log\ndata leads to an interesting and important research question: How can logs be\nsummarised and monitored over time? While this has been a fundamental topic of\nresearch in the software engineering community, work has typically focused on\nheuristic-, syntax-, or static-based methods. In this work, we suggest an\nonline semantic-based clustering approach to error logs that dynamically\nupdates the log clusters to enable monitoring code error life-cycles. We also\nintroduce a novel metric to evaluate the performance of temporal log clusters.\nWe test our system and evaluation metric with an industrial dataset and find\nthat our solution outperforms similar systems. We hope that our work encourages\nfurther temporal exploration in defect datasets.\n","authors":["Rares Dolga","Ran Zmigrod","Rui Silva","Salwa Alamir","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.08358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09702v2","updated":"2024-03-13T09:11:15Z","published":"2023-11-16T09:27:36Z","title":"Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?","summary":"  Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.\n","authors":["Bangzheng Li","Ben Zhou","Fei Wang","Xingyu Fu","Dan Roth","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.09702v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.08345v1","updated":"2024-03-13T08:50:15Z","published":"2024-03-13T08:50:15Z","title":"From human experts to machines: An LLM supported approach to ontology\n  and knowledge graph construction","summary":"  The conventional process of building Ontologies and Knowledge Graphs (KGs)\nheavily relies on human domain experts to define entities and relationship\ntypes, establish hierarchies, maintain relevance to the domain, fill the ABox\n(or populate with instances), and ensure data quality (including amongst others\naccuracy and completeness). On the other hand, Large Language Models (LLMs)\nhave recently gained popularity for their ability to understand and generate\nhuman-like natural language, offering promising ways to automate aspects of\nthis process. This work explores the (semi-)automatic construction of KGs\nfacilitated by open-source LLMs. Our pipeline involves formulating competency\nquestions (CQs), developing an ontology (TBox) based on these CQs, constructing\nKGs using the developed ontology, and evaluating the resultant KG with minimal\nto no involvement of human experts. We showcase the feasibility of our\nsemi-automated pipeline by creating a KG on deep learning methodologies by\nexploiting scholarly publications. To evaluate the answers generated via\nRetrieval-Augmented-Generation (RAG) as well as the KG concepts automatically\nextracted using LLMs, we design a judge LLM, which rates the generated content\nbased on ground truth. Our findings suggest that employing LLMs could\npotentially reduce the human effort involved in the construction of KGs,\nalthough a human-in-the-loop approach is recommended to evaluate automatically\ngenerated KGs.\n","authors":["Vamsi Krishna Kommineni","Birgitta König-Ries","Sheeba Samuel"],"pdf_url":"https://arxiv.org/pdf/2403.08345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08852v2","updated":"2024-03-13T08:49:54Z","published":"2023-06-15T04:41:28Z","title":"BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection","summary":"  This paper introduces a novel method leveraging bi-encoder-based detectors\nalong with a comprehensive study comparing different out-of-distribution (OOD)\ndetection methods in NLP using different feature extractors. The feature\nextraction stage employs popular methods such as Universal Sentence Encoder\n(USE), BERT, MPNET, and GLOVE to extract informative representations from\ntextual data. The evaluation is conducted on several datasets, including\nCLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using\nmetrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental\nresults demonstrate that the proposed bi-encoder-based detectors outperform\nother methods, both those that require OOD labels in training and those that do\nnot, across all datasets, showing great potential for OOD detection in NLP. The\nsimplicity of the training process and the superior detection performance make\nthem applicable to real-world scenarios. The presented methods and benchmarking\nmetrics serve as a valuable resource for future research in OOD detection,\nenabling further advancements in this field. The code and implementation\ndetails can be found on our GitHub repository:\nhttps://github.com/yellowmessenger/ood-detection.\n","authors":["Louis Owen","Biddwan Ahmed","Abhay Kumar"],"pdf_url":"https://arxiv.org/pdf/2306.08852v2.pdf","comment":"Published in IEEE: https://ieeexplore.ieee.org/document/10389907"},{"id":"http://arxiv.org/abs/2311.18765v3","updated":"2024-03-13T08:47:32Z","published":"2023-11-30T18:05:52Z","title":"MLLMs-Augmented Visual-Language Representation Learning","summary":"  Visual-language pre-training has achieved remarkable success in many\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that Multi-modal Large\nLanguage Models (MLLMs) can enhance visual-language representation learning by\nestablishing richer image-text associations for image-text datasets. Our\napproach is simple, utilizing MLLMs to extend multiple diverse captions for\neach image. To prevent the bias introduced by MLLMs' hallucinations and\nmonotonous language styles, we propose \"text shearing\" to maintain the quality\nand availability of extended captions. In image-text retrieval, without\nintroducing additional training cost, our method consistently obtains 5.6 ~\n35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and\nzero-shot settings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n","authors":["Yanqing Liu","Kai Wang","Wenqi Shao","Ping Luo","Yu Qiao","Mike Zheng Shou","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2311.18765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01188v2","updated":"2024-03-13T08:45:53Z","published":"2023-10-02T13:26:43Z","title":"Quantifying the Plausibility of Context Reliance in Neural Machine\n  Translation","summary":"  Establishing whether language models can use contextual information in a\nhuman-plausible way is important to ensure their trustworthiness in real-world\nsettings. However, the questions of when and which parts of the context affect\nmodel generations are typically tackled separately, with current plausibility\nevaluations being practically limited to a handful of artificial benchmarks. To\naddress this, we introduce Plausibility Evaluation of Context Reliance\n(PECoRe), an end-to-end interpretability framework designed to quantify context\nusage in language models' generations. Our approach leverages model internals\nto (i) contrastively identify context-sensitive target tokens in generated\ntexts and (ii) link them to contextual cues justifying their prediction. We use\n\\pecore to quantify the plausibility of context-aware machine translation\nmodels, comparing model rationales with human annotations across several\ndiscourse-level phenomena. Finally, we apply our method to unannotated model\ntranslations to identify context-mediated predictions and highlight instances\nof (im)plausible context usage throughout generation.\n","authors":["Gabriele Sarti","Grzegorz Chrupała","Malvina Nissim","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2310.01188v2.pdf","comment":"ICLR 2024 Camera Ready. Code: https://github.com/gsarti/pecore.\n  Artifacts:\n  https://huggingface.co/collections/gsarti/pecore-iclr-2024-65edab42e28439e21b612c2e"},{"id":"http://arxiv.org/abs/2403.08332v1","updated":"2024-03-13T08:34:53Z","published":"2024-03-13T08:34:53Z","title":"Autoregressive Score Generation for Multi-trait Essay Scoring","summary":"  Recently, encoder-only pre-trained models such as BERT have been successfully\napplied in automated essay scoring (AES) to predict a single overall score.\nHowever, studies have yet to explore these models in multi-trait AES, possibly\ndue to the inefficiency of replicating BERT-based models for each trait.\nBreaking away from the existing sole use of encoder, we propose an\nautoregressive prediction of multi-trait scores (ArTS), incorporating a\ndecoding process by leveraging the pre-trained T5. Unlike prior regression or\nclassification methods, we redefine AES as a score-generation task, allowing a\nsingle model to predict multiple scores. During decoding, the subsequent trait\nprediction can benefit by conditioning on the preceding trait scores.\nExperimental results proved the efficacy of ArTS, showing over 5% average\nimprovements in both prompts and traits.\n","authors":["Heejin Do","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08332v1.pdf","comment":"Accepted at EACL2024 Findings"},{"id":"http://arxiv.org/abs/2307.09249v2","updated":"2024-03-13T08:20:34Z","published":"2023-07-18T13:28:31Z","title":"UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model\n  in Data Science","summary":"  Recent advancements in NLP have witnessed the groundbreaking impact of\npretrained models, yielding impressive outcomes across various tasks. This\nstudy seeks to extend the power of pretraining methodologies to facilitating\nthe prediction over tables in data science, a domain traditionally overlooked,\nyet inherently challenging due to the plethora of table schemas intrinsic to\ndifferent tasks. The primary research questions underpinning this work revolve\naround the establishment of a universal pretraining protocol for tables with\nvaried structures, the generalizability and transferability of learned\nknowledge across tasks, the adaptation to diverse downstream applications, and\nthe incorporation of incremental columns over time. In response to these\nchallenges, we introduce UniTabE, a straightforward yet effective method\ndesigned to process tables in a uniform manner, devoid of constraints imposed\nby specific table structures. UniTabE's core concept relies on representing\neach basic table element with a module, termed TabUnit. This is subsequently\nfollowed by a Transformer encoder to refine the representation. Moreover, our\nmodel is designed to facilitate pretraining and finetuning through the\nutilization of free-form prompts. In order to implement the pretraining phase,\nwe curated an expansive tabular dataset comprising approximately 13B samples,\nmeticulously gathered from the Kaggle platform. This research primarily centers\non classification and regression tasks involving tabular data, and conducts\nrigorous experimental testing and analyses to validate the effectiveness of our\nmethodology. The experimental results demonstrate UniTabE's superior\nperformance against several baselines across massive benchmarks. This,\ntherefore, underscores UniTabE's potential to significantly enhance the\nsemantic representation of tabular data, thereby marking a significant stride\nfor tabular data analysis.\n","authors":["Yazheng Yang","Yuqi Wang","Guang Liu","Ledell Wu","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2307.09249v2.pdf","comment":"ICLR 2024, 9 pages"},{"id":"http://arxiv.org/abs/2310.05209v2","updated":"2024-03-13T08:14:47Z","published":"2023-10-08T15:50:36Z","title":"Scaling Laws of RoPE-based Extrapolation","summary":"  The extrapolation capability of Large Language Models (LLMs) based on Rotary\nPosition Embedding is currently a topic of considerable interest. The\nmainstream approach to addressing extrapolation with LLMs involves modifying\nRoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the\noriginal RoPE, with a larger value and providing longer fine-tuning text. In\nthis work, we first observe that fine-tuning a RoPE-based LLM with either a\nsmaller or larger base in pre-training context length could significantly\nenhance its extrapolation performance. After that, we propose\n\\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework\nfrom the periodic perspective, to describe the relationship between the\nextrapolation performance and base value as well as tuning context length. In\nthis process, we also explain the origin of the RoPE-based extrapolation issue\nby \\textbf{\\textit{critical dimension for extrapolation}}. Besides these\nobservations and analyses, we achieve extrapolation up to 1 million context\nlength within only 16K training length on LLaMA2 7B and 13B.\n","authors":["Xiaoran Liu","Hang Yan","Shuo Zhang","Chenxin An","Xipeng Qiu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05209v2.pdf","comment":"26 pages, 12 figures, Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08319v1","updated":"2024-03-13T08:02:23Z","published":"2024-03-13T08:02:23Z","title":"Knowledge Conflicts for LLMs: A Survey","summary":"  This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.\n","authors":["Rongwu Xu","Zehan Qi","Cunxiang Wang","Hongru Wang","Yue Zhang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.08319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08314v1","updated":"2024-03-13T07:49:50Z","published":"2024-03-13T07:49:50Z","title":"Is Context Helpful for Chat Translation Evaluation?","summary":"  Despite the recent success of automatic metrics for assessing translation\nquality, their application in evaluating the quality of machine-translated\nchats has been limited. Unlike more structured texts like news, chat\nconversations are often unstructured, short, and heavily reliant on contextual\ninformation. This poses questions about the reliability of existing\nsentence-level metrics in this domain as well as the role of context in\nassessing the translation quality. Motivated by this, we conduct a\nmeta-evaluation of existing sentence-level automatic metrics, primarily\ndesigned for structured domains such as news, to assess the quality of\nmachine-translated chats. We find that reference-free metrics lag behind\nreference-based ones, especially when evaluating translation quality in\nout-of-English settings. We then investigate how incorporating conversational\ncontextual information in these metrics affects their performance. Our findings\nshow that augmenting neural learned metrics with contextual information helps\nimprove correlation with human judgments in the reference-free scenario and\nwhen evaluating translations in out-of-English settings. Finally, we propose a\nnew evaluation metric, Context-MQM, that utilizes bilingual context with a\nlarge language model (LLM) and further validate that adding context helps even\nfor LLM-based evaluation metrics.\n","authors":["Sweta Agrawal","Amin Farajian","Patrick Fernandes","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2403.08314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08312v1","updated":"2024-03-13T07:44:14Z","published":"2024-03-13T07:44:14Z","title":"StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses","summary":"  Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200k or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200k of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.\n","authors":["Jia-Nan Li","Quan Tu","Cunli Mao","Zhengtao Yu","Ji-Rong Wen","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.08312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06185v4","updated":"2024-03-13T07:35:18Z","published":"2023-12-11T07:56:25Z","title":"KnowGPT: Knowledge Injection for Large Language Models","summary":"  Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Daochen Zha","Zailiang Yu","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.06185v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08305v1","updated":"2024-03-13T07:31:20Z","published":"2024-03-13T07:31:20Z","title":"Towards Personalized Evaluation of Large Language Models with An\n  Anonymous Crowd-Sourcing Platform","summary":"  Large language model evaluation plays a pivotal role in the enhancement of\nits capacity. Previously, numerous methods for evaluating large language models\nhave been proposed in this area. Despite their effectiveness, these existing\nworks mainly focus on assessing objective questions, overlooking the capability\nto evaluate subjective questions which is extremely common for large language\nmodels. Additionally, these methods predominantly utilize centralized datasets\nfor evaluation, with question banks concentrated within the evaluation\nplatforms themselves. Moreover, the evaluation processes employed by these\nplatforms often overlook personalized factors, neglecting to consider the\nindividual characteristics of both the evaluators and the models being\nevaluated. To address these limitations, we propose a novel anonymous\ncrowd-sourcing evaluation platform, BingJian, for large language models that\nemploys a competitive scoring mechanism where users participate in ranking\nmodels based on their performance. This platform stands out not only for its\nsupport of centralized evaluations to assess the general capabilities of models\nbut also for offering an open evaluation gateway. Through this gateway, users\nhave the opportunity to submit their questions, testing the models on a\npersonalized and potentially broader range of capabilities. Furthermore, our\nplatform introduces personalized evaluation scenarios, leveraging various forms\nof human-computer interaction to assess large language models in a manner that\naccounts for individual user preferences and contexts. The demonstration of\nBingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.\n","authors":["Mingyue Cheng","Hao Zhang","Jiqian Yang","Qi Liu","Li Li","Xin Huang","Liwei Song","Zhi Li","Zhenya Huang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07759v3","updated":"2024-03-13T07:27:10Z","published":"2023-09-14T14:45:47Z","title":"PROGrasp: Pragmatic Human-Robot Communication for Object Grasping","summary":"  Interactive Object Grasping (IOG) is the task of identifying and grasping the\ndesired object via human-robot natural language interaction. Current IOG\nsystems assume that a human user initially specifies the target object's\ncategory (e.g., bottle). Inspired by pragmatics, where humans often convey\ntheir intentions by relying on context to achieve goals, we introduce a new IOG\ntask, Pragmatic-IOG, and the corresponding dataset, Intention-oriented\nMulti-modal Dialogue (IM-Dial). In our proposed task scenario, an\nintention-oriented utterance (e.g., \"I am thirsty\") is initially given to the\nrobot. The robot should then identify the target object by interacting with a\nhuman user. Based on the task setup, we propose a new robotic system that can\ninterpret the user's intention and pick up the target object, Pragmatic Object\nGrasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules\nfor visual grounding, question asking, object grasping, and most importantly,\nanswer interpretation for pragmatic inference. Experimental results show that\nPROGrasp is effective in offline (i.e., target object discovery) and online\n(i.e., IOG with a physical robot arm) settings. Code and data are available at\nhttps://github.com/gicheonkang/prograsp.\n","authors":["Gi-Cheon Kang","Junghyun Kim","Jaein Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07759v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2402.11550v2","updated":"2024-03-13T07:16:42Z","published":"2024-02-18T11:46:52Z","title":"LongAgent: Scaling Language Models to 128k Context through Multi-Agent\n  Collaboration","summary":"  Large language models (LLMs) have demonstrated impressive performance in\nunderstanding language and executing complex reasoning tasks. However, LLMs\nwith long context windows have been notorious for their expensive training\ncosts and high inference latency. Even the most advanced models such as GPT-4\nand Claude2 often make mistakes when processing inputs of over $100k$ tokens, a\nphenomenon also known as \\textit{lost in the middle}. In this paper, we propose\n\\textsc{LongAgent}, a method based on multi-agent collaboration, which scales\nLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority\nin long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is\nresponsible for understanding user intent and directing team members to acquire\ninformation from documents. Due to members' hallucinations, it is non-trivial\nfor a leader to obtain accurate information from the responses of dozens to\nhundreds of members. To address this, we develop an \\textit{inter-member\ncommunication} mechanism to resolve response conflicts caused by hallucinations\nthrough information sharing. Our experimental results indicate that\n\\textsc{LongAgent} offers a promising alternative for long-text processing. The\nagent team instantiated with LLaMA-7B achieves significant improvements in\ntasks such as 128k-long text retrieval, multi-hop question answering, compared\nto GPT-4.\n","authors":["Jun Zhao","Can Zu","Hao Xu","Yi Lu","Wei He","Yiwen Ding","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.11550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08295v1","updated":"2024-03-13T06:59:16Z","published":"2024-03-13T06:59:16Z","title":"Gemma: Open Models Based on Gemini Research and Technology","summary":"  This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.\n","authors":[" Gemma Team","Thomas Mesnard","Cassidy Hardin","Robert Dadashi","Surya Bhupatiraju","Shreya Pathak","Laurent Sifre","Morgane Rivière","Mihir Sanjay Kale","Juliette Love","Pouya Tafti","Léonard Hussenot","Aakanksha Chowdhery","Adam Roberts","Aditya Barua","Alex Botev","Alex Castro-Ros","Ambrose Slone","Amélie Héliou","Andrea Tacchetti","Anna Bulanova","Antonia Paterson","Beth Tsai","Bobak Shahriari","Charline Le Lan","Christopher A. Choquette-Choo","Clément Crepy","Daniel Cer","Daphne Ippolito","David Reid","Elena Buchatskaya","Eric Ni","Eric Noland","Geng Yan","George Tucker","George-Christian Muraru","Grigory Rozhdestvenskiy","Henryk Michalewski","Ian Tenney","Ivan Grishchenko","Jacob Austin","James Keeling","Jane Labanowski","Jean-Baptiste Lespiau","Jeff Stanway","Jenny Brennan","Jeremy Chen","Johan Ferret","Justin Chiu","Justin Mao-Jones","Katherine Lee","Kathy Yu","Katie Millican","Lars Lowe Sjoesund","Lisa Lee","Lucas Dixon","Machel Reid","Maciej Mikuła","Mateo Wirth","Michael Sharman","Nikolai Chinaev","Nithum Thain","Olivier Bachem","Oscar Chang","Oscar Wahltinez","Paige Bailey","Paul Michel","Petko Yotov","Pier Giuseppe Sessa","Rahma Chaabouni","Ramona Comanescu","Reena Jana","Rohan Anil","Ross McIlroy","Ruibo Liu","Ryan Mullins","Samuel L Smith","Sebastian Borgeaud","Sertan Girgin","Sholto Douglas","Shree Pandya","Siamak Shakeri","Soham De","Ted Klimenko","Tom Hennigan","Vlad Feinberg","Wojciech Stokowiec","Yu-hui Chen","Zafarali Ahmed","Zhitao Gong","Tris Warkentin","Ludovic Peran","Minh Giang","Clément Farabet","Oriol Vinyals","Jeff Dean","Koray Kavukcuoglu","Demis Hassabis","Zoubin Ghahramani","Douglas Eck","Joelle Barral","Fernando Pereira","Eli Collins","Armand Joulin","Noah Fiedel","Evan Senter","Alek Andreev","Kathleen Kenealy"],"pdf_url":"https://arxiv.org/pdf/2403.08295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08293v1","updated":"2024-03-13T06:54:47Z","published":"2024-03-13T06:54:47Z","title":"Generative Pretrained Structured Transformers: Unsupervised Syntactic\n  Language Models at Scale","summary":"  A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.\n","authors":["Xiang Hu","Pengyu Ji","Qingyang Zhu","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2403.08293v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.04945v2","updated":"2024-03-13T06:20:47Z","published":"2024-03-07T23:20:56Z","title":"Electrocardiogram Instruction Tuning for Report Generation","summary":"  Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool\nfor cardiac conditions monitoring, are crucial in assisting clinicians. Recent\nstudies have concentrated on classifying cardiac conditions using ECG data but\nhave overlooked ECG report generation, which is not only time-consuming but\nalso requires clinical expertise. To automate ECG report generation and ensure\nits versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)\nframework, the \\textit{first} attempt to tackle ECG report generation with LLMs\nand multimodal instructions. To facilitate future research, we establish a\nbenchmark to evaluate MEIT with various LLMs backbones across two large-scale\nECG datasets. Our approach uniquely aligns the representations of the ECG\nsignal and the report, and we conduct extensive experiments to benchmark MEIT\nwith nine open source LLMs, using more than 800,000 ECG reports. MEIT's results\nunderscore the superior performance of instruction-tuned LLMs, showcasing their\nproficiency in quality report generation, zero-shot capabilities, and\nresilience to signal perturbation. These findings emphasize the efficacy of our\nMEIT framework and its potential for real-world clinical application.\n","authors":["Zhongwei Wan","Che Liu","Xin Wang","Chaofan Tao","Hui Shen","Zhenwu Peng","Jie Fu","Rossella Arcucci","Huaxiu Yao","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.04945v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.08281v1","updated":"2024-03-13T06:18:48Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08272v1","updated":"2024-03-13T05:51:57Z","published":"2024-03-13T05:51:57Z","title":"RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education","summary":"  The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale and real-world interactions between students and AI\nsystems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE\nfor University), a dataset sourced from a semester-long experiment with 212\ncollege students in English as Foreign Language (EFL) writing courses. During\nthe study, students engaged in dialogues with ChatGPT to revise their essays.\nRECIPE4U includes comprehensive records of these interactions, including\nconversation logs, students' intent, students' self-rated satisfaction, and\nstudents' essay edit histories. In particular, we annotate the students'\nutterances in RECIPE4U with 13 intention labels based on our coding schemes. We\nestablish baseline results for two subtasks in task-oriented dialogue systems\nwithin educational contexts: intent detection and satisfaction estimation. As a\nfoundational step, we explore student-ChatGPT interaction patterns through\nRECIPE4U and analyze them by focusing on students' dialogue, essay data\nstatistics, and students' essay edits. We further illustrate potential\napplications of RECIPE4U dataset for enhancing the incorporation of LLMs in\neducational frameworks. RECIPE4U is publicly available at\nhttps://zeunie.github.io/RECIPE4U/.\n","authors":["Jieun Han","Haneul Yoo","Junho Myung","Minsun Kim","Tak Yeon Lee","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.08272v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2309.13243"},{"id":"http://arxiv.org/abs/2309.17428v2","updated":"2024-03-13T05:39:25Z","published":"2023-09-29T17:40:26Z","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized\n  Toolsets","summary":"  Large language models (LLMs) are often augmented with tools to solve complex\ntasks. By generating code snippets and executing them through task-specific\nApplication Programming Interfaces (APIs), they can offload certain functions\nto dedicated external modules, such as image encoding and performing\ncalculations. However, most existing approaches to augment LLMs with tools are\nconstrained by general-purpose APIs and lack the flexibility for tailoring them\nto specific tasks. In this work, we present CRAFT, a general tool creation and\nretrieval framework for LLMs. It creates toolsets specifically curated for the\ntasks and equips LLMs with a component that retrieves tools from these sets to\nenhance their capability to solve complex tasks. For each task, we collect\nspecific code solutions by prompting GPT-4 to solve the training examples.\nFollowing a validation step ensuring the correctness, these solutions are\nabstracted into code snippets to enhance reusability, and deduplicated for\nhigher quality. At inference time, the language model retrieves snippets from\nthe toolsets and then executes them or generates the output conditioning on the\nretrieved snippets. Our method is designed to be flexible and offers a\nplug-and-play approach to adapt off-the-shelf LLMs to unseen domains and\nmodalities, without any finetuning. Experiments on vision-language, tabular\nprocessing, and mathematical reasoning tasks show that our approach achieves\nsubstantial improvements compared to strong baselines. In addition, our\nin-depth analysis reveals that: (1) consistent performance improvement can be\nachieved by scaling up the number of tools and the capability of the backbone\nmodels; (2) each component of our approach contributes to the performance\ngains; (3) the created tools are well-structured and reliable with low\ncomplexity and atomicity. The code is available at\nhttps://github.com/lifan-yuan/CRAFT.\n","authors":["Lifan Yuan","Yangyi Chen","Xingyao Wang","Yi R. Fung","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2309.17428v2.pdf","comment":"Accepted to ICLR 2024. Code is available at\n  https://github.com/lifan-yuan/CRAFT"},{"id":"http://arxiv.org/abs/2403.08258v1","updated":"2024-03-13T05:20:45Z","published":"2024-03-13T05:20:45Z","title":"Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition","summary":"  Conformer-based attention models have become the de facto backbone model for\nAutomatic Speech Recognition tasks. A blank symbol is usually introduced to\nalign the input and output sequences for CTC or RNN-T models. Unfortunately,\nthe long input length overloads computational budget and memory consumption\nquadratically by attention mechanism. In this work, we propose a\n\"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze\nsequence input length dynamically and inhomogeneously. Skipformer uses an\nintermediate CTC output as criteria to split frames into three groups: crucial,\nskipping and ignoring. The crucial group feeds into next conformer blocks and\nits output joint with skipping group by original temporal order as the final\nencoder output. Experiments show that our model reduces the input sequence\nlength by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,\nthe model can achieve better recognition accuracy and faster inference speed\nthan recent baseline models. Our code is open-sourced and available online.\n","authors":["Wenjing Zhu","Sining Sun","Changhao Shan","Peng Fan","Qing Yang"],"pdf_url":"https://arxiv.org/pdf/2403.08258v1.pdf","comment":"Accepted by ICME2024"},{"id":"http://arxiv.org/abs/2310.19208v2","updated":"2024-03-13T05:11:57Z","published":"2023-10-30T00:30:34Z","title":"LitCab: Lightweight Language Model Calibration over Short- and Long-form\n  Responses","summary":"  A model is considered well-calibrated when its probability estimate aligns\nwith the actual likelihood of the output being correct. Calibrating language\nmodels (LMs) is crucial, as it plays a vital role in detecting and mitigating\nhallucinations of LMs as well as building more trustworthy models. However,\nstandard calibration techniques may not be suited for LM calibration. For\ninstance, post-processing methods such as temperature scaling do not reorder\nthe candidate generations. On the other hand, training-based methods require\nfine-tuning the entire model, which is impractical for LMs of large scale. We\npresent LitCab, a lightweight calibration mechanism consisting of a single\nlinear layer that takes the input text representation and predicts a bias term,\nwhich is then added to the LM output logits. LitCab improves model calibration\nby only adding < 2% of the original model parameters. For evaluation, we\nconstruct CaT, a benchmark consisting of eight text generation tasks, covering\nresponses ranging from short phrases to paragraphs. We test LitCab with\nLlama2-7B, where it improves calibration across all tasks, reducing the average\nECE score by as large as 30%. We further conduct a comprehensive evaluation\nwith multiple popular open-sourced LMs from GPT and LLaMA families, yielding\nthe following key findings: (i) Larger models within the same family exhibit\nbetter calibration on tasks with short generation tasks, but not necessarily\nfor longer ones. (ii) GPT-family models show superior calibration compared to\nLLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii)\nFine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose\n(e.g., conversations) may lead to worse calibration, highlighting the\nimportance of fine-tuning setups for calibrating LMs.\n","authors":["Xin Liu","Muhammad Khalifa","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19208v2.pdf","comment":"accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08229v1","updated":"2024-03-13T04:14:33Z","published":"2024-03-13T04:14:33Z","title":"Boosting Disfluency Detection with Large Language Model as Disfluency\n  Generator","summary":"  Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness.\n","authors":["Zhenrong Cheng","Jiayan Guo","Hao Sun","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08217v1","updated":"2024-03-13T03:31:26Z","published":"2024-03-13T03:31:26Z","title":"Research on the Application of Deep Learning-based BERT Model in\n  Sentiment Analysis","summary":"  This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.\n","authors":["Yichao Wu","Zhengyu Jin","Chenxi Shi","Penghao Liang","Tong Zhan"],"pdf_url":"https://arxiv.org/pdf/2403.08217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08213v1","updated":"2024-03-13T03:22:02Z","published":"2024-03-13T03:22:02Z","title":"Can Large Language Models Identify Authorship?","summary":"  The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis, encompassing\nauthorship verification and attribution, remains underexplored. This paper\nconducts a comprehensive evaluation of LLMs in these critical tasks.\nTraditional studies have depended on hand-crafted stylistic features, whereas\nstate-of-the-art approaches leverage text embeddings from pre-trained language\nmodels. These methods, which typically require fine-tuning on labeled data,\noften suffer from performance degradation in cross-domain applications and\nprovide limited explainability. This work seeks to address three research\nquestions: (1) Can LLMs perform zero-shot, end-to-end authorship verification\neffectively? (2) Are LLMs capable of accurately attributing authorship among\nmultiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide\nexplainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our extensive\nassessment demonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing insights into their decision-making via\na detailed analysis of linguistic features. This establishes a new benchmark\nfor future research on LLM-based authorship analysis. The code and data are\navailable at https://github.com/baixianghuang/authorship-llm.\n","authors":["Baixiang Huang","Canyu Chen","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2403.08213v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.08211v1","updated":"2024-03-13T03:15:05Z","published":"2024-03-13T03:15:05Z","title":"Large Language Models are Contrastive Reasoners","summary":"  Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on two large language models show that zero-shot\ncontrastive prompting improves performance on a range of arithmetic,\ncommonsense, and symbolic reasoning tasks without any hand-crafted few-shot\nexamples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and\nAQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method\nnot only surpasses zero-shot CoT and few-shot CoT in most arithmetic and\ncommonsense reasoning tasks but also can seamlessly integrate with existing\nprompting methods, resulting in improved or comparable results when compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp\n","authors":["Liang Yao"],"pdf_url":"https://arxiv.org/pdf/2403.08211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08198v1","updated":"2024-03-13T02:46:17Z","published":"2024-03-13T02:46:17Z","title":"Validating and Exploring Large Geographic Corpora","summary":"  This paper investigates the impact of corpus creation decisions on large\nmulti-lingual geographic web corpora. Beginning with a 427 billion word corpus\nderived from the Common Crawl, three methods are used to improve the quality of\nsub-corpora representing specific language-country pairs like New Zealand\nEnglish: (i) the agreement of independent language identification systems, (ii)\nhash-based deduplication, and (iii) location-specific outlier detection. The\nimpact of each of these steps is then evaluated at the language level and the\ncountry level by using corpus similarity measures to compare each resulting\ncorpus with baseline data sets. The goal is to understand the impact of\nupstream data cleaning decisions on downstream corpora with a specific focus on\nunder-represented languages and populations. The evaluation shows that the\nvalidity of sub-corpora is improved with each stage of cleaning but that this\nimprovement is unevenly distributed across languages and populations. This\nresult shows how standard corpus creation techniques can accidentally exclude\nunder-represented populations.\n","authors":["Jonathan Dunn"],"pdf_url":"https://arxiv.org/pdf/2403.08198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08196v1","updated":"2024-03-13T02:41:53Z","published":"2024-03-13T02:41:53Z","title":"SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech\n  Recognition Evaluation","summary":"  In the wake of the surging tide of deep learning over the past decade,\nAutomatic Speech Recognition (ASR) has garnered substantial attention, leading\nto the emergence of numerous publicly accessible ASR systems that are actively\nbeing integrated into our daily lives. Nonetheless, the impartial and\nreplicable evaluation of these ASR systems encounters challenges due to various\ncrucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a\ngeneral-purpose, open-source platform designed for ASR evaluation. With this\nplatform: (i) We report a comprehensive benchmark, unveiling the current\nstate-of-the-art panorama for ASR systems, covering both open-source models and\nindustrial commercial services. (ii) We quantize how distinct nuances in the\nscoring pipeline influence the final benchmark outcomes. These include nuances\nrelated to capitalization, punctuation, interjection, contraction, synonym\nusage, compound words, etc. These issues have gained prominence in the context\nof the transition towards an End-to-End future. (iii) We propose a practical\nmodification to the conventional Token-Error-Rate (TER) evaluation metric, with\ninspirations from Kolmogorov complexity and Normalized Information Distance\n(NID). This adaptation, called modified-TER (mTER), achieves proper\nnormalization and symmetrical treatment of reference and hypothesis. By\nleveraging this platform as a large-scale testing ground, this study\ndemonstrates the robustness and backward compatibility of mTER when compared to\nTER. The SpeechColab Leaderboard is accessible at\nhttps://github.com/SpeechColab/Leaderboard\n","authors":["Jiayu Du","Jinpeng Li","Guoguo Chen","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11522v2","updated":"2024-03-13T02:40:21Z","published":"2024-02-18T09:42:41Z","title":"Unveiling the Secrets of Engaging Conversations: Factors that Keep Users\n  Hooked on Role-Playing Dialog Agents","summary":"  With the growing humanlike nature of dialog agents, people are now engaging\nin extended conversations that can stretch from brief moments to substantial\nperiods of time. Understanding the factors that contribute to sustaining these\ninteractions is crucial, yet existing studies primarily focusing on short-term\nsimulations that rarely explore such prolonged and real conversations.\n  In this paper, we investigate the factors influencing retention rates in real\ninteractions with roleplaying models. By analyzing a large dataset of\ninteractions between real users and thousands of characters, we systematically\nexamine multiple factors and assess their impact on user retention rate.\nSurprisingly, we find that the degree to which the bot embodies the roles it\nplays has limited influence on retention rates, while the length of each turn\nit speaks significantly affects retention rates. This study sheds light on the\ncritical aspects of user engagement with role-playing models and provides\nvaluable insights for future improvements in the development of large language\nmodels for role-playing purposes.\n","authors":["Shuai Zhang","Yu Lu","Junwen Liu","Jia Yu","Huachuan Qiu","Yuming Yan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2402.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16480v3","updated":"2024-03-13T02:31:30Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text\npairs for visual-language models by recognizing and cleaning pathology reports\nwhich narrate diagnostic slides in TCGA. On the model end, we propose the\nmultiple instance generative model (MI-Gen) which can produce pathology reports\nfor gigapixel WSIs. We benchmark our model on the largest subset of\nTCGA-PathoText. Experimental results show our model can generate pathology\nreports which contain multiple clinical clues and achieve competitive\nperformance on certain slide-level tasks. We observe that simple semantic\nextraction from the pathology reports can achieve the best performance (0.838\nof F1 score) on BRCA subtyping surpassing previous state-of-the-art approaches.\nOur collected dataset and related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08192v1","updated":"2024-03-13T02:26:16Z","published":"2024-03-13T02:26:16Z","title":"MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular\n  Comprehension","summary":"  Large language models are playing an increasingly significant role in\nmolecular research, yet existing models often generate erroneous information,\nposing challenges to accurate molecular comprehension. Traditional evaluation\nmetrics for generated content fail to assess a model's accuracy in molecular\nunderstanding. To rectify the absence of factual evaluation, we present\nMoleculeQA, a novel question answering (QA) dataset which possesses 62K QA\npairs over 23K molecules. Each QA pair, composed of a manual question, a\npositive option and three negative options, has consistent semantics with a\nmolecular description from authoritative molecular corpus. MoleculeQA is not\nonly the first benchmark for molecular factual bias evaluation but also the\nlargest QA dataset for molecular research. A comprehensive evaluation on\nMoleculeQA for existing molecular LLMs exposes their deficiencies in specific\nareas and pinpoints several particularly crucial factors for molecular\nunderstanding.\n","authors":["Xingyu Lu","He Cao","Zijing Liu","Shengyuan Bai","Leqing Chen","Yuan Yao","Hai-Tao Zheng","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2403.08192v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.08189v1","updated":"2024-03-13T02:23:13Z","published":"2024-03-13T02:23:13Z","title":"Embedded Translations for Low-resource Automated Glossing","summary":"  We investigate automatic interlinear glossing in low-resource settings. We\naugment a hard-attentional neural model with embedded translation information\nextracted from interlinear glossed text. After encoding these translations\nusing large language models, specifically BERT and T5, we introduce a\ncharacter-level decoder for generating glossed output. Aided by these\nenhancements, our model demonstrates an average improvement of 3.97\\%-points\nover the previous state of the art on datasets from the SIGMORPHON 2023 Shared\nTask on Interlinear Glossing. In a simulated ultra low-resource setting,\ntrained on as few as 100 sentences, our system achieves an average 9.78\\%-point\nimprovement over the plain hard-attentional baseline. These results highlight\nthe critical role of translation information in boosting the system's\nperformance, especially in processing and interpreting modest data sources. Our\nfindings suggest a promising avenue for the documentation and preservation of\nlanguages, with our experiments on shared task datasets indicating significant\nadvancements over the existing state of the art.\n","authors":["Changbing Yang","Garrett Nicolai","Miikka Silfverberg"],"pdf_url":"https://arxiv.org/pdf/2403.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08187v1","updated":"2024-03-13T02:20:05Z","published":"2024-03-13T02:20:05Z","title":"Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of\n  Speech Sound Disorders in Korean children","summary":"  This study presents a model of automatic speech recognition (ASR) designed to\ndiagnose pronunciation issues in children with speech sound disorders (SSDs) to\nreplace manual transcriptions in clinical procedures. Since ASR models trained\nfor general purposes primarily predict input speech into real words, employing\na well-known high-performance ASR model for evaluating pronunciation in\nchildren with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to\nrecognize speech as pronounced rather than as existing words. The model was\nfine-tuned with a speech dataset from 137 children with inadequate speech\nproduction pronouncing 73 Korean words selected for actual clinical diagnosis.\nThe model's predictions of the pronunciations of the words matched the human\nannotations with about 90% accuracy. While the model still requires improvement\nin recognizing unclear pronunciation, this study demonstrates that ASR models\ncan streamline complex pronunciation error diagnostic procedures in clinical\nfields.\n","authors":["Taekyung Ahn","Yeonjung Hong","Younggon Im","Do Hyung Kim","Dayoung Kang","Joo Won Jeong","Jae Won Kim","Min Jung Kim","Ah-ra Cho","Dae-Hyun Jang","Hosung Nam"],"pdf_url":"https://arxiv.org/pdf/2403.08187v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.08174v1","updated":"2024-03-13T01:56:32Z","published":"2024-03-13T01:56:32Z","title":"Rethinking Loss Functions for Fact Verification","summary":"  We explore loss functions for fact verification in the FEVER shared task.\nWhile the cross-entropy loss is a standard objective for training verdict\npredictors, it fails to capture the heterogeneity among the FEVER verdict\nclasses. In this paper, we develop two task-specific objectives tailored to\nFEVER. Experimental results confirm that the proposed objective functions\noutperform the standard cross-entropy. Performance is further improved when\nthese objectives are combined with simple class weighting, which effectively\novercomes the imbalance in the training data. The souce code is available at\nhttps://github.com/yuta-mukobara/RLF-KGAT\n","authors":["Yuta Mukobara","Yutaro Shigeto","Masashi Shimbo"],"pdf_url":"https://arxiv.org/pdf/2403.08174v1.pdf","comment":"Accepted to EACL 2024 (short paper). The souce code is available at\n  https://github.com/yuta-mukobara/RLF-KGAT"},{"id":"http://arxiv.org/abs/2403.06199v2","updated":"2024-03-13T01:56:18Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08167v1","updated":"2024-03-13T01:38:42Z","published":"2024-03-13T01:38:42Z","title":"MolBind: Multimodal Alignment of Language, Molecules, and Proteins","summary":"  Recent advancements in biology and chemistry have leveraged multi-modal\nlearning, integrating molecules and their natural language descriptions to\nenhance drug discovery. However, current pre-training frameworks are limited to\ntwo modalities, and designing a unified network to process different modalities\n(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and\n3D proteins) remains challenging due to inherent gaps among them. In this work,\nwe propose MolBind, a framework that trains encoders for multiple modalities\nthrough contrastive learning, mapping all modalities to a shared feature space\nfor multi-modal semantic alignment. To facilitate effective pre-training of\nMolBind on multiple modalities, we also build and collect a high-quality\ndataset with four modalities, MolBind-M4, including graph-language,\nconformation-language, graph-conformation, and conformation-protein paired\ndata. MolBind shows superior zero-shot learning performance across a wide range\nof tasks, demonstrating its strong capability of capturing the underlying\nsemantics of multiple modalities.\n","authors":["Teng Xiao","Chao Cui","Huaisheng Zhu","Vasant G. Honavar"],"pdf_url":"https://arxiv.org/pdf/2403.08167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06487v2","updated":"2024-03-13T00:41:36Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v2.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2401.15127v2","updated":"2024-03-13T23:51:13Z","published":"2024-01-26T13:15:24Z","title":"Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness","summary":"  Knowledge sharing about emerging threats is crucial in the rapidly advancing\nfield of cybersecurity and forms the foundation of Cyber Threat Intelligence\n(CTI). In this context, Large Language Models are becoming increasingly\nsignificant in the field of cybersecurity, presenting a wide range of\nopportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,\nStanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary\nclassification and Named Entity Recognition (NER) tasks performed using Open\nSource INTelligence (OSINT). We utilize well-established data collected in\nprevious research from Twitter to assess the competitiveness of these chatbots\nwhen compared to specialized models trained for those tasks. In binary\nclassification experiments, Chatbot GPT-4 as a commercial model achieved an\nacceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1\nscore of 0.90. However, concerning cybersecurity entity recognition, all\nevaluated chatbots have limitations and are less effective. This study\ndemonstrates the capability of chatbots for OSINT binary classification and\nshows that they require further improvement in NER to effectively replace\nspecially trained models. Our results shed light on the limitations of the LLM\nchatbots when compared to specialized models, and can help researchers improve\nchatbots technology with the objective to reduce the required effort to\nintegrate machine learning in OSINT-based CTI tools.\n","authors":["Samaneh Shafee","Alysson Bessani","Pedro M. Ferreira"],"pdf_url":"https://arxiv.org/pdf/2401.15127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07311v2","updated":"2024-03-13T23:44:30Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v2.pdf","comment":"23 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.09862v2","updated":"2024-03-13T23:32:32Z","published":"2023-11-16T12:45:41Z","title":"Which Modality should I use -- Text, Motif, or Image? : Understanding\n  Graphs with Large Language Models","summary":"  Our research integrates graph data with Large Language Models (LLMs), which,\ndespite their advancements in various fields using large text corpora, face\nlimitations in encoding entire graphs due to context size constraints. This\npaper introduces a new approach to encoding a graph with diverse modalities,\nsuch as text, image, and motif, coupled with prompts to approximate a graph's\nglobal connectivity, thereby enhancing LLMs' efficiency in processing complex\ngraph structures. The study also presents GraphTMI, a novel benchmark for\nevaluating LLMs in graph structure analysis, focusing on homophily, motif\npresence, and graph difficulty. Key findings indicate that the image modality,\nespecially with vision-language models like GPT-4V, is superior to text in\nbalancing token limits and preserving essential information and outperforms\nprior graph neural net (GNN) encoders. Furthermore, the research assesses how\nvarious factors affect the performance of each encoding modality and outlines\nthe existing challenges and potential future developments for LLMs in graph\nunderstanding and reasoning tasks. All data will be publicly available upon\nacceptance.\n","authors":["Debarati Das","Ishaan Gupta","Jaideep Srivastava","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2311.09862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08994v1","updated":"2024-03-13T23:25:30Z","published":"2024-03-13T23:25:30Z","title":"Ethos: Rectifying Language Models in Orthogonal Parameter Space","summary":"  Language models (LMs) have greatly propelled the research on natural language\nprocessing. However, LMs also raise concerns regarding the generation of biased\nor toxic content and the potential disclosure of private information from the\ntraining dataset. In this work, we present a new efficient approach, Ethos,\nthat rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy\nleakage. Ethos is built on task arithmetic. However, unlike current task\narithmetic algorithms, Ethos distinguishes general beneficial and undesired\nknowledge when reconstructing task vectors. Specifically, Ethos first obtains a\nset of principal components from the pre-trained models using singular value\ndecomposition. Then, by projecting the task vector onto principal components,\nEthos identifies the principal components that encode general or undesired\nknowledge. Ethos performs negating using the task vector with undesired\nknowledge only, thereby minimizing collateral damage on general model utility.\nWe demonstrate the efficacy of our approach on three different tasks:\ndebiasing, detoxification, and memorization unlearning. Evaluations show Ethos\nis more effective in removing undesired knowledge and maintaining the overall\nmodel performance compared to current task arithmetic methods.\n","authors":["Lei Gao","Yue Niu","Tingting Tang","Salman Avestimehr","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2403.08994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04363v2","updated":"2024-03-13T22:48:14Z","published":"2023-10-06T16:36:08Z","title":"Amortizing intractable inference in large language models","summary":"  Autoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits\ntractable querying of this knowledge to start-to-end autoregressive sampling.\nHowever, many tasks of interest -- including sequence continuation, infilling,\nand other forms of constrained generation -- involve sampling from intractable\nposterior distributions. We address this limitation by using amortized Bayesian\ninference to sample from these intractable posteriors. Such amortization is\nalgorithmically achieved by fine-tuning LLMs via diversity-seeking\nreinforcement learning algorithms: generative flow networks (GFlowNets). We\nempirically demonstrate that this distribution-matching paradigm of LLM\nfine-tuning can serve as an effective alternative to maximum-likelihood\ntraining and reward-maximizing policy optimization. As an important\napplication, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient\nadaptation of LLMs to tasks that require multi-step rationalization and tool\nuse.\n","authors":["Edward J. Hu","Moksh Jain","Eric Elmoznino","Younesse Kaddar","Guillaume Lajoie","Yoshua Bengio","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2310.04363v2.pdf","comment":"ICLR 2024; 23 pages; code: https://github.com/GFNOrg/gfn-lm-tuning"},{"id":"http://arxiv.org/abs/2310.02226v2","updated":"2024-03-13T22:33:41Z","published":"2023-10-03T17:32:41Z","title":"Think before you speak: Training Language Models With Pause Tokens","summary":"  Language models generate responses by producing a series of tokens in\nimmediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$\nhidden vectors per layer, one vector per preceding token. What if instead we\nwere to let the model manipulate say, $K+10$ hidden vectors, before it outputs\nthe $(K+1)^{th}$ token? We operationalize this idea by performing training and\ninference on language models with a (learnable) $\\textit{pause}$ token, a\nsequence of which is appended to the input prefix. We then delay extracting the\nmodel's outputs until the last pause token is seen, thereby allowing the model\nto process extra computation before committing to an answer. We empirically\nevaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M\nparameters with causal pretraining on C4, and on downstream tasks covering\nreasoning, question-answering, general understanding and fact recall. Our main\nfinding is that inference-time delays show gains when the model is both\npre-trained and finetuned with delays. For the 1B model, we witness gains on 8\nof 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of\nSQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of\nGSM8k. Our work raises a range of conceptual and practical future research\nquestions on making delayed next-token prediction a widely applicable new\nparadigm.\n","authors":["Sachin Goyal","Ziwei Ji","Ankit Singh Rawat","Aditya Krishna Menon","Sanjiv Kumar","Vaishnavh Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2310.02226v2.pdf","comment":"Published at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08978v1","updated":"2024-03-13T22:06:03Z","published":"2024-03-13T22:06:03Z","title":"AutoGuide: Automated Generation and Selection of State-Aware Guidelines\n  for Large Language Model Agents","summary":"  The primary limitation of large language models (LLMs) is their restricted\nunderstanding of the world. This poses significant difficulties for LLM-based\nagents, particularly in domains where pre-trained LLMs lack sufficient\nknowledge. In this paper, we introduce a novel framework, called AutoGuide,\nthat bridges the knowledge gap in pre-trained LLMs by leveraging implicit\nknowledge in offline experiences. Specifically, AutoGuide effectively extracts\nknowledge embedded in offline data by extracting a set of state-aware\nguidelines. Importantly, each state-aware guideline is expressed in concise\nnatural language and follows a conditional structure, clearly describing the\nstate where it is applicable. As such, the resulting guidelines enable a\nprincipled way to provide helpful knowledge pertinent to an agent's current\ndecision-making process. We show that our approach outperforms competitive\nLLM-based baselines by a large margin in sequential decision-making benchmarks.\n","authors":["Yao Fu","Dong-Ki Kim","Jaekyeom Kim","Sungryull Sohn","Lajanugen Logeswaran","Kyunghoon Bae","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2403.08978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18235v4","updated":"2024-03-13T21:58:59Z","published":"2023-10-27T16:20:10Z","title":"Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-to-Image Generation","summary":"  Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand VQA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics, which is adaptable to any QG/A frameworks. DSG produces\natomic and unique questions organized in dependency graphs, which (i) ensure\nappropriate semantic coverage and (ii) sidestep inconsistent answers. With\nextensive experimentation and human evaluation on a range of model\nconfigurations (LLM, VQA, and T2I), we empirically demonstrate that DSG\naddresses the challenges noted above. Finally, we present DSG-1k, an\nopen-sourced evaluation benchmark that includes 1,060 prompts, covering a wide\nrange of fine-grained semantic categories with a balanced distribution. We\nrelease the DSG-1k prompts and the corresponding DSG questions.\n","authors":["Jaemin Cho","Yushi Hu","Roopal Garg","Peter Anderson","Ranjay Krishna","Jason Baldridge","Mohit Bansal","Jordi Pont-Tuset","Su Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18235v4.pdf","comment":"ICLR 2024; Project website: https://google.github.io/dsg"},{"id":"http://arxiv.org/abs/2403.01193v2","updated":"2024-03-13T21:57:19Z","published":"2024-03-02T12:19:04Z","title":"RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots","summary":"  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.\n","authors":["Philip Feldman. James R. Foulds","Shimei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.01193v2.pdf","comment":"7 Pages, 1 Figure, 1 Table"},{"id":"http://arxiv.org/abs/2403.08946v1","updated":"2024-03-13T20:25:27Z","published":"2024-03-13T20:25:27Z","title":"Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM\n  Era","summary":"  Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the \"black\nbox\" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.\n","authors":["Xuansheng Wu","Haiyan Zhao","Yaochen Zhu","Yucheng Shi","Fan Yang","Tianming Liu","Xiaoming Zhai","Wenlin Yao","Jundong Li","Mengnan Du","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08946v1.pdf","comment":"38 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.08943v1","updated":"2024-03-13T20:19:30Z","published":"2024-03-13T20:19:30Z","title":"LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots","summary":"  Since the breakthrough of ChatGPT, large language models (LLMs) have garnered\nsignificant attention in the research community. With the development of LLMs,\nthe question of text style transfer for conversational models has emerged as a\nnatural extension, where chatbots may possess their own styles or even\ncharacters. However, standard evaluation metrics have not yet been established\nfor this new settings. This paper aims to address this issue by proposing the\nLMStyle Benchmark, a novel evaluation framework applicable to chat-style text\nstyle transfer (C-TST), that can measure the quality of style transfer for LLMs\nin an automated and scalable manner. In addition to conventional style strength\nmetrics, LMStyle Benchmark further considers a novel aspect of metrics called\nappropriateness, a high-level metrics take account of coherence, fluency and\nother implicit factors without the aid of reference samples. Our experiments\ndemonstrate that the new evaluation methods introduced by LMStyle Benchmark\nhave a higher correlation with human judgments in terms of appropriateness.\nBased on LMStyle Benchmark, we present a comprehensive list of evaluation\nresults for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their\nstylistic properties, such as formality and sentiment strength, along with\ntheir appropriateness.\n","authors":["Jianlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13492v2","updated":"2024-03-13T20:09:46Z","published":"2024-02-21T03:05:50Z","title":"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval\n  Augmentation to Language Models","summary":"  While large language models (LMs) demonstrate remarkable performance, they\nencounter challenges in providing accurate responses when queried for\ninformation beyond their pre-trained memorization. Although augmenting them\nwith relevant external information can mitigate these issues, failure to\nconsider the necessity of retrieval may adversely affect overall performance.\nPrevious research has primarily focused on examining how entities influence\nretrieval models and knowledge recall in LMs, leaving other aspects relatively\nunexplored. In this work, our goal is to offer a more detailed, fact-centric\nanalysis by exploring the effects of combinations of entities and relations. To\nfacilitate this, we construct a new question answering (QA) dataset called\nWiTQA (Wikipedia Triple Question Answers). This dataset includes questions\nabout entities and relations of various popularity levels, each accompanied by\na supporting passage. Our extensive experiments with diverse LMs and retrievers\nreveal when retrieval does not consistently enhance LMs from the viewpoints of\nfact-centric popularity.Confirming earlier findings, we observe that larger LMs\nexcel in recalling popular facts. However, they notably encounter difficulty\nwith infrequent entity-relation pairs compared to retrievers. Interestingly,\nthey can effectively retain popular relations of less common entities. We\ndemonstrate the efficacy of our finer-grained metric and insights through an\nadaptive retrieval system that selectively employs retrieval and recall based\non the frequencies of entities and relations in the question.\n","authors":["Seiji Maekawa","Hayate Iso","Sairam Gurajada","Nikita Bhutani"],"pdf_url":"https://arxiv.org/pdf/2402.13492v2.pdf","comment":"NAACL2024 (main)"},{"id":"http://arxiv.org/abs/2403.08904v1","updated":"2024-03-13T18:47:00Z","published":"2024-03-13T18:47:00Z","title":"Detecting Hallucination and Coverage Errors in Retrieval Augmented\n  Generation for Controversial Topics","summary":"  We explore a strategy to handle controversial topics in LLM-based chatbots\nbased on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the\nabsence of a single true answer and surface multiple perspectives. We frame\nthis as retrieval augmented generation, where perspectives are retrieved from a\nknowledge base and the LLM is tasked with generating a fluent and faithful\nresponse from the given perspectives. As a starting point, we use a\ndeterministic retrieval system and then focus on common LLM failure modes that\narise during this approach to text generation, namely hallucination and\ncoverage errors. We propose and evaluate three methods to detect such errors\nbased on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our\nresults demonstrate that LLM-based classifiers, even when trained only on\nsynthetic errors, achieve high error detection performance, with ROC AUC scores\nof 95.3% for hallucination and 90.5% for coverage error detection on\nunambiguous error cases. We show that when no training data is available, our\nother methods still yield good results on hallucination (84.0%) and coverage\nerror (85.2%) detection.\n","authors":["Tyler A. Chang","Katrin Tomanek","Jessica Hoffmann","Nithum Thain","Erin van Liemt","Kathleen Meier-Hellstern","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2403.08904v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08890v1","updated":"2024-03-13T18:20:24Z","published":"2024-03-13T18:20:24Z","title":"From \"um\" to \"yeah\": Producing, predicting, and regulating information\n  flow in human conversation","summary":"  Conversation demands attention. Speakers must call words to mind, listeners\nmust make sense of them, and both together must negotiate this flow of\ninformation, all in fractions of a second. We used large language models to\nstudy how this works in a large-scale dataset of English-language conversation,\nthe CANDOR corpus. We provide a new estimate of the information density of\nunstructured conversation, of approximately 13 bits/second, and find\nsignificant effects associated with the cognitive load of both retrieving, and\npresenting, that information. We also reveal a role for backchannels -- the\nbrief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the\nproduction of novelty: the lead-up to a backchannel is associated with\ndeclining information rate, while speech downstream rebounds to previous rates.\nOur results provide new insights into long-standing theories of how we respond\nto fluctuating demands on cognitive resources, and how we negotiate those\ndemands in partnership with others.\n","authors":["Claire Augusta Bergey","Simon DeDeo"],"pdf_url":"https://arxiv.org/pdf/2403.08890v1.pdf","comment":"18 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.08851v1","updated":"2024-03-13T18:00:00Z","published":"2024-03-13T18:00:00Z","title":"PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models","summary":"  We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.\n","authors":["Siddharth Mishra-Sharma","Yiding Song","Jesse Thaler"],"pdf_url":"https://arxiv.org/pdf/2403.08851v1.pdf","comment":"17+6 pages, 3+1 figures, 5+2 tables"}]},"2024-03-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.09636v1","updated":"2024-03-14T17:59:26Z","published":"2024-03-14T17:59:26Z","title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","summary":"  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for on-line key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression rates in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to ~3.7x throughput increase in auto-regressive inference on a\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. We find\nthat DMC preserves the original downstream performance with up to 4x cache\ncompression, outperforming up-trained grouped-query attention (GQA). GQA and\nDMC can be even combined to obtain compounded gains. As a result DMC fits\nlonger contexts and larger batches within any given memory budget.\n","authors":["Piotr Nawrot","Adrian Łańcucki","Marcin Chochowski","David Tarjan","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2403.09636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09635v1","updated":"2024-03-14T17:59:14Z","published":"2024-03-14T17:59:14Z","title":"Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models","summary":"  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n","authors":["Akhil Kedia","Mohd Abbas Zaidi","Sushil Khyalia","Jungho Jung","Harshith Goka","Haejun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09635v1.pdf","comment":"Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.\n  Source code is available at\n  https://github.com/akhilkedia/TranformersGetStable"},{"id":"http://arxiv.org/abs/2403.09631v1","updated":"2024-03-14T17:58:41Z","published":"2024-03-14T17:58:41Z","title":"3D-VLA: A 3D Vision-Language-Action Generative World Model","summary":"  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n","authors":["Haoyu Zhen","Xiaowen Qiu","Peihao Chen","Jincheng Yang","Xin Yan","Yilun Du","Yining Hong","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09631v1.pdf","comment":"Project page: https://vis-www.cs.umass.edu/3dvla/"},{"id":"http://arxiv.org/abs/2403.09629v1","updated":"2024-03-14T17:58:16Z","published":"2024-03-14T17:58:16Z","title":"Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking","summary":"  When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.\n","authors":["Eric Zelikman","Georges Harik","Yijia Shao","Varuna Jayasiri","Nick Haber","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09613v1","updated":"2024-03-14T17:51:54Z","published":"2024-03-14T17:51:54Z","title":"Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training","summary":"  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.\n","authors":["Yanlai Yang","Matt Jones","Michael C. Mozer","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09613v1.pdf","comment":"19 pages, 18 figures"},{"id":"http://arxiv.org/abs/2403.09611v1","updated":"2024-03-14T17:51:32Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09606v1","updated":"2024-03-14T17:47:20Z","published":"2024-03-14T17:47:20Z","title":"Large Language Models and Causal Inference in Collaboration: A\n  Comprehensive Survey","summary":"  Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.\n","authors":["Xiaoyu Liu","Paiheng Xu","Junda Wu","Jiaxin Yuan","Yifan Yang","Yuhang Zhou","Fuxiao Liu","Tianrui Guan","Haoliang Wang","Tong Yu","Julian McAuley","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2403.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07769v2","updated":"2024-03-14T17:16:18Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v2","updated":"2024-03-14T16:57:37Z","published":"2024-03-12T17:55:38Z","title":"Exploring Safety Generalization Challenges of Large Language Models via\n  Code","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Yu Qiao","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09559v1","updated":"2024-03-14T16:47:25Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01308v2","updated":"2024-03-14T16:37:37Z","published":"2024-03-02T20:40:11Z","title":"VBART: The Turkish LLM","summary":"  We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.\n","authors":["Meliksah Turker","Mehmet Erdi Ari","Aydin Han"],"pdf_url":"https://arxiv.org/pdf/2403.01308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09539v1","updated":"2024-03-14T16:27:49Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12749v3","updated":"2024-03-14T16:13:36Z","published":"2024-02-20T06:37:31Z","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","summary":"  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\ngreat promise in many AI applications. However, their performance on medical\ntasks is suboptimal and can be improved by training on extensive\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\nthat includes foundation models - Me LLaMA 13/70B, along with their\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\ndomain-specific data suite for training and evaluation includes a large-scale,\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\nLLaMA models achieve overall better performance than existing open-source\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\nwith a slight variance of within 3%, and yet falls short when compared to\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\nour results show that Me LLaMA models outperform other open-source medical LLMs\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\nfoundation LLMs that use both biomedical and clinical data. It exhibits\nsuperior performance across both general and medical tasks compared to other\nopen-source medical LLMs, rendering it an attractive choice for medical AI\napplications. We release our models, datasets, and evaluation scripts at:\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\n","authors":["Qianqian Xie","Qingyu Chen","Aokun Chen","Cheng Peng","Yan Hu","Fongci Lin","Xueqing Peng","Jimin Huang","Jeffrey Zhang","Vipina Keloth","Xingyu Zhou","Huan He","Lucila Ohno-Machado","Yonghui Wu","Hua Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2402.12749v3.pdf","comment":"21 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.09530v1","updated":"2024-03-14T16:13:00Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v1.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.09522v1","updated":"2024-03-14T16:07:39Z","published":"2024-03-14T16:07:39Z","title":"MT-PATCHER: Selective and Extendable Knowledge Distillation from Large\n  Language Models for Machine Translation","summary":"  Large Language Models (LLM) have demonstrated their strong ability in the\nfield of machine translation (MT), yet they suffer from high computational cost\nand latency. Therefore, transferring translation knowledge from giant LLMs to\nmedium-sized machine translation models is a promising research direction.\nHowever, traditional knowledge distillation methods do not take the capability\nof student and teacher models into consideration, therefore repeatedly teaching\nstudent models on the knowledge they have learned, and failing to extend to\nnovel contexts and knowledge. In this paper, we propose a framework called\nMT-Patcher, which transfers knowledge from LLMs to existing MT models in a\nselective, comprehensive and proactive manner. Considering the current\ntranslation ability of student MT models, we only identify and correct their\ntranslation errors, instead of distilling the whole translation from the\nteacher. Leveraging the strong language abilities of LLMs, we instruct LLM\nteachers to synthesize diverse contexts and anticipate more potential errors\nfor the student. Experiment results on translating both specific language\nphenomena and general MT benchmarks demonstrate that finetuning the student MT\nmodel on about 10% examples can achieve comparable results to the traditional\nknowledge distillation method, and synthesized potential errors and diverse\ncontexts further improve translation performances on unseen contexts and words.\n","authors":["Jiahuan Li","Shanbo Cheng","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.09522v1.pdf","comment":"Accepted to NAACL-2024"},{"id":"http://arxiv.org/abs/2403.09516v1","updated":"2024-03-14T15:58:36Z","published":"2024-03-14T15:58:36Z","title":"Leveraging Prototypical Representations for Mitigating Social Bias\n  without Demographic Information","summary":"  Mitigating social biases typically requires identifying the social groups\nassociated with each data sample. In this paper, we present DAFair, a novel\napproach to address social bias in language models. Unlike traditional methods\nthat rely on explicit demographic labels, our approach does not require any\nsuch information. Instead, we leverage predefined prototypical demographic\ntexts and incorporate a regularization term during the fine-tuning process to\nmitigate bias in the model's representations. Our empirical results across two\ntasks and two models demonstrate the effectiveness of our method compared to\nprevious approaches that do not rely on labeled data. Moreover, with limited\ndemographic-annotated data, our approach outperforms common debiasing\napproaches.\n","authors":["Shadi Iskander","Kira Radinsky","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.09516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08664v2","updated":"2024-03-14T15:57:59Z","published":"2024-03-13T16:17:09Z","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records","summary":"  The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.\n","authors":["Erlend Frayling","Jake Lever","Graham McDonald"],"pdf_url":"https://arxiv.org/pdf/2403.08664v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.09498v1","updated":"2024-03-14T15:40:13Z","published":"2024-03-14T15:40:13Z","title":"From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\n  Fake News","summary":"  In the digital era, the rapid propagation of fake news and rumors via social\nnetworks brings notable societal challenges and impacts public opinion\nregulation. Traditional fake news modeling typically forecasts the general\npopularity trends of different groups or numerically represents opinions shift.\nHowever, these methods often oversimplify real-world complexities and overlook\nthe rich semantic information of news text. The advent of large language models\n(LLMs) provides the possibility of modeling subtle dynamics of opinion.\nConsequently, in this work, we introduce a Fake news Propagation Simulation\nframework (FPS) based on LLM, which studies the trends and control of fake news\npropagation in detail. Specifically, each agent in the simulation represents an\nindividual with a distinct personality. They are equipped with both short-term\nand long-term memory, as well as a reflective mechanism to mimic human-like\nthinking. Every day, they engage in random opinion exchanges, reflect on their\nthinking, and update their opinions. Our simulation results uncover patterns in\nfake news propagation related to topic relevance, and individual traits,\naligning with real-world observations. Additionally, we evaluate various\nintervention strategies and demonstrate that early and appropriately frequent\ninterventions strike a balance between governance cost and effectiveness,\noffering valuable insights for practical applications. Our study underscores\nthe significant utility and potential of LLMs in combating fake news.\n","authors":["Yuhan Liu","Xiuying Chen","Xiaoqing Zhang","Xing Gao","Ji Zhang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2403.09498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11093v2","updated":"2024-03-14T15:36:17Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.01980v2","updated":"2024-03-14T15:30:41Z","published":"2024-02-03T01:33:16Z","title":"SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks","summary":"  Social science NLP tasks, such as emotion or humor detection, are required to\ncapture the semantics along with the implicit pragmatics from text, often with\nlimited amounts of training data. Instruction tuning has been shown to improve\nthe many capabilities of large language models (LLMs) such as commonsense\nreasoning, reading comprehension, and computer programming. However, little is\nknown about the effectiveness of instruction tuning on the social domain where\nimplicit pragmatic cues are often needed to be captured. We explore the use of\ninstruction tuning for social science NLP tasks and introduce Socialite-Llama\n-- an open-source, instruction-tuned Llama. On a suite of 20 social science\ntasks, Socialite-Llama improves upon the performance of Llama as well as\nmatches or improves upon the performance of a state-of-the-art, multi-task\nfinetuned model on a majority of them. Further, Socialite-Llama also leads to\nimprovement on 5 out of 6 related social tasks as compared to Llama, suggesting\ninstruction tuning can lead to generalized social understanding. All resources\nincluding our code, model and dataset can be found through\nbit.ly/socialitellama.\n","authors":["Gourab Dey","Adithya V Ganesan","Yash Kumar Lal","Manal Shah","Shreyashee Sinha","Matthew Matero","Salvatore Giorgi","Vivek Kulkarni","H. Andrew Schwartz"],"pdf_url":"https://arxiv.org/pdf/2402.01980v2.pdf","comment":"Short paper accepted to EACL 2024. 4 pgs, 2 tables"},{"id":"http://arxiv.org/abs/2403.09490v1","updated":"2024-03-14T15:30:25Z","published":"2024-03-14T15:30:25Z","title":"Hyper-CL: Conditioning Sentence Representations with Hypernetworks","summary":"  While the introduction of contrastive learning frameworks in sentence\nrepresentation learning has significantly contributed to advancements in the\nfield, it still remains unclear whether state-of-the-art sentence embeddings\ncan capture the fine-grained semantics of sentences, particularly when\nconditioned on specific perspectives. In this paper, we introduce Hyper-CL, an\nefficient methodology that integrates hypernetworks with contrastive learning\nto compute conditioned sentence representations. In our proposed approach, the\nhypernetwork is responsible for transforming pre-computed condition embeddings\ninto corresponding projection layers. This enables the same sentence embeddings\nto be projected differently according to various conditions. Evaluation on two\nrepresentative conditioning benchmarks, namely conditional semantic text\nsimilarity and knowledge graph completion, demonstrates that Hyper-CL is\neffective in flexibly conditioning sentence representations, showcasing its\ncomputational efficiency at the same time. We also provide a comprehensive\nanalysis of the inner workings of our approach, leading to a better\ninterpretation of its mechanisms.\n","authors":["Young Hyun Yoo","Jii Cha","Changhyeon Kim","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.09490v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.09488v1","updated":"2024-03-14T15:30:14Z","published":"2024-03-14T15:30:14Z","title":"Rectifying Demonstration Shortcut in In-Context Learning","summary":"  Large language models (LLMs) are able to solve various tasks with only a few\ndemonstrations utilizing their in-context learning (ICL) abilities. However,\nLLMs often rely on their pre-trained semantic priors of demonstrations rather\nthan on the input-label relationships to proceed with ICL prediction. In this\nwork, we term this phenomenon as the `Demonstration Shortcut'. While previous\nworks have primarily focused on improving ICL prediction results for predefined\ntasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM\nto effectively learn new input-label relationships from demonstrations. To\nachieve this, we introduce In-Context Calibration, a demonstration-aware\ncalibration method. We evaluate the effectiveness of the proposed method in two\nsettings: (1) the Original ICL Task using the standard label space and (2) the\nTask Learning setting, where the label space is replaced with semantically\nunrelated tokens. In both settings, In-Context Calibration demonstrates\nsubstantial improvements, with results generalized across three LLM families\n(OPT, GPT, and Llama2) under various configurations.\n","authors":["Joonwon Jang","Sanghwan Jang","Wonbin Kweon","Minjin Jeon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09472v1","updated":"2024-03-14T15:12:38Z","published":"2024-03-14T15:12:38Z","title":"Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision","summary":"  Current AI alignment methodologies rely on human-provided demonstrations or\njudgments, and the learned capabilities of AI systems would be upper-bounded by\nhuman capabilities as a result. This raises a challenging research question:\nHow can we keep improving the systems when their capabilities have surpassed\nthe levels of humans? This paper answers this question in the context of\ntackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from\nhuman annotations on easier tasks (e.g., level 1-3 MATH problems), which we\nterm as \\textit{easy-to-hard generalization}. Our key insight is that an\nevaluator (reward model) trained on supervisions for easier tasks can be\neffectively used for scoring candidate solutions of harder tasks and hence\nfacilitating easy-to-hard generalization over different levels of tasks. Based\non this insight, we propose a novel approach to scalable alignment, which\nfirstly trains the process-supervised reward models on easy problems (e.g.,\nlevel 1-3), and then uses them to evaluate the performance of policy models on\nhard problems. We show that such \\textit{easy-to-hard generalization from\nevaluators} can enable \\textit{easy-to-hard generalizations in generators}\neither through re-ranking or reinforcement learning (RL). Notably, our\nprocess-supervised 7b RL model achieves an accuracy of 34.0\\% on MATH500,\ndespite only using human supervision on easy problems. Our approach suggests a\npromising path toward AI systems that advance beyond the frontier of human\nsupervision.\n","authors":["Zhiqing Sun","Longhui Yu","Yikang Shen","Weiyang Liu","Yiming Yang","Sean Welleck","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2403.09472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13718v5","updated":"2024-03-14T15:05:08Z","published":"2023-05-23T06:13:10Z","title":"Exploring Self-supervised Logic-enhanced Training for Large Language\n  Models","summary":"  Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.\n","authors":["Fangkai Jiao","Zhiyang Teng","Bosheng Ding","Zhengyuan Liu","Nancy F. Chen","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2305.13718v5.pdf","comment":"16 pages, NAACL 2024"},{"id":"http://arxiv.org/abs/2210.17437v3","updated":"2024-03-14T14:55:48Z","published":"2022-10-31T16:06:48Z","title":"Learning New Tasks from a Few Examples with Soft-Label Prototypes","summary":"  Existing approaches to few-shot learning in NLP rely on large language models\nand fine-tuning of these to generalise on out-of-distribution data. In this\nwork, we propose a simple yet powerful approach to \"extreme\" few-shot learning,\nwherein models are exposed to as little as 4 examples per class, based on\nsoft-label prototypes that collectively capture the distribution of different\nclasses across the input domain space. Inspired by previous work (Sucholutsky\net al., 2021) on univariate or simple multivariate (synthetic) data, we propose\na novel approach that is effective on large, high-dimensional and real-world\ndatasets. We learn soft-label prototypes within a neural framework (DeepSLP)\nand we experimentally demonstrate that it achieves superior performance on\n31/48 tested tasks and few-shot settings while closely matching the performance\nof strong baselines on the rest. We focus on learning previously unseen NLP\ntasks from very few examples (4, 8, 16) per label and present an in-depth\nanalysis of the effectiveness of our approach.\n","authors":["Avyav Kumar Singh","Ekaterina Shutova","Helen Yannakoudakis"],"pdf_url":"https://arxiv.org/pdf/2210.17437v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19791v3","updated":"2024-03-14T14:54:54Z","published":"2023-10-30T17:55:02Z","title":"LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code","summary":"  While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.\n","authors":["Gabriel Grand","Lionel Wong","Matthew Bowers","Theo X. Olausson","Muxin Liu","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.19791v3.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2306.10322v3","updated":"2024-03-14T14:33:51Z","published":"2023-06-17T11:44:04Z","title":"CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot\n  Vision-and-Language Navigation","summary":"  Understanding and following natural language instructions while navigating\nthrough complex, real-world environments poses a significant challenge for\ngeneral-purpose robots. These environments often include obstacles and\npedestrians, making it essential for autonomous agents to possess the\ncapability of self-corrected planning to adjust their actions based on feedback\nfrom the surroundings. However, the majority of existing vision-and-language\nnavigation (VLN) methods primarily operate in less realistic simulator settings\nand do not incorporate environmental feedback into their decision-making\nprocesses. To address this gap, we introduce a novel zero-shot framework called\nCorNav, utilizing a large language model for decision-making and comprising two\nkey components: 1) incorporating environmental feedback for refining future\nplans and adjusting its actions, and 2) multiple domain experts for parsing\ninstructions, scene understanding, and refining predicted actions. In addition\nto the framework, we develop a 3D simulator that renders realistic scenarios\nusing Unreal Engine 5. To evaluate the effectiveness and generalization of\nnavigation agents in a zero-shot multi-task setting, we create a benchmark\ncalled NavBench. Extensive experiments demonstrate that CorNav consistently\noutperforms all baselines by a significant margin across all tasks. On average,\nCorNav achieves a success rate of 28.1\\%, surpassing the best baseline's\nperformance of 20.5\\%.\n","authors":["Xiwen Liang","Liang Ma","Shanshan Guo","Jianhua Han","Hang Xu","Shikui Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.10322v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.00795v2","updated":"2024-03-14T14:25:13Z","published":"2024-02-23T05:31:36Z","title":"Executing Natural Language-Described Algorithms with Large Language\n  Models: An Investigation","summary":"  Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.\n","authors":["Xin Zheng","Qiming Zhu","Hongyu Lin","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.00795v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09409v1","updated":"2024-03-14T14:01:26Z","published":"2024-03-14T14:01:26Z","title":"\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS\n  Students using Large Language Models","summary":"  Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.\n","authors":["Seth Bernstein","Paul Denny","Juho Leinonen","Lauren Kan","Arto Hellas","Matt Littlefield Sami Sarsa","Stephen MacNeil"],"pdf_url":"https://arxiv.org/pdf/2403.09409v1.pdf","comment":"7 pages, 2 figures, ITiCSE 2024 preprint"},{"id":"http://arxiv.org/abs/2201.12191v5","updated":"2024-03-14T13:46:29Z","published":"2022-01-28T15:45:13Z","title":"Kernelized Concept Erasure","summary":"  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n","authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12191v5.pdf","comment":"Accepted as a long paper in EMNLP22"},{"id":"http://arxiv.org/abs/2210.10012v4","updated":"2024-03-14T13:39:35Z","published":"2022-10-18T17:30:02Z","title":"Log-linear Guardedness and its Implications","summary":"  Methods for erasing human-interpretable concepts from neural representations\nthat assume linearity have been found to be tractable and useful. However, the\nimpact of this removal on the behavior of downstream classifiers trained on the\nmodified representations is not fully understood. In this work, we formally\ndefine the notion of log-linear guardedness as the inability of an adversary to\npredict the concept directly from the representation, and study its\nimplications. We show that, in the binary case, under certain assumptions, a\ndownstream log-linear model cannot recover the erased concept. However, we\ndemonstrate that a multiclass log-linear model \\emph{can} be constructed that\nindirectly recovers the concept in some cases, pointing to the inherent\nlimitations of log-linear guardedness as a downstream bias mitigation\ntechnique. These findings shed light on the theoretical limitations of linear\nerasure methods and highlight the need for further research on the connections\nbetween intrinsic and extrinsic bias in neural models.\n","authors":["Shauli Ravfogel","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2210.10012v4.pdf","comment":"Accepted as a long paper in ACL 2023"},{"id":"http://arxiv.org/abs/2403.04369v2","updated":"2024-03-14T13:25:48Z","published":"2024-03-07T09:57:42Z","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge\n  Prediction","summary":"  Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.\n","authors":["Ang Li","Qiangchao Chen","Yiquan Wu","Ming Cai","Xiang Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.04369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09362v1","updated":"2024-03-14T13:12:21Z","published":"2024-03-14T13:12:21Z","title":"Komodo: A Linguistic Expedition into Indonesia's Regional Languages","summary":"  The recent breakthroughs in Large Language Models (LLMs) have mostly focused\non languages with easily available and sufficient resources, such as English.\nHowever, there remains a significant gap for languages that lack sufficient\nlinguistic resources in the public domain. Our work introduces Komodo-7B,\n7-billion-parameter Large Language Models designed to address this gap by\nseamlessly operating across Indonesian, English, and 11 regional languages in\nIndonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and\nKomodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art\nperformance in various tasks and languages, outperforming the benchmarks set by\nOpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,\nMixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only\ndemonstrates superior performance in both language-specific and overall\nassessments but also highlights its capability to excel in linguistic\ndiversity. Our commitment to advancing language models extends beyond\nwell-resourced languages, aiming to bridge the gap for those with limited\nlinguistic assets. Additionally, Komodo-7B-Instruct's better cross-language\nunderstanding contributes to addressing educational disparities in Indonesia,\noffering direct translations from English to 11 regional languages, a\nsignificant improvement compared to existing language translation services.\nKomodo-7B represents a crucial step towards inclusivity and effectiveness in\nlanguage models, providing to the linguistic needs of diverse communities.\n","authors":["Louis Owen","Vishesh Tripathi","Abhay Kumar","Biddwan Ahmed"],"pdf_url":"https://arxiv.org/pdf/2403.09362v1.pdf","comment":"30 Pages, 8 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2305.15083v3","updated":"2024-03-14T13:04:49Z","published":"2023-05-24T12:00:24Z","title":"Eliciting the Translation Ability of Large Language Models via\n  Multilingual Finetuning with Translation Instructions","summary":"  Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that multilingual LLMs have stronger translation\nabilities than previously demonstrated. For a certain language, the performance\ndepends on its similarity to English and the amount of data used in the\npretraining phase. Secondly, we find that LLMs' ability to carry out\ntranslation instructions relies on the understanding of translation\ninstructions and the alignment among different languages. With multilingual\nfinetuning, LLMs could learn to perform the translation task well even for\nthose language pairs unseen during the instruction tuning phase.\n","authors":["Jiahuan Li","Hao Zhou","Shujian Huang","Shanbo Cheng","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2305.15083v3.pdf","comment":"accepted by Transaction of ACL, pre-MIT version"},{"id":"http://arxiv.org/abs/2310.05116v3","updated":"2024-03-14T12:39:14Z","published":"2023-10-08T11:09:16Z","title":"Utilizing Contextual Clues and Role Correlations for Enhancing\n  Document-level Event Argument Extraction","summary":"  Document-level event argument extraction is a crucial yet challenging task\nwithin the field of information extraction. Current mainstream approaches\nprimarily focus on the information interaction between event triggers and their\narguments, facing two limitations: insufficient context interaction and the\nignorance of event correlations. Here, we introduce a novel framework named\nCARLG (Contextual Aggregation of clues and Role-based Latent Guidance),\ncomprising two innovative components: the Contextual Clues Aggregation (CCA)\nand the Role-based Latent Information Guidance (RLIG). The CCA module leverages\nthe attention weights derived from a pre-trained encoder to adaptively\nassimilates broader contextual information, while the RLIG module aims to\ncapture the semantic correlations among event roles. We then instantiate the\nCARLG framework into two variants based on two types of current mainstream EAE\napproaches. Notably, our CARLG framework introduces less than 1% new parameters\nyet significantly improving the performance. Comprehensive experiments across\nthe RAMS, WikiEvents, and MLEE datasets confirm the superiority of CARLG,\nshowing significant superiority in terms of both performance and inference\nspeed compared to major benchmarks. Further analyses demonstrate the\neffectiveness of the proposed modules.\n","authors":["Wanlong Liu","Dingyi Zeng","Li Zhou","Yichen Xiao","Weishan Kong","Malu Zhang","Shaohuan Cheng","Hongyang Zhao","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05116v3.pdf","comment":"pre-submission"},{"id":"http://arxiv.org/abs/2403.03750v2","updated":"2024-03-14T12:30:54Z","published":"2024-03-06T14:37:30Z","title":"German also Hallucinates! Inconsistency Detection in News Summaries with\n  the Absinth Dataset","summary":"  The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.\n","authors":["Laura Mascarell","Ribin Chalumattu","Annette Rios"],"pdf_url":"https://arxiv.org/pdf/2403.03750v2.pdf","comment":"11 pages, 2 figures, 7 tables, conference: Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024), Turin, Italy, May 20-25, 2024"},{"id":"http://arxiv.org/abs/2402.18603v4","updated":"2024-03-14T12:10:43Z","published":"2024-02-28T08:29:42Z","title":"MMSR: Symbolic Regression is a Multimodal Task","summary":"  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n","authors":["Yanjie Li","Jingyi Liu","Weijun Li","Lina Yu","Min Wu","Wenqiang Li","Meilan Hao","Su Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2402.18603v4.pdf","comment":"12 page"},{"id":"http://arxiv.org/abs/2403.09298v1","updated":"2024-03-14T11:37:02Z","published":"2024-03-14T11:37:02Z","title":"More than words: Advancements and challenges in speech recognition for\n  singing","summary":"  This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2403.09298v1.pdf","comment":"Conference on Electronic Speech Signal Processing (ESSV) 2024,\n  Keynote"},{"id":"http://arxiv.org/abs/2403.00774v2","updated":"2024-03-14T11:34:28Z","published":"2024-02-14T02:33:17Z","title":"Regional inflation analysis using social network data","summary":"  Inflation is one of the most important macroeconomic indicators that have a\ngreat impact on the population of any country and region. Inflation is\ninfluenced by range of factors, one of which is inflation expectations. Many\ncentral banks take this factor into consideration while implementing monetary\npolicy within the inflation targeting regime. Nowadays, a lot of people are\nactive users of the Internet, especially social networks. There is a hypothesis\nthat people search, read, and discuss mainly only those issues that are of\nparticular interest to them. It is logical to assume that the dynamics of\nprices may also be in the focus of user discussions. So, such discussions could\nbe regarded as an alternative source of more rapid information about inflation\nexpectations. This study is based on unstructured data from Vkontakte social\nnetwork to analyze upward and downward inflationary trends (on the example of\nthe Omsk region). The sample of more than 8.5 million posts was collected\nbetween January 2010 and May 2022. The authors used BERT neural networks to\nsolve the problem. These models demonstrated better results than the benchmarks\n(e.g., logistic regression, decision tree classifier, etc.). It makes possible\nto define pro-inflationary and disinflationary types of keywords in different\ncontexts and get their visualization with SHAP method. This analysis provides\nadditional operational information about inflationary processes at the regional\nlevel The proposed approach can be scaled for other regions. At the same time\nthe limitation of the work is the time and power costs for the initial training\nof similar models for all regions of Russia.\n","authors":["Vasilii Chsherbakov","Ilia Karpov"],"pdf_url":"https://arxiv.org/pdf/2403.00774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09294v1","updated":"2024-03-14T11:29:47Z","published":"2024-03-14T11:29:47Z","title":"Anatomical Structure-Guided Medical Vision-Language Pre-training","summary":"  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n","authors":["Qingqiu Li","Xiaohan Yan","Jilan Xu","Runtian Yuan","Yuejie Zhang","Rui Feng","Quanli Shen","Xiaobo Zhang","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.09294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01334v2","updated":"2024-03-14T11:01:15Z","published":"2023-10-02T16:51:32Z","title":"Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy","summary":"  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.\n","authors":["Pingzhi Li","Zhenyu Zhang","Prateek Yadav","Yi-Lin Sung","Yu Cheng","Mohit Bansal","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.01334v2.pdf","comment":"This paper is accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2311.17371v2","updated":"2024-03-14T10:56:50Z","published":"2023-11-29T05:54:41Z","title":"Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs","summary":"  Recent advancements in large language models (LLMs) underscore their\npotential for responding to inquiries in various domains. However, ensuring\nthat generative agents provide accurate and reliable answers remains an ongoing\nchallenge. In this context, multi-agent debate (MAD) has emerged as a promising\nstrategy for enhancing the truthfulness of LLMs. We benchmark a range of\ndebating and prompting strategies to explore the trade-offs between cost, time,\nand accuracy. Importantly, we find that multi-agent debating systems, in their\ncurrent form, do not reliably outperform other proposed prompting strategies,\nsuch as self-consistency and ensembling using multiple reasoning paths.\nHowever, when performing hyperparameter tuning, several MAD systems, such as\nMulti-Persona, perform better. This suggests that MAD protocols might not be\ninherently worse than other approaches, but that they are more sensitive to\ndifferent hyperparameter settings and difficult to optimize. We build on these\nresults to offer insights into improving debating strategies, such as adjusting\nagent agreement levels, which can significantly enhance performance and even\nsurpass all other non-debate protocols we evaluated. We provide an open-source\nrepository to the community with several state-of-the-art protocols together\nwith evaluation scripts to benchmark across popular research datasets.\n","authors":["Andries Smit","Paul Duckworth","Nathan Grinsztajn","Thomas D. Barrett","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2311.17371v2.pdf","comment":"2 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.08103v2","updated":"2024-03-14T10:47:32Z","published":"2024-03-12T22:23:08Z","title":"Contextual Clarity: Generating Sentences with Transformer Models using\n  Context-Reverso Data","summary":"  In the age of information abundance, the ability to provide users with\ncontextually relevant and concise information is crucial. Keyword in Context\n(KIC) generation is a task that plays a vital role in and generation\napplications, such as search engines, personal assistants, and content\nsummarization. In this paper, we present a novel approach to generating\nunambiguous and brief sentence-contexts for given keywords using the T5\ntransformer model, leveraging data obtained from the Context-Reverso API. The\ncode is available at https://github.com/Rusamus/word2context/tree/main .\n","authors":["Ruslan Musaev"],"pdf_url":"https://arxiv.org/pdf/2403.08103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14000v2","updated":"2024-03-14T10:35:57Z","published":"2022-07-28T10:44:46Z","title":"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study\n  on Out-of-Distribution Generalisation","summary":"  Combining deep learning with symbolic logic reasoning aims to capitalize on\nthe success of both fields and is drawing increasing attention. Inspired by\nDeepLogic, an end-to-end model trained to perform inference on logic programs,\nwe introduce IMA-GloVe-GA, an iterative neural inference network for multi-step\nreasoning expressed in natural language. In our model, reasoning is performed\nusing an iterative memory neural network based on RNN with a gate attention\nmechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES\nV1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention\ncan achieve higher test accuracy than DeepLogic and other RNN baseline models.\nOur model achieves better out-of-distribution generalisation than RoBERTa-Large\nwhen the rules have been shuffled. Furthermore, to address the issue of\nunbalanced distribution of reasoning depths in the current multi-step reasoning\ndatasets, we develop PARARULE-Plus, a large dataset with more examples that\nrequire deeper reasoning steps. Experimental results show that the addition of\nPARARULE-Plus can increase the model's performance on examples requiring deeper\nreasoning depths. The source code and data are available at\nhttps://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.\n","authors":["Qiming Bao","Alex Yuxuan Peng","Tim Hartill","Neset Tan","Zhenyun Deng","Michael Witbrock","Jiamou Liu"],"pdf_url":"https://arxiv.org/pdf/2207.14000v2.pdf","comment":"10 pages, 3 figures, The 2nd International Joint Conference on\n  Learning & Reasoning and 16th International Workshop on Neural-Symbolic\n  Learning and Reasoning (IJCLR-NeSy 2022)"},{"id":"http://arxiv.org/abs/2403.09259v1","updated":"2024-03-14T10:33:28Z","published":"2024-03-14T10:33:28Z","title":"To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation","summary":"  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n","authors":["Abdul Hameed Azeemi","Ihsan Ayyub Qazi","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2403.09259v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2309.17167v3","updated":"2024-03-14T09:52:16Z","published":"2023-09-29T12:04:14Z","title":"DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks","summary":"  Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.\n","authors":["Kaijie Zhu","Jiaao Chen","Jindong Wang","Neil Zhenqiang Gong","Diyi Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2309.17167v3.pdf","comment":"ICLR 2024 spotlight; 38 pages; code is at aka.ms/dyval"},{"id":"http://arxiv.org/abs/2403.09226v1","updated":"2024-03-14T09:45:05Z","published":"2024-03-14T09:45:05Z","title":"Retrieval augmented text-to-SQL generation for epidemiological question\n  answering using electronic health records","summary":"  Electronic health records (EHR) and claims data are rich sources of\nreal-world data that reflect patient health status and healthcare utilization.\nQuerying these databases to answer epidemiological questions is challenging due\nto the intricacy of medical terminology and the need for complex SQL queries.\nHere, we introduce an end-to-end methodology that combines text-to-SQL\ngeneration with retrieval augmented generation (RAG) to answer epidemiological\nquestions using EHR and claims data. We show that our approach, which\nintegrates a medical coding step into the text-to-SQL process, significantly\nimproves the performance over simple prompting. Our findings indicate that\nalthough current language models are not yet sufficiently accurate for\nunsupervised use, RAG offers a promising direction for improving their\ncapabilities, as shown in a realistic industry setting.\n","authors":["Angelo Ziletti","Leonardo D'Ambrosi"],"pdf_url":"https://arxiv.org/pdf/2403.09226v1.pdf","comment":"6 pages, 1 figure"},{"id":"http://arxiv.org/abs/2309.10272v2","updated":"2024-03-14T09:32:16Z","published":"2023-09-19T02:59:41Z","title":"Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi","summary":"  One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n","authors":["Md Nishat Raihan","Dhiman Goswami","Antara Mahmud"],"pdf_url":"https://arxiv.org/pdf/2309.10272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09207v1","updated":"2024-03-14T09:21:25Z","published":"2024-03-14T09:21:25Z","title":"TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic\n  Tasks","summary":"  In this paper, we explore the capabilities of LLMs in capturing\nlexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model\nand test it on multiple lexical semantic tasks. As the outcome of our\nexperiments, we present TaxoLLaMA, the everything-in-one model, lightweight due\nto 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results\nout of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy\nConstruction, and Lexical Entailment tasks. Moreover, it demonstrates very\nstrong zero-shot performance on Lexical Entailment and Taxonomy Construction\nwith no fine-tuning. We also explore its hidden multilingual and domain\nadaptation capabilities with a little tuning or few-shot learning. All\ndatasets, code, and model are available online at\nhttps://github.com/VityaVitalich/TaxoLLaMA\n","authors":["Viktor Moskvoretskii","Ekaterina Neminova","Alina Lobanova","Alexander Panchenko","Irina Nikishina"],"pdf_url":"https://arxiv.org/pdf/2403.09207v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.09167v1","updated":"2024-03-14T08:27:32Z","published":"2024-03-14T08:27:32Z","title":"Dial-insight: Fine-tuning Large Language Models with High-Quality\n  Domain-Specific Data Preventing Capability Collapse","summary":"  The efficacy of large language models (LLMs) is heavily dependent on the\nquality of the underlying data, particularly within specialized domains. A\ncommon challenge when fine-tuning LLMs for domain-specific applications is the\npotential degradation of the model's generalization capabilities. To address\nthese issues, we propose a two-stage approach for the construction of\nproduction prompts designed to yield high-quality data. This method involves\nthe generation of a diverse array of prompts that encompass a broad spectrum of\ntasks and exhibit a rich variety of expressions. Furthermore, we introduce a\ncost-effective, multi-dimensional quality assessment framework to ensure the\nintegrity of the generated labeling data. Utilizing a dataset comprised of\nservice provider and customer interactions from the real estate sector, we\ndemonstrate a positive correlation between data quality and model performance.\nNotably, our findings indicate that the domain-specific proficiency of general\nLLMs can be enhanced through fine-tuning with data produced via our proposed\nmethod, without compromising their overall generalization abilities, even when\nexclusively domain-specific data is employed for fine-tuning.\n","authors":["Jianwei Sun","Chaoyang Mei","Linlin Wei","Kaiyu Zheng","Na Liu","Ming Cui","Tianyi Li"],"pdf_url":"https://arxiv.org/pdf/2403.09167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09164v1","updated":"2024-03-14T08:20:40Z","published":"2024-03-14T08:20:40Z","title":"Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine\n  Knowledge","summary":"  No previous work has studied the performance of Large Language Models (LLMs)\nin the context of Traditional Chinese Medicine (TCM), an essential and distinct\nbranch of medical knowledge with a rich history. To bridge this gap, we present\na TCM question dataset named TCM-QA, which comprises three question types:\nsingle choice, multiple choice, and true or false, to examine the LLM's\ncapacity for knowledge recall and comprehensive reasoning within the TCM\ndomain. In our study, we evaluate two settings of the LLM, zero-shot and\nfew-shot settings, while concurrently discussing the differences between\nEnglish and Chinese prompts. Our results indicate that ChatGPT performs best in\ntrue or false questions, achieving the highest precision of 0.688 while scoring\nthe lowest precision is 0.241 in multiple-choice questions. Furthermore, we\nobserved that Chinese prompts outperformed English prompts in our evaluations.\nAdditionally, we assess the quality of explanations generated by ChatGPT and\ntheir potential contribution to TCM knowledge comprehension. This paper offers\nvaluable insights into the applicability of LLMs in specialized domains and\npaves the way for future research in leveraging these powerful models to\nadvance TCM.\n","authors":["Li Yizhen","Huang Shaohan","Qi Jiaxing","Quan Lei","Han Dongran","Luan Zhongzhi"],"pdf_url":"https://arxiv.org/pdf/2403.09164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09163v1","updated":"2024-03-14T08:19:41Z","published":"2024-03-14T08:19:41Z","title":"Caveat Lector: Large Language Models in Legal Practice","summary":"  The current fascination with large language models, or LLMs, derives from the\nfact that many users lack the expertise to evaluate the quality of the\ngenerated text. LLMs may therefore appear more capable than they actually are.\nThe dangerous combination of fluency and superficial plausibility leads to the\ntemptation to trust the generated text and creates the risk of overreliance.\nWho would not trust perfect legalese? Relying recent findings in both technical\nand legal scholarship, this Article counterbalances the overly optimistic\npredictions as to the role of LLMs in legal practice. Integrating LLMs into\nlegal workstreams without a better comprehension of their limitations, will\ncreate inefficiencies if not outright risks. Notwithstanding their\nunprecedented ability to generate text, LLMs do not understand text. Without\nthe ability to understand meaning, LLMs will remain unable to use language, to\nacquire knowledge and to perform complex reasoning tasks. Trained to model\nlanguage on the basis of stochastic word predictions, LLMs cannot distinguish\nfact from fiction. Their knowledge of the law is limited to word strings\nmemorized in their parameters. It is also incomplete and largely incorrect.\nLLMs operate at the level of word distributions, not at the level of verified\nfacts. The resulting propensity to hallucinate, to produce statements that are\nincorrect but appear helpful and relevant, is alarming in high-risk areas like\nlegal services. At present, lawyers should beware of relying on text generated\nby LLMs.\n","authors":["Eliza Mik"],"pdf_url":"https://arxiv.org/pdf/2403.09163v1.pdf","comment":"Vol 19 Rutgers Bus L R 2 2024 (forthcoming)"},{"id":"http://arxiv.org/abs/2403.09162v1","updated":"2024-03-14T08:18:59Z","published":"2024-03-14T08:18:59Z","title":"Unveiling the Generalization Power of Fine-Tuned Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n","authors":["Haoran Yang","Yumeng Zhang","Jiaqi Xu","Hongyuan Lu","Pheng Ann Heng","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.09162v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2305.13068v3","updated":"2024-03-14T08:15:27Z","published":"2023-05-22T14:37:05Z","title":"Making Language Models Better Tool Learners with Execution Feedback","summary":"  Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.\n","authors":["Shuofei Qiao","Honghao Gui","Chengfei Lv","Qianghuai Jia","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13068v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09159v1","updated":"2024-03-14T08:12:47Z","published":"2024-03-14T08:12:47Z","title":"Basque and Spanish Counter Narrative Generation: Data Creation and\n  Evaluation","summary":"  Counter Narratives (CNs) are non-negative textual responses to Hate Speech\n(HS) aiming at defusing online hatred and mitigating its spreading across\nmedia. Despite the recent increase in HS content posted online, research on\nautomatic CN generation has been relatively scarce and predominantly focused on\nEnglish. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset\nfor CN generation developed by means of Machine Translation (MT) and\nprofessional post-edition. Being a parallel corpus, also with respect to the\noriginal English CONAN, it allows to perform novel research on multilingual and\ncrosslingual automatic generation of CNs. Our experiments on CN generation with\nmT5, a multilingual encoder-decoder model, show that generation greatly\nbenefits from training on post-edited data, as opposed to relying on silver MT\ndata only. These results are confirmed by their correlation with a qualitative\nmanual evaluation, demonstrating that manually revised training data remains\ncrucial for the quality of the generated CNs. Furthermore, multilingual data\naugmentation improves results over monolingual settings for structurally\nsimilar languages such as English and Spanish, while being detrimental for\nBasque, a language isolate. Similar findings occur in zero-shot crosslingual\nevaluations, where model transfer (fine-tuning in English and generating in a\ndifferent target language) outperforms fine-tuning mT5 on machine translated\ndata for Spanish but not for Basque. This provides an interesting insight into\nthe asymmetry in the multilinguality of generative models, a challenging topic\nwhich is still open to research.\n","authors":["Jaione Bengoetxea","Yi-Ling Chung","Marco Guerini","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2403.09159v1.pdf","comment":"Accepted for the Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024"},{"id":"http://arxiv.org/abs/2310.20703v3","updated":"2024-03-14T08:05:18Z","published":"2023-10-31T17:59:05Z","title":"Vanishing Gradients in Reinforcement Finetuning of Language Models","summary":"  Pretrained language models are commonly aligned with human preferences and\ndownstream tasks via reinforcement finetuning (RFT), which refers to maximizing\na (possibly learned) reward function using policy gradient algorithms. This\nwork identifies a fundamental optimization obstacle in RFT: we prove that the\nexpected gradient for an input vanishes when its reward standard deviation\nunder the model is small, even if the expected reward is far from optimal.\nThrough experiments on an RFT benchmark and controlled environments, as well as\na theoretical analysis, we then demonstrate that vanishing gradients due to\nsmall reward standard deviation are prevalent and detrimental, leading to\nextremely slow reward maximization. Lastly, we explore ways to overcome\nvanishing gradients in RFT. We find the common practice of an initial\nsupervised finetuning (SFT) phase to be the most promising candidate, which\nsheds light on its importance in an RFT pipeline. Moreover, we show that a\nrelatively small number of SFT optimization steps on as few as 1% of the input\nsamples can suffice, indicating that the initial SFT phase need not be\nexpensive in terms of compute and data labeling efforts. Overall, our results\nemphasize that being mindful for inputs whose expected gradient vanishes, as\nmeasured by the reward standard deviation, is crucial for successful execution\nof RFT.\n","authors":["Noam Razin","Hattie Zhou","Omid Saremi","Vimal Thilak","Arwen Bradley","Preetum Nakkiran","Joshua Susskind","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2310.20703v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08495v2","updated":"2024-03-14T08:05:08Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v2.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2311.05296v2","updated":"2024-03-14T08:04:17Z","published":"2023-11-09T11:53:52Z","title":"BeLLM: Backward Dependency Enhanced Large Language Model for Sentence\n  Embeddings","summary":"  Sentence embeddings are crucial in measuring semantic similarity. Most recent\nstudies employed large language models (LLMs) to learn sentence embeddings.\nExisting LLMs mainly adopted autoregressive architecture without explicit\nbackward dependency modeling. Therefore, we examined the effects of backward\ndependencies in LLMs for semantic similarity measurements. Concretely, we\npropose a novel model: backward dependency enhanced large language model\n(BeLLM). It learns sentence embeddings via transforming specific attention\nlayers from uni- to bi-directional. We extensively experiment across various\nsemantic textual similarity (STS) tasks and downstream applications. BeLLM\nachieves state-of-the-art performance in varying scenarios. It shows that\nauto-regressive LLMs benefit from backward dependencies for sentence\nembeddings.\n","authors":["Xianming Li","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2311.05296v2.pdf","comment":"Accepted by NAACL24 Main Conference"},{"id":"http://arxiv.org/abs/2403.04321v2","updated":"2024-03-14T08:02:29Z","published":"2024-03-07T08:37:33Z","title":"Discriminative Probing and Tuning for Text-to-Image Generation","summary":"  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n","authors":["Leigang Qu","Wenjie Wang","Yongqi Li","Hanwang Zhang","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.04321v2.pdf","comment":"CVPR 2024; project page: https://dpt-t2i.github.io/"},{"id":"http://arxiv.org/abs/2403.05101v3","updated":"2024-03-14T08:00:51Z","published":"2024-03-08T07:06:43Z","title":"Rule-driven News Captioning","summary":"  News captioning task aims to generate sentences by describing named entities\nor concrete events for an image with its news article. Existing methods have\nachieved remarkable results by relying on the large-scale pre-trained models,\nwhich primarily focus on the correlations between the input news content and\nthe output predictions. However, the news captioning requires adhering to some\nfundamental rules of news reporting, such as accurately describing the\nindividuals and actions associated with the event. In this paper, we propose\nthe rule-driven news captioning method, which can generate image descriptions\nfollowing designated rule signal. Specifically, we first design the news-aware\nsemantic rule for the descriptions. This rule incorporates the primary action\ndepicted in the image (e.g., \"performing\") and the roles played by named\nentities involved in the action (e.g., \"Agent\" and \"Place\"). Second, we inject\nthis semantic rule into the large-scale pre-trained model, BART, with the\nprefix-tuning strategy, where multiple encoder layers are embedded with\nnews-aware semantic rule. Finally, we can effectively guide BART to generate\nnews sentences that comply with the designated rule. Extensive experiments on\ntwo widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the\neffectiveness of our method.\n","authors":["Ning Xu","Tingting Zhang","Hongshuo Tian","An-An Liu"],"pdf_url":"https://arxiv.org/pdf/2403.05101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09148v1","updated":"2024-03-14T07:58:27Z","published":"2024-03-14T07:58:27Z","title":"Evaluating LLMs for Gender Disparities in Notable Persons","summary":"  This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.\n","authors":["Lauren Rhue","Sofie Goethals","Arun Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2403.09148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07691v2","updated":"2024-03-14T07:47:08Z","published":"2024-03-12T14:34:08Z","title":"ORPO: Monolithic Preference Optimization without Reference Model","summary":"  While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).\n","authors":["Jiwoo Hong","Noah Lee","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2403.07691v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.09131v1","updated":"2024-03-14T06:49:16Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.05502v2","updated":"2024-03-14T05:55:44Z","published":"2023-10-09T08:07:04Z","title":"XAL: EXplainable Active Learning Makes Classifiers Better Low-resource\n  Learners","summary":"  Active learning (AL), which aims to construct an effective training set by\niteratively curating the most formative unlabeled data for annotation, has been\nwidely used in low-resource tasks. Most active learning techniques in\nclassification rely on the model's uncertainty or disagreement to choose\nunlabeled data, suffering from the problem of over-confidence in superficial\npatterns and a lack of exploration. Inspired by the cognitive processes in\nwhich humans deduce and predict through causal information, we take an initial\nattempt towards integrating rationales into AL and propose a novel Explainable\nActive Learning framework (XAL) for low-resource text classification, which\naims to encourage classifiers to justify their inferences and delve into\nunlabeled data for which they cannot provide reasonable explanations.\nSpecifically, besides using a pre-trained bi-directional encoder for\nclassification, we employ a pre-trained uni-directional decoder to generate and\nscore the explanation. We further facilitate the alignment of the model with\nhuman reasoning preference through a proposed ranking loss. During the\nselection of unlabeled data, the predicted uncertainty of the encoder and the\nexplanation score of the decoder complement each other as the final metric to\nacquire informative data. Extensive experiments on six datasets show that XAL\nachieves consistent improvement over 9 strong baselines. Analysis indicates\nthat the proposed method can generate corresponding explanations for its\npredictions.\n","authors":["Yun Luo","Zhen Yang","Fandong Meng","Yingjie Li","Fang Guo","Qinglin Qi","Jie Zhou","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05502v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09113v1","updated":"2024-03-14T05:29:35Z","published":"2024-03-14T05:29:35Z","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based\n  on Meta Learning","summary":"  Large-scale pretraining followed by task-specific finetuning has achieved\ngreat success in various NLP tasks. Since finetuning all parameters of large\npretrained models poses substantial computational and memory challenges,\nseveral efficient finetuning methods have been developed. Among them, low-rank\nadaptation (LoRA), which finetunes low-rank incremental update matrices on top\nof frozen pretrained weights, has proven particularly effective. Nonetheless,\nLoRA's uniform rank assignment across all layers, along with its reliance on an\nexhaustive search to find the best rank, leads to high computation costs and\nsuboptimal finetuning performance. To address these limitations, we introduce\nAutoLoRA, a meta learning based framework for automatically identifying the\noptimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a\nlow-rank update matrix with a selection variable, which determines whether the\nrank-1 matrix should be discarded. A meta learning based method is developed to\nlearn these selection variables. The optimal rank is determined by thresholding\nthe values of these variables. Our comprehensive experiments on natural\nlanguage understanding, generation, and sequence labeling demonstrate the\neffectiveness of AutoLoRA.\n","authors":["Ruiyi Zhang","Rushi Qiang","Sai Ashish Somayajula","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.09113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09097v1","updated":"2024-03-14T04:43:02Z","published":"2024-03-14T04:43:02Z","title":"AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI\n  Publications","summary":"  Identifying scientific publications that are within a dynamic field of\nresearch often requires costly annotation by subject-matter experts. Resources\nlike widely-accepted classification criteria or field taxonomies are\nunavailable for a domain like artificial intelligence (AI), which spans\nemerging topics and technologies. We address these challenges by inferring a\nfunctional definition of AI research from existing expert labels, and then\nevaluating state-of-the-art chatbot models on the task of expert data\nannotation. Using the arXiv publication database as ground-truth, we experiment\nwith prompt engineering for GPT chatbot models to identify an alternative,\nautomated expert annotation pipeline that assigns AI labels with 94% accuracy.\nFor comparison, we fine-tune SPECTER, a transformer language model pre-trained\non scientific publications, that achieves 96% accuracy (only 2% higher than\nGPT) on classifying AI publications. Our results indicate that with effective\nprompt engineering, chatbots can be used as reliable data annotators even where\nsubject-area expertise is required. To evaluate the utility of\nchatbot-annotated datasets on downstream classification tasks, we train a new\nclassifier on GPT-labeled data and compare its performance to the arXiv-trained\nmodel. The classifier trained on GPT-labeled data outperforms the arXiv-trained\nmodel by nine percentage points, achieving 82% accuracy.\n","authors":["Autumn Toney-Wails","Christian Schoeberl","James Dunham"],"pdf_url":"https://arxiv.org/pdf/2403.09097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09092v1","updated":"2024-03-14T04:32:13Z","published":"2024-03-14T04:32:13Z","title":"MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection","summary":"  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n","authors":["Yupeng Li","Haorui He","Jin Bai","Dacheng Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09092v1.pdf","comment":"Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset\n  available: https://github.com/TrustworthyComp"},{"id":"http://arxiv.org/abs/2310.01798v2","updated":"2024-03-14T04:27:52Z","published":"2023-10-03T04:56:12Z","title":"Large Language Models Cannot Self-Correct Reasoning Yet","summary":"  Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance even\ndegrades after self-correction. Drawing from these insights, we offer\nsuggestions for future research and practical applications in this field.\n","authors":["Jie Huang","Xinyun Chen","Swaroop Mishra","Huaixiu Steven Zheng","Adams Wei Yu","Xinying Song","Denny Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.01798v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09085v1","updated":"2024-03-14T04:06:13Z","published":"2024-03-14T04:06:13Z","title":"Meaningful Learning: Advancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance","summary":"  Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nsimple questions supported by a generic fact, LLMs often fail to provide\nconsistent and precise answers, indicating a deficiency in abstract reasoning\nabilities. This has sparked a vigorous debate about whether LLMs are genuinely\nreasoning or merely memorizing. In light of this, we design a preliminary study\nto quantify and delve into the abstract reasoning abilities of existing LLMs.\nOur findings reveal a substantial discrepancy between their general reasoning\nand abstract reasoning performances. To relieve this problem, we tailor an\nabstract reasoning dataset (AbsR) together with a meaningful learning paradigm\nto teach LLMs how to leverage generic facts for reasoning purposes. The results\nshow that our approach not only boosts the general reasoning performance of\nLLMs but also makes considerable strides towards their capacity for abstract\nreasoning, moving beyond simple memorization or imitation to a more nuanced\nunderstanding and application of generic facts.\n","authors":["Kai Xiong","Xiao Ding","Ting Liu","Bing Qin","Dongliang Xu","Qing Yang","Hongtao Liu","Yixin Cao"],"pdf_url":"https://arxiv.org/pdf/2403.09085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09077v1","updated":"2024-03-14T03:49:36Z","published":"2024-03-14T03:49:36Z","title":"Information Extraction: An application to the domain of hyper-local\n  financial data on developing countries","summary":"  Despite the need for financial data on company activities in developing\ncountries for development research and economic analysis, such data does not\nexist. In this project, we develop and evaluate two Natural Language Processing\n(NLP) based techniques to address this issue. First, we curate a custom dataset\nspecific to the domain of financial text data on developing countries and\nexplore multiple approaches for information extraction. We then explore a\ntext-to-text approach with the transformer-based T5 model with the goal of\nundertaking simultaneous NER and relation extraction. We find that this model\nis able to learn the custom text structure output data corresponding to the\nentities and their relations, resulting in an accuracy of 92.44\\%, a precision\nof 68.25\\% and a recall of 54.20\\% from our best T5 model on the combined task.\nSecondly, we explore an approach with sequential NER and relation extration.\nFor the NER, we run pre-trained and fine-tuned models using SpaCy, and we\ndevelop a custom relation extraction model using SpaCy's Dependency Parser\noutput and some heuristics to determine entity relationships \\cite{spacy}. We\nobtain an accuracy of 84.72\\%, a precision of 6.06\\% and a recall of 5.57\\% on\nthis sequential task.\n","authors":["Abuzar Royesh","Olamide Oladeji"],"pdf_url":"https://arxiv.org/pdf/2403.09077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04614v3","updated":"2024-03-14T03:48:08Z","published":"2024-02-07T06:32:50Z","title":"Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\n  from Large Language Models","summary":"  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n","authors":["Chirag Agarwal","Sree Harsha Tanneru","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09073v1","updated":"2024-03-14T03:33:46Z","published":"2024-03-14T03:33:46Z","title":"Large Language Models are Parallel Multilingual Learners","summary":"  In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence.\n","authors":["Yongyu Mu","Peinan Feng","Zhiquan Cao","Yuzhang Wu","Bei Li","Chenglong Wang","Tong Xiao","Kai Song","Tongran Liu","Chunliang Zhang","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.09073v1.pdf","comment":"Working in process"},{"id":"http://arxiv.org/abs/2403.09072v1","updated":"2024-03-14T03:29:58Z","published":"2024-03-14T03:29:58Z","title":"UniCode: Learning a Unified Codebook for Multimodal Large Language\n  Models","summary":"  In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.\n","authors":["Sipeng Zheng","Bohan Zhou","Yicheng Feng","Ye Wang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09072v1.pdf","comment":"14 pages, 2 figures, 11 tables"},{"id":"http://arxiv.org/abs/2310.08992v3","updated":"2024-03-14T03:29:09Z","published":"2023-10-13T10:17:48Z","title":"CodeChain: Towards Modular Code Generation Through Chain of\n  Self-revisions with Representative Sub-modules","summary":"  Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.\n","authors":["Hung Le","Hailin Chen","Amrita Saha","Akash Gokul","Doyen Sahoo","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2310.08992v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.08715v2","updated":"2024-03-14T03:13:20Z","published":"2024-03-13T17:17:48Z","title":"SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language\n  Agents","summary":"  Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.\n","authors":["Ruiyi Wang","Haofei Yu","Wenxin Zhang","Zhengyang Qi","Maarten Sap","Graham Neubig","Yonatan Bisk","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.08715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09059v1","updated":"2024-03-14T02:56:38Z","published":"2024-03-14T02:56:38Z","title":"LAMP: A Language Model on the Map","summary":"  Large Language Models (LLMs) are poised to play an increasingly important\nrole in our lives, providing assistance across a wide array of tasks. In the\ngeospatial domain, LLMs have demonstrated the ability to answer generic\nquestions, such as identifying a country's capital; nonetheless, their utility\nis hindered when it comes to answering fine-grained questions about specific\nplaces, such as grocery stores or restaurants, which constitute essential\naspects of people's everyday lives. This is mainly because the places in our\ncities haven't been systematically fed into LLMs, so as to understand and\nmemorize them. This study introduces a novel framework for fine-tuning a\npre-trained model on city-specific data, to enable it to provide accurate\nrecommendations, while minimizing hallucinations. We share our model, LAMP, and\nthe data used to train it. We conduct experiments to analyze its ability to\ncorrectly retrieving spatial objects, and compare it to well-known open- and\nclosed- source language models, such as GPT-4. Finally, we explore its emerging\ncapabilities through a case study on day planning.\n","authors":["Pasquale Balsebre","Weiming Huang","Gao Cong"],"pdf_url":"https://arxiv.org/pdf/2403.09059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07022v2","updated":"2024-03-14T02:56:32Z","published":"2023-04-14T09:31:17Z","title":"Label Dependencies-aware Set Prediction Networks for Multi-label Text\n  Classification","summary":"  Multi-label text classification involves extracting all relevant labels from\na sentence. Given the unordered nature of these labels, we propose approaching\nthe problem as a set prediction task. To address the correlation between\nlabels, we leverage Graph Convolutional Networks and construct an adjacency\nmatrix based on the statistical relations between labels. Additionally, we\nenhance recall ability by applying the Bhattacharyya distance to the output\ndistributions of the set prediction networks. We evaluate the effectiveness of\nour approach on two multi-label datasets and demonstrate its superiority over\nprevious baselines through experimental results.\n","authors":["Du Xinkai","Han Quanjie","Sun Yalin","Lv Chao","Sun Maosong"],"pdf_url":"https://arxiv.org/pdf/2304.07022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09057v1","updated":"2024-03-14T02:55:37Z","published":"2024-03-14T02:55:37Z","title":"A Continued Pretrained LLM Approach for Automatic Medical Note\n  Generation","summary":"  LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like\nGPT-4, is too costly for most domain-specific scenarios. We present the first\ncontinuously trained 13B Llama2-based LLM that is purpose-built for medical\nconversations and measured on automated scribing. Our results show that our\nmodel outperforms GPT-4 in PubMedQA with 76.6\\% accuracy and matches its\nperformance in summarizing medical conversations into SOAP notes. Notably, our\nmodel exceeds GPT-4 in capturing a higher number of correct medical concepts\nand outperforms human scribes with higher correctness and completeness.\n","authors":["Dong Yuan","Eti Rastogi","Gautam Naik","Jai Chintagunta","Sree Prasanna Rajagopal","Fen Zhao","Sagar Goyal","Jeff Ward"],"pdf_url":"https://arxiv.org/pdf/2403.09057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07556v2","updated":"2024-03-14T02:40:22Z","published":"2024-03-12T11:40:44Z","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts","summary":"  Although large language models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by the untruthful context\nprovided by users or knowledge augmentation tools, thereby producing\nhallucinations. To alleviate the LLMs from being misled by untruthful\ninformation and take advantage of knowledge augmentation, we propose\nTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\ncontext from the inputs. TACS begins by performing truth detection on the input\ncontext, leveraging the parameterized knowledge within the LLM. Subsequently,\nit constructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results show that TACS can effectively\nfilter information in context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2403.07556v2.pdf","comment":"Code is available at: https://github.com/ictnlp/TACS"},{"id":"http://arxiv.org/abs/2308.10792v5","updated":"2024-03-14T02:28:22Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v5.pdf","comment":"V2; Last update: March 12, 2024"},{"id":"http://arxiv.org/abs/2403.09040v1","updated":"2024-03-14T02:26:31Z","published":"2024-03-14T02:26:31Z","title":"RAGGED: Towards Informed Design of Retrieval Augmented Generation\n  Systems","summary":"  Retrieval-augmented generation (RAG) greatly benefits language models (LMs)\nby providing additional context for tasks such as document-based question\nanswering (DBQA). Despite its potential, the power of RAG is highly dependent\non its configuration, raising the question: What is the optimal RAG\nconfiguration? To answer this, we introduce the RAGGED framework to analyze and\noptimize RAG systems. On a set of representative DBQA tasks, we study two\nclassic sparse and dense retrievers, and four top-performing LMs in\nencoder-decoder and decoder-only architectures. Through RAGGED, we uncover that\ndifferent models suit substantially varied RAG setups. While encoder-decoder\nmodels monotonically improve with more documents, we find decoder-only models\ncan only effectively use < 5 documents, despite often having a longer context\nwindow. RAGGED offers further insights into LMs' context utilization habits,\nwhere we find that encoder-decoder models rely more on contexts and are thus\nmore sensitive to retrieval quality, while decoder-only models tend to rely on\nknowledge memorized during training.\n","authors":["Jennifer Hsia","Afreen Shaikh","Zhiruo Wang","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2403.09040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09037v1","updated":"2024-03-14T02:25:35Z","published":"2024-03-14T02:25:35Z","title":"The First to Know: How Token Distributions Reveal Hidden Knowledge in\n  Large Vision-Language Models?","summary":"  Large vision-language models (LVLMs), designed to interpret and respond to\nhuman instructions, occasionally generate hallucinated or harmful content due\nto inappropriate instructions. This study uses linear probing to shed light on\nthe hidden knowledge at the output layer of LVLMs. We demonstrate that the\nlogit distributions of the first tokens contain sufficient information to\ndetermine whether to respond to the instructions, including recognizing\nunanswerable visual questions, defending against multi-modal jailbreaking\nattack, and identifying deceptive questions. Such hidden knowledge is gradually\nlost in logits of subsequent tokens during response generation. Then, we\nillustrate a simple decoding strategy at the generation of the first token,\neffectively improving the generated content. In experiments, we find a few\ninteresting insights: First, the CLIP model already contains a strong signal\nfor solving these tasks, indicating potential bias in the existing datasets.\nSecond, we observe performance improvement by utilizing the first logit\ndistributions on three additional tasks, including indicting uncertainty in\nmath solving, mitigating hallucination, and image classification. Last, with\nthe same training data, simply finetuning LVLMs improve models' performance but\nis still inferior to linear probing on these tasks.\n","authors":["Qinyu Zhao","Ming Xu","Kartik Gupta","Akshay Asthana","Liang Zheng","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2403.09037v1.pdf","comment":"Under review. Project page:\n  https://github.com/Qinyu-Allen-Zhao/LVLM-LP"},{"id":"http://arxiv.org/abs/2308.07702v2","updated":"2024-03-14T02:07:11Z","published":"2023-08-15T11:08:30Z","title":"Better Zero-Shot Reasoning with Role-Play Prompting","summary":"  Modern large language models (LLMs) exhibit a remarkable capacity for\nrole-playing, enabling them to embody not only human characters but also\nnon-human entities. This versatility allows them to simulate complex human-like\ninteractions and behaviors within various contexts, as well as to emulate\nspecific objects or systems. While these capabilities have enhanced user\nengagement and introduced novel modes of interaction, the influence of\nrole-playing on LLMs' reasoning abilities remains underexplored. In this study,\nwe introduce a strategically designed role-play prompting methodology and\nassess its performance under the zero-shot setting across twelve diverse\nreasoning benchmarks. Our empirical results illustrate that role-play prompting\nconsistently surpasses the standard zero-shot approach across most datasets.\nNotably, in experiments conducted using ChatGPT, accuracy on AQuA rises from\n53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison\nwith the Zero-Shot-CoT technique, which prompts the model to \"think step by\nstep\", our study demonstrates that role-play prompting acts as a more effective\ntrigger for the CoT process. This highlights its potential to augment the\nreasoning capabilities of LLMs. We release our code at\nhttps://github.com/NKU-HLT/Role-Play-Prompting.\n","authors":["Aobo Kong","Shiwan Zhao","Hao Chen","Qicheng Li","Yong Qin","Ruiqi Sun","Xin Zhou","Enzhi Wang","Xiaohang Dong"],"pdf_url":"https://arxiv.org/pdf/2308.07702v2.pdf","comment":"NAACL 2024, Main Conference"},{"id":"http://arxiv.org/abs/2403.07708v2","updated":"2024-03-14T02:02:31Z","published":"2024-03-12T14:51:57Z","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive\n  Rewards","summary":"  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.\n","authors":["Wei Shen","Xiaoying Zhang","Yuanshun Yao","Rui Zheng","Hongyi Guo","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.07708v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v1","updated":"2024-03-14T01:51:35Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires assessing intricate\ntextual LLMs' outputs. By relying on automated metrics and static analysis\ntools, existing benchmarks fail to assess nuances in user instructions and LLM\noutputs, highlighting the need for large-scale datasets and benchmarks for LLM\npreference alignment. In this paper, we introduce CodeUltraFeedback, a\npreference dataset of 10,000 complex instructions to tune and align LLMs to\ncoding preferences through AI feedback. We generate responses to the\ninstructions using a pool of 14 diverse LLMs, which we then annotate according\nto their alignment with five coding preferences using the LLM-as-a-Judge\napproach with GPT-3.5, producing both numerical and textual feedback. We also\npresent CODAL-Bench, a benchmark for assessing LLM alignment with these coding\npreferences. Our results show that CodeLlama-7B-Instruct, aligned through\nreinforcement learning from AI feedback (RLAIF) with direct preference\noptimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B\nLLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference\ntuning. Furthermore, we show our DPO-aligned CodeLlama model improves\nfunctional correctness on HumanEval+ compared to the unaligned base model.\nTherefore, our contributions bridge the gap in preference tuning of LLMs for\ncode and set the stage for further advancements in model alignment and RLAIF\nfor code intelligence. Our code and data are available at\nhttps://github.com/martin-wey/CodeUltraFeedback.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09028v1","updated":"2024-03-14T01:40:23Z","published":"2024-03-14T01:40:23Z","title":"ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning","summary":"  Charts provide visual representations of data and are widely used for\nanalyzing information, addressing queries, and conveying insights to others.\nVarious chart-related downstream tasks have emerged recently, such as\nquestion-answering and summarization. A common strategy to solve these tasks is\nto fine-tune various models originally trained on vision tasks language.\nHowever, such task-specific models are not capable of solving a wide range of\nchart-related tasks, constraining their real-world applicability. To overcome\nthese challenges, we introduce ChartInstruct: a novel chart-specific\nvision-language Instruction-following dataset comprising 191K instructions\ngenerated with 71K charts. We then present two distinct systems for instruction\ntuning on such datasets: (1) an end-to-end model that connects a vision encoder\nfor chart understanding with a LLM; and (2) a pipeline model that employs a\ntwo-step approach to extract chart data tables and input them into the LLM. In\nexperiments on four downstream tasks, we first show the effectiveness of our\nmodel--achieving a new set of state-of-the-art results. Further evaluation\nshows that our instruction-tuning approach supports a wide array of real-world\nchart comprehension and reasoning scenarios, thereby expanding the scope and\napplicability of our models to new kinds of tasks.\n","authors":["Ahmed Masry","Mehrad Shahmohammadi","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2403.09028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01579v2","updated":"2024-03-14T01:39:58Z","published":"2023-05-02T16:28:10Z","title":"Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models\n  against Counterfactual Noise","summary":"  Most existing retrieval-augmented language models (LMs) assume a naive\ndichotomy within a retrieved document set: query-relevance and irrelevance. Our\nwork investigates a more challenging scenario in which even the \"relevant\"\ndocuments may contain misleading or incorrect information, causing conflict\namong the retrieved documents and thereby negatively influencing model\ndecisions as noise. We observe that existing LMs are highly brittle to the\npresence of conflicting information in both the fine-tuning and in-context\nfew-shot learning scenarios. We propose approaches for handling knowledge\nconflicts among retrieved documents by explicitly fine-tuning a discriminator\nor prompting GPT-3.5 to elicit its discriminative capability. Our empirical\nresults on open-domain QA show that these approaches significantly enhance\nmodel robustness. We also provide our findings on incorporating the fine-tuned\ndiscriminator's decision into the in-context learning process, proposing a way\nto exploit the benefits of two disparate learning schemes. Alongside our\nfindings, we provide MacNoise, a machine-generated, conflict-induced dataset to\nfurther encourage research in this direction.\n","authors":["Giwon Hong","Jeonghwan Kim","Junmo Kang","Sung-Hyon Myaeng","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2305.01579v2.pdf","comment":"NAACL 2024 (Findings; Long Paper)"},{"id":"http://arxiv.org/abs/2403.09024v1","updated":"2024-03-14T01:28:13Z","published":"2024-03-14T01:28:13Z","title":"Semiparametric Token-Sequence Co-Supervision","summary":"  In this work, we introduce a semiparametric token-sequence co-supervision\ntraining method. It trains a language model by simultaneously leveraging\nsupervision from the traditional next token prediction loss which is calculated\nover the parametric token embedding space and the next sequence prediction loss\nwhich is calculated over the nonparametric sequence embedding space. The\nnonparametric sequence embedding space is constructed by a separate language\nmodel tasked to condense an input text into a single representative embedding.\nOur experiments demonstrate that a model trained via both supervisions\nconsistently surpasses models trained via each supervision independently.\nAnalysis suggests that this co-supervision encourages a broader generalization\ncapability across the model. Especially, the robustness of parametric token\nspace which is established during the pretraining step tends to effectively\nenhance the stability of nonparametric sequence embedding space, a new space\nestablished by another language model.\n","authors":["Hyunji Lee","Doyoung Kim","Jihoon Jun","Sejune Joo","Joel Jang","Kyoung-Woon On","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2403.09024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09017v1","updated":"2024-03-14T00:45:24Z","published":"2024-03-14T00:45:24Z","title":"AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic","summary":"  The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n516 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. By introducing\nAraTrust, we aim to promote collaborative efforts to create safer and more\ntrustworthy LLMs for Arabic users. We evaluated a set of LLMs against our\nbenchmark to assess its trustworthiness. GPT-4 showed to be the most\ntrustworthy regarding Arabic language.\n","authors":["Emad A. Alghamdi","Reem I. Masoud","Deema Alnuhait","Afnan Y. Alomairi","Ahmed Ashraf","Mohamed Zaytoon"],"pdf_url":"https://arxiv.org/pdf/2403.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13897v2","updated":"2024-03-14T00:21:09Z","published":"2024-02-21T16:09:25Z","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning","summary":"  Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.\n","authors":["Loïc Rakotoson","Sylvain Massip","Fréjus A. A. Laleye"],"pdf_url":"https://arxiv.org/pdf/2402.13897v2.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.09479v1","updated":"2024-03-14T15:20:54Z","published":"2024-03-14T15:20:54Z","title":"Laying the Foundation First? Investigating the Generalization from\n  Atomic Skills to Complex Reasoning Tasks","summary":"  Current language models have demonstrated their capability to develop basic\nreasoning, but struggle in more complicated reasoning tasks that require a\ncombination of atomic skills, such as math word problem requiring skills like\narithmetic and unit conversion. Previous methods either do not improve the\ninherent atomic skills of models or not attempt to generalize the atomic skills\nto complex reasoning tasks. In this paper, we first propose a probing framework\nto investigate whether the atomic skill can spontaneously generalize to complex\nreasoning tasks. Then, we introduce a hierarchical curriculum learning training\nstrategy to achieve better skill generalization. In our experiments, we find\nthat atomic skills can not spontaneously generalize to compositional tasks. By\nleveraging hierarchical curriculum learning, we successfully induce\ngeneralization, significantly improve the performance of open-source LMs on\ncomplex reasoning tasks. Promisingly, the skill generalization exhibit\neffective in cross-dataset and cross-domain scenarios. Complex reasoning can\nalso help enhance atomic skills. Our findings offer valuable guidance for\ndesigning better training strategies for complex reasoning tasks.\n","authors":["Yuncheng Huang","Qianyu He","Yipei Xu","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09054v1","updated":"2024-03-14T02:42:42Z","published":"2024-03-14T02:42:42Z","title":"Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient\n  Generative Inference","summary":"  Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.\n","authors":["Muhammad Adnan","Akhil Arunkumar","Gaurav Jain","Prashant J. Nair","Ilya Soloveychik","Purushotham Kamath"],"pdf_url":"https://arxiv.org/pdf/2403.09054v1.pdf","comment":"A collaborative effort by d-matrix and the University of British\n  Columbia"},{"id":"http://arxiv.org/abs/2403.06487v3","updated":"2024-03-14T23:59:59Z","published":"2024-03-11T07:50:29Z","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","summary":"  This paper investigates the application of voice activity projection (VAP), a\npredictive turn-taking model for spoken dialogue, on multilingual data,\nencompassing English, Mandarin, and Japanese. The VAP model continuously\npredicts the upcoming voice activities of participants in dyadic dialogue,\nleveraging a cross-attention Transformer to capture the dynamic interplay\nbetween participants. The results show that a monolingual VAP model trained on\none language does not make good predictions when applied to other languages.\nHowever, a multilingual model, trained on all three languages, demonstrates\npredictive performance on par with monolingual models across all languages.\nFurther analyses show that the multilingual model has learned to discern the\nlanguage of the input signal. We also analyze the sensitivity to pitch, a\nprosodic cue that is thought to be important for turn-taking. Finally, we\ncompare two different audio encoders, contrastive predictive coding (CPC)\npre-trained on English, with a recent model based on multilingual wav2vec 2.0\n(MMS).\n","authors":["Koji Inoue","Bing'er Jiang","Erik Ekstedt","Tatsuya Kawahara","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2403.06487v3.pdf","comment":"This paper has been accepted for presentation at The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation (LREC-COLING 2024) and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2403.09919v1","updated":"2024-03-14T23:40:56Z","published":"2024-03-14T23:40:56Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"  In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.\n","authors":["Aonan Zhang","Chong Wang","Yi Wang","Xuanyu Zhang","Yunfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09919v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.01989v2","updated":"2024-03-14T23:20:33Z","published":"2024-01-03T21:38:40Z","title":"Revisiting Zero-Shot Abstractive Summarization in the Era of Large\n  Language Models from the Perspective of Position Bias","summary":"  We characterize and study zero-shot abstractive summarization in Large\nLanguage Models (LLMs) by measuring position bias, which we propose as a\ngeneral formulation of the more restrictive lead bias phenomenon studied\npreviously in the literature. Position bias captures the tendency of a model\nunfairly prioritizing information from certain parts of the input text over\nothers, leading to undesirable behavior. Through numerous experiments on four\ndiverse real-world datasets, we study position bias in multiple LLM models such\nas GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained\nencoder-decoder abstractive summarization models such as Pegasus and BART. Our\nfindings lead to novel insights and discussion on performance and position bias\nof models for zero-shot summarization tasks.\n","authors":["Anshuman Chhabra","Hadi Askari","Prasant Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2401.01989v2.pdf","comment":"Accepted to NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2309.11063v2","updated":"2024-03-14T22:23:14Z","published":"2023-09-20T04:58:59Z","title":"XATU: A Fine-grained Instruction-based Benchmark for Explainable Text\n  Updates","summary":"  Text editing is a crucial task of modifying text to better align with user\nintents. However, existing text editing benchmark datasets contain only\ncoarse-grained instructions and lack explainability, thus resulting in outputs\nthat deviate from the intended changes outlined in the gold reference. To\ncomprehensively investigate the text editing capabilities of large language\nmodels (LLMs), this paper introduces XATU, the first benchmark specifically\ndesigned for fine-grained instruction-based explainable text editing. XATU\nconsiders finer-grained text editing tasks of varying difficulty\n(simplification, grammar check, fact-check, etc.), incorporating lexical,\nsyntactic, semantic, and knowledge-intensive edit aspects. To enhance\ninterpretability, we combine LLM-based annotation and human annotation,\nresulting in a benchmark that includes fine-grained instructions and\ngold-standard edit explanations. By evaluating existing LLMs against our\nbenchmark, we demonstrate the effectiveness of instruction tuning and the\nimpact of underlying architecture across various editing tasks. Furthermore,\nextensive experimentation reveals the significant role of explanations in\nfine-tuning language models for text editing tasks. The benchmark will be\nopen-sourced to support reproduction and facilitate future research\nat~\\url{https://github.com/megagonlabs/xatu}.\n","authors":["Haopeng Zhang","Hayate Iso","Sairam Gurajada","Nikita Bhutani"],"pdf_url":"https://arxiv.org/pdf/2309.11063v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09892v1","updated":"2024-03-14T21:55:17Z","published":"2024-03-14T21:55:17Z","title":"Geographically-Informed Language Identification","summary":"  This paper develops an approach to language identification in which the set\nof languages considered by the model depends on the geographic origin of the\ntext in question. Given that many digital corpora can be geo-referenced at the\ncountry level, this paper formulates 16 region-specific models, each of which\ncontains the languages expected to appear in countries within that region.\nThese regional models also each include 31 widely-spoken international\nlanguages in order to ensure coverage of these linguae francae regardless of\nlocation. An upstream evaluation using traditional language identification\ntesting data shows an improvement in f-score ranging from 1.7 points (Southeast\nAsia) to as much as 10.4 points (North Africa). A downstream evaluation on\nsocial media data shows that this improved performance has a significant impact\non the language labels which are applied to large real-world corpora. The\nresult is a highly-accurate model that covers 916 languages at a sample size of\n50 characters, the performance improved by incorporating geographic information\ninto the model.\n","authors":["Jonathan Dunn","Lane Edwards-Brown"],"pdf_url":"https://arxiv.org/pdf/2403.09892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09891v1","updated":"2024-03-14T21:52:26Z","published":"2024-03-14T21:52:26Z","title":"Fisher Mask Nodes for Language Model Merging","summary":"  Fine-tuning pre-trained models provides significant advantages in downstream\nperformance. The ubiquitous nature of pre-trained models such as BERT and its\nderivatives in natural language processing has also led to a proliferation of\ntask-specific fine-tuned models. As these models typically only perform one\ntask well, additional training or ensembling is required in multi-task\nscenarios. The growing field of model merging provides a solution, dealing with\nthe challenge of combining multiple task-specific models into a single\nmulti-task model. In this study, we introduce a novel model merging method for\nTransformers, combining insights from previous work in Fisher-weighted\naveraging and the use of Fisher information in model pruning. Utilizing the\nFisher information of mask nodes within the Transformer architecture, we devise\na computationally efficient weighted-averaging scheme. Our method exhibits a\nregular and significant performance increase across various models in the BERT\nfamily, outperforming full-scale Fisher-weighted averaging in a fraction of the\ncomputational cost, with baseline performance improvements of up to +6.5 and a\nspeedup of 57.4x. Our results prove the potential of our method in current\nmulti-task learning environments and suggest its scalability and adaptability\nto new model architectures and learning scenarios.\n","authors":["Thennal D K","Ganesh Nathan","Suchithra M S"],"pdf_url":"https://arxiv.org/pdf/2403.09891v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.04849v2","updated":"2024-03-14T21:46:37Z","published":"2023-09-09T17:30:35Z","title":"Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect\n  Representations","summary":"  We propose EmoDistill, a novel speech emotion recognition (SER) framework\nthat leverages cross-modal knowledge distillation during training to learn\nstrong linguistic and prosodic representations of emotion from speech. During\ninference, our method only uses a stream of speech signals to perform unimodal\nSER thus reducing computation overhead and avoiding run-time transcription and\nprosodic feature extraction errors. During training, our method distills\ninformation at both embedding and logit levels from a pair of pre-trained\nProsodic and Linguistic teachers that are fine-tuned for SER. Experiments on\nthe IEMOCAP benchmark demonstrate that our method outperforms other unimodal\nand multimodal techniques by a considerable margin, and achieves\nstate-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted\naccuracy. Detailed ablation studies demonstrate the impact of each component of\nour method.\n","authors":["Debaditya Shome","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2309.04849v2.pdf","comment":"Accepted at ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.09887v1","updated":"2024-03-14T21:44:48Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05359v3","updated":"2024-03-14T21:12:42Z","published":"2024-02-08T02:37:30Z","title":"Prompting Large Language Models with Divide-and-Conquer Program for\n  Discerning Problem Solving","summary":"  Foundation models, such as Large language Models (LLMs), have attracted\nsignificant amount of interest due to their large number of applications.\nExisting works show that appropriate prompt design, such as Chain-of-Thoughts,\ncan unlock LLM's powerful capacity in diverse areas. However, when handling\ntasks involving repetitive sub-tasks and/or deceptive contents, such as\narithmetic calculation and article-level fake news detection, existing\nprompting strategies either suffers from insufficient expressive power or\nintermediate errors triggered by hallucination. To make LLM more discerning to\nsuch intermediate errors, we propose to guide LLM with a Divide-and-Conquer\nprogram that simultaneously ensures superior expressive power and disentangles\ntask decomposition, sub-task resolution, and resolution assembly process.\nTheoretic analysis reveals that our strategy can guide LLM to extend the\nexpressive power of fixed-depth Transformer. Experiments indicate that our\nproposed method can achieve better performance than typical prompting\nstrategies in tasks bothered by intermediate errors and deceptive contents,\nsuch as large integer multiplication, hallucination detection and\nmisinformation detection.\n","authors":["Yizhou Zhang","Lun Du","Defu Cao","Qiang Fu","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2402.05359v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2308.07317v2","updated":"2024-03-14T20:56:23Z","published":"2023-08-14T17:59:56Z","title":"Platypus: Quick, Cheap, and Powerful Refinement of LLMs","summary":"  We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large\nLanguage Models (LLMs) that achieves the strongest performance and currently\nstands at first place in HuggingFace's Open LLM Leaderboard as of the release\ndate of this work. In this work we describe (1) our curated dataset\n$\\textbf{Open-Platypus}$, that is a subset of other open datasets and which\n$\\textit{we release to the public}$ (2) our process of fine-tuning and merging\nLoRA modules in order to conserve the strong prior of pretrained LLMs, while\nbringing specific domain knowledge to the surface (3) our efforts in checking\nfor test data leaks and contamination in the training data, which can inform\nfuture research. Specifically, the Platypus family achieves strong performance\nin quantitative LLM metrics across model sizes, topping the global Open LLM\nleaderboard while using just a fraction of the fine-tuning data and overall\ncompute that are required for other state-of-the-art fine-tuned LLMs. In\nparticular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU\nusing 25k questions in 5 hours. This is a testament of the quality of our\nOpen-Platypus dataset, and opens opportunities for more improvements in the\nfield. Project page: https://platypus-llm.github.io\n","authors":["Ariel N. Lee","Cole J. Hunter","Nataniel Ruiz"],"pdf_url":"https://arxiv.org/pdf/2308.07317v2.pdf","comment":"Workshop on Instruction Tuning and Instruction Following at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2403.09858v1","updated":"2024-03-14T20:39:26Z","published":"2024-03-14T20:39:26Z","title":"FakeWatch: A Framework for Detecting Fake News to Ensure Credible\n  Elections","summary":"  In today's technologically driven world, the rapid spread of fake news,\nparticularly during critical events like elections, poses a growing threat to\nthe integrity of information. To tackle this challenge head-on, we introduce\nFakeWatch, a comprehensive framework carefully designed to detect fake news.\nLeveraging a newly curated dataset of North American election-related news\narticles, we construct robust classification models. Our framework integrates a\nmodel hub comprising of both traditional machine learning (ML) techniques and\ncutting-edge Language Models (LMs) to discern fake news effectively. Our\noverarching objective is to provide the research community with adaptable and\nprecise classification models adept at identifying the ever-evolving landscape\nof misinformation. Quantitative evaluations of fake news classifiers on our\ndataset reveal that, while state-of-the-art LMs exhibit a slight edge over\ntraditional ML models, classical models remain competitive due to their balance\nof accuracy and computational efficiency. Additionally, qualitative analyses\nshed light on patterns within fake news articles. This research lays the\ngroundwork for future endeavors aimed at combating misinformation, particularly\nconcerning electoral processes. We provide our labeled data and model publicly\nfor use and reproducibility.\n","authors":["Shaina Raza","Tahniat Khan","Drai Paulen-Patterson","Veronica Chatrath","Mizanur Rahman","Oluwanifemi Bamgbose"],"pdf_url":"https://arxiv.org/pdf/2403.09858v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.03730"},{"id":"http://arxiv.org/abs/2305.07019v2","updated":"2024-03-14T20:30:52Z","published":"2023-05-11T17:57:49Z","title":"Musketeer: Joint Training for Multi-task Vision Language Model with Task\n  Explanation Prompts","summary":"  We present a vision-language model whose parameters are jointly trained on\nall tasks and fully shared among multiple heterogeneous tasks which may\ninterfere with each other, resulting in a single model which we named\nMusketeer. The integration of knowledge across heterogeneous tasks is enabled\nby a novel feature called Task Explanation Prompt (TEP). With rich and\nstructured information such as task input/output format, TEP reduces\ninterference among tasks, allowing the model to focus on their shared\nstructure. With a single model, Musketeer achieves results comparable to or\nbetter than strong baselines trained on single tasks, almost uniformly across\nmultiple tasks.\n","authors":["Zhaoyang Zhang","Yantao Shen","Kunyu Shi","Zhaowei Cai","Jun Fang","Siqi Deng","Hao Yang","Davide Modolo","Zhuowen Tu","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2305.07019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09849v1","updated":"2024-03-14T20:17:10Z","published":"2024-03-14T20:17:10Z","title":"Self-Consistency Boosts Calibration for Math Reasoning","summary":"  Calibration, which establishes the correlation between accuracy and model\nconfidence, is important for LLM development. We design three off-the-shelf\ncalibration methods based on self-consistency (Wang et al., 2022) for math\nreasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using\nstrong open-source LLMs (Mistral and LLaMA2), our methods better bridge model\nconfidence and accuracy than existing methods based on p(True) (Kadavath et\nal., 2022) or logit (Kadavath et al., 2022).\n","authors":["Ante Wang","Linfeng Song","Ye Tian","Baolin Peng","Lifeng Jin","Haitao Mi","Jinsong Su","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09832v1","updated":"2024-03-14T19:39:10Z","published":"2024-03-14T19:39:10Z","title":"Scaling Behavior of Machine Translation with Large Language Models under\n  Prompt Injection Attacks","summary":"  Large Language Models (LLMs) are increasingly becoming the preferred\nfoundation platforms for many Natural Language Processing tasks such as Machine\nTranslation, owing to their quality often comparable to or better than\ntask-specific models, and the simplicity of specifying the task through natural\nlanguage instructions or in-context examples. Their generality, however, opens\nthem up to subversion by end users who may embed into their requests\ninstructions that cause the model to behave in unauthorized and possibly unsafe\nways. In this work we study these Prompt Injection Attacks (PIAs) on multiple\nfamilies of LLMs on a Machine Translation task, focusing on the effects of\nmodel size on the attack success rates. We introduce a new benchmark data set\nand we discover that on multiple language pairs and injected prompts written in\nEnglish, larger models under certain conditions may become more susceptible to\nsuccessful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et\nal., 2023). To our knowledge, this is the first work to study non-trivial LLM\nscaling behaviour in a multi-lingual setting.\n","authors":["Zhifan Sun","Antonio Valerio Miceli-Barone"],"pdf_url":"https://arxiv.org/pdf/2403.09832v1.pdf","comment":"15 pages, 18 figures, First Workshop on the Scaling Behavior of Large\n  Language Models (SCALE-LLM 2024)"},{"id":"http://arxiv.org/abs/2310.00898v3","updated":"2024-03-14T19:27:23Z","published":"2023-10-02T04:29:40Z","title":"Enabling Language Models to Implicitly Learn Self-Improvement","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.\n","authors":["Ziqi Wang","Le Hou","Tianjian Lu","Yuexin Wu","Yunxuan Li","Hongkun Yu","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2310.00898v3.pdf","comment":"28 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.19119v2","updated":"2024-03-14T19:18:43Z","published":"2024-02-29T12:56:18Z","title":"VIXEN: Visual Text Comparison Network for Image Difference Captioning","summary":"  We present VIXEN - a technique that succinctly summarizes in text the visual\ndifferences between a pair of images in order to highlight any content\nmanipulation present. Our proposed network linearly maps image features in a\npairwise manner, constructing a soft prompt for a pretrained large language\nmodel. We address the challenge of low volume of training data and lack of\nmanipulation variety in existing image difference captioning (IDC) datasets by\ntraining on synthetically manipulated images from the recent InstructPix2Pix\ndataset generated via prompt-to-prompt editing framework. We augment this\ndataset with change summaries produced via GPT-3. We show that VIXEN produces\nstate-of-the-art, comprehensible difference captions for diverse image contents\nand edit types, offering a potential mitigation against misinformation\ndisseminated via manipulated image content. Code and data are available at\nhttp://github.com/alexblck/vixen\n","authors":["Alexander Black","Jing Shi","Yifei Fan","Tu Bui","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2402.19119v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2403.09795v1","updated":"2024-03-14T18:27:43Z","published":"2024-03-14T18:27:43Z","title":"Helpful or Harmful? Exploring the Efficacy of Large Language Models for\n  Online Grooming Prevention","summary":"  Powerful generative Large Language Models (LLMs) are becoming popular tools\namongst the general public as question-answering systems, and are being\nutilised by vulnerable groups such as children. With children increasingly\ninteracting with these tools, it is imperative for researchers to scrutinise\nthe safety of LLMs, especially for applications that could lead to serious\noutcomes, such as online child safety queries. In this paper, the efficacy of\nLLMs for online grooming prevention is explored both for identifying and\navoiding grooming through advice generation, and the impact of prompt design on\nmodel performance is investigated by varying the provided context and prompt\nspecificity. In results reflecting over 6,000 LLM interactions, we find that no\nmodels were clearly appropriate for online grooming prevention, with an\nobserved lack of consistency in behaviours, and potential for harmful answer\ngeneration, especially from open-source models. We outline where and how models\nfall short, providing suggestions for improvement, and identify prompt designs\nthat heavily altered model performance in troubling ways, with findings that\ncan be used to inform best practice usage guides.\n","authors":["Ellie Prosser","Matthew Edwards"],"pdf_url":"https://arxiv.org/pdf/2403.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09792v1","updated":"2024-03-14T18:24:55Z","published":"2024-03-14T18:24:55Z","title":"Images are Achilles' Heel of Alignment: Exploiting Visual\n  Vulnerabilities for Jailbreaking Multimodal Large Language Models","summary":"  In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models~(MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly\nreleased.\n","authors":["Yifan Li","Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09792v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2109.10952v2","updated":"2024-03-14T18:23:47Z","published":"2021-09-22T18:17:06Z","title":"Cross-linguistically Consistent Semantic and Syntactic Annotation of\n  Child-directed Speech","summary":"  This paper proposes a methodology for constructing such corpora of child\ndirected speech (CDS) paired with sentential logical forms, and uses this\nmethod to create two such corpora, in English and Hebrew. The approach enforces\na cross-linguistically consistent representation, building on recent advances\nin dependency representation and semantic parsing. Specifically, the approach\ninvolves two steps. First, we annotate the corpora using the Universal\nDependencies (UD) scheme for syntactic annotation, which has been developed to\napply consistently to a wide variety of domains and typologically diverse\nlanguages. Next, we further annotate these data by applying an automatic method\nfor transducing sentential logical forms (LFs) from UD structures. The UD and\nLF representations have complementary strengths: UD structures are\nlanguage-neutral and support consistent and reliable annotation by multiple\nannotators, whereas LFs are neutral as to their syntactic derivation and\ntransparently encode semantic relations.\n  Using this approach, we provide syntactic and semantic annotation for two\ncorpora from CHILDES: Brown's Adam corpus (English; we annotate ~80% of its\nchild-directed utterances), all child-directed utterances from Berman's Hagar\ncorpus (Hebrew). We verify the quality of the UD annotation using an\ninter-annotator agreement study, and manually evaluate the transduced meaning\nrepresentations. We then demonstrate the utility of the compiled corpora\nthrough (1) a longitudinal corpus study of the prevalence of different\nsyntactic and semantic phenomena in the CDS, and (2) applying an existing\ncomputational model of language acquisition to the two corpora and briefly\ncomparing the results across languages.\n","authors":["Ida Szubert","Omri Abend","Nathan Schneider","Samuel Gibbon","Louis Mahon","Sharon Goldwater","Mark Steedman"],"pdf_url":"https://arxiv.org/pdf/2109.10952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13011v2","updated":"2024-03-14T18:07:10Z","published":"2023-10-17T01:31:59Z","title":"Compositional preference models for aligning LMs","summary":"  As language models (LMs) become more capable, it is increasingly important to\nalign them with human preferences. However, the dominant paradigm for training\nPreference Models (PMs) for that purpose suffers from fundamental limitations,\nsuch as lack of transparency and scalability, along with susceptibility to\noverfitting the preference dataset. We propose Compositional Preference Models\n(CPMs), a novel PM framework that decomposes one global preference assessment\ninto several interpretable features, obtains scalar scores for these features\nfrom a prompted LM, and aggregates these scores using a logistic regression\nclassifier. Through these simple steps, CPMs allow to control which properties\nof the preference data are used to train the preference model and to build it\nbased on features that are believed to underlie the human preference judgment.\nOur experiments show that CPMs not only improve generalization and are more\nrobust to overoptimization than standard PMs, but also that best-of-n samples\nobtained using CPMs tend to be preferred over samples obtained using\nconventional PMs. Overall, our approach demonstrates the benefits of endowing\nPMs with priors about which features determine human preferences while relying\non LM capabilities to extract those features in a scalable and robust way.\n","authors":["Dongyoung Go","Tomasz Korbak","Germán Kruszewski","Jos Rozen","Marc Dymetman"],"pdf_url":"https://arxiv.org/pdf/2310.13011v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.09762v1","updated":"2024-03-14T15:58:13Z","published":"2024-03-14T15:58:13Z","title":"Emotional Intelligence Through Artificial Intelligence : NLP and Deep\n  Learning in the Analysis of Healthcare Texts","summary":"  This manuscript presents a methodical examination of the utilization of\nArtificial Intelligence in the assessment of emotions in texts related to\nhealthcare, with a particular focus on the incorporation of Natural Language\nProcessing and deep learning technologies. We scrutinize numerous research\nstudies that employ AI to augment sentiment analysis, categorize emotions, and\nforecast patient outcomes based on textual information derived from clinical\nnarratives, patient feedback on medications, and online health discussions. The\nreview demonstrates noteworthy progress in the precision of algorithms used for\nsentiment classification, the prognostic capabilities of AI models for\nneurodegenerative diseases, and the creation of AI-powered systems that offer\nsupport in clinical decision-making. Remarkably, the utilization of AI\napplications has exhibited an enhancement in personalized therapy plans by\nintegrating patient sentiment and contributing to the early identification of\nmental health disorders. There persist challenges, which encompass ensuring the\nethical application of AI, safeguarding patient confidentiality, and addressing\npotential biases in algorithmic procedures. Nevertheless, the potential of AI\nto revolutionize healthcare practices is unmistakable, offering a future where\nhealthcare is not only more knowledgeable and efficient but also more\nempathetic and centered around the needs of patients. This investigation\nunderscores the transformative influence of AI on healthcare, delivering a\ncomprehensive comprehension of its role in examining emotional content in\nhealthcare texts and highlighting the trajectory towards a more compassionate\napproach to patient care. The findings advocate for a harmonious synergy\nbetween AI's analytical capabilities and the human aspects of healthcare.\n","authors":["Prashant Kumar Nag","Amit Bhagat","R. Vishnu Priya","Deepak kumar Khare"],"pdf_url":"https://arxiv.org/pdf/2403.09762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09751v1","updated":"2024-03-14T09:38:12Z","published":"2024-03-14T09:38:12Z","title":"What Was Your Prompt? A Remote Keylogging Attack on AI Assistants","summary":"  AI assistants are becoming an integral part of society, used for asking\nadvice or help in personal and confidential issues. In this paper, we unveil a\nnovel side-channel that can be used to read encrypted responses from AI\nAssistants over the web: the token-length side-channel. We found that many\nvendors, including OpenAI and Microsoft, have this side-channel.\n  However, inferring the content of a response from a token-length sequence\nalone proves challenging. This is because tokens are akin to words, and\nresponses can be several sentences long leading to millions of grammatically\ncorrect sentences. In this paper, we show how this can be overcome by (1)\nutilizing the power of a large language model (LLM) to translate these\nsequences, (2) providing the LLM with inter-sentence context to narrow the\nsearch space and (3) performing a known-plaintext attack by fine-tuning the\nmodel on the target model's writing style.\n  Using these methods, we were able to accurately reconstruct 29\\% of an AI\nassistant's responses and successfully infer the topic from 55\\% of them. To\ndemonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and\nMicrosoft's Copilot on both browser and API traffic.\n","authors":["Roy Weiss","Daniel Ayzenshteyn","Guy Amit","Yisroel Mirsky"],"pdf_url":"https://arxiv.org/pdf/2403.09751v1.pdf","comment":null}]},"2024-03-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2204.09601v2","updated":"2024-03-15T17:59:17Z","published":"2022-03-08T21:20:19Z","title":"Extraction of Sleep Information from Clinical Notes of Patients with\n  Alzheimer's Disease Using Natural Language Processing","summary":"  Alzheimer's Disease (AD) is the most common form of dementia in the United\nStates. Sleep is one of the lifestyle-related factors that has been shown\ncritical for optimal cognitive function in old age. However, there is a lack of\nresearch studying the association between sleep and AD incidence. A major\nbottleneck for conducting such research is that the traditional way to acquire\nsleep information is time-consuming, inefficient, non-scalable, and limited to\npatients' subjective experience. A gold standard dataset is created from manual\nannotation of 570 randomly sampled clinical note documents from the adSLEEP, a\ncorpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved\nfrom the University of Pittsburgh Medical Center (UPMC). We developed a\nrule-based Natural Language Processing (NLP) algorithm, machine learning\nmodels, and Large Language Model(LLM)-based NLP algorithms to automate the\nextraction of sleep-related concepts, including snoring, napping, sleep\nproblem, bad sleep quality, daytime sleepiness, night wakings, and sleep\nduration, from the gold standard dataset. Rule-based NLP algorithm achieved the\nbest performance of F1 across all sleep-related concepts. In terms of Positive\nPredictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime\nsleepiness and sleep duration, machine learning models: 0.95 and for napping,\n0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning\nachieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for\nsleep duration. The results show that the rule-based NLP algorithm consistently\nachieved the best performance for all sleep concepts. This study focused on the\nclinical notes of patients with AD, but could be extended to general sleep\ninformation extraction for other diseases.\n","authors":["Sonish Sivarajkumar","Thomas Yu CHow Tam","Haneef Ahamed Mohammad","Samual Viggiano","David Oniani","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2204.09601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15302v4","updated":"2024-03-15T17:57:58Z","published":"2024-02-23T13:03:12Z","title":"How (un)ethical are instruction-centric responses of LLMs? Unveiling the\n  vulnerabilities of safety guardrails to harmful queries","summary":"  In this study, we tackle a growing concern around the safety and ethical use\nof large language models (LLMs). Despite their potential, these models can be\ntricked into producing harmful or unethical content through various\nsophisticated methods, including 'jailbreaking' techniques and targeted\nmanipulation. Our work zeroes in on a specific issue: to what extent LLMs can\nbe led astray by asking them to generate responses that are instruction-centric\nsuch as a pseudocode, a program or a software snippet as opposed to vanilla\ntext. To investigate this question, we introduce TechHazardQA, a dataset\ncontaining complex queries which should be answered in both text and\ninstruction-centric formats (e.g., pseudocodes), aimed at identifying triggers\nfor unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,\nMistral-V2 and Mistral 8X7B -- and ask them to generate both text and\ninstruction-centric responses. For evaluation we report the harmfulness score\nmetric as well as judgements from GPT-4 and humans. Overall, we observe that\nasking LLMs to produce instruction-centric responses enhances the unethical\nresponse generation by ~2-38% across the models. As an additional objective, we\ninvestigate the impact of model editing using the ROME technique, which further\nincreases the propensity for generating undesirable content. In particular,\nasking edited LLMs to generate instruction-centric responses further increases\nthe unethical response generation by ~3-16% across the different models.\n","authors":["Somnath Banerjee","Sayan Layek","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.15302v4.pdf","comment":"Under review.\n  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}"},{"id":"http://arxiv.org/abs/2403.10517v1","updated":"2024-03-15T17:57:52Z","published":"2024-03-15T17:57:52Z","title":"VideoAgent: Long-form Video Understanding with Large Language Model as\n  Agent","summary":"  Long-form video understanding represents a significant challenge within\ncomputer vision, demanding a model capable of reasoning over long multi-modal\nsequences. Motivated by the human cognitive process for long-form video\nunderstanding, we emphasize interactive reasoning and planning over the ability\nto process lengthy visual inputs. We introduce a novel agent-based system,\nVideoAgent, that employs a large language model as a central agent to\niteratively identify and compile crucial information to answer a question, with\nvision-language foundation models serving as tools to translate and retrieve\nvisual information. Evaluated on the challenging EgoSchema and NExT-QA\nbenchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only\n8.4 and 8.2 frames used on average. These results demonstrate superior\neffectiveness and efficiency of our method over the current state-of-the-art\nmethods, highlighting the potential of agent-based approaches in advancing\nlong-form video understanding.\n","authors":["Xiaohan Wang","Yuhui Zhang","Orr Zohar","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.10517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13466v2","updated":"2024-03-15T17:55:52Z","published":"2023-03-22T13:46:16Z","title":"Mining Clinical Notes for Physical Rehabilitation Exercise Information:\n  Natural Language Processing Algorithm Development and Validation Study","summary":"  Post-stroke patient rehabilitation requires precise, personalized treatment\nplans. Natural Language Processing (NLP) offers potential to extract valuable\nexercise information from clinical notes, aiding in the development of more\neffective rehabilitation strategies. Objective: This study aims to develop and\nevaluate a variety of NLP algorithms to extract and categorize physical\nrehabilitation exercise information from the clinical notes of post-stroke\npatients treated at the University of Pittsburgh Medical Center. A cohort of\n13,605 patients diagnosed with stroke was identified, and their clinical notes\ncontaining rehabilitation therapy notes were retrieved. A comprehensive\nclinical ontology was created to represent various aspects of physical\nrehabilitation exercises. State-of-the-art NLP algorithms were then developed\nand compared, including rule-based, machine learning-based algorithms, and\nlarge language model (LLM)-based algorithms (ChatGPT). Analysis was conducted\non a dataset comprising 23,724 notes with detailed demographic and clinical\ncharacteristics. The rule-based NLP algorithm demonstrated superior performance\nin most areas, particularly in detecting the 'Right Side' location with an F1\nscore of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting\nexcelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing\nrule-based NLP by 0.023. It also showed notable performance in 'Passive Range\nof Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.\nThe rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'\nwith F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot\nprompts, achieved high recall but generally lower precision and F1 scores.\nHowever, it notably excelled in 'Backward Plane' motion detection, achieving an\nF1 score of 0.846, surpassing the rule-based algorithm's 0.720.\n","authors":["Sonish Sivarajkumar","Fengyi Gao","Parker E. Denny","Bayan M. Aldhahwani","Shyam Visweswaran","Allyn Bove","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v3","updated":"2024-03-15T17:48:05Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10499v1","updated":"2024-03-15T17:33:49Z","published":"2024-03-15T17:33:49Z","title":"Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A\n  Pilot Study","summary":"  Pre-training image representations from the raw text about images enables\nzero-shot vision transfer to downstream tasks. Through pre-training on millions\nof samples collected from the internet, multimodal foundation models, such as\nCLIP, produce state-of-the-art zero-shot results that often reach\ncompetitiveness with fully supervised methods without the need for\ntask-specific training. Besides the encouraging performance on classification\naccuracy, it is reported that these models close the robustness gap by matching\nthe performance of supervised models trained on ImageNet under natural\ndistribution shift. Because robustness is critical to real-world applications,\nespecially safety-critical ones, in this paper, we present a comprehensive\nevaluation based on a large-scale robustness benchmark covering 7 natural, 3\nsynthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a\npilot study. We show that CLIP leads to a significant robustness drop compared\nto supervised ImageNet models on our benchmark, especially under synthetic\ndistribution shift and adversarial attacks. Furthermore, data overlap analysis\nsuggests that the observed robustness under natural distribution shifts could\nbe attributed, at least in part, to data overlap. In summary, our evaluation\nshows a comprehensive evaluation of robustness is necessary; and there is a\nsignificant need to improve the robustness of zero-shot multimodal models.\n","authors":["Chenguang Wang","Ruoxi Jia","Xin Liu","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2403.10499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19791v4","updated":"2024-03-15T16:55:47Z","published":"2023-10-30T17:55:02Z","title":"LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code","summary":"  While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.\n","authors":["Gabriel Grand","Lionel Wong","Maddy Bowers","Theo X. Olausson","Muxin Liu","Joshua B. Tenenbaum","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2310.19791v4.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2310.18913v3","updated":"2024-03-15T16:39:26Z","published":"2023-10-29T05:50:03Z","title":"Debiasing Algorithm through Model Adaptation","summary":"  Large language models are becoming the go-to solution for the ever-growing\nnumber of tasks. However, with growing capacity, models are prone to rely on\nspurious correlations stemming from biases and stereotypes present in the\ntraining data. This work proposes a novel method for detecting and mitigating\ngender bias in language models. We perform causal analysis to identify\nproblematic model components and discover that mid-upper feed-forward layers\nare most prone to convey bias. Based on the analysis results, we intervene in\nthe model by applying a linear projection to the weight matrices of these\nlayers. Our titular method, DAMA, significantly decreases bias as measured by\ndiverse metrics while maintaining the model's performance on downstream tasks.\nWe release code for our method and models, which retrain LLaMA's\nstate-of-the-art performance while being significantly less biased.\n","authors":["Tomasz Limisiewicz","David Mareček","Tomáš Musil"],"pdf_url":"https://arxiv.org/pdf/2310.18913v3.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2403.10446v1","updated":"2024-03-15T16:30:14Z","published":"2024-03-15T16:30:14Z","title":"Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A\n  Case Study on Domain-Specific Queries in Private Knowledge-Bases","summary":"  We proposed an end-to-end system design towards utilizing Retrieval Augmented\nGeneration (RAG) to improve the factual accuracy of Large Language Models\n(LLMs) for domain-specific and time-sensitive queries related to private\nknowledge-bases. Our system integrates RAG pipeline with upstream datasets\nprocessing and downstream performance evaluation. Addressing the challenge of\nLLM hallucinations, we finetune models with a curated dataset which originates\nfrom CMU's extensive resources and annotated with the teacher model. Our\nexperiments demonstrate the system's effectiveness in generating more accurate\nanswers to domain-specific and time-sensitive inquiries. The results also\nrevealed the limitations of fine-tuning LLMs with small-scale and skewed\ndatasets. This research highlights the potential of RAG systems in augmenting\nLLMs with external datasets for improved performance in knowledge-intensive\ntasks. Our code and models are available on Github.\n","authors":["Jiarui Li","Ye Yuan","Zehua Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10446v1.pdf","comment":"These authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.10444v1","updated":"2024-03-15T16:28:22Z","published":"2024-03-15T16:28:22Z","title":"Optimal Block-Level Draft Verification for Accelerating Speculative\n  Decoding","summary":"  Speculative decoding has shown to be an effective method for lossless\nacceleration of large language models (LLMs) during inference. In each\niteration, the algorithm first uses a smaller model to draft a block of tokens.\nThe tokens are then verified by the large model in parallel and only a subset\nof tokens will be kept to guarantee that the final output follows the\ndistribution of the large model. In all of the prior speculative decoding\nworks, the draft verification is performed token-by-token independently. In\nthis work, we propose a better draft verification algorithm that provides\nadditional wall-clock speedup without incurring additional computation cost and\ndraft tokens. We first formulate the draft verification step as a block-level\noptimal transport problem. The block-level formulation allows us to consider a\nwider range of draft verification algorithms and obtain a higher number of\naccepted tokens in expectation in one draft block. We propose a verification\nalgorithm that achieves the optimal accepted length for the block-level\ntransport problem. We empirically evaluate our proposed block-level\nverification algorithm in a wide range of tasks and datasets, and observe\nconsistent improvements in wall-clock speedup when compared to token-level\nverification algorithm. To the best of our knowledge, our work is the first to\nestablish improvement over speculative decoding through a better draft\nverification algorithm.\n","authors":["Ziteng Sun","Jae Hun Ro","Ahmad Beirami","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2403.10444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12192v3","updated":"2024-03-15T16:03:46Z","published":"2022-12-23T08:23:32Z","title":"CinPatent: Datasets for Patent Classification","summary":"  Patent classification is the task that assigns each input patent into several\ncodes (classes). Due to its high demand, several datasets and methods have been\nintroduced. However, the lack of both systematic performance comparison of\nbaselines and access to some datasets creates a gap for the task. To fill the\ngap, we introduce two new datasets in English and Japanese collected by using\nCPC codes. The English dataset includes 45,131 patent documents with 425 labels\nand the Japanese dataset contains 54,657 documents with 523 labels. To\nfacilitate the next studies, we compare the performance of strong multi-label\ntext classification methods on the two datasets. Experimental results show that\nAttentionXML is consistently better than other strong baselines. The ablation\nstudy is also conducted in two aspects: the contribution of different parts\n(title, abstract, description, and claims) of a patent and the behavior of\nbaselines in terms of performance with different training data segmentation. We\nrelease the two new datasets with the code of the baselines.\n","authors":["Minh-Tien Nguyen","Nhung Bui","Manh Tran-Tien","Linh Le","Huy-The Vu"],"pdf_url":"https://arxiv.org/pdf/2212.12192v3.pdf","comment":"This paper describes an on-going work"},{"id":"http://arxiv.org/abs/2309.02427v3","updated":"2024-03-15T15:44:11Z","published":"2023-09-05T17:56:20Z","title":"Cognitive Architectures for Language Agents","summary":"  Recent efforts have augmented large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning, leading to a new class of\nlanguage agents. While these agents have achieved substantial empirical\nsuccess, we lack a systematic framework to organize existing agents and plan\nfuture developments. In this paper, we draw on the rich history of cognitive\nscience and symbolic artificial intelligence to propose Cognitive Architectures\nfor Language Agents (CoALA). CoALA describes a language agent with modular\nmemory components, a structured action space to interact with internal memory\nand external environments, and a generalized decision-making process to choose\nactions. We use CoALA to retrospectively survey and organize a large body of\nrecent work, and prospectively identify actionable directions towards more\ncapable agents. Taken together, CoALA contextualizes today's language agents\nwithin the broader history of AI and outlines a path towards language-based\ngeneral intelligence.\n","authors":["Theodore R. Sumers","Shunyu Yao","Karthik Narasimhan","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2309.02427v3.pdf","comment":"v3 is TMLR camera ready version. 19 pages of main content, 5 figures.\n  The first two authors contributed equally, order decided by coin flip. A\n  CoALA-based repo of recent work on language agents:\n  https://github.com/ysymyth/awesome-language-agents"},{"id":"http://arxiv.org/abs/2311.10112v2","updated":"2024-03-15T15:38:07Z","published":"2023-11-15T21:25:15Z","title":"zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with\n  Large Language Models","summary":"  Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become\na heated topic. Various methods have been proposed to forecast links on TKGs.\nMost of them are embedding-based, where hidden representations are learned to\nrepresent knowledge graph (KG) entities and relations based on the observed\ngraph contexts. Although these methods show strong performance on traditional\nTKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the\nunseen zero-shot relations that have no prior graph context. In this paper, we\ntry to mitigate this problem as follows. We first input the text descriptions\nof KG relations into large language models (LLMs) for generating relation\nrepresentations, and then introduce them into embedding-based TKGF methods.\nLLM-empowered representations can capture the semantic information in the\nrelation descriptions. This makes the relations, whether seen or unseen, with\nsimilar semantic meanings stay close in the embedding space, enabling TKGF\nmodels to recognize zero-shot relations even without any observed graph\ncontext. Experimental results show that our approach helps TKGF models to\nachieve much better performance in forecasting the facts with previously unseen\nrelations, while still maintaining their ability in link forecasting regarding\nseen relations.\n","authors":["Zifeng Ding","Heling Cai","Jingpei Wu","Yunpu Ma","Ruotong Liao","Bo Xiong","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2311.10112v2.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.10381v1","updated":"2024-03-15T15:10:41Z","published":"2024-03-15T15:10:41Z","title":"Monotonic Representation of Numeric Properties in Language Models","summary":"  Language models (LMs) can express factual knowledge involving numeric\nproperties such as Karl Popper was born in 1902. However, how this information\nis encoded in the model's internal representations is not understood well.\nHere, we introduce a simple method for finding and editing representations of\nnumeric properties such as an entity's birth year. Empirically, we find\nlow-dimensional subspaces that encode numeric properties monotonically, in an\ninterpretable and editable fashion. When editing representations along\ndirections in these subspaces, LM output changes accordingly. For example, by\npatching activations along a \"birthyear\" direction we can make the LM express\nan increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was\nborn in 1957, Karl Popper was born in 1968. Property-encoding directions exist\nacross several numeric properties in all models under consideration, suggesting\nthe possibility that monotonic representation of numeric properties\nconsistently emerges during LM pretraining. Code:\nhttps://github.com/bheinzerling/numeric-property-repr\n","authors":["Benjamin Heinzerling","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2403.10381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10378v1","updated":"2024-03-15T15:08:39Z","published":"2024-03-15T15:08:39Z","title":"EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for\n  Evaluating Vision Language Models","summary":"  We introduce EXAMS-V, a new challenging multi-discipline multimodal\nmultilingual exam benchmark for evaluating vision language models. It consists\nof 20,932 multiple-choice questions across 20 school disciplines covering\nnatural science, social science, and other miscellaneous studies, e.g.,\nreligion, fine arts, business, etc. EXAMS-V includes a variety of multimodal\nfeatures such as text, images, tables, figures, diagrams, maps, scientific\nsymbols, and equations. The questions come in 11 languages from 7 language\nfamilies. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering\nschool exam questions from various countries, with a variety of education\nsystems. This distinctive approach calls for intricate reasoning across diverse\nlanguages and relies on region-specific knowledge. Solving the problems in the\ndataset requires advanced perception and joint reasoning over the text and the\nvisual content of the image. Our evaluation results demonstrate that this is a\nchallenging dataset, which is difficult even for advanced vision-text models\nsuch as GPT-4V and Gemini; this underscores the inherent complexity of the\ndataset and its significance as a future benchmark.\n","authors":["Rocktim Jyoti Das","Simeon Emilov Hristov","Haonan Li","Dimitar Iliyanov Dimitrov","Ivan Koychev","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2403.10378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07516v3","updated":"2024-03-15T15:03:40Z","published":"2023-06-30T17:05:11Z","title":"Voting-based Multimodal Automatic Deception Detection","summary":"  Automatic Deception Detection has been a hot research topic for a long time,\nusing machine learning and deep learning to automatically detect deception,\nbrings new light to this old field. In this paper, we proposed a voting-based\nmethod for automatic deception detection from videos using audio, visual and\nlexical features. Experiments were done on two datasets, the Real-life trial\ndataset by Michigan University and the Miami University deception detection\ndataset. Video samples were split into frames of images, audio, and\nmanuscripts. Our Voting-based Multimodal proposed solution consists of three\nmodels. The first model is CNN for detecting deception from images, the second\nmodel is Support Vector Machine (SVM) on Mel spectrograms for detecting\ndeception from audio and the third model is Word2Vec on Support Vector Machine\n(SVM) for detecting deception from manuscripts. Our proposed solution\noutperforms state of the art. Best results achieved on images, audio and text\nwere 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%\non video, audio and text respectively on Miami University Deception Detection.\n","authors":["Lana Touma","Mohammad Al Horani","Manar Tailouni","Anas Dahabiah","Khloud Al Jallad"],"pdf_url":"https://arxiv.org/pdf/2307.07516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10351v1","updated":"2024-03-15T14:36:38Z","published":"2024-03-15T14:36:38Z","title":"TriSum: Learning Summarization Ability from Large Language Models with\n  Structured Rationale","summary":"  The advent of large language models (LLMs) has significantly advanced natural\nlanguage processing tasks like text summarization. However, their large size\nand computational demands, coupled with privacy concerns in data transmission,\nlimit their use in resource-constrained and privacy-centric settings. To\novercome this, we introduce TriSum, a framework for distilling LLMs' text\nsummarization abilities into a compact, local model. Initially, LLMs extract a\nset of aspect-triple rationales and summaries, which are refined using a\ndual-scoring method for quality. Next, a smaller local model is trained with\nthese tasks, employing a curriculum learning strategy that evolves from simple\nto complex tasks. Our method enhances local model performance on various\nbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by\n4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by\nproviding insights into the summarization rationale.\n","authors":["Pengcheng Jiang","Cao Xiao","Zifeng Wang","Parminder Bhatia","Jimeng Sun","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2403.10351v1.pdf","comment":"NAACL'24"},{"id":"http://arxiv.org/abs/2403.10338v1","updated":"2024-03-15T14:25:59Z","published":"2024-03-15T14:25:59Z","title":"Investigating grammatical abstraction in language models using few-shot\n  learning of novel noun gender","summary":"  Humans can learn a new word and infer its grammatical properties from very\nfew examples. They have an abstract notion of linguistic properties like\ngrammatical gender and agreement rules that can be applied to novel syntactic\ncontexts and words. Drawing inspiration from psycholinguistics, we conduct a\nnoun learning experiment to assess whether an LSTM and a decoder-only\ntransformer can achieve human-like abstraction of grammatical gender in French.\nLanguage models were tasked with learning the gender of a novel noun embedding\nfrom a few examples in one grammatical agreement context and predicting\nagreement in another, unseen context. We find that both language models\neffectively generalise novel noun gender from one to two learning examples and\napply the learnt gender across agreement contexts, albeit with a bias for the\nmasculine gender category. Importantly, the few-shot updates were only applied\nto the embedding layers, demonstrating that models encode sufficient gender\ninformation within the word embedding space. While the generalisation behaviour\nof models suggests that they represent grammatical gender as an abstract\ncategory, like humans, further work is needed to explore the details of how\nexactly this is implemented. For a comparative perspective with human\nbehaviour, we conducted an analogous one-shot novel noun gender learning\nexperiment, which revealed that native French speakers, like language models,\nalso exhibited a masculine gender bias and are not excellent one-shot learners\neither.\n","authors":["Priyanka Sukumaran","Conor Houghton","Nina Kazanina"],"pdf_url":"https://arxiv.org/pdf/2403.10338v1.pdf","comment":"EACL 2024; Findings of the Association for Computational Linguistics"},{"id":"http://arxiv.org/abs/2403.10326v1","updated":"2024-03-15T14:14:26Z","published":"2024-03-15T14:14:26Z","title":"CDGP: Automatic Cloze Distractor Generation based on Pre-trained\n  Language Model","summary":"  Manually designing cloze test consumes enormous time and efforts. The major\nchallenge lies in wrong option (distractor) selection. Having carefully-design\ndistractors improves the effectiveness of learner ability assessment. As a\nresult, the idea of automatically generating cloze distractor is motivated. In\nthis paper, we investigate cloze distractor generation by exploring the\nemployment of pre-trained language models (PLMs) as an alternative for\ncandidate distractor generation. Experiments show that the PLM-enhanced model\nbrings a substantial performance improvement. Our best performing model\nadvances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our\ncode and dataset is available at https://github.com/AndyChiangSH/CDGP.\n","authors":["Shang-Hsuan Chiang","Ssu-Cheng Wang","Yao-Chung Fan"],"pdf_url":"https://arxiv.org/pdf/2403.10326v1.pdf","comment":"Findings of short paper, EMNLP 2022"},{"id":"http://arxiv.org/abs/2403.10301v1","updated":"2024-03-15T13:43:47Z","published":"2024-03-15T13:43:47Z","title":"Uni-SMART: Universal Science Multimodal Analysis and Research\n  Transformer","summary":"  In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as molecular\nstructure, tables, and charts, which are hard for text-focused LLMs to\nunderstand and analyze. This issue points to the urgent need for new solutions\nthat can fully understand and analyze multimodal content in scientific\nliterature. To answer this demand, we present Uni-SMART (Universal Science\nMultimodal Analysis and Research Transformer), an innovative model designed for\nin-depth understanding of multimodal scientific literature. Through rigorous\nquantitative evaluation across several domains, Uni-SMART demonstrates superior\nperformance over leading text-focused LLMs. Furthermore, our exploration\nextends to practical applications, including patent infringement detection and\nnuanced analysis of charts. These applications not only highlight Uni-SMART's\nadaptability but also its potential to revolutionize how we interact with\nscientific literature.\n","authors":["Hengxing Cai","Xiaochen Cai","Shuwen Yang","Jiankun Wang","Lin Yao","Zhifeng Gao","Junhan Chang","Sihang Li","Mingjun Xu","Changxin Wang","Hongshuai Wang","Yongge Li","Mujie Lin","Yaqi Li","Yuqi Yin","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2403.10301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10293v1","updated":"2024-03-15T13:33:10Z","published":"2024-03-15T13:33:10Z","title":"MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank","summary":"  Despite the success of the Universal Dependencies (UD) project exemplified by\nits impressive language breadth, there is still a lack in `within-language\nbreadth': most treebanks focus on standard languages. Even for German, the\nlanguage with the most annotations in UD, so far no treebank exists for one of\nits language varieties spoken by over 10M people: Bavarian. To contribute to\nclosing this gap, we present the first multi-dialect Bavarian treebank\n(MaiBaam) manually annotated with part-of-speech and syntactic dependency\ninformation in UD, covering multiple text genres (wiki, fiction, grammar\nexamples, social, non-fiction). We highlight the morphosyntactic differences\nbetween the closely-related Bavarian and German and showcase the rich\nvariability of speakers' orthographies. Our corpus includes 15k tokens,\ncovering dialects from all Bavarian-speaking areas spanning three countries. We\nprovide baseline parsing and POS tagging results, which are lower than results\nobtained on German and vary substantially between different graph-based\nparsers. To support further research on Bavarian syntax, we make our dataset,\nlanguage-specific guidelines and code publicly available.\n","authors":["Verena Blaschke","Barbara Kovačić","Siyao Peng","Hinrich Schütze","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.10293v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.01976v2","updated":"2024-03-15T13:27:31Z","published":"2024-03-04T12:19:28Z","title":"SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis","summary":"  Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nnatural language understanding and generation, igniting a surge of interest in\nleveraging these technologies in the field of scientific literature analysis.\nExisting benchmarks, however, inadequately evaluate the proficiency of LLMs in\nscientific literature analysis, especially in scenarios involving complex\ncomprehension and multimodal data. In response, we introduced SciAssess, a\nbenchmark tailored for the in-depth analysis of scientific literature, crafted\nto provide a thorough assessment of LLMs' efficacy. SciAssess focuses on\nevaluating LLMs' abilities in memorization, comprehension, and analysis within\nthe context of scientific literature analysis. It includes representative tasks\nfrom diverse scientific fields, such as general chemistry, organic materials,\nand alloy materials. And rigorous quality control measures ensure its\nreliability in terms of correctness, anonymization, and copyright compliance.\nSciAssess evaluates leading LLMs, including GPT-4, GPT-3.5, and Gemini,\nidentifying their strengths and aspects for improvement and supporting the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are made available at https://sci-assess.github.io,\noffering a valuable tool for advancing LLM capabilities in scientific\nliterature analysis.\n","authors":["Hengxing Cai","Xiaochen Cai","Junhan Chang","Sihang Li","Lin Yao","Changxin Wang","Zhifeng Gao","Hongshuai Wang","Yongge Li","Mujie Lin","Shuwen Yang","Jiankun Wang","Yuqi Yin","Yaqi Li","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2403.01976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10281v1","updated":"2024-03-15T13:24:28Z","published":"2024-03-15T13:24:28Z","title":"Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification\n  with Fine-Tuning","summary":"  In this paper, we present Pre-CoFactv3, a comprehensive framework comprised\nof Question Answering and Text Classification components for fact verification.\nLeveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and\nthe FakeNet model, we address the challenges of fact verification. Our\nexperiments explore diverse approaches, comparing different Pre-trained LLMs,\nintroducing FakeNet, and implementing various ensemble methods. Notably, our\nteam, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop,\nsurpassing the baseline accuracy by 103% and maintaining a 70% lead over the\nsecond competitor. This success underscores the efficacy of our approach and\nits potential contributions to advancing fact verification research.\n","authors":["Shang-Hsuan Chiang","Ming-Chih Lo","Lin-Wei Chao","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2403.10281v1.pdf","comment":"Accepted by AAAI 2024 Workshop: FACTIFY 3.0 - Workshop Series on\n  Multimodal Fact-Checking and Hate Speech Detection"},{"id":"http://arxiv.org/abs/2403.08604v2","updated":"2024-03-15T13:23:34Z","published":"2024-03-13T15:13:44Z","title":"DevBench: A Comprehensive Benchmark for Software Development","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced their coding capabilities. However, existing benchmarks predominantly\nfocused on simplified or isolated aspects of programming, such as single-file\ncode generation or repository issue debugging, falling short of measuring the\nfull spectrum of challenges raised by real-world programming activities. To\nthis end, we propose DevBench, a comprehensive benchmark that evaluates LLMs\nacross various stages of the software development lifecycle, including software\ndesign, environment setup, implementation, acceptance testing, and unit\ntesting. DevBench features a wide range of programming languages and domains,\nhigh-quality data collection, and carefully designed and verified metrics for\neach task. Empirical studies show that current LLMs, including GPT-4-Turbo,\nfail to solve the challenges presented within DevBench. Analyses reveal that\nmodels struggle with understanding the complex structures in the repository,\nmanaging the compilation process, and grasping advanced programming concepts.\nOur findings offer actionable insights for the future development of LLMs\ntoward real-world programming applications. Our benchmark is available at\nhttps://github.com/open-compass/DevBench\n","authors":["Bowen Li","Wenhan Wu","Ziwei Tang","Lin Shi","John Yang","Jinyang Li","Shunyu Yao","Chen Qian","Binyuan Hui","Qicheng Zhang","Zhiyin Yu","He Du","Ping Yang","Dahua Lin","Chao Peng","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08604v2.pdf","comment":"Our data and code are available at\n  https://github.com/open-compass/DevBench"},{"id":"http://arxiv.org/abs/2403.10275v1","updated":"2024-03-15T13:15:23Z","published":"2024-03-15T13:15:23Z","title":"A Question on the Explainability of Large Language Models and the\n  Word-Level Univariate First-Order Plausibility Assumption","summary":"  The explanations of large language models have recently been shown to be\nsensitive to the randomness used for their training, creating a need to\ncharacterize this sensitivity. In this paper, we propose a characterization\nthat questions the possibility to provide simple and informative explanations\nfor such models. To this end, we give statistical definitions for the\nexplanations' signal, noise and signal-to-noise ratio. We highlight that, in a\ntypical case study where word-level univariate explanations are analyzed with\nfirst-order statistical tools, the explanations of simple feature-based models\ncarry more signal and less noise than those of transformer ones. We then\ndiscuss the possibility to improve these results with alternative definitions\nof signal and noise that would capture more complex explanations and analysis\nmethods, while also questioning the tradeoff with their plausibility for\nreaders.\n","authors":["Jeremie Bogaert","Francois-Xavier Standaert"],"pdf_url":"https://arxiv.org/pdf/2403.10275v1.pdf","comment":"7 pages, 10 figures, Accepted and presented at AAAI 2024 (ReLM\n  workshop)"},{"id":"http://arxiv.org/abs/2403.10258v1","updated":"2024-03-15T12:47:39Z","published":"2024-03-15T12:47:39Z","title":"Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models","summary":"  Large language models (LLMs) have demonstrated strong multilingual\ncapabilities; yet, they are mostly English-centric due to the imbalanced\ntraining corpora. Existing works leverage this phenomenon to improve their\nmultilingual performances on NLP tasks. In this work, we extend the evaluation\nfrom NLP tasks to real user queries. We find that even though translation into\nEnglish can help improve the performance of multilingual NLP tasks for\nEnglish-centric LLMs, it may not be optimal for all scenarios. For\nculture-related tasks that need deep language understanding, prompting in the\nnative language proves to be more promising since it can capture the nuances\nrelated to culture and language. Therefore, we advocate for more efforts\ntowards the development of strong multilingual LLMs instead of just\nEnglish-centric LLMs.\n","authors":["Chaoqun Liu","Wenxuan Zhang","Yiran Zhao","Anh Tuan Luu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2403.10258v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.10239v1","updated":"2024-03-15T12:12:54Z","published":"2024-03-15T12:12:54Z","title":"A Big Data Approach to Understand Sub-national Determinants of FDI in\n  Africa","summary":"  Various macroeconomic and institutional factors hinder FDI inflows, including\ncorruption, trade openness, access to finance, and political instability.\nExisting research mostly focuses on country-level data, with limited\nexploration of firm-level data, especially in developing countries. Recognizing\nthis gap, recent calls for research emphasize the need for qualitative data\nanalysis to delve into FDI determinants, particularly at the regional level.\nThis paper proposes a novel methodology, based on text mining and social\nnetwork analysis, to get information from more than 167,000 online news\narticles to quantify regional-level (sub-national) attributes affecting FDI\nownership in African companies. Our analysis extends information on obstacles\nto industrial development as mapped by the World Bank Enterprise Surveys.\nFindings suggest that regional (sub-national) structural and institutional\ncharacteristics can play an important role in determining foreign ownership.\n","authors":["A. Fronzetti Colladon","R. Vestrelli","S. Bait","M. M. Schiraldi"],"pdf_url":"https://arxiv.org/pdf/2403.10239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10237v1","updated":"2024-03-15T12:08:58Z","published":"2024-03-15T12:08:58Z","title":"A comprehensive study on Frequent Pattern Mining and Clustering\n  categories for topic detection in Persian text stream","summary":"  Topic detection is a complex process and depends on language because it\nsomehow needs to analyze text. There have been few studies on topic detection\nin Persian, and the existing algorithms are not remarkable. Therefore, we aimed\nto study topic detection in Persian. The objectives of this study are: 1) to\nconduct an extensive study on the best algorithms for topic detection, 2) to\nidentify necessary adaptations to make these algorithms suitable for the\nPersian language, and 3) to evaluate their performance on Persian social\nnetwork texts. To achieve these objectives, we have formulated two research\nquestions: First, considering the lack of research in Persian, what\nmodifications should be made to existing frameworks, especially those developed\nin English, to make them compatible with Persian? Second, how do these\nalgorithms perform, and which one is superior? There are various topic\ndetection methods that can be categorized into different categories. Frequent\npattern and clustering are selected for this research, and a hybrid of both is\nproposed as a new category. Then, ten methods from these three categories are\nselected. All of them are re-implemented from scratch, changed, and adapted\nwith Persian. These ten methods encompass different types of topic detection\nmethods and have shown good performance in English. The text of Persian social\nnetwork posts is used as the dataset. Additionally, a new multiclass evaluation\ncriterion, called FS, is used in this paper for the first time in the field of\ntopic detection. Approximately 1.4 billion tokens are processed during\nexperiments. The results indicate that if we are searching for keyword-topics\nthat are easily understandable by humans, the hybrid category is better.\nHowever, if the aim is to cluster posts for further analysis, the frequent\npattern category is more suitable.\n","authors":["Elnaz Zafarani-Moattar","Mohammad Reza Kangavari","Amir Masoud Rahmani"],"pdf_url":"https://arxiv.org/pdf/2403.10237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17876v3","updated":"2024-03-15T12:01:25Z","published":"2023-03-31T08:18:30Z","title":"WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset","summary":"  We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed as the first webcam-based eye-tracking corpus of reading to\nsupport the development of explainable computational language processing\nmodels. WebQAmGaze includes webcam eye-tracking data from 600 participants of a\nwide age range naturally reading English, German, Spanish, and Turkish texts.\nEach participant performs two reading tasks composed of five texts each, a\nnormal reading and an information-seeking task, followed by a comprehension\nquestion. We compare the collected webcam data to high-quality eye-tracking\nrecordings. The results show a moderate to strong correlation between the eye\nmovement measures obtained with the webcam compared to those obtained with a\ncommercial eye-tracking device. When validating the data, we find that higher\nfixation duration on relevant text spans accurately indicates correctness when\nanswering the corresponding questions. This dataset advances webcam-based\nreading studies and opens avenues to low-cost and diverse data collection.\nWebQAmGaze is beneficial to learn about the cognitive processes behind\nquestion-answering and to apply these insights to computational models of\nlanguage understanding.\n","authors":["Tiago Ribeiro","Stephanie Brandl","Anders Søgaard","Nora Hollenstein"],"pdf_url":"https://arxiv.org/pdf/2303.17876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10228v1","updated":"2024-03-15T11:58:18Z","published":"2024-03-15T11:58:18Z","title":"HawkEye: Training Video-Text LLMs for Grounding Text in Videos","summary":"  Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.\n","authors":["Yueqian Wang","Xiaojun Meng","Jianxin Liang","Yuxuan Wang","Qun Liu","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.10228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07769v3","updated":"2024-03-15T11:44:51Z","published":"2024-03-12T15:56:10Z","title":"Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations","summary":"  This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.\n","authors":["Carlos Jose Xavier Cruz"],"pdf_url":"https://arxiv.org/pdf/2403.07769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10214v1","updated":"2024-03-15T11:32:44Z","published":"2024-03-15T11:32:44Z","title":"Enhanced Coherence-Aware Network with Hierarchical Disentanglement for\n  Aspect-Category Sentiment Analysis","summary":"  Aspect-category-based sentiment analysis (ACSA), which aims to identify\naspect categories and predict their sentiments has been intensively studied due\nto its wide range of NLP applications. Most approaches mainly utilize\nintrasentential features. However, a review often includes multiple different\naspect categories, and some of them do not explicitly appear in the review.\nEven in a sentence, there is more than one aspect category with its sentiments,\nand they are entangled intra-sentence, which makes the model fail to\ndiscriminately preserve all sentiment characteristics. In this paper, we\npropose an enhanced coherence-aware network with hierarchical disentanglement\n(ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture\nthe contexts across the whole review and to help the implicit aspect and\nsentiment identification. To address the issue of multiple aspect categories\nand sentiment entanglement, we propose a hierarchical disentanglement module to\nextract distinct categories and sentiment features. Extensive experimental and\nvisualization results show that our ECAN effectively decouples multiple\ncategories and sentiments entangled in the coherence representations and\nachieves state-of-the-art (SOTA) performance. Our codes and data are available\nonline: \\url{https://github.com/cuijin-23/ECAN}.\n","authors":["Jin Cui","Fumiyo Fukumoto","Xinfeng Wang","Yoshimi Suzuki","Jiyi Li","Noriko Tomuro","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2403.10214v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.13607v2","updated":"2024-03-15T11:19:30Z","published":"2024-02-21T08:21:12Z","title":"CODIS: Benchmarking Context-Dependent Visual Comprehension for\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have demonstrated promising results\nin a variety of tasks that combine vision and language. As these models become\nmore integral to research and applications, conducting comprehensive\nevaluations of their capabilities has grown increasingly important. However,\nmost existing benchmarks fail to consider that, in certain situations, images\nneed to be interpreted within a broader context. In this work, we introduce a\nnew benchmark, named as CODIS, designed to assess the ability of models to use\ncontext provided in free-form text to enhance visual comprehension. Our\nfindings indicate that MLLMs consistently fall short of human performance on\nthis benchmark. Further analysis confirms that these models struggle to\neffectively extract and utilize contextual information to improve their\nunderstanding of images. This underscores the pressing need to enhance the\nability of MLLMs to comprehend visuals in a context-dependent manner. View our\nproject website at https://thunlp-mt.github.io/CODIS.\n","authors":["Fuwen Luo","Chi Chen","Zihao Wan","Zhaolu Kang","Qidong Yan","Yingjie Li","Xiaolong Wang","Siyu Wang","Ziyue Wang","Xiaoyue Mi","Peng Li","Ning Ma","Maosong Sun","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.13607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10205v1","updated":"2024-03-15T11:11:57Z","published":"2024-03-15T11:11:57Z","title":"Read between the lines -- Functionality Extraction From READMEs","summary":"  While text summarization is a well-known NLP task, in this paper, we\nintroduce a novel and useful variant of it called functionality extraction from\nGit README files. Though this task is a text2text generation at an abstract\nlevel, it involves its own peculiarities and challenges making existing\ntext2text generation systems not very useful. The motivation behind this task\nstems from a recent surge in research and development activities around the use\nof large language models for code-related tasks, such as code refactoring, code\nsummarization, etc. We also release a human-annotated dataset called FuncRead,\nand develop a battery of models for the task. Our exhaustive experimentation\nshows that small size fine-tuned models beat any baseline models that can be\ndesigned using popular black-box or white-box large language models (LLMs) such\nas ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70%\nand 20% gain on the F1 score against ChatGPT and Bard respectively.\n","authors":["Prince Kumar","Srikanth Tamilselvam","Dinesh Garg"],"pdf_url":"https://arxiv.org/pdf/2403.10205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10185v1","updated":"2024-03-15T10:46:00Z","published":"2024-03-15T10:46:00Z","title":"Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based\n  Deception","summary":"  If a person firmly believes in a non-factual statement, such as \"The Earth is\nflat\", and argues in its favor, there is no inherent intention to deceive. As\nthe argumentation stems from genuine belief, it may be unlikely to exhibit the\nlinguistic properties associated with deception or lying. This interplay of\nfactuality, personal belief, and intent to deceive remains an understudied\narea. Disentangling the influence of these variables in argumentation is\ncrucial to gain a better understanding of the linguistic properties attributed\nto each of them. To study the relation between deception and factuality, based\non belief, we present the DeFaBel corpus, a crowd-sourced resource of\nbelief-based deception. To create this corpus, we devise a study in which\nparticipants are instructed to write arguments supporting statements like\n\"eating watermelon seeds can cause indigestion\", regardless of its factual\naccuracy or their personal beliefs about the statement. In addition to the\ngeneration task, we ask them to disclose their belief about the statement. The\ncollected instances are labelled as deceptive if the arguments are in\ncontradiction to the participants' personal beliefs. Each instance in the\ncorpus is thus annotated (or implicitly labelled) with personal beliefs of the\nauthor, factuality of the statement, and the intended deceptiveness. The\nDeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive\nand 388 are non-deceptive. It is the first publicly available corpus for\nstudying deception in German. In our analysis, we find that people are more\nconfident in the persuasiveness of their arguments when the statement is\naligned with their belief, but surprisingly less confident when they are\ngenerating arguments in favor of facts. The DeFaBel corpus can be obtained from\nhttps://www.ims.uni-stuttgart.de/data/defabel\n","authors":["Aswathy Velutharambath","Amelie Wührl","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2403.10185v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.11657v3","updated":"2024-03-15T10:28:13Z","published":"2023-04-23T13:54:39Z","title":"Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in\n  Large Language Models","summary":"  Large language models (LLMs) can achieve highly effective performance on\nvarious reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting as demonstrations. However, the reasoning chains of demonstrations\ngenerated by LLMs are prone to errors, which can subsequently lead to incorrect\nreasoning during inference. Furthermore, inappropriate exemplars (overly\nsimplistic or complex), can affect overall performance among varying levels of\ndifficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts\nPrompting), an iterative bootstrapping approach for selecting exemplars and\ngenerating reasoning chains. By utilizing iterative bootstrapping, our approach\nenables LLMs to autonomously rectify errors, resulting in more precise and\ncomprehensive reasoning chains. Simultaneously, our approach selects\nchallenging yet answerable questions accompanied by reasoning chains as\nexemplars with a moderate level of difficulty, which enhances the LLMs'\ngeneralizability across varying levels of difficulty. Experimental results\nindicate that Iter-CoT exhibits superiority, achieving competitive performance\nacross three distinct reasoning tasks on ten datasets.\n","authors":["Jiashuo Sun","Yi Luo","Yeyun Gong","Chen Lin","Yelong Shen","Jian Guo","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2304.11657v3.pdf","comment":"Accepted by NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2311.09022v2","updated":"2024-03-15T10:00:04Z","published":"2023-11-15T15:12:15Z","title":"Exploring the Potential of Large Language Models in Computational\n  Argumentation","summary":"  Computational argumentation has become an essential tool in various fields,\nincluding artificial intelligence, law, and public policy. It is an emerging\nresearch field in natural language processing that attracts increasing\nattention. Research on computational argumentation mainly involves two types of\ntasks: argument mining and argument generation. As large language models have\ndemonstrated strong abilities in understanding context and generating natural\nlanguage, it is worthwhile to evaluate the performance of LLMs on various\ncomputational argumentation tasks. This work aims to embark on an assessment of\nLLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and\nfew-shot settings within the realm of computational argumentation. We organize\nexisting tasks into six main categories and standardise the format of fourteen\nopen-sourced datasets. In addition, we present a new benchmark dataset on\ncounter speech generation, that aims to holistically evaluate the end-to-end\nperformance of LLMs on argument mining and argument generation. Extensive\nexperiments show that LLMs exhibit commendable performance across most of these\ndatasets, demonstrating their capabilities in the field of argumentation. Our\nanalysis offers valuable suggestions for evaluating computational argumentation\nand its integration with LLMs in future research endeavors.\n","authors":["Guizhen Chen","Liying Cheng","Luu Anh Tuan","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2311.09022v2.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.10144v1","updated":"2024-03-15T09:43:52Z","published":"2024-03-15T09:43:52Z","title":"NLP Verification: Towards a General Methodology for Certifying\n  Robustness","summary":"  Deep neural networks have exhibited substantial success in the field of\nNatural Language Processing (NLP) and ensuring their safety and reliability is\ncrucial: there are safety critical contexts where such models must be robust to\nvariability or attack, and give guarantees over their output. Unlike Computer\nVision, NLP lacks a unified verification methodology and, despite recent\nadvancements in literature, they are often light on the pragmatical issues of\nNLP verification. In this paper, we make an attempt to distil and evaluate\ngeneral components of an NLP verification pipeline, that emerges from the\nprogress in the field to date. Our contributions are two-fold. Firstly, we give\na general characterisation of verifiable subspaces that result from embedding\nsentences into continuous spaces. We identify, and give an effective method to\ndeal with, the technical challenge of semantic generalisability of verified\nsubspaces; and propose it as a standard metric in the NLP verification\npipelines (alongside with the standard metrics of model accuracy and model\nverifiability). Secondly, we propose a general methodology to analyse the\neffect of the embedding gap, a problem that refers to the discrepancy between\nverification of geometric subpspaces on the one hand, and semantic meaning of\nsentences which the geometric subspaces are supposed to represent, on the other\nhand. In extreme cases, poor choices in embedding of sentences may invalidate\nverification results. We propose a number of practical NLP methods that can\nhelp to identify the effects of the embedding gap; and in particular we propose\nthe metric of falsifiability of semantic subpspaces as another fundamental\nmetric to be reported as part of the NLP verification pipeline. We believe that\ntogether these general principles pave the way towards a more consolidated and\neffective development of this new domain.\n","authors":["Marco Casadio","Tanvi Dinkar","Ekaterina Komendantskaya","Luca Arnaboldi","Omri Isac","Matthew L. Daggitt","Guy Katz","Verena Rieser","Oliver Lemon"],"pdf_url":"https://arxiv.org/pdf/2403.10144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07910v2","updated":"2024-03-15T09:30:29Z","published":"2024-02-27T02:00:28Z","title":"MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained\n  Identification of Expressions","summary":"  Media bias detection poses a complex, multifaceted problem traditionally\ntackled using single-task models and small in-domain datasets, consequently\nlacking generalizability. To address this, we introduce MAGPIE, the first\nlarge-scale multi-task pre-training approach explicitly tailored for media bias\ndetection. To enable pre-training at scale, we present Large Bias Mixture\n(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous\napproaches in media bias detection on the Bias Annotation By Experts (BABE)\ndataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs\nbetter than previous models on 5 out of 8 tasks in the Media Bias\nIdentification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%\nof finetuning steps compared to single-task approaches. Our evaluation shows,\nfor instance, that tasks like sentiment and emotionality boost all learning,\nall tasks enhance fake news detection, and scaling tasks leads to the best\nresults. MAGPIE confirms that MTL is a promising approach for addressing media\nbias detection, enhancing the accuracy and efficiency of existing models.\nFurthermore, LBM is the first available resource collection focused on media\nbias MTL.\n","authors":["Tomáš Horych","Martin Wessel","Jan Philip Wahle","Terry Ruas","Jerome Waßmuth","André Greiner-Petter","Akiko Aizawa","Bela Gipp","Timo Spinde"],"pdf_url":"https://arxiv.org/pdf/2403.07910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10135v1","updated":"2024-03-15T09:28:19Z","published":"2024-03-15T09:28:19Z","title":"The Whole is Better than the Sum: Using Aggregated Demonstrations in\n  In-Context Learning for Sequential Recommendation","summary":"  Large language models (LLMs) have shown excellent performance on various NLP\ntasks. To use LLMs as strong sequential recommenders, we explore the in-context\nlearning approach to sequential recommendation. We investigate the effects of\ninstruction format, task consistency, demonstration selection, and number of\ndemonstrations. As increasing the number of demonstrations in ICL does not\nimprove accuracy despite using a long prompt, we propose a novel method called\nLLMSRec-Syn that incorporates multiple demonstration users into one aggregated\ndemonstration. Our experiments on three recommendation datasets show that\nLLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation\nmethods. In some cases, LLMSRec-Syn can perform on par with or even better than\nsupervised learning methods. Our code is publicly available at\nhttps://github.com/demoleiwang/LLMSRec_Syn.\n","authors":["Lei Wang","Ee-Peng Lim"],"pdf_url":"https://arxiv.org/pdf/2403.10135v1.pdf","comment":"NAACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2403.10131v1","updated":"2024-03-15T09:26:02Z","published":"2024-03-15T09:26:02Z","title":"RAFT: Adapting Language Model to Domain Specific RAG","summary":"  Pretraining Large Language Models (LLMs) on large corpora of textual data is\nnow a standard paradigm. When using these LLMs for many downstream\napplications, it is common to additionally bake in new knowledge (e.g.,\ntime-critical news, or private domain knowledge) into the pretrained model\neither through RAG-based-prompting, or fine-tuning. However, the optimal\nmethodology for the model to gain such new knowledge remains an open question.\nIn this paper, we present Retrieval Augmented FineTuning (RAFT), a training\nrecipe that improves the model's ability to answer questions in a \"open-book\"\nin-domain settings. In RAFT, given a question, and a set of retrieved\ndocuments, we train the model to ignore those documents that don't help in\nanswering the question, which we call, distractor documents. RAFT accomplishes\nthis by citing verbatim the right sequence from the relevant document that\nwould help answer the question. This coupled with RAFT's chain-of-thought-style\nresponse helps improve the model's ability to reason. In domain-specific RAG,\nRAFT consistently improves the model's performance across PubMed, HotpotQA, and\nGorilla datasets, presenting a post-training recipe to improve pre-trained LLMs\nto in-domain RAG. RAFT's code and demo are open-sourced at\ngithub.com/ShishirPatil/gorilla.\n","authors":["Tianjun Zhang","Shishir G. Patil","Naman Jain","Sheng Shen","Matei Zaharia","Ion Stoica","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2403.10131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07593v2","updated":"2024-03-15T08:58:05Z","published":"2023-11-10T05:24:07Z","title":"Follow-Up Differential Descriptions: Language Models Resolve Ambiguities\n  for Image Classification","summary":"  A promising approach for improving the performance of vision-language models\nlike CLIP for image classification is to extend the class descriptions (i.e.,\nprompts) with related attributes, e.g., using brown sparrow instead of sparrow.\nHowever, current zero-shot methods select a subset of attributes regardless of\ncommonalities between the target classes, potentially providing no useful\ninformation that would have helped to distinguish between them. For instance,\nthey may use color instead of bill shape to distinguish between sparrows and\nwrens, which are both brown. We propose Follow-up Differential Descriptions\n(FuDD), a zero-shot approach that tailors the class descriptions to each\ndataset and leads to additional attributes that better differentiate the target\nclasses. FuDD first identifies the ambiguous classes for each image, and then\nuses a Large Language Model (LLM) to generate new class descriptions that\ndifferentiate between them. The new class descriptions resolve the initial\nambiguity and help predict the correct label. In our experiments, FuDD\nconsistently outperforms generic description ensembles and naive LLM-generated\ndescriptions on 12 datasets. We show that differential descriptions are an\neffective tool to resolve class ambiguities, which otherwise significantly\ndegrade the performance. We also show that high quality natural language class\ndescriptions produced by FuDD result in comparable performance to few-shot\nadaptation methods.\n","authors":["Reza Esfandiarpoor","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2311.07593v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.06412v3","updated":"2024-03-15T08:53:31Z","published":"2024-03-11T03:54:33Z","title":"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in\n  Korean","summary":"  Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.\n","authors":["Eunsu Kim","Juyoung Suk","Philhoon Oh","Haneul Yoo","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2403.06412v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16259v6","updated":"2024-03-15T08:31:05Z","published":"2023-05-25T17:13:44Z","title":"Neural Natural Language Processing for Long Texts: A Survey on\n  Classification and Summarization","summary":"  The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded online renders automated\nunderstanding of lengthy texts a critical issue. Relevant applications include\nautomated Web mining, legal document review, medical records analysis,\nfinancial reports analysis, contract management, environmental impact\nassessment, news aggregation, etc. Despite the relatively recent development of\nefficient algorithms for analyzing long documents, practical tools in this\nfield are currently flourishing. This article serves as an entry point into\nthis dynamic domain and aims to achieve two objectives. First of all, it\nprovides an introductory overview of the relevant neural building blocks,\nserving as a concise tutorial for the field. Secondly, it offers a brief\nexamination of the current state-of-the-art in two key long document analysis\ntasks: document classification and document summarization. Sentiment analysis\nfor long texts is also covered, since it is typically treated as a particular\ncase of document classification. Consequently, this article presents an\nintroductory exploration of document-level analysis, addressing the primary\nchallenges, concerns, and existing solutions. Finally, it offers a concise\ndefinition of \"long text/document\", presents an original overarching taxonomy\nof common deep neural methods for long document analysis and lists publicly\navailable annotated datasets that can facilitate further research in this area.\n","authors":["Dimitrios Tsirmpas","Ioannis Gkionis","Georgios Th. Papadopoulos","Ioannis Mademlis"],"pdf_url":"https://arxiv.org/pdf/2305.16259v6.pdf","comment":"65 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2311.07445v2","updated":"2024-03-15T08:30:30Z","published":"2023-11-13T16:19:42Z","title":"Think Before You Speak: Cultivating Communication Skills of Large\n  Language Models via Inner Monologue","summary":"  The emergence of large language models (LLMs) further improves the\ncapabilities of open-domain dialogue systems and can generate fluent, coherent,\nand diverse responses. However, LLMs still lack a crucial ability:\ncommunication skills. This limitation renders them more like information\nseeking tools rather than anthropomorphic chatbots. Communication skills, such\nas topic transition, proactively asking questions, concept guidance, empathy,\nand summarising often should be taken into consideration, to make LLMs more\nanthropomorphic and proactive during the conversation, thereby increasing the\ninterest of users and attracting them to chat for longer. However, enabling\nthese communication skills in black-box LLMs remains a key challenge because\nthey do not have the same utterance formation mode as real people: think before\nspeaking. Inspired by linguistics and cognitive science, we empower LLMs with\ncommunication skills through inner monologues. To evaluate various\ncommunication skills, we construct a benchmark named Cskills, which can also\nmore comprehensively evaluate the dialogue generation ability of the model.\nExperimental results show that the proposed CSIM strategy improves the backbone\nmodels and outperforms the baselines.\n","authors":["Junkai Zhou","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2311.07445v2.pdf","comment":"Accepted by NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.10088v1","updated":"2024-03-15T08:03:49Z","published":"2024-03-15T08:03:49Z","title":"Intent-conditioned and Non-toxic Counterspeech Generation using\n  Multi-Task Instruction Tuning with RLAIF","summary":"  Counterspeech, defined as a response to mitigate online hate speech, is\nincreasingly used as a non-censorial solution. Addressing hate speech\neffectively involves dispelling the stereotypes, prejudices, and biases often\nsubtly implied in brief, single-sentence statements or abuses. These implicit\nexpressions challenge language models, especially in seq2seq tasks, as model\nperformance typically excels with longer contexts. Our study introduces CoARL,\na novel framework enhancing counterspeech generation by modeling the pragmatic\nimplications underlying social biases in hateful statements. CoARL's first two\nphases involve sequential multi-instruction tuning, teaching the model to\nunderstand intents, reactions, and harms of offensive statements, and then\nlearning task-specific low-rank adapter weights for generating\nintent-conditioned counterspeech. The final phase uses reinforcement learning\nto fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms\nexisting benchmarks in intent-conditioned counterspeech generation, showing an\naverage improvement of 3 points in intent-conformity and 4 points in\nargument-quality metrics. Extensive human evaluation supports CoARL's efficacy\nin generating superior and more context-appropriate responses compared to\nexisting systems, including prominent LLMs like ChatGPT.\n","authors":["Amey Hengle","Aswini Kumar","Sahajpreet Singh","Anil Bandhakavi","Md Shad Akhtar","Tanmoy Chakroborty"],"pdf_url":"https://arxiv.org/pdf/2403.10088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10081v1","updated":"2024-03-15T07:45:37Z","published":"2024-03-15T07:45:37Z","title":"DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time\n  Information Needs of Large Language Models","summary":"  Dynamic retrieval augmented generation (RAG) paradigm actively decides when\nand what to retrieve during the text generation process of Large Language\nModels (LLMs). There are two key elements of this paradigm: identifying the\noptimal moment to activate the retrieval module (deciding when to retrieve) and\ncrafting the appropriate query once retrieval is triggered (determining what to\nretrieve). However, current dynamic RAG methods fall short in both aspects.\nFirstly, the strategies for deciding when to retrieve often rely on static\nrules. Moreover, the strategies for deciding what to retrieve typically limit\nthemselves to the LLM's most recent sentence or the last few tokens, while the\nLLM's real-time information needs may span across the entire context. To\novercome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic\nRetrieval Augmented Generation based on the real-time Information Needs of\nLLMs. Our framework is specifically designed to make decisions on when and what\nto retrieve based on the LLM's real-time information needs during the text\ngeneration process. We evaluate DRAGIN along with existing methods\ncomprehensively over 4 knowledge-intensive generation datasets. Experimental\nresults show that DRAGIN achieves superior performance on all tasks,\ndemonstrating the effectiveness of our method. We have open-sourced all the\ncode, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08281v2","updated":"2024-03-15T07:22:31Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06443v2","updated":"2024-03-15T07:17:10Z","published":"2024-01-12T08:31:42Z","title":"BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via\n  Graph Representation Pretraining","summary":"  The current research direction in generative models, such as the recently\ndeveloped GPT4, aims to find relevant knowledge information for multimodal and\nmultilingual inputs to provide answers. Under these research circumstances, the\ndemand for multilingual evaluation of visual question answering (VQA) tasks, a\nrepresentative task of multimodal systems, has increased. Accordingly, we\npropose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that\ncan be extended to multilingualism. The proposed data include 17K images, 17K\nquestion-answer pairs for both Korean and English and 280K instances of\nknowledge information related to question-answer content. We also present a\nframework that can effectively inject knowledge information into a VQA system\nby pretraining the knowledge information of BOK-VQA data in the form of graph\nembeddings. Finally, through in-depth analysis, we demonstrated the actual\neffect of the knowledge information contained in the constructed training data\non VQA.\n","authors":["Minjun Kim","Seungwoo Song","Youhan Lee","Haneol Jang","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2401.06443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10065v1","updated":"2024-03-15T07:15:48Z","published":"2024-03-15T07:15:48Z","title":"Triple GNNs: Introducing Syntactic and Semantic Information for\n  Conversational Aspect-Based Quadruple Sentiment Analysis","summary":"  Conversational Aspect-Based Sentiment Analysis (DiaASQ) aims to detect\nquadruples \\{target, aspect, opinion, sentiment polarity\\} from given\ndialogues. In DiaASQ, elements constituting these quadruples are not\nnecessarily confined to individual sentences but may span across multiple\nutterances within a dialogue. This necessitates a dual focus on both the\nsyntactic information of individual utterances and the semantic interaction\namong them. However, previous studies have primarily focused on coarse-grained\nrelationships between utterances, thus overlooking the potential benefits of\ndetailed intra-utterance syntactic information and the granularity of\ninter-utterance relationships. This paper introduces the Triple GNNs network to\nenhance DiaAsQ. It employs a Graph Convolutional Network (GCN) for modeling\nsyntactic dependencies within utterances and a Dual Graph Attention Network\n(DualGATs) to construct interactions between utterances. Experiments on two\nstandard datasets reveal that our model significantly outperforms\nstate-of-the-art baselines. The code is available at\n\\url{https://github.com/nlperi2b/Triple-GNNs-}.\n","authors":["Binbin Li","Yuqing Li","Siyu Jia","Bingnan Ma","Yu Ding","Zisen Qi","Xingbang Tan","Menghan Guo","Shenghui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10065v1.pdf","comment":"Accepted by CSCWD2024"},{"id":"http://arxiv.org/abs/2403.10059v1","updated":"2024-03-15T06:59:43Z","published":"2024-03-15T06:59:43Z","title":"Repoformer: Selective Retrieval for Repository-Level Code Completion","summary":"  Recent advances in retrieval-augmented generation (RAG) have initiated a new\nera in repository-level code completion. However, the invariable use of\nretrieval in existing methods exposes issues in both efficiency and robustness,\nwith a large proportion of the retrieved contexts proving unhelpful or harmful\nto code language models (code LMs). To tackle the challenges, this paper\nproposes a selective RAG framework where retrieval is avoided when unnecessary.\nTo power this framework, we design a self-supervised learning approach that\nenables a code LM to accurately self-evaluate whether retrieval can improve its\noutput quality and robustly leverage the potentially noisy retrieved contexts.\nUsing this LM as both the selective retrieval policy and the generation model,\nour framework consistently outperforms the state-of-the-art prompting with an\ninvariable retrieval approach on diverse benchmarks including RepoEval,\nCrossCodeEval, and a new benchmark. Meanwhile, our selective retrieval strategy\nresults in strong efficiency improvements by as much as 70% inference speedup\nwithout harming the performance. We demonstrate that our framework effectively\naccommodates different generation models, retrievers, and programming\nlanguages. These advancements position our framework as an important step\ntowards more accurate and efficient repository-level code completion.\n","authors":["Di Wu","Wasi Uddin Ahmad","Dejiao Zhang","Murali Krishna Ramanathan","Xiaofei Ma"],"pdf_url":"https://arxiv.org/pdf/2403.10059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10056v1","updated":"2024-03-15T06:54:20Z","published":"2024-03-15T06:54:20Z","title":"Don't Half-listen: Capturing Key-part Information in Continual\n  Instruction Tuning","summary":"  Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks.\n","authors":["Yongquan He","Xuancheng Huang","Minghao Tang","Lingxun Meng","Xiang Li","Wei Lin","Wenyuan Zhang","Yifu Gao"],"pdf_url":"https://arxiv.org/pdf/2403.10056v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.12689v3","updated":"2024-03-15T06:51:28Z","published":"2024-01-23T11:54:09Z","title":"Energy-based Automated Model Evaluation","summary":"  The conventional evaluation protocols on machine learning models rely heavily\non a labeled, i.i.d-assumed testing dataset, which is not often present in real\nworld applications. The Automated Model Evaluation (AutoEval) shows an\nalternative to this traditional workflow, by forming a proximal prediction\npipeline of the testing performance without the presence of ground-truth\nlabels. Despite its recent successes, the AutoEval frameworks still suffer from\nan overconfidence issue, substantial storage and computational cost. In that\nregard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that\nallows the AutoEval framework to be both more efficient and effective. The core\nof the MDE is to establish a meta-distribution statistic, on the information\n(energy) associated with individual samples, then offer a smoother\nrepresentation enabled by energy-based learning. We further provide our\ntheoretical insights by connecting the MDE with the classification loss. We\nprovide extensive experiments across modalities, datasets and different\narchitectural backbones to validate MDE's validity, together with its\nsuperiority compared with prior approaches. We also prove MDE's versatility by\nshowing its seamless integration with large-scale models, and easy adaption to\nlearning scenarios with noisy- or imbalanced- labels. Code and data are\navailable: https://github.com/pengr/Energy_AutoEval\n","authors":["Ru Peng","Heming Zou","Haobo Wang","Yawen Zeng","Zenan Huang","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.12689v3.pdf","comment":"ICLR2024 poster paper"},{"id":"http://arxiv.org/abs/2401.07518v3","updated":"2024-03-15T05:10:05Z","published":"2024-01-15T07:48:42Z","title":"Survey of Natural Language Processing for Education: Taxonomy,\n  Systematic Review, and Future Trends","summary":"  Natural Language Processing (NLP) aims to analyze text or speech via\ntechniques in the computer science field. It serves the applications in domains\nof healthcare, commerce, education and so on. Particularly, NLP has been widely\napplied to the education domain and its applications have enormous potential to\nhelp teaching and learning. In this survey, we review recent advances in NLP\nwith the focus on solving problems relevant to the education domain. In detail,\nwe begin with introducing the related background and the real-world scenarios\nin education where NLP techniques could contribute. Then, we present a taxonomy\nof NLP in the education domain and highlight typical NLP applications including\nquestion answering, question construction, automated assessment, and error\ncorrection. Next, we illustrate the task definition, challenges, and\ncorresponding cutting-edge techniques based on the above taxonomy. In\nparticular, LLM-involved methods are included for discussion due to the wide\nusage of LLMs in diverse NLP applications. After that, we showcase some\noff-the-shelf demonstrations in this domain. At last, we conclude with six\npromising directions for future research, including more datasets in education\ndomain, controllable usage of LLMs, intervention of difficulty-level control,\ninterpretable educational NLP, methods with adaptive learning, and integrated\nsystems for education. We organize all relevant datasets and papers in the\nopen-available Github Link for better\nreview~\\url{https://github.com/LiXinyuan1015/NLP-for-Education}.\n","authors":["Yunshi Lan","Xinyuan Li","Hanyue Du","Xuesong Lu","Ming Gao","Weining Qian","Aoying Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.07518v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10020v1","updated":"2024-03-15T05:06:21Z","published":"2024-03-15T05:06:21Z","title":"Lost in Overlap: Exploring Watermark Collision in LLMs","summary":"  The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread use of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks like question answering and paraphrasing. This study focuses on dual\nwatermark collisions, where two watermarks are present simultaneously in the\nsame text. The research demonstrates that watermark collision poses a threat to\ndetection performance for detectors of both upstream and downstream watermark\nalgorithms.\n","authors":["Yiyang Luo","Ke Lin","Chao Gu"],"pdf_url":"https://arxiv.org/pdf/2403.10020v1.pdf","comment":"Short Paper, 4 pages"},{"id":"http://arxiv.org/abs/2308.03449v2","updated":"2024-03-15T04:51:19Z","published":"2023-08-07T10:11:42Z","title":"Accurate Retraining-free Pruning for Pretrained Encoder-based Language\n  Models","summary":"  Given a pretrained encoder-based language model, how can we accurately\ncompress it without retraining? Retraining-free structured pruning algorithms\nare crucial in pretrained language model compression due to their significantly\nreduced pruning cost and capability to prune large language models. However,\nexisting retraining-free algorithms encounter severe accuracy degradation, as\nthey fail to handle pruning errors, especially at high compression rates. In\nthis paper, we propose K-prune (Knowledge-preserving pruning), an accurate\nretraining-free structured pruning algorithm for pretrained encoder-based\nlanguage models. K-prune focuses on preserving the useful knowledge of the\npretrained model to minimize pruning errors through a carefully designed\niterative pruning process composed of knowledge measurement,\nknowledge-preserving mask search, and knowledge-preserving weight-tuning. As a\nresult, K-prune shows significant accuracy improvements up to 58.02%p higher F1\nscore compared to existing retraining-free pruning algorithms under a high\ncompression rate of 80% on the SQuAD benchmark without any retraining process.\n","authors":["Seungcheol Park","Hojun Choi","U Kang"],"pdf_url":"https://arxiv.org/pdf/2308.03449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02992v2","updated":"2024-03-15T04:38:21Z","published":"2023-10-04T17:28:44Z","title":"Kosmos-G: Generating Images in Context with Multimodal Large Language\n  Models","summary":"  Recent advancements in subject-driven image generation have made significant\nstrides. However, current methods still fall short in diverse application\nscenarios, as they require test-time tuning and cannot accept interleaved\nmulti-image and text input. These limitations keep them far from the ultimate\ngoal of \"image as a foreign language in image generation.\" This paper presents\nKosmos-G, a model that leverages the advanced multimodal perception\ncapabilities of Multimodal Large Language Models (MLLMs) to tackle the\naforementioned challenge. Our approach aligns the output space of MLLM with\nCLIP using the textual modality as an anchor and performs compositional\ninstruction tuning on curated data. Kosmos-G demonstrates an impressive\ncapability of zero-shot subject-driven generation with interleaved multi-image\nand text input. Notably, the score distillation instruction tuning requires no\nmodifications to the image decoder. This allows for a seamless substitution of\nCLIP and effortless integration with a myriad of U-Net techniques ranging from\nfine-grained controls to personalized image decoder variants. We posit Kosmos-G\nas an initial attempt towards the goal of \"image as a foreign language in image\ngeneration.\" The code can be found at https://aka.ms/Kosmos-G\n","authors":["Xichen Pan","Li Dong","Shaohan Huang","Zhiliang Peng","Wenhu Chen","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.02992v2.pdf","comment":"Code: https://aka.ms/Kosmos-G Project Page:\n  https://xichenpan.github.io/kosmosg"},{"id":"http://arxiv.org/abs/2403.09997v1","updated":"2024-03-15T03:43:07Z","published":"2024-03-15T03:43:07Z","title":"Identifying Health Risks from Family History: A Survey of Natural\n  Language Processing Techniques","summary":"  Electronic health records include information on patients' status and medical\nhistory, which could cover the history of diseases and disorders that could be\nhereditary. One important use of family history information is in precision\nhealth, where the goal is to keep the population healthy with preventative\nmeasures. Natural Language Processing (NLP) and machine learning techniques can\nassist with identifying information that could assist health professionals in\nidentifying health risks before a condition is developed in their later years,\nsaving lives and reducing healthcare costs.\n  We survey the literature on the techniques from the NLP field that have been\ndeveloped to utilise digital health records to identify risks of familial\ndiseases. We highlight that rule-based methods are heavily investigated and are\nstill actively used for family history extraction. Still, more recent efforts\nhave been put into building neural models based on large-scale pre-trained\nlanguage models. In addition to the areas where NLP has successfully been\nutilised, we also identify the areas where more research is needed to unlock\nthe value of patients' records regarding data collection, task formulation and\ndownstream applications.\n","authors":["Xiang Dai","Sarvnaz Karimi","Nathan O'Callaghan"],"pdf_url":"https://arxiv.org/pdf/2403.09997v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2312.02436v3","updated":"2024-03-15T03:11:44Z","published":"2023-12-05T02:32:08Z","title":"MUFFIN: Curating Multi-Faceted Instructions for Improving\n  Instruction-Following","summary":"  In the realm of large language models (LLMs), enhancing instruction-following\ncapability often involves curating expansive training data. This is achieved\nthrough two primary schemes: i) Scaling-Inputs: Amplifying (input, output)\npairs per task instruction, aiming for better instruction adherence. ii)\nScaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,\noutput) pair (without requiring a separate input anymore). However, LLMs under\nScaling-Inputs tend to be overly sensitive to inputs, leading to\nmisinterpretation or non-compliance with instructions. Conversely, Scaling\nInput-Free Tasks demands a substantial number of tasks but is less effective in\ninstruction following when dealing with instances in Scaling-Inputs. This work\nintroduces MUFFIN, a new scheme of instruction-following dataset curation.\nSpecifically, we automatically Scale Tasks per Input by diversifying these\ntasks with various input facets. Experimental results across four zero-shot\nbenchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,\nreveal that LLMs, at various scales, trained on MUFFIN generally demonstrate\nsuperior instruction-following capabilities compared to those trained on the\ntwo aforementioned schemes.\n","authors":["Renze Lou","Kai Zhang","Jian Xie","Yuxuan Sun","Janice Ahn","Hanzi Xu","Yu Su","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2312.02436v3.pdf","comment":"ICLR 2024. Data, model, and code are available at:\n  https://renzelou.github.io/Muffin/"},{"id":"http://arxiv.org/abs/2403.07378v2","updated":"2024-03-15T02:59:10Z","published":"2024-03-12T07:31:18Z","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression","summary":"  The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitate LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the remaining model parameters\nafter SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening strategy to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na layer-wise closed-form model parameter update strategy to compensate for\naccuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total\nof 11 datasets and seven models from three different LLM families at four\ndifferent scales. Our results demonstrate the superiority of SVD-LLM over\nstate-of-the-arts, especially at high model compression ratios. The source code\nis available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.\n","authors":["Xin Wang","Yu Zheng","Zhongwei Wan","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07378v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.09974v1","updated":"2024-03-15T02:40:13Z","published":"2024-03-15T02:40:13Z","title":"GET: Unlocking the Multi-modal Potential of CLIP for Generalized\n  Category Discovery","summary":"  Given unlabelled datasets containing both old and new categories, generalized\ncategory discovery (GCD) aims to accurately discover new classes while\ncorrectly classifying old classes, leveraging the class concepts learned from\nlabeled samples. Current GCD methods only use a single visual modality of\ninformation, resulting in poor classification of visually similar classes.\nThough certain classes are visually confused, their text information might be\ndistinct, motivating us to introduce text information into the GCD task.\nHowever, the lack of class names for unlabelled data makes it impractical to\nutilize text information. To tackle this challenging problem, in this paper, we\npropose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings\nfor unlabelled samples. Specifically, our TES leverages the property that CLIP\ncan generate aligned vision-language features, converting visual embeddings\ninto tokens of the CLIP's text encoder to generate pseudo text embeddings.\nBesides, we employ a dual-branch framework, through the joint learning and\ninstance consistency of different modality branches, visual and semantic\ninformation mutually enhance each other, promoting the interaction and fusion\nof visual and text embedding space. Our method unlocks the multi-modal\npotentials of CLIP and outperforms the baseline methods by a large margin on\nall GCD benchmarks, achieving new state-of-the-art. The code will be released\nat \\url{https://github.com/enguangW/GET}.\n","authors":["Enguang Wang","Zhimao Peng","Zhengyuan Xie","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09972v1","updated":"2024-03-15T02:38:26Z","published":"2024-03-15T02:38:26Z","title":"Think Twice Before Assure: Confidence Estimation for Large Language\n  Models through Reflection on Multiple Answers","summary":"  Confidence estimation aiming to evaluate output trustability is crucial for\nthe application of large language models (LLM), especially the black-box ones.\nExisting confidence estimation of LLM is typically not calibrated due to the\noverconfidence of LLM on its generated incorrect answers. Existing approaches\naddressing the overconfidence issue are hindered by a significant limitation\nthat they merely consider the confidence of one answer generated by LLM. To\ntackle this limitation, we propose a novel paradigm that thoroughly evaluates\nthe trustability of multiple candidate answers to mitigate the overconfidence\non incorrect answers. Building upon this paradigm, we introduce a two-step\nframework, which firstly instructs LLM to reflect and provide justifications\nfor each answer, and then aggregates the justifications for comprehensive\nconfidence estimation. This framework can be integrated with existing\nconfidence estimation approaches for superior calibration. Experimental results\non six datasets of three tasks demonstrate the rationality and effectiveness of\nthe proposed framework.\n","authors":["Moxin Li","Wenjie Wang","Fuli Feng","Fengbin Zhu","Qifan Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2403.09972v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.05502v3","updated":"2024-03-15T02:30:28Z","published":"2023-10-09T08:07:04Z","title":"XAL: EXplainable Active Learning Makes Classifiers Better Low-resource\n  Learners","summary":"  Active learning (AL), which aims to construct an effective training set by\niteratively curating the most formative unlabeled data for annotation, has been\nwidely used in low-resource tasks. Most active learning techniques in\nclassification rely on the model's uncertainty or disagreement to choose\nunlabeled data, suffering from the problem of over-confidence in superficial\npatterns and a lack of exploration. Inspired by the cognitive processes in\nwhich humans deduce and predict through causal information, we take an initial\nattempt towards integrating rationales into AL and propose a novel Explainable\nActive Learning framework (XAL) for low-resource text classification, which\naims to encourage classifiers to justify their inferences and delve into\nunlabeled data for which they cannot provide reasonable explanations.\nSpecifically, besides using a pre-trained bi-directional encoder for\nclassification, we employ a pre-trained uni-directional decoder to generate and\nscore the explanation. We further facilitate the alignment of the model with\nhuman reasoning preference through a proposed ranking loss. During the\nselection of unlabeled data, the predicted uncertainty of the encoder and the\nexplanation score of the decoder complement each other as the final metric to\nacquire informative data. Extensive experiments on six datasets show that XAL\nachieves consistent improvement over 9 strong baselines. Analysis indicates\nthat the proposed method can generate corresponding explanations for its\npredictions.\n","authors":["Yun Luo","Zhen Yang","Fandong Meng","Yingjie Li","Fang Guo","Qinglin Qi","Jie Zhou","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05502v3.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.09539v2","updated":"2024-03-15T02:07:30Z","published":"2024-03-14T16:27:49Z","title":"Logits of API-Protected LLMs Leak Proprietary Information","summary":"  The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.\n","authors":["Matthew Finlayson","Xiang Ren","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2403.09539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09963v1","updated":"2024-03-15T02:04:35Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. However, the extent and impact of prompt bias\nwithin the model remain underexplored. In response, this paper quantifies the\nbias with various types of prompts and assesses their impact on different\nbenchmarks. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). Our findings shed\nnew light on the underlying predicting mechanisms of prompt-based queries in\nPLMs. Hopefully, our plug-and-play approach can be a golden standard to\nstrengthen PLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2310.00535v3","updated":"2024-03-15T02:03:21Z","published":"2023-10-01T01:21:35Z","title":"JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and\n  Attention","summary":"  We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical\nframework to understand the training procedure of multilayer Transformer\narchitectures. This is achieved by integrating out the self-attention layer in\nTransformers, producing a modified dynamics of MLP layers only. JoMA removes\nunrealistic assumptions in previous analysis (e.g., lack of residual\nconnection) and predicts that the attention first becomes sparse (to learn\nsalient tokens), then dense (to learn less salient tokens) in the presence of\nnonlinear activations, while in the linear case, it is consistent with existing\nworks that show attention becomes sparse over time. We leverage JoMA to\nqualitatively explains how tokens are combined to form hierarchies in\nmultilayer Transformers, when the input tokens are generated by a latent\nhierarchical generative model. Experiments on models trained from real-world\ndataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)\nverify our theoretical findings. Code can be found in\nhttps://github.com/facebookresearch/luckmatters/tree/yuandong3.\n","authors":["Yuandong Tian","Yiping Wang","Zhenyu Zhang","Beidi Chen","Simon Du"],"pdf_url":"https://arxiv.org/pdf/2310.00535v3.pdf","comment":"ICLR'24 camera ready. Improve theorem 3 and theorem 4. Polish writing\n  and add code link"},{"id":"http://arxiv.org/abs/2402.10948v2","updated":"2024-03-15T02:02:02Z","published":"2024-02-09T09:44:06Z","title":"Zero-shot Explainable Mental Health Analysis on Social Media by\n  Incorporating Mental Scales","summary":"  Traditional discriminative approaches in mental health analysis are known for\ntheir strong capacity but lack interpretability and demand large-scale\nannotated data. The generative approaches, such as those based on large\nlanguage models (LLMs), have the potential to get rid of heavy annotations and\nprovide explanations but their capabilities still fall short compared to\ndiscriminative approaches, and their explanations may be unreliable due to the\nfact that the generation of explanation is a black-box process. Inspired by the\npsychological assessment practice of using scales to evaluate mental states,\nour method which is called Mental Analysis by Incorporating Mental Scales\n(MAIMS), incorporates two procedures via LLMs. First, the patient completes\nmental scales, and second, the psychologist interprets the collected\ninformation from the mental scales and makes informed decisions. Experimental\nresults show that MAIMS outperforms other zero-shot methods. MAIMS can generate\nmore rigorous explanation based on the outputs of mental scales\n","authors":["Wenyu Li","Yinuo Zhu","Xin Lin","Ming Li","Ziyue Jiang","Ziqian Zeng"],"pdf_url":"https://arxiv.org/pdf/2402.10948v2.pdf","comment":"4 pages,2 figures"},{"id":"http://arxiv.org/abs/2402.16363v5","updated":"2024-03-15T01:58:58Z","published":"2024-02-26T07:33:05Z","title":"LLM Inference Unveiled: Survey and Roofline Model Insights","summary":"  The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.\n","authors":["Zhihang Yuan","Yuzhang Shang","Yang Zhou","Zhen Dong","Zhe Zhou","Chenhao Xue","Bingzhe Wu","Zhikai Li","Qingyi Gu","Yong Jae Lee","Yan Yan","Beidi Chen","Guangyu Sun","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2402.16363v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14602v3","updated":"2024-03-15T01:53:58Z","published":"2022-08-31T02:38:16Z","title":"Continuous QA Learning with Structured Prompts","summary":"  QA models with lifelong learning (LL) abilities are important for practical\nQA applications, and architecture-based LL methods are reported to be an\neffective implementation for these models. However, it is non-trivial to extend\nprevious approaches to QA tasks since they either require access to task\nidentities in the testing phase or do not explicitly model samples from unseen\ntasks. In this paper, we propose Diana: a dynamic architecture-based lifelong\nQA model that tries to learn a sequence of QA tasks with a prompt enhanced\nlanguage model. Four types of hierarchically organized prompts are used in\nDiana to capture QA knowledge from different granularities. Specifically, we\ndedicate task-level prompts to capture task-specific knowledge to retain high\nLL performances and maintain instance-level prompts to learn knowledge shared\nacross different input samples to improve the model's generalization\nperformance. Moreover, we dedicate separate prompts to explicitly model unseen\ntasks and introduce a set of prompt key vectors to facilitate knowledge sharing\nbetween tasks. Extensive experiments demonstrate that Diana outperforms\nstate-of-the-art lifelong QA models, especially in handling unseen tasks.\n","authors":["Yinhe Zheng"],"pdf_url":"https://arxiv.org/pdf/2208.14602v3.pdf","comment":"Duplicate of arXiv:2305.06555 (Please cite arXiv:2305.06555 since it\n  is the camera ready version)"}]},"2024-03-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.12968v1","updated":"2024-03-19T17:59:56Z","published":"2024-03-19T17:59:56Z","title":"LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression","summary":"  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Menglin Xia","Xufang Luo","Jue Zhang","Qingwei Lin","Victor Rühle","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12964v1","updated":"2024-03-19T17:59:39Z","published":"2024-03-19T17:59:39Z","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language\n  Models","summary":"  Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.\n","authors":["Ce Zhang","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2403.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12958v1","updated":"2024-03-19T17:57:58Z","published":"2024-03-19T17:57:58Z","title":"Dated Data: Tracing Knowledge Cutoffs in Large Language Models","summary":"  Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models.\n","authors":["Jeffrey Cheng","Marc Marone","Orion Weller","Dawn Lawrie","Daniel Khashabi","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2403.12958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10081v2","updated":"2024-03-19T17:51:45Z","published":"2023-11-16T18:37:29Z","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback","summary":"  We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2311.10081v2.pdf","comment":"CVPR 2024. The feedback datasets are released at:\n  https://huggingface.co/datasets/YangyiYY/LVLM_NLF"},{"id":"http://arxiv.org/abs/2403.12936v1","updated":"2024-03-19T17:43:08Z","published":"2024-03-19T17:43:08Z","title":"Automatic Information Extraction From Employment Tribunal Judgements\n  Using Large Language Models","summary":"  Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.\n","authors":["Joana Ribeiro de Faria","Huiyuan Xie","Felix Steffek"],"pdf_url":"https://arxiv.org/pdf/2403.12936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12924v1","updated":"2024-03-19T17:28:51Z","published":"2024-03-19T17:28:51Z","title":"Supporting Energy Policy Research with Large Language Models","summary":"  The recent growth in renewable energy development in the United States has\nbeen accompanied by a simultaneous surge in renewable energy siting ordinances.\nThese zoning laws play a critical role in dictating the placement of wind and\nsolar resources that are critical for achieving low-carbon energy futures. In\nthis context, efficient access to and management of siting ordinance data\nbecomes imperative. The National Renewable Energy Laboratory (NREL) recently\nintroduced a public wind and solar siting database to fill this need. This\npaper presents a method for harnessing Large Language Models (LLMs) to automate\nthe extraction of these siting ordinances from legal documents, enabling this\ndatabase to maintain accurate up-to-date information in the rapidly changing\nenergy policy landscape. A novel contribution of this research is the\nintegration of a decision tree framework with LLMs. Our results show that this\napproach is 85 to 90% accurate with outputs that can be used directly in\ndownstream quantitative modeling. We discuss opportunities to use this work to\nsupport similar large-scale policy research in the energy sector. By unlocking\nnew efficiencies in the extraction and analysis of legal documents using LLMs,\nthis study enables a path forward for automated large-scale energy policy\nresearch.\n","authors":["Grant Buster","Pavlo Pinchuk","Jacob Barrons","Ryan McKeever","Aaron Levine","Anthony Lopez"],"pdf_url":"https://arxiv.org/pdf/2403.12924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12918v1","updated":"2024-03-19T17:21:29Z","published":"2024-03-19T17:21:29Z","title":"Generalizable and Stable Finetuning of Pretrained Language Models on\n  Low-Resource Texts","summary":"  Pretrained Language Models (PLMs) have advanced Natural Language Processing\n(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses\nsignificant challenges such as instability and overfitting. Previous methods\ntackle these issues by finetuning a strategically chosen subnetwork on a\ndownstream task, while keeping the remaining weights fixed to the pretrained\nweights. However, they rely on a suboptimal criteria for sub-network selection,\nleading to suboptimal solutions. To address these limitations, we propose a\nregularization method based on attention-guided weight mixup for finetuning\nPLMs. Our approach represents each network weight as a mixup of task-specific\nweight and pretrained weight, controlled by a learnable attention parameter,\nproviding finer control over sub-network selection. Furthermore, we employ a\nbi-level optimization (BLO) based framework on two separate splits of the\ntraining dataset, improving generalization and combating overfitting. We\nvalidate the efficacy of our proposed method through extensive experiments,\ndemonstrating its superiority over previous methods, particularly in the\ncontext of finetuning PLMs on low-resource datasets.\n","authors":["Sai Ashish Somayajula","Youwei Liang","Abhishek Singh","Li Zhang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.12918v1.pdf","comment":"Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11\n  tables, 3 figures"},{"id":"http://arxiv.org/abs/2309.13173v2","updated":"2024-03-19T17:11:41Z","published":"2023-09-22T20:29:34Z","title":"BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP","summary":"  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.\n","authors":["Mohsinul Kabir","Mohammed Saidul Islam","Md Tahmid Rahman Laskar","Mir Tafseer Nayeem","M Saiful Bari","Enamul Hoque"],"pdf_url":"https://arxiv.org/pdf/2309.13173v2.pdf","comment":"Accepted by LREC-COLING 2024. The first two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2306.08666v2","updated":"2024-03-19T17:01:03Z","published":"2023-06-14T17:57:24Z","title":"Radiology-GPT: A Large Language Model for Radiology","summary":"  We introduce Radiology-GPT, a large language model for radiology. Using an\ninstruction tuning approach on an extensive dataset of radiology domain\nknowledge, Radiology-GPT demonstrates superior performance compared to general\nlanguage models such as StableLM, Dolly and LLaMA. It exhibits significant\nversatility in radiological diagnosis, research, and communication. This work\nserves as a catalyst for future developments in clinical NLP. The successful\nimplementation of Radiology-GPT is indicative of the potential of localizing\ngenerative large language models, specifically tailored for distinctive medical\nspecialties, while ensuring adherence to privacy standards such as HIPAA. The\nprospect of developing individualized, large-scale language models that cater\nto specific needs of various hospitals presents a promising direction. The\nfusion of conversational competence and domain-specific knowledge in these\nmodels is set to foster future development in healthcare AI. A demo of\nRadiology-GPT is available at\nhttps://huggingface.co/spaces/allen-eric/radiology-gpt.\n","authors":["Zhengliang Liu","Aoxiao Zhong","Yiwei Li","Longtao Yang","Chao Ju","Zihao Wu","Chong Ma","Peng Shu","Cheng Chen","Sekeun Kim","Haixing Dai","Lin Zhao","Lichao Sun","Dajiang Zhu","Jun Liu","Wei Liu","Dinggang Shen","Xiang Li","Quanzheng Li","Tianming Liu"],"pdf_url":"https://arxiv.org/pdf/2306.08666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10311v4","updated":"2024-03-19T16:57:20Z","published":"2024-02-15T20:24:39Z","title":"The optimal placement of the head in the noun phrase. The case of\n  demonstrative, numeral, adjective and noun","summary":"  The word order of a sentence is shaped by multiple principles. The principle\nof syntactic dependency distance minimization is in conflict with the principle\nof surprisal minimization (or predictability maximization) in single head\nsyntactic dependency structures: while the former predicts that the head should\nbe placed at the center of the linear arrangement, the latter predicts that the\nhead should be placed at one of the ends (either first or last). A critical\nquestion is when surprisal minimization (or predictability maximization) should\nsurpass syntactic dependency distance minimization. In the context of single\nhead structures, it has been predicted that this is more likely to happen when\ntwo conditions are met, i.e. (a) fewer words are involved and (b) words are\nshorter. Here we test the prediction on the noun phrase when it is composed of\na demonstrative, a numeral, an adjective and a noun. We find that, across\npreferred orders in languages, the noun tends to be placed at one of the ends,\nconfirming the theoretical prediction. We also show evidence of anti locality\neffects: syntactic dependency distances in preferred orders are longer than\nexpected by chance.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2402.10311v4.pdf","comment":"Many typos corrected"},{"id":"http://arxiv.org/abs/2403.12900v1","updated":"2024-03-19T16:53:53Z","published":"2024-03-19T16:53:53Z","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly\n  Large Language Model Inference","summary":"  The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.\n","authors":["Baolin Li","Yankai Jiang","Vijay Gadepally","Devesh Tiwari"],"pdf_url":"https://arxiv.org/pdf/2403.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07875v3","updated":"2024-03-19T16:50:50Z","published":"2023-09-14T17:23:37Z","title":"Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\n  Models that Follow Instructions","summary":"  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.\n","authors":["Federico Bianchi","Mirac Suzgun","Giuseppe Attanasio","Paul Röttger","Dan Jurafsky","Tatsunori Hashimoto","James Zou"],"pdf_url":"https://arxiv.org/pdf/2309.07875v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10109v2","updated":"2024-03-19T16:43:09Z","published":"2024-02-15T17:05:48Z","title":"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction","summary":"  Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.\n","authors":["Denis Jered McInerney","William Dickinson","Lucy C. Flynn","Andrea C. Young","Geoffrey S. Young","Jan-Willem van de Meent","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2402.10109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09611v2","updated":"2024-03-19T16:37:13Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09684v2","updated":"2024-03-19T16:27:37Z","published":"2023-11-16T08:54:52Z","title":"Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation","summary":"  This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.\n","authors":["Zonghai Yao","Ahmed Jaafar","Beining Wang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.09684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12881v1","updated":"2024-03-19T16:26:10Z","published":"2024-03-19T16:26:10Z","title":"Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for\n  Large Language Models","summary":"  Open-sourced Large Language Models (LLMs) have achieved great success in\nvarious NLP tasks, however, they are still far inferior to API-based models\nwhen acting as agents. How to integrate agent ability into general LLMs becomes\na crucial and urgent problem. This paper first delivers three key observations:\n(1) the current agent training corpus is entangled with both formats following\nand agent reasoning, which significantly shifts from the distribution of its\npre-training data; (2) LLMs exhibit different learning speeds on the\ncapabilities required by agent tasks; and (3) current approaches have\nside-effects when improving agent abilities by introducing hallucinations.\nBased on the above findings, we propose Agent-FLAN to effectively Fine-tune\nLANguage models for Agents. Through careful decomposition and redesign of the\ntraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by\n3.5\\% across various agent evaluation datasets. With comprehensively\nconstructed negative samples, Agent-FLAN greatly alleviates the hallucination\nissues based on our established evaluation benchmark. Besides, it consistently\nimproves the agent capability of LLMs when scaling model sizes while slightly\nenhancing the general capability of LLMs. The code will be available at\nhttps://github.com/InternLM/Agent-FLAN.\n","authors":["Zehui Chen","Kuikun Liu","Qiuchen Wang","Wenwei Zhang","Jiangning Liu","Dahua Lin","Kai Chen","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12881v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2402.09283v2","updated":"2024-03-19T16:23:20Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.11366v2","updated":"2024-03-19T16:19:49Z","published":"2024-03-17T23:02:04Z","title":"JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n  Fine-Tuning","summary":"  The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.\n","authors":["Anique Tahir","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16635v2","updated":"2024-03-19T16:14:04Z","published":"2023-05-26T05:19:24Z","title":"Impossible Distillation: from Low-Quality Model to High-Quality Dataset\n  & Model for Summarization and Paraphrasing","summary":"  We present Impossible Distillation, a novel framework for paraphrasing and\nsentence summarization, that distills a high-quality dataset and model from a\nlow-quality teacher that itself cannot perform these tasks. Unlike prior works\nthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specific\narchitecture, we hypothesize and verify the paraphrastic proximity intrinsic to\npre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in\nthe LM distribution. By identifying and distilling generations from these\nsubspaces, Impossible Distillation produces a high-quality dataset and model\neven from GPT2-scale LMs. We evaluate our method on multiple benchmarks\nspanning unconstrained / syntax-controlled paraphrase generation and sentence\nsummarization. Our model with 770M parameters consistently outperforms strong\nbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPT\nitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher\ndiversity and fidelity than up to 13 times larger datasets.\n","authors":["Jaehun Jung","Peter West","Liwei Jiang","Faeze Brahman","Ximing Lu","Jillian Fisher","Taylor Sorensen","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2305.16635v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2301.13853v2","updated":"2024-03-19T16:08:36Z","published":"2023-01-26T05:25:34Z","title":"LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with\n  Conditional Generative Adversarial Neural Network","summary":"  Education is a right of all, however, every individual is different than\nothers. Teachers in post-communism era discover inherent individualism to\nequally train all towards job market of fourth industrial revolution. We can\nconsider scenario of ethnic minority education in academic practices. Ethnic\nminority group has grown in their own culture and would prefer to be taught in\ntheir native way. We have formulated such linguistic anthropology(how people\nlearn)based engagement as semi-supervised problem. Then, we have developed an\nconditional deep generative adversarial network algorithm namely LA-GAN to\nclassify linguistic ethnographic features in student engagement. Theoretical\njustification proves the objective, regularization and loss function of our\nsemi-supervised adversarial model. Survey questions are prepared to reach some\nform of assumptions about z-generation and ethnic minority group, whose\nlearning style, learning approach and preference are our main area of interest.\n","authors":["Rossi Kamal","Zuzana Kubincova"],"pdf_url":"https://arxiv.org/pdf/2301.13853v2.pdf","comment":"This submission has been withdrawn by arXiv administrators as the\n  second author was added without their knowledge or consent"},{"id":"http://arxiv.org/abs/2403.12862v1","updated":"2024-03-19T16:06:10Z","published":"2024-03-19T16:06:10Z","title":"Epistemology of Language Models: Do Language Models Have Holistic\n  Knowledge?","summary":"  This paper investigates the inherent knowledge in language models from the\nperspective of epistemological holism. The purpose of this paper is to explore\nwhether LLMs exhibit characteristics consistent with epistemological holism.\nThese characteristics suggest that core knowledge, such as general scientific\nknowledge, each plays a specific role, serving as the foundation of our\nknowledge system and being difficult to revise. To assess these traits related\nto holism, we created a scientific reasoning dataset and examined the\nepistemology of language models through three tasks: Abduction, Revision, and\nArgument Generation. In the abduction task, the language models explained\nsituations while avoiding revising the core knowledge. However, in other tasks,\nthe language models were revealed not to distinguish between core and\nperipheral knowledge, showing an incomplete alignment with holistic knowledge\nprinciples.\n","authors":["Minsu Kim","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2403.12862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01811v2","updated":"2024-03-19T15:40:52Z","published":"2024-03-04T07:58:26Z","title":"Enhancing Multi-Domain Automatic Short Answer Grading through an\n  Explainable Neuro-Symbolic Pipeline","summary":"  Grading short answer questions automatically with interpretable reasoning\nbehind the grading decision is a challenging goal for current transformer\napproaches. Justification cue detection, in combination with logical reasoners,\nhas shown a promising direction for neuro-symbolic architectures in ASAG. But,\none of the main challenges is the requirement of annotated justification cues\nin the students' responses, which only exist for a few ASAG datasets. To\novercome this challenge, we contribute (1) a weakly supervised annotation\nprocedure for justification cues in ASAG datasets, and (2) a neuro-symbolic\nmodel for explainable ASAG based on justification cues. Our approach improves\nupon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short\nAnswer Feedback dataset in a bilingual, multi-domain, and multi-question\ntraining setup. This result shows that our approach provides a promising\ndirection for generating high-quality grades and accompanying explanations for\nfuture research in ASAG and educational NLP.\n","authors":["Felix Künnecke","Anna Filighera","Colin Leong","Tim Steuer"],"pdf_url":"https://arxiv.org/pdf/2403.01811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12809v1","updated":"2024-03-19T15:07:22Z","published":"2024-03-19T15:07:22Z","title":"Comparing Explanation Faithfulness between Multilingual and Monolingual\n  Fine-tuned Language Models","summary":"  In many real natural language processing application scenarios, practitioners\nnot only aim to maximize predictive performance but also seek faithful\nexplanations for the model predictions. Rationales and importance distribution\ngiven by feature attribution methods (FAs) provide insights into how different\nparts of the input contribute to a prediction. Previous studies have explored\nhow different factors affect faithfulness, mainly in the context of monolingual\nEnglish models. On the other hand, the differences in FA faithfulness between\nmultilingual and monolingual models have yet to be explored. Our extensive\nexperiments, covering five languages and five popular FAs, show that FA\nfaithfulness varies between multilingual and monolingual models. We find that\nthe larger the multilingual model, the less faithful the FAs are compared to\nits counterpart monolingual models.Our further analysis shows that the\nfaithfulness disparity is potentially driven by the differences between model\ntokenizers. Our code is available:\nhttps://github.com/casszhao/multilingual-faith.\n","authors":["Zhixue Zhao","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2403.12809v1.pdf","comment":"Accepted at NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.12805v1","updated":"2024-03-19T15:06:53Z","published":"2024-03-19T15:06:53Z","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","summary":"  Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.\n","authors":["Pierre Dognin","Jesus Rios","Ronny Luss","Inkit Padhi","Matthew D Riemer","Miao Liu","Prasanna Sattigeri","Manish Nagireddy","Kush R. Varshney","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2403.12805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12799v1","updated":"2024-03-19T15:01:14Z","published":"2024-03-19T15:01:14Z","title":"Investigating Text Shortening Strategy in BERT: Truncation vs\n  Summarization","summary":"  The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.\n","authors":["Mirza Alim Mutasodirin","Radityo Eko Prasojo"],"pdf_url":"https://arxiv.org/pdf/2403.12799v1.pdf","comment":"The 13th International Conference on Advanced Computer Science and\n  Information Systems (ICACSIS 2021)"},{"id":"http://arxiv.org/abs/2403.12776v1","updated":"2024-03-19T14:44:45Z","published":"2024-03-19T14:44:45Z","title":"Automated Data Curation for Robust Language Model Fine-Tuning","summary":"  Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).\n","authors":["Jiuhai Chen","Jonas Mueller"],"pdf_url":"https://arxiv.org/pdf/2403.12776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11453v3","updated":"2024-03-19T14:44:22Z","published":"2024-02-18T04:28:28Z","title":"MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific\n  Data Visualization","summary":"  Scientific data visualization plays a crucial role in research by enabling\nthe direct display of complex information and assisting researchers in\nidentifying implicit patterns. Despite its importance, the use of Large\nLanguage Models (LLMs) for scientific data visualization remains rather\nunexplored. In this study, we introduce MatPlotAgent, an efficient\nmodel-agnostic LLM agent framework designed to automate scientific data\nvisualization tasks. Leveraging the capabilities of both code LLMs and\nmulti-modal LLMs, MatPlotAgent consists of three core modules: query\nunderstanding, code generation with iterative debugging, and a visual feedback\nmechanism for error correction. To address the lack of benchmarks in this\nfield, we present MatPlotBench, a high-quality benchmark consisting of 100\nhuman-verified test cases. Additionally, we introduce a scoring approach that\nutilizes GPT-4V for automatic evaluation. Experimental results demonstrate that\nMatPlotAgent can improve the performance of various LLMs, including both\ncommercial and open-source models. Furthermore, the proposed evaluation method\nshows a strong correlation with human-annotated scores.\n","authors":["Zhiyu Yang","Zihan Zhou","Shuo Wang","Xin Cong","Xu Han","Yukun Yan","Zhenghao Liu","Zhixing Tan","Pengyuan Liu","Dong Yu","Zhiyuan Liu","Xiaodong Shi","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.11453v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2310.16226v2","updated":"2024-03-19T14:17:54Z","published":"2023-10-24T22:41:14Z","title":"TiC-CLIP: Continual Training of CLIP Models","summary":"  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to \\emph{continually} train these models. This problem is\nexacerbated by the lack of any large scale continual learning benchmarks or\nbaselines. We introduce the first set of web-scale Time-Continual (TiC)\nbenchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and\nTiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped\nimage-text pairs spanning 9 years (2014--2022). We first use our benchmarks to\ncurate various \\emph{dynamic} evaluations to measure temporal robustness of\nexisting models. We show OpenAI's CLIP (trained on data up to 2020) loses\n$\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from 2021--2022\ncompared with more recently trained models in OpenCLIP repository. We then\nstudy how to efficiently train models on time-continuous data. We demonstrate\nthat a simple rehearsal-based approach that continues training from the last\ncheckpoint and replays old data reduces compute by $2.5\\times$ when compared to\nthe standard practice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.\n","authors":["Saurabh Garg","Mehrdad Farajtabar","Hadi Pouransari","Raviteja Vemulapalli","Sachin Mehta","Oncel Tuzel","Vaishaal Shankar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2310.16226v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12749v1","updated":"2024-03-19T14:12:54Z","published":"2024-03-19T14:12:54Z","title":"Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian\n  Dialectal Data","summary":"  Named Entity Recognition (NER) is a fundamental task to extract key\ninformation from texts, but annotated resources are scarce for dialects. This\npaper introduces the first dialectal NER dataset for German, BarNER, with 161K\ntokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets\n(bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The\nBavarian dialect differs from standard German in lexical distribution,\nsyntactic construction, and entity information. We conduct in-domain,\ncross-domain, sequential, and joint experiments on two Bavarian and three\nGerman corpora and present the first comprehensive NER results on Bavarian.\nIncorporating knowledge from the larger German NER (sub-)datasets notably\nimproves on bar-wiki and moderately on bar-tweet. Inversely, training first on\nBavarian contributes slightly to the seminal German CoNLL 2006 corpus.\nMoreover, with gold dialect labels on Bavarian tweets, we assess multi-task\nlearning between five NER and two Bavarian-German dialect identification tasks\nand achieve NER SOTA on bar-wiki. We substantiate the necessity of our\nlow-resource BarNER corpus and the importance of diversity in dialects, genres,\nand topics in enhancing model performance.\n","authors":["Siyao Peng","Zihang Sun","Huangyan Shan","Marie Kolm","Verena Blaschke","Ekaterina Artemova","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.12749v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.12744v1","updated":"2024-03-19T14:07:28Z","published":"2024-03-19T14:07:28Z","title":"Instructing Large Language Models to Identify and Ignore Irrelevant\n  Conditions","summary":"  Math word problem (MWP) solving requires generating a reasoning path based on\na given problem description that often contains irrelevant conditions. Existing\nchain-of-thought (CoT) prompting methods elicited multi-step reasoning\nabilities of large language models (LLMs) to solve MWPs. However, they were\nseriously confused by the irrelevant conditions, resulting in low accuracy. In\nthis paper, we propose a novel approach named I$^3$C that instructs LLMs to\nidentify and ignore irrelevant conditions. It identifies a set of irrelevant\ncondition candidates that have a weak semantic relevance with the question.\nThen it prompts LLMs to verify the irrelevant conditions. Lastly it instructs\nthe LLMs with the verification on relevant and irrelevant conditions to avoid\nconfusion and improve reasoning paths. Moreover, we propose to select (problem,\nreasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot\nreasoning. We develop I$^3$C-Select that selects the most confusing problems\nbased on the semantic relevance measurement. We conduct extensive experiments\non eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to\nimprove the performance of solving MWPs. Notably, with GPT-3.5-Turbo and\nI$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and\nGSM-ICM-1K, respectively, significantly outperforming the state-of-the-art\nfew-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is\nmade publicly available at https://wzy6642.github.io/I3C.github.io/.\n","authors":["Zhenyu Wu","Chao Shen","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12744v1.pdf","comment":"NAACL 2024 - Camera Ready"},{"id":"http://arxiv.org/abs/2403.12721v1","updated":"2024-03-19T13:30:47Z","published":"2024-03-19T13:30:47Z","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched\n  with Linguistic and Genre Annotation","summary":"  This paper presents a collection of highly comparable web corpora of\nSlovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,\ncovering thereby the whole spectrum of official languages in the South Slavic\nlanguage space. The collection of these corpora comprises a total of 13 billion\ntokens of texts from 26 million documents. The comparability of the corpora is\nensured by a comparable crawling setup and the usage of identical crawling and\npost-processing technology. All the corpora were linguistically annotated with\nthe state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and\nenriched with document-level genre information via the Transformer-based\nmultilingual X-GENRE classifier, which further enhances comparability at the\nlevel of linguistic annotation and metadata enrichment. The genre-focused\nanalysis of the resulting corpora shows a rather consistent distribution of\ngenres throughout the seven corpora, with variations in the most prominent\ngenre categories being well-explained by the economic strength of each language\ncommunity. A comparison of the distribution of genre categories across the\ncorpora indicates that web corpora from less developed countries primarily\nconsist of news articles. Conversely, web corpora from economically more\ndeveloped countries exhibit a smaller proportion of news content, with a\ngreater presence of promotional and opinionated texts.\n","authors":["Nikola Ljubešić","Taja Kuzman"],"pdf_url":"https://arxiv.org/pdf/2403.12721v1.pdf","comment":"Accepted to the LREC-COLING 2024 conference"},{"id":"http://arxiv.org/abs/2310.08559v3","updated":"2024-03-19T13:18:22Z","published":"2023-10-12T17:51:10Z","title":"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of\n  Language Models with Hypothesis Refinement","summary":"  The ability to derive underlying principles from a handful of observations\nand then generalize to novel situations -- known as inductive reasoning -- is\ncentral to human intelligence. Prior work suggests that language models (LMs)\noften fall short on inductive reasoning, despite achieving impressive success\non research benchmarks. In this work, we conduct a systematic study of the\ninductive reasoning capabilities of LMs through iterative hypothesis\nrefinement, a technique that more closely mirrors the human inductive process\nthan standard input-output prompting. Iterative hypothesis refinement employs a\nthree-step process: proposing, selecting, and refining hypotheses in the form\nof textual rules. By examining the intermediate rules, we observe that LMs are\nphenomenal hypothesis proposers (i.e., generating candidate rules), and when\ncoupled with a (task-specific) symbolic interpreter that is able to\nsystematically filter the proposed set of rules, this hybrid approach achieves\nstrong results across inductive reasoning benchmarks that require inducing\ncausal relations, language-like instructions, and symbolic concepts. However,\nthey also behave as puzzling inductive reasoners, showing notable performance\ngaps between rule induction (i.e., identifying plausible rules) and rule\napplication (i.e., applying proposed rules to instances), suggesting that LMs\nare proposing hypotheses without being able to actually apply the rules.\nThrough empirical and human analyses, we further reveal several discrepancies\nbetween the inductive reasoning processes of LMs and humans, shedding light on\nboth the potentials and limitations of using LMs in inductive reasoning tasks.\n","authors":["Linlu Qiu","Liwei Jiang","Ximing Lu","Melanie Sclar","Valentina Pyatkin","Chandra Bhagavatula","Bailin Wang","Yoon Kim","Yejin Choi","Nouha Dziri","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2310.08559v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.05326v3","updated":"2024-03-19T12:53:27Z","published":"2024-03-08T14:05:36Z","title":"ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in\n  Dialogues","summary":"  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.\n","authors":["Yiding Liu","Jingjing Wang","Jiamin Luo","Tao Zeng","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05326v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12725v3","updated":"2024-03-19T12:30:53Z","published":"2023-06-22T07:57:19Z","title":"Generative Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask.\n","authors":["Senbao Shi","Zhenran Xu","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.12725v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.07269v2","updated":"2024-03-19T12:27:33Z","published":"2023-08-14T16:52:42Z","title":"EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models","summary":"  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.\n","authors":["Peng Wang","Ningyu Zhang","Bozhong Tian","Zekun Xi","Yunzhi Yao","Ziwen Xu","Mengru Wang","Shengyu Mao","Xiaohan Wang","Siyuan Cheng","Kangwei Liu","Yuansheng Ni","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.07269v2.pdf","comment":"Code: https://github.com/zjunlp/EasyEdit HF Demo:\n  https://huggingface.co/spaces/zjunlp/EasyEdit Video:\n  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit"},{"id":"http://arxiv.org/abs/2403.12678v1","updated":"2024-03-19T12:24:20Z","published":"2024-03-19T12:24:20Z","title":"Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights","summary":"  The Canadian air travel sector has seen a significant increase in flight\ndelays, cancellations, and other issues concerning passenger rights.\nRecognizing this demand, we present a chatbot to assist passengers and educate\nthem about their rights. Our system breaks a complex user input into simple\nqueries which are used to retrieve information from a collection of documents\ndetailing air travel regulations. The most relevant passages from these\ndocuments are presented along with links to the original documents and the\ngenerated queries, enabling users to dissect and leverage the information for\ntheir unique circumstances. The system successfully overcomes two predominant\nchallenges: understanding complex user inputs, and delivering accurate answers,\nfree of hallucinations, that passengers can rely on for making informed\ndecisions. A user study comparing the chatbot to a Google search demonstrated\nthe chatbot's usefulness and ease of use. Beyond the primary goal of providing\naccurate and timely information to air passengers regarding their rights, we\nhope that this system will also enable further research exploring the tradeoff\nbetween the user-friendly conversational interface of chatbots and the accuracy\nof retrieval systems.\n","authors":["Maksym Taranukhin","Sahithya Ravi","Gabor Lukacs","Evangelos Milios","Vered Shwartz"],"pdf_url":"https://arxiv.org/pdf/2403.12678v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2403.12675v1","updated":"2024-03-19T12:21:20Z","published":"2024-03-19T12:21:20Z","title":"Pragmatic Competence Evaluation of Large Language Models for Korean","summary":"  The current evaluation of Large Language Models (LLMs) predominantly relies\non benchmarks focusing on their embedded knowledge by testing through\nmultiple-choice questions (MCQs), a format inherently suited for automated\nevaluation. Our study extends this evaluation to explore LLMs' pragmatic\ncompetence--a facet previously underexamined before the advent of sophisticated\nLLMs, specifically in the context of Korean. We employ two distinct evaluation\nsetups: the conventional MCQ format, adapted for automatic evaluation, and\nOpen-Ended Questions (OEQs), assessed by human experts, to examine LLMs'\nnarrative response capabilities without predefined options. Our findings reveal\nthat GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups,\nrespectively, with HyperCLOVA X, an LLM optimized for Korean, closely\nfollowing, especially in the OEQ setup, demonstrating a score of 81.56 with a\nmarginal difference of 4.13 points compared to GPT-4. Furthermore, while\nfew-shot learning strategies generally enhance LLM performance,\nChain-of-Thought (CoT) prompting introduces a bias toward literal\ninterpretations, hindering accurate pragmatic inference. Considering the\ngrowing expectation for LLMs to understand and produce language that aligns\nwith human communicative norms, our findings emphasize the importance for\nadvancing LLMs' abilities to grasp and convey sophisticated meanings beyond\nmere literal interpretations.\n","authors":["Dojun Park","Jiwoo Lee","Hyeyun Jeong","Seohyun Park","Sungeun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.12675v1.pdf","comment":"9 pages, submitted for publication"},{"id":"http://arxiv.org/abs/2403.12666v1","updated":"2024-03-19T12:02:38Z","published":"2024-03-19T12:02:38Z","title":"Multi-Dimensional Machine Translation Evaluation: Model Evaluation and\n  Resource for Korean","summary":"  Almost all frameworks for the manual or automatic evaluation of machine\ntranslation characterize the quality of an MT output with a single number. An\nexception is the Multidimensional Quality Metrics (MQM) framework which offers\na fine-grained ontology of quality dimensions for scoring (such as style,\nfluency, accuracy, and terminology). Previous studies have demonstrated the\nfeasibility of MQM annotation but there are, to our knowledge, no computational\nmodels that predict MQM scores for novel texts, due to a lack of resources. In\nthis paper, we address these shortcomings by (a) providing a 1200-sentence MQM\nevaluation benchmark for the language pair English-Korean and (b) reframing MT\nevaluation as the multi-task problem of simultaneously predicting several MQM\nscores using SOTA language models, both in a reference-based MT evaluation\nsetup and a reference-free quality estimation (QE) setup. We find that\nreference-free setup outperforms its counterpart in the style dimension while\nreference-based models retain an edge regarding accuracy. Overall, RemBERT\nemerges as the most promising model. Through our evaluation, we offer an\ninsight into the translation quality in a more fine-grained, interpretable\nmanner.\n","authors":["Dojun Park","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2403.12666v1.pdf","comment":"9 pages, accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.10314v2","updated":"2024-03-19T11:53:15Z","published":"2023-05-17T15:53:31Z","title":"LeTI: Learning to Generate from Textual Interactions","summary":"  Fine-tuning pre-trained language models (LMs) is essential for enhancing\ntheir capabilities. Existing techniques commonly fine-tune on input-output\npairs (e.g., instruction tuning) or with numerical rewards that gauge the\noutput quality (e.g., RLHF). We explore LMs' potential to learn from textual\ninteractions (LETI) that not only check their correctness with binary labels\nbut also pinpoint and explain errors in their outputs through textual feedback.\nOur focus is the code generation task, where the model produces code based on\nnatural language instructions. This setting invites a natural and scalable way\nto acquire textual feedback: the error messages and stack traces from code\nexecution using a Python interpreter. LETI iteratively fine-tunes the model,\nusing the LM objective, on a concatenation of natural language instructions,\nLM-generated programs, and textual feedback. Prepended to this fine-tuning\ntext, a binary reward token is used to differentiate correct and buggy\nsolutions. LETI requires no ground-truth outputs for training and even\noutperforms a fine-tuned baseline that does. LETI not only improves the\nperformance of LMs on a code generation dataset MBPP, but also generalizes to\nother datasets. Trained on MBPP, it achieves comparable or better performance\nthan the base LMs on unseen problems in HumanEval. Furthermore, compared to\nbinary feedback, we observe that textual feedback leads to improved generation\nquality and sample efficiency, achieving the same performance with fewer than\nhalf of the gradient steps. LETI is equally applicable in natural language\ntasks when they can be formulated as code generation, which we empirically\nverified on event argument extraction.\n","authors":["Xingyao Wang","Hao Peng","Reyhaneh Jabbarvand","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2305.10314v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.06097v2","updated":"2024-03-19T11:36:26Z","published":"2024-03-10T05:12:16Z","title":"Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese\n  Address Entity Recognition Dataset for UAV Delivery","summary":"  We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.\n","authors":["Yuxuan Yao","Sichun Luo","Haohan Zhao","Guanzhi Deng","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2403.06097v2.pdf","comment":"Accepted by TheWebConf'24 (WWW'24) as a Resource Paper"},{"id":"http://arxiv.org/abs/2403.07311v4","updated":"2024-03-19T11:08:02Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Chong Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v4.pdf","comment":"23 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.12601v1","updated":"2024-03-19T10:11:14Z","published":"2024-03-19T10:11:14Z","title":"LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation\n  Benchmark for Chinese Large Language Models","summary":"  Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.\n","authors":["Chuang Liu","Renren Jin","Yuqi Ren","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.12601v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11183v2","updated":"2024-03-19T10:09:20Z","published":"2024-03-17T12:12:33Z","title":"Decoding Continuous Character-based Language from Non-invasive Brain\n  Recordings","summary":"  Deciphering natural language from brain activity through non-invasive devices\nremains a formidable challenge. Previous non-invasive decoders either require\nmultiple experiments with identical stimuli to pinpoint cortical regions and\nenhance signal-to-noise ratios in brain activity, or they are limited to\ndiscerning basic linguistic elements such as letters and words. We propose a\nnovel approach to decoding continuous language from single-trial non-invasive\nfMRI recordings, in which a three-dimensional convolutional network augmented\nwith information bottleneck is developed to automatically identify responsive\nvoxels to stimuli, and a character-based decoder is designed for the semantic\nreconstruction of continuous language characterized by inherent character\nstructures. The resulting decoder can produce intelligible textual sequences\nthat faithfully capture the meaning of perceived speech both within and across\nsubjects, while existing decoders exhibit significantly inferior performance in\ncross-subject contexts. The ability to decode continuous language from single\ntrials across subjects demonstrates the promising applications of non-invasive\nlanguage brain-computer interfaces in both healthcare and neuroscience.\n","authors":["Cenyuan Zhang","Xiaoqing Zheng","Ruicheng Yin","Shujie Geng","Jianhan Xu","Xuan Gao","Changze Lv","Zixuan Ling","Xuanjing Huang","Miao Cao","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.11183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12596v1","updated":"2024-03-19T10:03:07Z","published":"2024-03-19T10:03:07Z","title":"Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs","summary":"  Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\n\\citet{chen2023pali3}, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\n\\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by \\citet{hsieh2023distilling}.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt \\cite{chen2023program}, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V.\n","authors":["Victor Carbune","Hassan Mansoor","Fangyu Liu","Rahul Aralikatte","Gilles Baechler","Jindong Chen","Abhanshu Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.12596v1.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2403.12582v1","updated":"2024-03-19T09:45:33Z","published":"2024-03-19T09:45:33Z","title":"AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented\n  Stock-Chain Framework","summary":"  The task of financial analysis primarily encompasses two key areas: stock\ntrend prediction and the corresponding financial question answering. Currently,\nmachine learning and deep learning algorithms (ML&DL) have been widely applied\nfor stock trend predictions, leading to significant progress. However, these\nmethods fail to provide reasons for predictions, lacking interpretability and\nreasoning processes. Also, they can not integrate textual information such as\nfinancial news or reports. Meanwhile, large language models (LLMs) have\nremarkable textual understanding and generation ability. But due to the\nscarcity of financial training datasets and limited integration with real-time\nknowledge, LLMs still suffer from hallucinations and are unable to keep up with\nthe latest information. To tackle these challenges, we first release AlphaFin\ndatasets, combining traditional research datasets, real-time financial data,\nand handwritten chain-of-thought (CoT) data. It has a positive impact on\ntraining LLMs for completing financial analysis. We then use AlphaFin datasets\nto benchmark a state-of-the-art method, called Stock-Chain, for effectively\ntackling the financial analysis task, which integrates retrieval-augmented\ngeneration (RAG) techniques. Extensive experiments are conducted to demonstrate\nthe effectiveness of our framework on financial analysis.\n","authors":["Xiang Li","Zhenyu Li","Chen Shi","Yong Xu","Qing Du","Mingkui Tan","Jun Huang","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2403.12582v1.pdf","comment":"COLING 2024. The first three authors contributed equally. Project\n  website: https://github.com/AlphaFin-proj/AlphaFin"},{"id":"http://arxiv.org/abs/2403.12563v1","updated":"2024-03-19T09:17:25Z","published":"2024-03-19T09:17:25Z","title":"Simple Hack for Transformers against Heavy Long-Text Classification on a\n  Time- and Memory-Limited GPU Service","summary":"  Many NLP researchers rely on free computational services, such as Google\nColab, to fine-tune their Transformer models, causing a limitation for\nhyperparameter optimization (HPO) in long-text classification due to the method\nhaving quadratic complexity and needing a bigger resource. In Indonesian, only\na few works were found on long-text classification using Transformers. Most\nonly use a small amount of data and do not report any HPO. In this study, using\n18k news articles, we investigate which pretrained models are recommended to\nuse based on the output length of the tokenizer. We then compare some hacks to\nshorten and enrich the sequences, which are the removals of stopwords,\npunctuation, low-frequency words, and recurring words. To get a fair\ncomparison, we propose and run an efficient and dynamic HPO procedure that can\nbe done gradually on a limited resource and does not require a long-running\noptimization library. Using the best hack found, we then compare 512, 256, and\n128 tokens length. We find that removing stopwords while keeping punctuation\nand low-frequency words is the best hack. Some of our setups manage to\noutperform taking 512 first tokens using a smaller 128 or 256 first tokens\nwhich manage to represent the same information while requiring less\ncomputational resources. The findings could help developers to efficiently\npursue optimal performance of the models using limited resources.\n","authors":["Mirza Alim Mutasodirin","Radityo Eko Prasojo","Achmad F. Abka","Hanif Rasyidi"],"pdf_url":"https://arxiv.org/pdf/2403.12563v1.pdf","comment":"The 10th International Conference on Advanced Informatics: Concepts,\n  Theory, and Applications (ICAICTA 2023)"},{"id":"http://arxiv.org/abs/2309.13345v3","updated":"2024-03-19T09:00:32Z","published":"2023-09-23T11:36:15Z","title":"BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling\n  Capacities of Large Language Models","summary":"  Large language models (LLMs) have achieved dramatic proficiency over NLP\ntasks with normal length. Recently, multiple studies have committed to\nextending the context length and enhancing the long text modeling capabilities\nof LLMs. To comprehensively evaluate the long context ability of LLMs, we\npropose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed\nwith four principles: comprehensive capacity evaluation, avoidance of data\ncontamination, accurate automatic evaluation, and different length levels. It\nconsists of 10 datasets from 5 different long text understanding tasks, i.e.\nquestion answering, hallucination detection, text sorting, language modeling,\nand code completion, to cover core capacities and various domains of LLMs. We\nconduct experiments with five long context models on BAMBOO and further discuss\nfour key research questions of long text. We also qualitatively analyze current\nlong context models and point out future directions for enhancing long text\nmodeling capacities. We release our data, prompts, and code at\nhttps://github.com/RUCAIBox/BAMBOO.\n","authors":["Zican Dong","Tianyi Tang","Junyi Li","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2309.13345v3.pdf","comment":"Accepted for the Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024"},{"id":"http://arxiv.org/abs/2403.12556v1","updated":"2024-03-19T09:00:23Z","published":"2024-03-19T09:00:23Z","title":"Factorized Learning Assisted with Large Language Model for Gloss-free\n  Sign Language Translation","summary":"  Previous Sign Language Translation (SLT) methods achieve superior performance\nby relying on gloss annotations. However, labeling high-quality glosses is a\nlabor-intensive task, which limits the further development of SLT. Although\nsome approaches work towards gloss-free SLT through jointly training the visual\nencoder and translation network, these efforts still suffer from poor\nperformance and inefficient use of the powerful Large Language Model (LLM).\nMost seriously, we find that directly introducing LLM into SLT will lead to\ninsufficient learning of visual representations as LLM dominates the learning\ncurve. To address these problems, we propose Factorized Learning assisted with\nLarge Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the\ntraining process into two stages. In the visual initialing stage, we employ a\nlightweight translation model after the visual encoder to pre-train the visual\nencoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the\nvisual encoder and integrate it with a pre-trained LLM to inspire the LLM's\ntranslation potential. This factorized training strategy proves to be highly\neffective as evidenced by significant improvements achieved across three SLT\ndatasets which are all conducted under the gloss-free setting.\n","authors":["Zhigang Chen","Benjia Zhou","Jun Li","Jun Wan","Zhen Lei","Ning Jiang","Quan Lu","Guoqing Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12556v1.pdf","comment":"Accepted by LREC-COLING-2024"},{"id":"http://arxiv.org/abs/2403.00241v2","updated":"2024-03-19T08:34:05Z","published":"2024-03-01T03:07:32Z","title":"CASIMIR: A Corpus of Scientific Articles enhanced with Multiple\n  Author-Integrated Revisions","summary":"  Writing a scientific article is a challenging task as it is a highly codified\nand specific genre, consequently proficiency in written communication is\nessential for effectively conveying research findings and ideas. In this\narticle, we propose an original textual resource on the revision step of the\nwriting process of scientific articles. This new dataset, called CASIMIR,\ncontains the multiple revised versions of 15,646 scientific articles from\nOpenReview, along with their peer reviews. Pairs of consecutive versions of an\narticle are aligned at sentence-level while keeping paragraph location\ninformation as metadata for supporting future revision studies at the discourse\nlevel. Each pair of revised sentences is enriched with automatically extracted\nedits and associated revision intention. To assess the initial quality on the\ndataset, we conducted a qualitative study of several state-of-the-art text\nrevision approaches and compared various evaluation metrics. Our experiments\nled us to question the relevance of the current evaluation methods for the text\nrevision task.\n","authors":["Leane Jourdan","Florian Boudin","Nicolas Hernandez","Richard Dufour"],"pdf_url":"https://arxiv.org/pdf/2403.00241v2.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2305.08473v2","updated":"2024-03-19T07:59:52Z","published":"2023-05-15T09:24:48Z","title":"Shared and Private Information Learning in Multimodal Sentiment Analysis\n  with Deep Modal Alignment and Self-supervised Multi-Task Learning","summary":"  Designing an effective representation learning method for multimodal\nsentiment analysis tasks is a crucial research direction. The challenge lies in\nlearning both shared and private information in a complete modal\nrepresentation, which is difficult with uniform multimodal labels and a raw\nfeature fusion approach. In this work, we propose a deep modal shared\ninformation learning module based on the covariance matrix to capture the\nshared information between modalities. Additionally, we use a label generation\nmodule based on a self-supervised learning strategy to capture the private\ninformation of the modalities. Our module is plug-and-play in multimodal tasks,\nand by changing the parameterization, it can adjust the information exchange\nrelationship between the modes and learn the private or shared information\nbetween the specified modes. We also employ a multi-task learning strategy to\nhelp the model focus its attention on the modal differentiation training data.\nWe provide a detailed formulation derivation and feasibility proof for the\ndesign of the deep modal shared information learning module. We conduct\nextensive experiments on three common multimodal sentiment analysis baseline\ndatasets, and the experimental results validate the reliability of our model.\nFurthermore, we explore more combinatorial techniques for the use of the\nmodule. Our approach outperforms current state-of-the-art methods on most of\nthe metrics of the three public datasets.\n","authors":["Songning Lai","Jiakang Li","Guinan Guo","Xifeng Hu","Yulong Li","Yuan Tan","Zichen Song","Yutong Liu","Zhaoxia Ren","Chun Wan","Danmin Miao","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2305.08473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12526v1","updated":"2024-03-19T07:56:42Z","published":"2024-03-19T07:56:42Z","title":"Prompt-based Graph Model for Joint Liberal Event Extraction and Event\n  Schema Induction","summary":"  Events are essential components of speech and texts, describing the changes\nin the state of entities. The event extraction task aims to identify and\nclassify events and find their participants according to event schemas.\nManually predefined event schemas have limited coverage and are hard to migrate\nacross domains. Therefore, the researchers propose Liberal Event Extraction\n(LEE), which aims to extract events and discover event schemas simultaneously.\nHowever, existing LEE models rely heavily on external language knowledge bases\nand require the manual development of numerous rules for noise removal and\nknowledge alignment, which is complex and laborious. To this end, we propose a\nPrompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we\nuse a prompt-based model to obtain candidate triggers and arguments, and then\nbuild heterogeneous event graphs to encode the structures within and between\nevents. Experimental results prove that our approach achieves excellent\nperformance with or without predefined event schemas, while the automatically\ndetected event schemas are proven high quality.\n","authors":["Haochen Li","Di Geng"],"pdf_url":"https://arxiv.org/pdf/2403.12526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12523v1","updated":"2024-03-19T07:50:32Z","published":"2024-03-19T07:50:32Z","title":"GraphERE: Jointly Multiple Event-Event Relation Extraction via\n  Graph-Enhanced Event Embeddings","summary":"  Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.\n","authors":["Haochen Li","Di Geng"],"pdf_url":"https://arxiv.org/pdf/2403.12523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12500v1","updated":"2024-03-19T07:07:13Z","published":"2024-03-19T07:07:13Z","title":"A Large Collection of Model-generated Contradictory Responses for\n  Consistency-aware Dialogue Systems","summary":"  Mitigating the generation of contradictory responses poses a substantial\nchallenge in dialogue response generation. The quality and quantity of\navailable contradictory response data play a vital role in suppressing these\ncontradictions, offering two significant benefits. First, having access to\nlarge contradiction data enables a comprehensive examination of their\ncharacteristics. Second, data-driven methods to mitigate contradictions may be\nenhanced with large-scale contradiction data for training. Nevertheless, no\nattempt has been made to build an extensive collection of model-generated\ncontradictory responses. In this paper, we build a large dataset of response\ngeneration models' contradictions for the first time. Then, we acquire valuable\ninsights into the characteristics of model-generated contradictions through an\nextensive analysis of the collected responses. Lastly, we also demonstrate how\nthis dataset substantially enhances the performance of data-driven\ncontradiction suppression methods.\n","authors":["Shiki Sato","Reina Akama","Jun Suzuki","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2403.12500v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2403.12482v1","updated":"2024-03-19T06:39:47Z","published":"2024-03-19T06:39:47Z","title":"Embodied LLM Agents Learn to Cooperate in Organized Teams","summary":"  Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.\n","authors":["Xudong Guo","Kaixuan Huang","Jiale Liu","Wenhui Fan","Natalia Vélez","Qingyun Wu","Huazheng Wang","Thomas L. Griffiths","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17231v2","updated":"2024-03-19T06:25:40Z","published":"2024-02-27T05:50:35Z","title":"MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical\n  Reasoning","summary":"  Tool-augmented Large Language Models (TALM) are known to enhance the skillset\nof large language models (LLM), thereby, leading to their improved reasoning\nabilities across many tasks. While, TALMs have been successfully employed in\ndifferent question-answering benchmarks, their efficacy on complex mathematical\nreasoning benchmarks, and the potential complimentary benefits offered by tools\nfor knowledge retrieval and mathematical equation solving, are open research\nquestions. In this work, we present MATHSENSEI, a tool-augmented large language\nmodel for mathematical reasoning. Augmented with tools for knowledge retrieval\n(Bing Web Search), program execution (Python), and symbolic equation solving\n(Wolfram-Alpha), we study the complimentary benefits of these tools through\nevaluations on mathematical reasoning datasets. We perform exhaustive ablations\non MATH,a popular dataset for evaluating mathematical reasoning on diverse\nmathematical disciplines. We also conduct experiments involving well-known tool\nplanners to study the impact of tool sequencing on the model performance.\nMATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with\nchain-of-thought on the MATH dataset. We further observe that TALMs are not as\neffective for simpler math word problems (in GSM-8k), and the benefit increases\nas the complexity and required knowledge increases (progressively over AQuA,\nMMLU-Math, and higher level complex questions in MATH). The code and data are\navailable at https://github.com/Debrup-61/MathSensei.\n","authors":["Debrup Das","Debopriyo Banerjee","Somak Aditya","Ashish Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2402.17231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12469v1","updated":"2024-03-19T06:01:02Z","published":"2024-03-19T06:01:02Z","title":"When Do \"More Contexts\" Help with Sarcasm Recognition?","summary":"  Sarcasm recognition is challenging because it needs an understanding of the\ntrue intention, which is opposite to or different from the literal meaning of\nthe words. Prior work has addressed this challenge by developing a series of\nmethods that provide richer $contexts$, e.g., sentiment or cultural nuances, to\nmodels. While shown to be effective individually, no study has systematically\nevaluated their collective effectiveness. As a result, it remains unclear to\nwhat extent additional contexts can improve sarcasm recognition. In this work,\nwe explore the improvements that existing methods bring by incorporating more\ncontexts into a model. To this end, we develop a framework where we can\nintegrate multiple contextual cues and test different approaches. In evaluation\nwith four approaches on three sarcasm recognition benchmarks, we achieve\nexisting state-of-the-art performances and also demonstrate the benefits of\nsequentially adding more contexts. We also identify inherent drawbacks of using\nmore contexts, highlighting that in the pursuit of even better results, the\nmodel may need to adopt societal biases.\n","authors":["Ojas Nimase","Sanghyun Hong"],"pdf_url":"https://arxiv.org/pdf/2403.12469v1.pdf","comment":"Accepted to LREC-COLING 2024 [Short]"},{"id":"http://arxiv.org/abs/2403.12468v1","updated":"2024-03-19T05:52:56Z","published":"2024-03-19T05:52:56Z","title":"CrossTune: Black-Box Few-Shot Classification with Label Enhancement","summary":"  Training or finetuning large-scale language models (LLMs) requires\nsubstantial computation resources, motivating recent efforts to explore\nparameter-efficient adaptation to downstream tasks. One approach is to treat\nthese models as black boxes and use forward passes (Inference APIs) to interact\nwith them. Current research focuses on adapting these black-box models to\ndownstream tasks using gradient-free prompt optimization, but this often\ninvolves an expensive process of searching task-specific prompts. Therefore, we\nare motivated to study black-box language model adaptation without prompt\nsearch. Specifically, we introduce a label-enhanced cross-attention network\ncalled CrossTune, which models the semantic relatedness between the input text\nsequence and task-specific label descriptions. Its effectiveness is examined in\nthe context of few-shot text classification. To improve the generalization of\nCrossTune, we utilize ChatGPT to generate additional training data through\nin-context learning. A switch mechanism is implemented to exclude low-quality\nChatGPT-generated data. Through extensive experiments on seven benchmark text\nclassification datasets, we demonstrate that our proposed approach outperforms\nthe previous state-of-the-art gradient-free black-box tuning method by 5.7% on\naverage. Even without using ChatGPT-augmented data, CrossTune performs better\nor comparably than previous black-box tuning methods, suggesting the\neffectiveness of our approach.\n","authors":["Danqing Luo","Chen Zhang","Yan Zhang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2403.12468v1.pdf","comment":"Accepted by LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2403.09362v2","updated":"2024-03-19T05:49:01Z","published":"2024-03-14T13:12:21Z","title":"Komodo: A Linguistic Expedition into Indonesia's Regional Languages","summary":"  The recent breakthroughs in Large Language Models (LLMs) have mostly focused\non languages with easily available and sufficient resources, such as English.\nHowever, there remains a significant gap for languages that lack sufficient\nlinguistic resources in the public domain. Our work introduces Komodo-7B,\n7-billion-parameter Large Language Models designed to address this gap by\nseamlessly operating across Indonesian, English, and 11 regional languages in\nIndonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and\nKomodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art\nperformance in various tasks and languages, outperforming the benchmarks set by\nOpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,\nMixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only\ndemonstrates superior performance in both language-specific and overall\nassessments but also highlights its capability to excel in linguistic\ndiversity. Our commitment to advancing language models extends beyond\nwell-resourced languages, aiming to bridge the gap for those with limited\nlinguistic assets. Additionally, Komodo-7B-Instruct's better cross-language\nunderstanding contributes to addressing educational disparities in Indonesia,\noffering direct translations from English to 11 regional languages, a\nsignificant improvement compared to existing language translation services.\nKomodo-7B represents a crucial step towards inclusivity and effectiveness in\nlanguage models, providing to the linguistic needs of diverse communities.\n","authors":["Louis Owen","Vishesh Tripathi","Abhay Kumar","Biddwan Ahmed"],"pdf_url":"https://arxiv.org/pdf/2403.09362v2.pdf","comment":"30 Pages, 8 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2402.13605v4","updated":"2024-03-19T04:00:56Z","published":"2024-02-21T08:12:26Z","title":"KorNAT: LLM Alignment Benchmark for Korean Social Values and Common\n  Knowledge","summary":"  For Large Language Models (LLMs) to be effectively deployed in a specific\ncountry, they must possess an understanding of the nation's culture and basic\nknowledge. To this end, we introduce National Alignment, which measures an\nalignment between an LLM and a targeted country from two aspects: social value\nalignment and common knowledge alignment. Social value alignment evaluates how\nwell the model understands nation-specific social values, while common\nknowledge alignment examines how well the model captures basic knowledge\nrelated to the nation. We constructed KorNAT, the first benchmark that measures\nnational alignment with South Korea. For the social value dataset, we obtained\nground truth labels from a large-scale survey involving 6,174 unique Korean\nparticipants. For the common knowledge dataset, we constructed samples based on\nKorean textbooks and GED reference materials. KorNAT contains 4K and 6K\nmultiple-choice questions for social value and common knowledge, respectively.\nOur dataset creation process is meticulously designed and based on statistical\nsampling theory and was refined through multiple rounds of human review. The\nexperiment results of seven LLMs reveal that only a few models met our\nreference score, indicating a potential for further enhancement. KorNAT has\nreceived government approval after passing an assessment conducted by a\ngovernment-affiliated organization dedicated to evaluating dataset quality.\nSamples and detailed evaluation protocols of our dataset can be found in\nhttps://selectstar.ai/ko/papers-national-alignment\n","authors":["Jiyoung Lee","Minwoo Kim","Seungho Kim","Junghwan Kim","Seunghyun Won","Hwaran Lee","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2402.13605v4.pdf","comment":"35 pages, 7 figures, 16 tables"},{"id":"http://arxiv.org/abs/2403.12416v1","updated":"2024-03-19T03:59:14Z","published":"2024-03-19T03:59:14Z","title":"Eye-gaze Guided Multi-modal Alignment Framework for Radiology","summary":"  In multi-modal frameworks, the alignment of cross-modal features presents a\nsignificant challenge. The predominant approach in multi-modal pre-training\nemphasizes either global or local alignment between modalities, utilizing\nextensive datasets. This bottom-up driven method often suffers from a lack of\ninterpretability, a critical concern in radiology. Previous studies have\nintegrated high-level labels in medical images or text, but these still rely on\nmanual annotation, a costly and labor-intensive process. Our work introduces a\nnovel approach by using eye-gaze data, collected synchronously by radiologists\nduring diagnostic evaluations. This data, indicating radiologists' focus areas,\nnaturally links chest X-rays to diagnostic texts. We propose the Eye-gaze\nGuided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for\nbetter alignment of image and text features, aiming to reduce reliance on\nmanual annotations and thus cut training costs. Our model demonstrates robust\nperformance, outperforming other state-of-the-art methods in zero-shot\nclassification and retrieval tasks. The incorporation of easily-obtained\neye-gaze data during routine radiological diagnoses signifies a step towards\nminimizing manual annotation dependency. Additionally, we explore the impact of\nvarying amounts of eye-gaze data on model performance, highlighting the\nfeasibility and utility of integrating this auxiliary data into multi-modal\npre-training.\n","authors":["Chong Ma","Hanqi Jiang","Wenting Chen","Zihao Wu","Xiaowei Yu","Fang Zeng","Lei Guo","Dajiang Zhu","Tuo Zhang","Dinggang Shen","Tianming Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12416v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.12413v1","updated":"2024-03-19T03:53:47Z","published":"2024-03-19T03:53:47Z","title":"Third-Party Language Model Performance Prediction from Instruction","summary":"  Language model-based instruction-following systems have lately shown\nincreasing performance on many benchmark tasks, demonstrating the capability of\nadapting to a broad variety of instructions. However, such systems are often\nnot designed to be transparent about their limitations; a user may easily\nprompt a model with an instruction without any idea of whether the responses\nshould be expected to be accurate, or if the system is even capable of\nperforming the task. We propose a third party performance prediction framework,\nwhere a separate model is trained to predict the metric resulting from\nevaluating an instruction-following system on a task while assuming access only\nto its inputs and outputs at inference time. We perform this analysis with a\nvariety of both open and closed instruction-following models as well as\nmultiple performance predictors, and examine the effect of various factors such\nas model size, number of training tasks, and prompt format. Our findings\nindicate that third-party performance prediction is very challenging, and much\nwork remains in developing predictors that can automatically reveal the\nlimitations of modern instruction-following natural language processing\nsystems.\n","authors":["Rahul Nadkarni","Yizhong Wang","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2403.12413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05881v2","updated":"2024-03-19T03:48:11Z","published":"2024-03-09T11:23:38Z","title":"KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge\n  Graphs and Ranking Techniques","summary":"  Large Language Models (LLMs) have significantly advanced healthcare\ninnovation on generation capabilities. However, their application in real\nclinical settings is challenging due to potential deviations from medical facts\nand inherent biases. In this work, we develop an augmented LLM framework,\nKG-Rank, which leverages a medical knowledge graph (KG) with ranking and\nre-ranking techniques, aiming to improve free-text question-answering (QA) in\nthe medical domain. Specifically, upon receiving a question, we initially\nretrieve triplets from a medical KG to gather factual information.\nSubsequently, we innovatively apply ranking methods to refine the ordering of\nthese triplets, aiming to yield more precise answers. To the best of our\nknowledge, KG-Rank is the first application of ranking models combined with KG\nin medical QA specifically for generating long answers. Evaluation of four\nselected medical QA datasets shows that KG-Rank achieves an improvement of over\n18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it\nrealizes a 14% improvement in ROUGE-L, showing the effectiveness and potential\nof KG-Rank.\n","authors":["Rui Yang","Haoran Liu","Edison Marrese-Taylor","Qingcheng Zeng","Yu He Ke","Wanxin Li","Lechao Cheng","Qingyu Chen","James Caverlee","Yutaka Matsuo","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2403.05881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02246v2","updated":"2024-03-19T03:42:31Z","published":"2024-03-04T17:34:34Z","title":"PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large\n  Language Models","summary":"  Recent advances in large language models (LLMs) demonstrate that their\ncapabilities are comparable, or even superior, to humans in many tasks in\nnatural language processing. Despite this progress, LLMs are still inadequate\nat social-cognitive reasoning, which humans are naturally good at. Drawing\ninspiration from psychological research on the links between certain\npersonality traits and Theory-of-Mind (ToM) reasoning, and from prompt\nengineering research on the hyper-sensitivity of prompts in affecting LLMs\ncapabilities, this study investigates how inducing personalities in LLMs using\nprompts affects their ToM reasoning capabilities. Our findings show that\ncertain induced personalities can significantly affect the LLMs' reasoning\ncapabilities in three different ToM tasks. In particular, traits from the Dark\nTriad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral\nacross the different ToM tasks. We find that LLMs that exhibit a higher\nvariance across personality prompts in ToM also tends to be more controllable\nin personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and\nMistral can be controllably adjusted through our personality prompts. In\ntoday's landscape where role-play is a common strategy when using LLMs, our\nresearch highlights the need for caution, as models that adopt specific\npersonas with personalities potentially also alter their reasoning abilities in\nan unexpected manner.\n","authors":["Fiona Anting Tan","Gerard Christopher Yeo","Fanyou Wu","Weijie Xu","Vinija Jain","Aman Chadha","Kokil Jaidka","Yang Liu","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.02246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12408v1","updated":"2024-03-19T03:35:20Z","published":"2024-03-19T03:35:20Z","title":"MSLM-S2ST: A Multitask Speech Language Model for Textless\n  Speech-to-Speech Translation with Speaker Style Preservation","summary":"  There have been emerging research interest and advances in speech-to-speech\ntranslation (S2ST), translating utterances from one language to another. This\nwork proposes Multitask Speech Language Model (MSLM), which is a decoder-only\nspeech language model trained in a multitask setting. Without reliance on text\ntraining data, our model is able to support multilingual S2ST with speaker\nstyle preserved.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12407v1","updated":"2024-03-19T03:35:18Z","published":"2024-03-19T03:35:18Z","title":"Cross-Lingual Transfer for Natural Language Inference via Multilingual\n  Prompt Translator","summary":"  Based on multilingual pre-trained models, cross-lingual transfer with prompt\nlearning has shown promising effectiveness, where soft prompt learned in a\nsource language is transferred to target languages for downstream tasks,\nparticularly in the low-resource scenario. To efficiently transfer soft prompt,\nwe propose a novel framework, Multilingual Prompt Translator (MPT), where a\nmultilingual prompt translator is introduced to properly process crucial\nknowledge embedded in prompt by changing language knowledge while retaining\ntask knowledge. Concretely, we first train prompt in source language and employ\ntranslator to translate it into target prompt. Besides, we extend an external\ncorpus as auxiliary data, on which an alignment task for predicted answer\nprobability is designed to convert language knowledge, thereby equipping target\nprompt with multilingual knowledge. In few-shot settings on XNLI, MPT\ndemonstrates superiority over baselines by remarkable improvements. MPT is more\nprominent compared with vanilla prompting when transferring to languages quite\ndistinct from source language.\n","authors":["Xiaoyu Qiu","Yuechen Wang","Jiaxin Shi","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.12407v1.pdf","comment":"6 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2403.12403v1","updated":"2024-03-19T03:22:35Z","published":"2024-03-19T03:22:35Z","title":"Towards Interpretable Hate Speech Detection using Large Language\n  Model-extracted Rationales","summary":"  Although social media platforms are a prominent arena for users to engage in\ninterpersonal discussions and express opinions, the facade and anonymity\noffered by social media may allow users to spew hate speech and offensive\ncontent. Given the massive scale of such platforms, there arises a need to\nautomatically identify and flag instances of hate speech. Although several hate\nspeech detection methods exist, most of these black-box methods are not\ninterpretable or explainable by design. To address the lack of\ninterpretability, in this paper, we propose to use state-of-the-art Large\nLanguage Models (LLMs) to extract features in the form of rationales from the\ninput text, to train a base hate speech classifier, thereby enabling faithful\ninterpretability by design. Our framework effectively combines the textual\nunderstanding capabilities of LLMs and the discriminative power of\nstate-of-the-art hate speech classifiers to make these classifiers faithfully\ninterpretable. Our comprehensive evaluation on a variety of social media hate\nspeech datasets demonstrate: (1) the goodness of the LLM-extracted rationales,\nand (2) the surprising retention of detector performance even after training to\nensure interpretability.\n","authors":["Ayushi Nirmal","Amrita Bhattacharjee","Paras Sheth","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12403v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.12402v1","updated":"2024-03-19T03:22:28Z","published":"2024-03-19T03:22:28Z","title":"An Empirical Study of Speech Language Models for Prompt-Conditioned\n  Speech Synthesis","summary":"  Speech language models (LMs) are promising for high-quality speech synthesis\nthrough in-context learning. A typical speech LM takes discrete semantic units\nas content and a short utterance as prompt, and synthesizes speech which\npreserves the content's semantics but mimics the prompt's style. However, there\nis no systematic understanding on how the synthesized audio is controlled by\nthe prompt and content. In this work, we conduct an empirical study of the\nwidely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and\nprovide insights into the prompt design and content semantic units. Our\nanalysis reveals that heterogeneous and nonstationary prompts hurt the audio\nquality in contrast to the previous finding that longer prompts always lead to\nbetter synthesis. Moreover, we find that the speaker style of the synthesized\naudio is also affected by the content in addition to the prompt. We further\nshow that semantic units carry rich acoustic information such as pitch, tempo,\nvolume and speech emphasis, which might be leaked from the content to the\nsynthesized audio.\n","authors":["Yifan Peng","Ilia Kulikov","Yilin Yang","Sravya Popuri","Hui Lu","Changhan Wang","Hongyu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.12402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12393v1","updated":"2024-03-19T03:00:03Z","published":"2024-03-19T03:00:03Z","title":"Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open\n  Domain Multi-Hop Question Answering","summary":"  Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in\nNatural Language Processing (NLP) by aiming to answer complex questions through\nmulti-step reasoning over retrieved information from external knowledge\nsources. Recently, Large Language Models (LLMs) have demonstrated remarkable\nperformance in solving ODMHQA owing to their capabilities including planning,\nreasoning, and utilizing tools. However, LLMs may generate off-topic answers\nwhen attempting to solve ODMHQA, namely the generated answers are irrelevant to\nthe original questions. This issue of off-topic answers accounts for\napproximately one-third of incorrect answers, yet remains underexplored despite\nits significance. To alleviate this issue, we propose the\nDiscriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.\nSpecifically, the Discriminator leverages the intrinsic capabilities of LLMs to\njudge whether the generated answers are off-topic. In cases where an off-topic\nanswer is detected, the Corrector performs step-wise revisions along the\nreversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final\nanswer becomes on-topic. Experimental results on the HotpotQA and\n2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably\nreduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving\nthe performance in Exact Match (EM) by nearly 3% compared to the baseline\nmethod without the Dr3 mechanism.\n","authors":["Yuan Gao","Yiheng Zhu","Yuanbin Cao","Yinzhi Zhou","Zhen Wu","Yujie Chen","Shenglan Wu","Haoyuan Hu","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2403.12393v1.pdf","comment":"LREC-COLING 2024, Long Paper"},{"id":"http://arxiv.org/abs/2403.12392v1","updated":"2024-03-19T02:59:58Z","published":"2024-03-19T02:59:58Z","title":"AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis","summary":"  Arabic poetry, with its rich linguistic features and profound cultural\nsignificance, presents a unique challenge to the Natural Language Processing\n(NLP) field. The complexity of its structure and context necessitates advanced\ncomputational models for accurate analysis. In this paper, we introduce\nAraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry\ntext. To demonstrate the effectiveness of the proposed model, we compared\nAraPoemBERT with 5 different Arabic language models on various NLP tasks\nrelated to Arabic poetry. The new model outperformed all other models and\nachieved state-of-the-art results in most of the downstream tasks. AraPoemBERT\nachieved unprecedented accuracy in two out of three novel tasks: poet's gender\nclassification (99.34\\% accuracy), and poetry sub-meter classification (97.79\\%\naccuracy). In addition, the model achieved an accuracy score in poems' rhyme\nclassification (97.73\\% accuracy) which is almost equivalent to the best score\nreported in this study. Moreover, the proposed model significantly outperformed\nprevious work and other comparative models in the tasks of poems' sentiment\nanalysis, achieving an accuracy of 78.95\\%, and poetry meter classification\n(99.03\\% accuracy), while significantly expanding the scope of these two\nproblems. The dataset used in this study, contains more than 2.09 million\nverses collected from online sources, each associated with various attributes\nsuch as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the\neffectiveness of the proposed model in understanding and analyzing Arabic\npoetry, achieving state-of-the-art results in several tasks and outperforming\nprevious works and other language models included in the study. AraPoemBERT\nmodel is publicly available on \\url{https://huggingface.co/faisalq}.\n","authors":["Faisal Qarah"],"pdf_url":"https://arxiv.org/pdf/2403.12392v1.pdf","comment":"28 pages, 11 figures, not published yet"},{"id":"http://arxiv.org/abs/2403.12386v1","updated":"2024-03-19T02:52:58Z","published":"2024-03-19T02:52:58Z","title":"Pipelined Biomedical Event Extraction Rivaling Joint Learning","summary":"  Biomedical event extraction is an information extraction task to obtain\nevents from biomedical text, whose targets include the type, the trigger, and\nthe respective arguments involved in an event. Traditional biomedical event\nextraction usually adopts a pipelined approach, which contains trigger\nidentification, argument role recognition, and finally event construction\neither using specific rules or by machine learning. In this paper, we propose\nan n-ary relation extraction method based on the BERT pre-training model to\nconstruct Binding events, in order to capture the semantic information about an\nevent's context and its participants. The experimental results show that our\nmethod achieves promising results on the GE11 and GE13 corpora of the BioNLP\nshared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates\nthat by significantly improving theperformance of Binding events, the overall\nperformance of the pipelined event extraction approach or even exceeds those of\ncurrent joint learning methods.\n","authors":["Pengchao Wu","Xuefeng Li","Jinghang Gu","Longhua Qian","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12374v1","updated":"2024-03-19T02:34:33Z","published":"2024-03-19T02:34:33Z","title":"Improving Generalizability of Extracting Social Determinants of Health\n  Using Large Language Models through Prompt-tuning","summary":"  The progress in natural language processing (NLP) using large language models\n(LLMs) has greatly improved patient information extraction from clinical\nnarratives. However, most methods based on the fine-tuning strategy have\nlimited transfer learning ability for cross-domain applications. This study\nproposed a novel approach that employs a soft prompt-based learning\narchitecture, which introduces trainable prompts to guide LLMs toward desired\noutputs. We examined two types of LLM architectures, including encoder-only\nGatorTron and decoder-only GatorTronGPT, and evaluated their performance for\nthe extraction of social determinants of health (SDoH) using a\ncross-institution dataset from the 2022 n2c2 challenge and a cross-disease\ndataset from the University of Florida (UF) Health. The results show that\ndecoder-only LLMs with prompt tuning achieved better performance in\ncross-domain applications. GatorTronGPT achieved the best F1 scores for both\ndatasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a\ncross-institution setting, and 5.5% and 14.5% in a cross-disease setting.\n","authors":["Cheng Peng","Zehao Yu","Kaleb E Smith","Wei-Hsuan Lo-Ciganic","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2403.12374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12373v1","updated":"2024-03-19T02:34:18Z","published":"2024-03-19T02:34:18Z","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners","summary":"  Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, which include deploying task-specific verifiers or voting over\nmultiple reasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13\\%. RankPrompt\nalso excels in LLM-based automatic evaluations for open-ended generation,\naligning with human preferences 74\\% of the time in the AlpacaEval set.\nMoreover, RankPrompt demonstrates robustness against variations in the\norderings and consistencies of responses.\n","authors":["Chi Hu","Yuan Ge","Xiangnan Ma","Hang Cao","Qiang Li","Yonghua Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12373v1.pdf","comment":"LREC-Coling 2024 Long Paper"},{"id":"http://arxiv.org/abs/2403.12368v1","updated":"2024-03-19T02:25:29Z","published":"2024-03-19T02:25:29Z","title":"Characteristic AI Agents via Large Language Models","summary":"  The advancement of Large Language Models (LLMs) has led to significant\nenhancements in the performance of chatbot systems. Many researchers have\ndedicated their efforts to the development of bringing characteristics to\nchatbots. While there have been commercial products for developing role-driven\nchatbots using LLMs, it is worth noting that academic research in this area\nremains relatively scarce. Our research focuses on investigating the\nperformance of LLMs in constructing Characteristic AI Agents by simulating\nreal-life individuals across different settings. Current investigations have\nprimarily focused on act on roles with simple profiles. In response to this\nresearch gap, we create a benchmark for the characteristic AI agents task,\nincluding dataset, techniques, and evaluation metrics. A dataset called\n``Character100'' is built for this benchmark, comprising the most-visited\npeople on Wikipedia for language models to role-play. With the constructed\ndataset, we conduct comprehensive assessment of LLMs across various settings.\nIn addition, we devise a set of automatic metrics for quantitative performance\nevaluation. The experimental results underscore the potential directions for\nfurther improvement in the capabilities of LLMs in constructing characteristic\nAI agents. The benchmark is available at\nhttps://github.com/nuaa-nlp/Character100.\n","authors":["Xi Wang","Hongliang Dai","Shen Gao","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2403.12368v1.pdf","comment":"COLING 2024,The benchmark is available at:\n  https://github.com/nuaa-nlp/Character100"},{"id":"http://arxiv.org/abs/2311.09585v2","updated":"2024-03-19T02:20:50Z","published":"2023-11-16T05:43:02Z","title":"LifeTox: Unveiling Implicit Toxicity in Life Advice","summary":"  As large language models become increasingly integrated into daily life,\ndetecting implicit toxicity across diverse contexts is crucial. To this end, we\nintroduce LifeTox, a dataset designed for identifying implicit toxicity within\na broad range of advice-seeking scenarios. Unlike existing safety datasets,\nLifeTox comprises diverse contexts derived from personal experiences through\nopen-ended questions. Experiments demonstrate that RoBERTa fine-tuned on\nLifeTox matches or surpasses the zero-shot performance of large language models\nin toxicity classification tasks. These results underscore the efficacy of\nLifeTox in addressing the complex challenges inherent in implicit toxicity. We\nopen-sourced the\ndataset\\footnote{\\url{https://huggingface.co/datasets/mbkim/LifeTox}} and the\nLifeTox moderator family; 350M, 7B, and 13B.\n","authors":["Minbeom Kim","Jahyun Koo","Hwanhee Lee","Joonsuk Park","Hwaran Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.09585v2.pdf","comment":"11 pages, 5 figures, NAACL 2024"},{"id":"http://arxiv.org/abs/2307.12418v2","updated":"2024-03-19T02:17:22Z","published":"2023-07-23T20:08:38Z","title":"HateModerate: Testing Hate Speech Detectors against Content Moderation\n  Policies","summary":"  To protect users from massive hateful content, existing works studied\nautomated hate speech detection. Despite the existing efforts, one question\nremains: do automated hate speech detectors conform to social media content\npolicies? A platform's content policies are a checklist of content moderated by\nthe social media platform. Because content moderation rules are often uniquely\ndefined, existing hate speech datasets cannot directly answer this question.\n  This work seeks to answer this question by creating HateModerate, a dataset\nfor testing the behaviors of automated content moderators against content\npolicies. First, we engage 28 annotators and GPT in a six-step annotation\nprocess, resulting in a list of hateful and non-hateful test suites matching\neach of Facebook's 41 hate speech policies. Second, we test the performance of\nstate-of-the-art hate speech detectors against HateModerate, revealing\nsubstantial failures these models have in their conformity to the policies.\nThird, using HateModerate, we augment the training data of a top-downloaded\nhate detector on HuggingFace. We observe significant improvement in the models'\nconformity to content policies while having comparable scores on the original\ntest data. Our dataset and code can be found in the attachment.\n","authors":["Jiangrui Zheng","Xueqing Liu","Guanqun Yang","Mirazul Haque","Xing Qian","Ravishka Rathnasuriya","Wei Yang","Girish Budhrani"],"pdf_url":"https://arxiv.org/pdf/2307.12418v2.pdf","comment":"NAACL 2024 Finding"},{"id":"http://arxiv.org/abs/2310.07968v3","updated":"2024-03-19T01:32:19Z","published":"2023-10-12T01:17:56Z","title":"Think, Act, and Ask: Open-World Interactive Personalized Robot\n  Navigation","summary":"  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards\nopen-vocabulary objects in unknown environments. The existing works of ZSON\nmainly focus on following individual instructions to find generic object\nclasses, neglecting the utilization of natural language interaction and the\ncomplexities of identifying user-specific objects. To address these\nlimitations, we introduce Zero-shot Interactive Personalized Object Navigation\n(ZIPON), where robots need to navigate to personalized goal objects while\nengaging in conversations with users. To solve ZIPON, we propose a new\nframework termed Open-woRld Interactive persOnalized Navigation (ORION), which\nuses Large Language Models (LLMs) to make sequential decisions to manipulate\ndifferent modules for perception, navigation and communication. Experimental\nresults show that the performance of interactive agents that can leverage user\nfeedback exhibits significant improvement. However, obtaining a good balance\nbetween task completion and the efficiency of navigation and interaction\nremains challenging for all methods. We further provide more findings on the\nimpact of diverse user feedback forms on the agents' performance.\n","authors":["Yinpei Dai","Run Peng","Sikai Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2310.07968v3.pdf","comment":"Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:\n  https://github.com/sled-group/navchat"},{"id":"http://arxiv.org/abs/2307.00852v2","updated":"2024-03-19T01:30:03Z","published":"2023-07-03T08:45:42Z","title":"VOLTA: Improving Generative Diversity by Variational Mutual Information\n  Maximizing Autoencoder","summary":"  The natural language generation domain has witnessed great success thanks to\nTransformer models. Although they have achieved state-of-the-art generative\nquality, they often neglect generative diversity. Prior attempts to tackle this\nissue suffer from either low model capacity or over-complicated architectures.\nSome recent methods employ the VAE framework to enhance diversity, but their\nlatent variables fully depend on the input context, restricting exploration of\nthe latent space. In this paper, we introduce VOLTA, a framework that elevates\ngenerative diversity by bridging Transformer with VAE via a more effective\ncross-attention-based connection, departing from conventional embedding\nconcatenation or summation. Additionally, we propose integrating InfoGAN-style\nlatent codes to enable input-independent variability, further diversifying the\ngeneration. Moreover, our framework accommodates discrete inputs alongside its\nexisting support for continuous inputs. We perform comprehensive experiments\nwith two types of Transformers on six datasets from three different NLG tasks\nto show that our approach can significantly improve generative diversity while\nmaintaining generative quality.\n","authors":["Yueen Ma","Dafeng Chi","Jingjing Li","Kai Song","Yuzheng Zhuang","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2307.00852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14565v4","updated":"2024-03-19T22:53:25Z","published":"2023-06-26T10:26:33Z","title":"Mitigating Hallucination in Large Multi-Modal Models via Robust\n  Instruction Tuning","summary":"  Despite the promising progress in multi-modal tasks, current large\nmulti-modal models (LMMs) are prone to hallucinating inconsistent descriptions\nwith respect to the associated image and human instructions. This paper\naddresses this issue by introducing the first large and diverse visual\ninstruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.\nOur dataset comprises 400k visual instructions generated by GPT4, covering 16\nvision-and-language tasks with open-ended instructions and answers. Unlike\nexisting studies that primarily focus on positive instruction samples, we\ndesign LRV-Instruction to include both positive and negative instructions for\nmore robust visual instruction tuning. Our negative instructions are designed\nat three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent\nObject Manipulation and (iii) Knowledge Manipulation. To efficiently measure\nthe hallucination generated by LMMs, we propose GPT4-Assisted Visual\nInstruction Evaluation (GAVIE), a stable approach to evaluate visual\ninstruction tuning like human experts. GAVIE does not require human-annotated\ngroundtruth answers and can adapt to diverse instruction formats. We conduct\ncomprehensive experiments to investigate the hallucination of LMMs. Our results\ndemonstrate existing LMMs exhibit significant hallucinations when presented\nwith our negative instructions, particularly Existent Object and Knowledge\nManipulation instructions. Moreover, we successfully mitigate hallucination by\nfinetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving\nperformance on several public datasets compared to state-of-the-art methods.\nAdditionally, we observed that a balanced ratio of positive and negative\ninstances in the training data leads to a more robust model. Code and data are\navailable at https://github.com/FuxiaoLiu/LRV-Instruction.\n","authors":["Fuxiao Liu","Kevin Lin","Linjie Li","Jianfeng Wang","Yaser Yacoob","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.14565v4.pdf","comment":"40 pages, 32 figures, ICLR 2024"},{"id":"http://arxiv.org/abs/2402.00746v6","updated":"2024-03-19T22:12:19Z","published":"2024-02-01T16:40:32Z","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction System","summary":"  Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.\nThe code is available at https://github.com/jmyissb/HealthLLM.\n","authors":["Mingyu Jin","Qinkai Yu","Dong Shu","Chong Zhang","Lizhou Fan","Wenyue Hua","Suiyuan Zhu","Yanda Meng","Zhenting Wang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00746v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04461v2","updated":"2024-03-19T21:48:59Z","published":"2023-09-08T17:49:44Z","title":"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language\n  Models","summary":"  Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.\n","authors":["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"],"pdf_url":"https://arxiv.org/pdf/2309.04461v2.pdf","comment":"NAACL 2024 Main Conference. The data is released at\n  https://github.com/Yangyi-Chen/CoTConsistency"},{"id":"http://arxiv.org/abs/2403.13169v1","updated":"2024-03-19T21:45:29Z","published":"2024-03-19T21:45:29Z","title":"Wav2Gloss: Generating Interlinear Glossed Text from Speech","summary":"  Thousands of the world's languages are in danger of extinction--a tremendous\nthreat to cultural identities and human language diversity. Interlinear Glossed\nText (IGT) is a form of linguistic annotation that can support documentation\nand resource creation for these languages' communities. IGT typically consists\nof (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4)\nfree translations to a majority language. We propose Wav2Gloss: a task to\nextract these four annotation components automatically from speech, and\nintroduce the first dataset to this end, Fieldwork: a corpus of speech with all\nthese annotations covering 37 languages with standard formatting and\ntrain/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods,\nwith analysis suggesting that pre-trained decoders assist with translation and\nglossing, that multi-task and multilingual approaches are underperformant, and\nthat end-to-end systems perform better than cascaded systems, despite the\ntext-only systems' advantages. We provide benchmarks to lay the ground work for\nfuture research on IGT generation from speech.\n","authors":["Taiqi He","Kwanghee Choi","Lindia Tjuatja","Nathaniel R. Robinson","Jiatong Shi","Shinji Watanabe","Graham Neubig","David R. Mortensen","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.13169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00785v3","updated":"2024-03-19T20:13:59Z","published":"2023-10-01T20:46:44Z","title":"BooookScore: A systematic exploration of book-length summarization in\n  the era of LLMs","summary":"  Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.\n","authors":["Yapei Chang","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2310.00785v3.pdf","comment":"ICLR 2024 camera-ready"},{"id":"http://arxiv.org/abs/2403.13130v1","updated":"2024-03-19T19:59:54Z","published":"2024-03-19T19:59:54Z","title":"Self-generated Replay Memories for Continual Neural Machine Translation","summary":"  Modern Neural Machine Translation systems exhibit strong performance in\nseveral different languages and are constantly improving. Their ability to\nlearn continuously is, however, still severely limited by the catastrophic\nforgetting issue. In this work, we leverage a key property of encoder-decoder\nTransformers, i.e. their generative ability, to propose a novel approach to\ncontinually learning Neural Machine Translation systems. We show how this can\neffectively learn on a stream of experiences comprising different languages, by\nleveraging a replay memory populated by using the model itself as a generator\nof parallel sentences. We empirically demonstrate that our approach can\ncounteract catastrophic forgetting without requiring explicit memorization of\ntraining data. Code will be publicly available upon publication. Code:\nhttps://github.com/m-resta/sg-rep\n","authors":["Michele Resta","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2403.13130v1.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.03861v2","updated":"2024-03-19T19:51:09Z","published":"2024-03-06T17:11:38Z","title":"Designing Informative Metrics for Few-Shot Example Selection","summary":"  Pretrained language models (PLMs) have shown remarkable few-shot learning\ncapabilities when provided with properly formatted examples. However, selecting\nthe \"best\" examples remains an open challenge. We propose a complexity-based\nprompt selection approach for sequence tagging tasks. This approach avoids the\ntraining of a dedicated model for selection of examples, and instead uses\ncertain metrics to align the syntactico-semantic complexity of test sentences\nand examples. We use both sentence- and word-level metrics to match the\ncomplexity of examples to the (test) sentence being considered. Our results\ndemonstrate that our approach extracts greater performance from PLMs: it\nachieves state-of-the-art performance on few-shot NER, achieving a 5% absolute\nimprovement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large\ngains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.\n","authors":["Rishabh Adiga","Lakshminarayanan Subramanian","Varun Chandrasekaran"],"pdf_url":"https://arxiv.org/pdf/2403.03861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13112v1","updated":"2024-03-19T19:27:23Z","published":"2024-03-19T19:27:23Z","title":"Encode Once and Decode in Parallel: Efficient Transformer Decoding","summary":"  Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment scenarios. Finetuned encoder-decoder models are popular\nin specialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and question-answering\ntasks where multiple outputs are required of a single input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes output in parallel,\nboosting both training and inference efficiency by avoiding duplicate input\nencoding, thereby reducing the decoder's memory footprint. We achieve\ncomputation reduction that roughly scales with the number of subtasks, gaining\nup to 4.6x speed-up over state-of-the-art models for dialogue state tracking,\nsummarization, and question-answering tasks with comparable or better\nperformance. We release our training/inference code and checkpoints.\n","authors":["Bo-Ru Lu","Nikita Haduong","Chien-Yu Lin","Hao Cheng","Noah A. Smith","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2403.13112v1.pdf","comment":"14 pages, 4 figures.\n  https://github.com/boru-roylu/encode-once-and-decode-in-parallel"},{"id":"http://arxiv.org/abs/2403.07693v2","updated":"2024-03-19T19:20:05Z","published":"2024-03-12T14:37:03Z","title":"Large, Small or Both: A Novel Data Augmentation Framework Based on\n  Language Models for Debiasing Opinion Summarization","summary":"  As more than 70$\\%$ of reviews in the existing opinion summary data set are\npositive, current opinion summarization approaches are reluctant to generate\nnegative summaries given the input of negative texts. To address such sentiment\nbias, a direct approach without the over-reliance on a specific framework is to\ngenerate additional data based on large language models to balance the\nemotional distribution of the dataset. However, data augmentation based on\nlarge language models faces two disadvantages: 1) the potential issues or\ntoxicity in the augmented data; 2) the expensive costs. Therefore, in this\npaper, we propose a novel data augmentation framework based on both large and\nsmall language models for debiasing opinion summarization. In specific, a small\nsize of synthesized negative reviews is obtained by rewriting the positive text\nvia a large language model. Then, a disentangle reconstruction model is trained\nbased on the generated data. After training, a large amount of synthetic data\ncan be obtained by decoding the new representation obtained from the\ncombination of different sample representations and filtering based on\nconfusion degree and sentiment classification. Experiments have proved that our\nframework can effectively alleviate emotional bias same as using only large\nmodels, but more economically.\n","authors":["Yanyue Zhang","Pengfei Li","Yilong Lai","Deyu Zhou","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2403.07693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13107v1","updated":"2024-03-19T19:15:13Z","published":"2024-03-19T19:15:13Z","title":"Towards Unsupervised Question Answering System with Multi-level\n  Summarization for Legal Text","summary":"  This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal\nArgument Reasoning in Civil Procedure. To address this Binary Classification\ntask, which was daunting due to the complexity of the Legal Texts involved, we\npropose a simple yet novel similarity and distance-based unsupervised approach\nto generate labels. Further, we explore the Multi-level fusion of Legal-Bert\nembeddings using ensemble features, including CNN, GRU, and LSTM. To address\nthe lengthy nature of Legal explanation in the dataset, we introduce T5-based\nsegment-wise summarization, which successfully retained crucial information,\nenhancing the model's performance. Our unsupervised system witnessed a 20-point\nincrease in macro F1-score on the development set and a 10-point increase on\nthe test set, which is promising given its uncomplicated architecture.\n","authors":["M Manvith Prabhu","Haricharana Srinivasa","Anand Kumar M"],"pdf_url":"https://arxiv.org/pdf/2403.13107v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.13106v1","updated":"2024-03-19T19:13:22Z","published":"2024-03-19T19:13:22Z","title":"Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying\n  Structure of Data","summary":"  Measuring nonlinear feature interaction is an established approach to\nunderstanding complex patterns of attribution in many models. In this paper, we\nuse Shapley Taylor interaction indices (STII) to analyze the impact of\nunderlying data structure on model representations in a variety of modalities,\ntasks, and architectures. Considering linguistic structure in masked and\nauto-regressive language models (MLMs and ALMs), we find that STII increases\nwithin idiomatic expressions and that MLMs scale STII with syntactic distance,\nrelying more on syntax in their nonlinear structure than ALMs do. Our speech\nmodel findings reflect the phonetic principal that the openness of the oral\ncavity determines how much a phoneme varies based on its context. Finally, we\nstudy image classifiers and illustrate that feature interactions intuitively\nreflect object boundaries. Our wide range of results illustrates the benefits\nof interdisciplinary work and domain expertise in interpretability research.\n","authors":["Divyansh Singhvi","Andrej Erkelens","Raghav Jain","Diganta Misra","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2403.13106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13089v1","updated":"2024-03-19T18:37:05Z","published":"2024-03-19T18:37:05Z","title":"Automatic Summarization of Doctor-Patient Encounter Dialogues Using\n  Large Language Model through Prompt Tuning","summary":"  Automatic text summarization (ATS) is an emerging technology to assist\nclinicians in providing continuous and coordinated care. This study presents an\napproach to summarize doctor-patient dialogues using generative large language\nmodels (LLMs). We developed prompt-tuning algorithms to instruct generative\nLLMs to summarize clinical text. We examined the prompt-tuning strategies, the\nsize of soft prompts, and the few-short learning ability of GatorTronGPT, a\ngenerative clinical LLM developed using 277 billion clinical and general\nEnglish words with up to 20 billion parameters. We compared GatorTronGPT with a\nprevious solution based on fine-tuning of a widely used T5 model, using a\nclinical benchmark dataset MTS-DIALOG. The experimental results show that the\nGatorTronGPT- 20B model achieved the best performance on all evaluation\nmetrics. The proposed solution has a low computing cost as the LLM parameters\nare not updated during prompt-tuning. This study demonstrates the efficiency of\ngenerative clinical LLMs for clinical ATS through prompt tuning.\n","authors":["Mengxian Lyu","Cheng Peng","Xiaohan Li","Patrick Balian","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2403.13089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09738v2","updated":"2024-03-19T18:35:40Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.13037v1","updated":"2024-03-19T14:11:20Z","published":"2024-03-19T14:11:20Z","title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient\n  Low-Rank Adaptation of Large Pre-trained Models","summary":"  Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale\npre-trained models in downstream tasks by learning low-rank incremental\nmatrices. Though LoRA and its variants effectively reduce the number of\ntrainable parameters compared to full fine-tuning methods, they often overfit\ntraining data, resulting in sub-optimal generalization on test data. To address\nthis problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning\napproach based on bi-level optimization (BLO). BiLoRA employs pseudo singular\nvalue decomposition to parameterize low-rank incremental matrices and splits\nthe training of pseudo singular vectors and values across two different subsets\nof training data. This division, embedded within separate levels of the BLO\nframework, mitigates the risk of overfitting to a single dataset. Tested on ten\ndatasets covering natural language understanding and generation tasks and\napplied to various well-known large pre-trained models, BiLoRA significantly\noutperforms LoRA methods and other fine-tuning approaches, with similar amounts\nof trainable parameters.\n","authors":["Rushi Qiang","Ruiyi Zhang","Pengtao Xie"],"pdf_url":"https://arxiv.org/pdf/2403.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13031v1","updated":"2024-03-19T07:25:02Z","published":"2024-03-19T07:25:02Z","title":"RigorLLM: Resilient Guardrails for Large Language Models against\n  Undesired Content","summary":"  Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.\n","authors":["Zhuowen Yuan","Zidi Xiong","Yi Zeng","Ning Yu","Ruoxi Jia","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.13031v1.pdf","comment":null}]},"2024-03-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2306.15006v2","updated":"2024-03-18T23:59:29Z","published":"2023-06-26T18:43:46Z","title":"DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species\n  Genome","summary":"  Decoding the linguistic intricacies of the genome is a crucial problem in\nbiology, and pre-trained foundational models such as DNABERT and Nucleotide\nTransformer have made significant strides in this area. Existing works have\nlargely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the\ntoken of the genome language due to its simplicity. However, we argue that the\ncomputation and sample inefficiencies introduced by k-mer tokenization are\nprimary obstacles in developing large genome foundational models. We provide\nconceptual and empirical insights into genome tokenization, building on which\nwe propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a\nstatistics-based data compression algorithm that constructs tokens by\niteratively merging the most frequent co-occurring genome segment in the\ncorpus. We demonstrate that BPE not only overcomes the limitations of k-mer\ntokenization but also benefits from the computational efficiency of\nnon-overlapping tokenization. Based on these insights, we introduce DNABERT-2,\na refined genome foundation model that adapts an efficient tokenizer and\nemploys multiple strategies to overcome input length constraints, reduce time\nand memory expenditure, and enhance model capability. Furthermore, we identify\nthe absence of a comprehensive and standardized benchmark for genome\nunderstanding as another significant impediment to fair comparative analysis.\nIn response, we propose the Genome Understanding Evaluation (GUE), a\ncomprehensive multi-species genome classification dataset that amalgamates $36$\ndistinct datasets across $9$ tasks, with input lengths ranging from $70$ to\n$10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate\nthat DNABERT-2 achieves comparable performance to the state-of-the-art model\nwith $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time\nin pre-training.\n","authors":["Zhihan Zhou","Yanrong Ji","Weijian Li","Pratik Dutta","Ramana Davuluri","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2306.15006v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2305.18026v2","updated":"2024-03-18T23:53:50Z","published":"2023-05-29T11:34:14Z","title":"Semantic Role Labeling Guided Out-of-distribution Detection","summary":"  Identifying unexpected domain-shifted instances in natural language\nprocessing is crucial in real-world applications. Previous works identify the\nout-of-distribution (OOD) instance by leveraging a single global feature\nembedding to represent the sentence, which cannot characterize subtle OOD\npatterns well. Another major challenge current OOD methods face is learning\neffective low-dimensional sentence representations to identify the hard OOD\ninstances that are semantically similar to the in-distribution (ID) data. In\nthis paper, we propose a new unsupervised OOD detection method, namely Semantic\nRole Labeling Guided Out-of-distribution Detection (SRLOOD), that separates,\nextracts, and learns the semantic role labeling (SRL) guided fine-grained local\nfeature representations from different arguments of a sentence and the global\nfeature representations of the full sentence using a margin-based contrastive\nloss. A novel self-supervised approach is also introduced to enhance such\nglobal-local feature learning by predicting the SRL extracted role. The\nresulting model achieves SOTA performance on four OOD benchmarks, indicating\nthe effectiveness of our approach. The code is publicly accessible via\n\\url{https://github.com/cytai/SRLOOD}.\n","authors":["Jinan Zou","Maihao Guo","Yu Tian","Yuhao Lin","Haiyao Cao","Lingqiao Liu","Ehsan Abbasnejad","Javen Qinfeng Shi"],"pdf_url":"https://arxiv.org/pdf/2305.18026v2.pdf","comment":"accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.12328v1","updated":"2024-03-18T23:48:33Z","published":"2024-03-18T23:48:33Z","title":"Methods for Generating Drift in Text Streams","summary":"  Systems and individuals produce data continuously. On the Internet, people\nshare their knowledge, sentiments, and opinions, provide reviews about services\nand products, and so on. Automatically learning from these textual data can\nprovide insights to organizations and institutions, thus preventing financial\nimpacts, for example. To learn from textual data over time, the machine\nlearning system must account for concept drift. Concept drift is a frequent\nphenomenon in real-world datasets and corresponds to changes in data\ndistribution over time. For instance, a concept drift occurs when sentiments\nchange or a word's meaning is adjusted over time. Although concept drift is\nfrequent in real-world applications, benchmark datasets with labeled drifts are\nrare in the literature. To bridge this gap, this paper provides four textual\ndrift generation methods to ease the production of datasets with labeled\ndrifts. These methods were applied to Yelp and Airbnb datasets and tested using\nincremental classifiers respecting the stream mining paradigm to evaluate their\nability to recover from the drifts. Results show that all methods have their\nperformance degraded right after the drifts, and the incremental SVM is the\nfastest to run and recover the previous performance levels regarding accuracy\nand Macro F1-Score.\n","authors":["Cristiano Mesquita Garcia","Alessandro Lameiras Koerich","Alceu de Souza Britto Jr","Jean Paul Barddal"],"pdf_url":"https://arxiv.org/pdf/2403.12328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12316v1","updated":"2024-03-18T23:21:37Z","published":"2024-03-18T23:21:37Z","title":"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and\n  Safety","summary":"  The rapid development of Chinese large language models (LLMs) poses big\nchallenges for efficient LLM evaluation. While current initiatives have\nintroduced new benchmarks or evaluation platforms for assessing Chinese LLMs,\nmany of these focus primarily on capabilities, usually overlooking potential\nalignment and safety issues. To address this gap, we introduce OpenEval, an\nevaluation testbed that benchmarks Chinese LLMs across capability, alignment\nand safety. For capability assessment, we include 12 benchmark datasets to\nevaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,\ncommonsense reasoning and mathematical reasoning. For alignment assessment,\nOpenEval contains 7 datasets that examines the bias, offensiveness and\nillegalness in the outputs yielded by Chinese LLMs. To evaluate safety,\nespecially anticipated risks (e.g., power-seeking, self-awareness) of advanced\nLLMs, we include 6 datasets. In addition to these benchmarks, we have\nimplemented a phased public evaluation and benchmark update strategy to ensure\nthat OpenEval is in line with the development of Chinese LLMs or even able to\nprovide cutting-edge benchmark datasets to guide the development of Chinese\nLLMs. In our first public evaluation, we have tested a range of Chinese LLMs,\nspanning from 7B to 72B parameters, including both open-source and proprietary\nmodels. Evaluation results indicate that while Chinese LLMs have shown\nimpressive performance in certain tasks, more attention should be directed\ntowards broader aspects such as commonsense reasoning, alignment, and safety.\n","authors":["Chuang Liu","Linhao Yu","Jiaxuan Li","Renren Jin","Yufei Huang","Ling Shi","Junhui Zhang","Xinmeng Ji","Tingting Cui","Tao Liu","Jinwang Song","Hongying Zan","Sun Li","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.12316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10668v2","updated":"2024-03-18T23:15:47Z","published":"2023-09-19T14:50:38Z","title":"Language Modeling Is Compression","summary":"  It has long been established that predictive models can be transformed into\nlossless compressors and vice versa. Incidentally, in recent years, the machine\nlearning community has focused on training increasingly large and powerful\nself-supervised (language) models. Since these large language models exhibit\nimpressive predictive capabilities, they are well-positioned to be strong\ncompressors. In this work, we advocate for viewing the prediction problem\nthrough the lens of compression and evaluate the compression capabilities of\nlarge (foundation) models. We show that large language models are powerful\ngeneral-purpose predictors and that the compression viewpoint provides novel\ninsights into scaling laws, tokenization, and in-context learning. For example,\nChinchilla 70B, while trained primarily on text, compresses ImageNet patches to\n43.4% and LibriSpeech samples to 16.4% of their raw size, beating\ndomain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.\nFinally, we show that the prediction-compression equivalence allows us to use\nany compressor (like gzip) to build a conditional generative model.\n","authors":["Grégoire Delétang","Anian Ruoss","Paul-Ambroise Duquenne","Elliot Catt","Tim Genewein","Christopher Mattern","Jordi Grau-Moya","Li Kevin Wenliang","Matthew Aitchison","Laurent Orseau","Marcus Hutter","Joel Veness"],"pdf_url":"https://arxiv.org/pdf/2309.10668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09891v2","updated":"2024-03-18T23:10:24Z","published":"2024-03-14T21:52:26Z","title":"Fisher Mask Nodes for Language Model Merging","summary":"  Fine-tuning pre-trained models provides significant advantages in downstream\nperformance. The ubiquitous nature of pre-trained models such as BERT and its\nderivatives in natural language processing has also led to a proliferation of\ntask-specific fine-tuned models. As these models typically only perform one\ntask well, additional training or ensembling is required in multi-task\nscenarios. The growing field of model merging provides a solution, dealing with\nthe challenge of combining multiple task-specific models into a single\nmulti-task model. In this study, we introduce a novel model merging method for\nTransformers, combining insights from previous work in Fisher-weighted\naveraging and the use of Fisher information in model pruning. Utilizing the\nFisher information of mask nodes within the Transformer architecture, we devise\na computationally efficient weighted-averaging scheme. Our method exhibits a\nregular and significant performance increase across various models in the BERT\nfamily, outperforming full-scale Fisher-weighted averaging in a fraction of the\ncomputational cost, with baseline performance improvements of up to +6.5 and a\nspeedup of 57.4x in the biggest model. Our results prove the potential of our\nmethod in current multi-task learning environments and suggest its scalability\nand adaptability to new model architectures and learning scenarios.\n","authors":["Thennal D K","Ganesh Nathan","Suchithra M S"],"pdf_url":"https://arxiv.org/pdf/2403.09891v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.12297v1","updated":"2024-03-18T22:39:03Z","published":"2024-03-18T22:39:03Z","title":"Leveraging Large Language Models to Extract Information on Substance Use\n  Disorder Severity from Clinical Notes: A Zero-shot Learning Approach","summary":"  Substance use disorder (SUD) poses a major concern due to its detrimental\neffects on health and society. SUD identification and treatment depend on a\nvariety of factors such as severity, co-determinants (e.g., withdrawal\nsymptoms), and social determinants of health. Existing diagnostic coding\nsystems used by American insurance providers, like the International\nClassification of Diseases (ICD-10), lack granularity for certain diagnoses,\nbut clinicians will add this granularity (as that found within the Diagnostic\nand Statistical Manual of Mental Disorders classification or DSM-5) as\nsupplemental unstructured text in clinical notes. Traditional natural language\nprocessing (NLP) methods face limitations in accurately parsing such diverse\nclinical language. Large Language Models (LLMs) offer promise in overcoming\nthese challenges by adapting to diverse language patterns. This study\ninvestigates the application of LLMs for extracting severity-related\ninformation for various SUD diagnoses from clinical notes. We propose a\nworkflow employing zero-shot learning of LLMs with carefully crafted prompts\nand post-processing techniques. Through experimentation with Flan-T5, an\nopen-source LLM, we demonstrate its superior recall compared to the rule-based\napproach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness\nof LLMs in extracting severity information, contributing to improved risk\nassessment and treatment planning for SUD patients.\n","authors":["Maria Mahbub","Gregory M. Dams","Sudarshan Srinivasan","Caitlin Rizy","Ioana Danciu","Jodie Trafton","Kathryn Knight"],"pdf_url":"https://arxiv.org/pdf/2403.12297v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.12294v1","updated":"2024-03-18T22:33:51Z","published":"2024-03-18T22:33:51Z","title":"A Comparative Investigation of Compositional Syntax and Semantics in\n  DALL-E 2","summary":"  In this study we compared how well DALL-E 2 visually represented the meaning\nof linguistic prompts also given to young children in comprehension tests.\nSentences representing fundamental components of grammatical knowledge were\nselected from assessment tests used with several hundred English-speaking\nchildren aged 2-7 years for whom we had collected original item-level data.\nDALL-E 2 was given these prompts five times to generate 20 cartoons per item,\nfor 9 adult judges to score. Results revealed no conditions in which DALL-E\n2-generated images that matched the semantic accuracy of children, even at the\nyoungest age (2 years). DALL-E 2 failed to assign the appropriate roles in\nreversible forms; it failed on negation despite an easier contrastive prompt\nthan the children received; it often assigned the adjective to the wrong noun;\nit ignored implicit agents in passives. This work points to a clear absence of\ncompositional sentence representations for DALL-E 2.\n","authors":["Elliot Murphy","Jill de Villiers","Sofia Lucero Morales"],"pdf_url":"https://arxiv.org/pdf/2403.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03132v2","updated":"2024-03-18T22:18:02Z","published":"2023-07-06T16:59:52Z","title":"T-MARS: Improving Visual Representations by Circumventing Text Feature\n  Learning","summary":"  Large web-sourced multimodal datasets have powered a slew of new methods for\nlearning general-purpose visual representations, advancing the state of the art\nin computer vision and revolutionizing zero- and few-shot recognition. One\ncrucial decision facing practitioners is how, if at all, to curate these\never-larger datasets. For example, the creators of the LAION-5B dataset chose\nto retain only image-caption pairs whose CLIP similarity score exceeded a\ndesignated threshold. In this paper, we propose a new state-of-the-art data\nfiltering approach motivated by our observation that nearly 40% of LAION's\nimages contain text that overlaps significantly with the caption. Intuitively,\nsuch data could be wasteful as it incentivizes models to perform optical\ncharacter recognition rather than learning visual features. However, naively\nremoving all such data could also be wasteful, as it throws away images that\ncontain visual features (in addition to overlapping text). Our simple and\nscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those\npairs where the text dominates the remaining visual features -- by first\nmasking out the text and then filtering out those with a low CLIP similarity\nscore of the masked image. Experimentally, T-MARS outperforms the top-ranked\nmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by a\nmargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic\nevaluation on various data pool sizes from 2M to 64M shows that the accuracy\ngains enjoyed by T-MARS linearly increase as data and compute are scaled\nexponentially. Code is available at https://github.com/locuslab/T-MARS.\n","authors":["Pratyush Maini","Sachin Goyal","Zachary C. Lipton","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2307.03132v2.pdf","comment":"Accepted to ICLR 2024. Oral at ICCV Datacomp 2023"},{"id":"http://arxiv.org/abs/2403.12285v1","updated":"2024-03-18T22:11:00Z","published":"2024-03-18T22:11:00Z","title":"FinLlama: Financial Sentiment Classification for Algorithmic Trading\n  Applications","summary":"  There are multiple sources of financial news online which influence market\nmovements and trader's decisions. This highlights the need for accurate\nsentiment analysis, in addition to having appropriate algorithmic trading\ntechniques, to arrive at better informed trading decisions. Standard lexicon\nbased sentiment approaches have demonstrated their power in aiding financial\ndecisions. However, they are known to suffer from issues related to context\nsensitivity and word ordering. Large Language Models (LLMs) can also be used in\nthis context, but they are not finance-specific and tend to require significant\ncomputational resources. To facilitate a finance specific LLM framework, we\nintroduce a novel approach based on the Llama 2 7B foundational model, in order\nto benefit from its generative nature and comprehensive language manipulation.\nThis is achieved by fine-tuning the Llama2 7B model on a small portion of\nsupervised financial sentiment analysis data, so as to jointly handle the\ncomplexities of financial lexicon and context, and further equipping it with a\nneural network based decision mechanism. Such a generator-classifier scheme,\nreferred to as FinLlama, is trained not only to classify the sentiment valence\nbut also quantify its strength, thus offering traders a nuanced insight into\nfinancial news articles. Complementing this, the implementation of\nparameter-efficient fine-tuning through LoRA optimises trainable parameters,\nthus minimising computational and memory requirements, without sacrificing\naccuracy. Simulation results demonstrate the ability of the proposed FinLlama\nto provide a framework for enhanced portfolio management decisions and\nincreased market returns. These results underpin the ability of FinLlama to\nconstruct high-return portfolios which exhibit enhanced resilience, even during\nvolatile periods and unpredictable market events.\n","authors":["Thanos Konstantinidis","Giorgos Iacovides","Mingxue Xu","Tony G. Constantinides","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2403.12285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03173v2","updated":"2024-03-18T21:44:47Z","published":"2023-10-04T21:40:36Z","title":"$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program\n  Synthesis","summary":"  Program synthesis aims to create accurate, executable programs from problem\nspecifications, specifically from natural language descriptions in our context.\nRecent studies have leveraged the power of reinforcement learning (RL) in\nconjunction with large language models (LLMs), significantly enhancing code\ngeneration capabilities. The application of RL focuses on directly optimizing\nfor functional correctness, offering an advantage over conventional supervised\nmethods. Despite policy-based RL methods dominating the literature on RL for\nprogram synthesis, the nature of program synthesis tasks hints at a natural\nalignment with value-based methods. This stems from the rich collection of\noff-policy programs, including those developed by human programmers and also\nhistorical samples, coupled with the straightforward verification of generated\nprograms through automated unit testing, meaning rewards are easy to obtain.\nDiverging from the dominant use of policy-based algorithms, our work explores\nthe feasibility of value-based approaches, leading to the development of our\n$\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based\nmethods presents challenges due to the enormous search space inherent to\nprogram synthesis. To this end, we introduce an initialization protocol for RL\nagents utilizing pre-trained LMs and a conservative Bellman operator to reduce\ntraining complexities. Moreover, we demonstrate how to leverage the learned\nvalue functions as a dual strategy to post-process generated programs. Our\nempirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in\nachieving state-of-the-art performance when compared to policy-based methods.\nRemarkably, this achievement is reached with minimal reward engineering effort,\nhighlighting the effectiveness of value-based RL, independent of reward\ndesigns.\n","authors":["Zishun Yu","Yunzhe Tao","Liyu Chen","Tao Sun","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.03173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04068v2","updated":"2024-03-18T21:37:45Z","published":"2024-02-06T15:13:17Z","title":"Retrieve to Explain: Evidence-driven Predictions with Language Models","summary":"  Machine learning models, particularly language models, are notoriously\ndifficult to introspect. Black-box models can mask both issues in model\ntraining and harmful biases. For human-in-the-loop processes, opaque\npredictions can drive lack of trust, limiting a model's impact even when it\nperforms effectively. To address these issues, we introduce Retrieve to Explain\n(R2E). R2E is a retrieval-based language model that prioritizes amongst a\npre-defined set of possible answers to a research question based on the\nevidence in a document corpus, using Shapley values to identify the relative\nimportance of pieces of evidence to the final prediction. R2E can adapt to new\nevidence without retraining, and incorporate structured data through templating\ninto natural language. We assess on the use case of drug target identification\nfrom published scientific literature, where we show that the model outperforms\nan industry-standard genetics-based approach on predicting clinical trial\noutcomes.\n","authors":["Ravi Patel","Angus Brayne","Rogier Hintzen","Daniel Jaroslawicz","Georgiana Neculae","Dane Corneil"],"pdf_url":"https://arxiv.org/pdf/2402.04068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12244v1","updated":"2024-03-18T20:50:26Z","published":"2024-03-18T20:50:26Z","title":"Zero-Shot Multi-task Hallucination Detection","summary":"  In recent studies, the extensive utilization of large language models has\nunderscored the importance of robust evaluation methodologies for assessing\ntext generation quality and relevance to specific tasks. This has revealed a\nprevalent issue known as hallucination, an emergent condition in the model\nwhere generated text lacks faithfulness to the source and deviates from the\nevaluation criteria. In this study, we formally define hallucination and\npropose a framework for its quantitative detection in a zero-shot setting,\nleveraging our definition and the assumption that model outputs entail task and\nsample specific inputs. In detecting hallucinations, our solution achieves an\naccuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.\nNotably, our solution maintains computational efficiency, requiring far less\ncomputational resources than other SOTA approaches, aligning with the trend\ntowards lightweight and compressed models.\n","authors":["Patanjali Bhamidipati","Advaith Malladi","Manish Shrivastava","Radhika Mamidi"],"pdf_url":"https://arxiv.org/pdf/2403.12244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12242v1","updated":"2024-03-18T20:47:10Z","published":"2024-03-18T20:47:10Z","title":"Reference-based Metrics Disprove Themselves in Question Generation","summary":"  Reference-based metrics such as BLEU and BERTScore are widely used to\nevaluate question generation (QG). In this study, on QG benchmarks such as\nSQuAD and HotpotQA, we find that using human-written references cannot\nguarantee the effectiveness of the reference-based metrics. Most QG benchmarks\nhave only one reference; we replicated the annotation process and collect\nanother reference. A good metric was expected to grade a human-validated\nquestion no worse than generated questions. However, the results of\nreference-based metrics on our newly collected reference disproved the metrics\nthemselves. We propose a reference-free metric consisted of multi-dimensional\ncriteria such as naturalness, answerability, and complexity, utilizing large\nlanguage models. These criteria are not constrained to the syntactic or\nsemantic of a single reference question, and the metric does not require a\ndiverse set of references. Experiments reveal that our metric accurately\ndistinguishes between high-quality questions and flawed ones, and achieves\nstate-of-the-art alignment with human judgment.\n","authors":["Bang Nguyen","Mengxia Yu","Yun Huang","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14855v2","updated":"2024-03-18T20:11:03Z","published":"2023-10-23T12:22:15Z","title":"Contextual Refinement of Translations: Large Language Models for\n  Sentence and Document-Level Post-Editing","summary":"  Large Language Models (LLM's) have demonstrated considerable success in\nvarious Natural Language Processing tasks, but they have yet to attain\nstate-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,\ntheir significant performance in tasks demanding a broad understanding and\ncontextual processing shows their potential for translation. To exploit these\nabilities, we investigate using LLM's for MT and explore recent\nparameter-efficient fine-tuning techniques. Surprisingly, our initial\nexperiments find that fine-tuning for translation purposes even led to\nperformance degradation. To overcome this, we propose an alternative approach:\nadapting LLM's as Automatic Post-Editors (APE) rather than direct translators.\nBuilding on the LLM's exceptional ability to process and generate lengthy\nsequences, we also propose extending our approach to document-level\ntranslation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can\nyield significant improvements across both sentence and document-level metrics\nwhile generalizing to out-of-domain data. Most notably, we achieve a\nstate-of-the-art accuracy rate of 89\\% on the ContraPro test set, which\nspecifically assesses the model's ability to resolve pronoun ambiguities when\ntranslating from English to German. Lastly, we investigate a practical scenario\ninvolving manual post-editing for document-level translation, where reference\ncontext is made available. Here, we demonstrate that leveraging human\ncorrections can significantly reduce the number of edits required for\nsubsequent translations (Interactive Demo for integrating manual feedback can\nbe found here:\nhttps://huggingface.co/spaces/skoneru/contextual_refinement_ende).\n","authors":["Sai Koneru","Miriam Exel","Matthias Huck","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2310.14855v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2401.01989v3","updated":"2024-03-18T20:09:01Z","published":"2024-01-03T21:38:40Z","title":"Revisiting Zero-Shot Abstractive Summarization in the Era of Large\n  Language Models from the Perspective of Position Bias","summary":"  We characterize and study zero-shot abstractive summarization in Large\nLanguage Models (LLMs) by measuring position bias, which we propose as a\ngeneral formulation of the more restrictive lead bias phenomenon studied\npreviously in the literature. Position bias captures the tendency of a model\nunfairly prioritizing information from certain parts of the input text over\nothers, leading to undesirable behavior. Through numerous experiments on four\ndiverse real-world datasets, we study position bias in multiple LLM models such\nas GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained\nencoder-decoder abstractive summarization models such as Pegasus and BART. Our\nfindings lead to novel insights and discussion on performance and position bias\nof models for zero-shot summarization tasks.\n","authors":["Anshuman Chhabra","Hadi Askari","Prasant Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2401.01989v3.pdf","comment":"Accepted to NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.12212v1","updated":"2024-03-18T19:53:56Z","published":"2024-03-18T19:53:56Z","title":"Evaluating Named Entity Recognition: Comparative Analysis of Mono- and\n  Multilingual Transformer Models on Brazilian Corporate Earnings Call\n  Transcriptions","summary":"  Named Entity Recognition (NER) is a Natural Language Processing technique for\nextracting information from textual documents. However, much of the existing\nresearch on NER has been centered around English-language documents, leaving a\ngap in the availability of datasets tailored to the financial domain in\nPortuguese. This study addresses the need for NER within the financial domain,\nfocusing on Portuguese-language texts extracted from earnings call\ntranscriptions of Brazilian banks. By curating a comprehensive dataset\ncomprising 384 transcriptions and leveraging weak supervision techniques for\nannotation, we evaluate the performance of monolingual models trained on\nPortuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5).\nNotably, we introduce a novel approach that reframes the token classification\ntask as a text generation problem, enabling fine-tuning and evaluation of T5\nmodels. Following the fine-tuning of the models, we conduct an evaluation on\nthe test dataset, employing performance and error metrics. Our findings reveal\nthat BERT-based models consistently outperform T5-based models. Furthermore,\nwhile the multilingual models exhibit comparable macro F1-scores, BERTimbau\ndemonstrates superior performance over PTT5. A manual analysis of sentences\ngenerated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to\n1.0, between the original and generated sentences. However, critical errors\nemerge as both models exhibit discrepancies, such as alterations to monetary\nand percentage values, underscoring the importance of accuracy and consistency\nin the financial domain. Despite these challenges, PTT5 and mT5 achieve\nimpressive macro F1-scores of 98.52% and 98.85%, respectively, with our\nproposed approach. Furthermore, our study sheds light on notable disparities in\nmemory and time consumption for inference across the models.\n","authors":["Ramon Abilio","Guilherme Palermo Coelho","Ana Estela Antunes da Silva"],"pdf_url":"https://arxiv.org/pdf/2403.12212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00840v2","updated":"2024-03-18T19:28:38Z","published":"2023-10-02T01:30:27Z","title":"Error Norm Truncation: Robust Training in the Presence of Data Noise for\n  Text Generation Models","summary":"  Text generation models are notoriously vulnerable to errors in the training\ndata. With the wide-spread availability of massive amounts of web-crawled data\nbecoming more commonplace, how can we enhance the robustness of models trained\non a massive amount of noisy web-crawled text? In our work, we propose Error\nNorm Truncation (ENT), a robust enhancement method to the standard training\nobjective that truncates noisy data. Compared to methods that only uses the\nnegative log-likelihood loss to estimate data quality, our method provides a\nmore accurate estimation by considering the distribution of non-target tokens,\nwhich is often overlooked by previous work. Through comprehensive experiments\nacross language modeling, machine translation, and text summarization, we show\nthat equipping text generation models with ENT improves generation quality over\nstandard training and previous soft and hard truncation methods. Furthermore,\nwe show that our method improves the robustness of models against two of the\nmost detrimental types of noise in machine translation, resulting in an\nincrease of more than 2 BLEU points over the MLE baseline when up to 50% of\nnoise is added to the data.\n","authors":["Tianjian Li","Haoran Xu","Philipp Koehn","Daniel Khashabi","Kenton Murray"],"pdf_url":"https://arxiv.org/pdf/2310.00840v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12173v1","updated":"2024-03-18T18:45:28Z","published":"2024-03-18T18:45:28Z","title":"TnT-LLM: Text Mining at Scale with Large Language Models","summary":"  Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.\n","authors":["Mengting Wan","Tara Safavi","Sujay Kumar Jauhar","Yujin Kim","Scott Counts","Jennifer Neville","Siddharth Suri","Chirag Shah","Ryen W White","Longqi Yang","Reid Andersen","Georg Buscher","Dhruv Joshi","Nagu Rangan"],"pdf_url":"https://arxiv.org/pdf/2403.12173v1.pdf","comment":"9 pages main content, 8 pages references and appendix"},{"id":"http://arxiv.org/abs/2403.12171v1","updated":"2024-03-18T18:39:53Z","published":"2024-03-18T18:39:53Z","title":"EasyJailbreak: A Unified Framework for Jailbreaking Large Language\n  Models","summary":"  Jailbreak attacks are crucial for identifying and mitigating the security\nvulnerabilities of Large Language Models (LLMs). They are designed to bypass\nsafeguards and elicit prohibited outputs. However, due to significant\ndifferences among various jailbreak methods, there is no standard\nimplementation framework available for the community, which limits\ncomprehensive security evaluations. This paper introduces EasyJailbreak, a\nunified framework simplifying the construction and evaluation of jailbreak\nattacks against LLMs. It builds jailbreak attacks using four components:\nSelector, Mutator, Constraint, and Evaluator. This modular framework enables\nresearchers to easily construct attacks from combinations of novel and existing\ncomponents. So far, EasyJailbreak supports 11 distinct jailbreak methods and\nfacilitates the security validation of a broad spectrum of LLMs. Our validation\nacross 10 distinct LLMs reveals a significant vulnerability, with an average\nbreach probability of 60% under various jailbreaking attacks. Notably, even\nadvanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success\nRates (ASR) of 57% and 33%, respectively. We have released a wealth of\nresources for researchers, including a web platform, PyPI published package,\nscreencast video, and experimental outputs.\n","authors":["Weikang Zhou","Xiao Wang","Limao Xiong","Han Xia","Yingshuang Gu","Mingxu Chai","Fukang Zhu","Caishuang Huang","Shihan Dou","Zhiheng Xi","Rui Zheng","Songyang Gao","Yicheng Zou","Hang Yan","Yifan Le","Ruohui Wang","Lijun Li","Jing Shao","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.12171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12151v1","updated":"2024-03-18T18:08:44Z","published":"2024-03-18T18:08:44Z","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification","summary":"  Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.\n","authors":["Filippos Gouidis","Katerina Papantoniou","Konstantinos Papoutsakis Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"pdf_url":"https://arxiv.org/pdf/2403.12151v1.pdf","comment":"Accepted at the AAAI-MAKE 24"},{"id":"http://arxiv.org/abs/2403.12145v1","updated":"2024-03-18T18:01:26Z","published":"2024-03-18T18:01:26Z","title":"Syn-QA2: Evaluating False Assumptions in Long-tail Questions with\n  Synthetic QA Datasets","summary":"  Sensitivity to false assumptions (or false premises) in information-seeking\nquestions is critical for robust question-answering (QA) systems. Recent work\nhas shown that false assumptions in naturally occurring questions pose\nchallenges to current models, with low performance on both generative QA and\nsimple detection tasks (Kim et al. 2023). However, the focus of existing work\non naturally occurring questions leads to a gap in the analysis of model\nbehavior on the long tail of the distribution of possible questions. To this\nend, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA\ndatasets: one generated using perturbed relations from Wikidata, and the other\nby perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range\nof large language models are threefold: (1) false assumptions in QA are\nchallenging, echoing the findings of prior work, (2) the binary detection task\nis challenging even compared to the difficulty of generative QA itself,\npossibly due to the linguistic structure of the problem, and (3) the detection\ntask is more challenging with long-tail questions compared to naturally\noccurring questions, highlighting the utility of our synthetic datasets and\ngeneration method.\n","authors":["Ashwin Daswani","Rohan Sawant","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.12145v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.12027v1","updated":"2024-03-18T17:57:09Z","published":"2024-03-18T17:57:09Z","title":"From Pixels to Insights: A Survey on Automatic Chart Understanding in\n  the Era of Large Foundation Models","summary":"  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models (LLMs), have revolutionized various natural language\nprocessing (NLP) tasks and are increasingly being applied to chart\nunderstanding tasks. This survey paper provides a comprehensive overview of the\nrecent developments, challenges, and future directions in chart understanding\nwithin the context of these foundation models. The paper begins by defining\nchart understanding, outlining problem formulations, and discussing fundamental\nbuilding blocks crucial for studying chart understanding tasks. In the section\non tasks and datasets, we explore various tasks within chart understanding and\ndiscuss their evaluation metrics and sources of both charts and textual inputs.\nModeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed in a dedicated\nsection, highlighting issues such as domain-specific charts, lack of efforts in\nevaluation, and agent-oriented settings. This survey paper serves to provide\nvaluable insights and directions for future research in chart understanding\nleveraging large foundation models. The studies mentioned in this paper, along\nwith emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n","authors":["Kung-Hsiang Huang","Hou Pong Chan","Yi R. Fung","Haoyi Qiu","Mingyang Zhou","Shafiq Joty","Shih-Fu Chang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.12027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12026v1","updated":"2024-03-18T17:57:02Z","published":"2024-03-18T17:57:02Z","title":"FlexCap: Generating Rich, Localized, and Flexible Captions in Images","summary":"  We introduce a versatile $\\textit{flexible-captioning}$ vision-language model\n(VLM) capable of generating region-specific descriptions of varying lengths.\nThe model, FlexCap, is trained to produce length-conditioned captions for input\nbounding boxes, and this allows control over the information density of its\noutput, with descriptions ranging from concise object labels to detailed\ncaptions. To achieve this we create large-scale training datasets of image\nregion descriptions of varying length, starting from captioned images. This\nflexible-captioning capability has several valuable applications.\n  First, FlexCap demonstrates superior performance in dense captioning tasks on\nthe Visual Genome dataset. Second, a visual question answering (VQA) system can\nbe built by employing FlexCap to generate localized descriptions as inputs to a\nlarge language model. The resulting system achieves state-of-the-art zero-shot\nperformance on a number of VQA datasets. We also demonstrate a\n$\\textit{localize-then-describe}$ approach with FlexCap can be better at\nopen-ended object detection than a $\\textit{describe-then-localize}$ approach\nwith other VLMs. We highlight a novel characteristic of FlexCap, which is its\nability to extract diverse visual information through prefix conditioning.\nFinally, we qualitatively demonstrate FlexCap's broad applicability in tasks\nsuch as image labeling, object attribute recognition, and visual dialog.\nProject webpage: https://flex-cap.github.io .\n","authors":["Debidatta Dwibedi","Vidhi Jain","Jonathan Tompson","Andrew Zisserman","Yusuf Aytar"],"pdf_url":"https://arxiv.org/pdf/2403.12026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12025v1","updated":"2024-03-18T17:56:37Z","published":"2024-03-18T17:56:37Z","title":"A Toolbox for Surfacing Health Equity Harms and Biases in Large Language\n  Models","summary":"  Large language models (LLMs) hold immense promise to serve complex health\ninformation needs but also have the potential to introduce harm and exacerbate\nhealth disparities. Reliably evaluating equity-related model failures is a\ncritical step toward developing systems that promote health equity. In this\nwork, we present resources and methodologies for surfacing biases with\npotential to precipitate equity-related harms in long-form, LLM-generated\nanswers to medical questions and then conduct an empirical case study with\nMed-PaLM 2, resulting in the largest human evaluation study in this area to\ndate. Our contributions include a multifactorial framework for human assessment\nof LLM-generated answers for biases, and EquityMedQA, a collection of seven\nnewly-released datasets comprising both manually-curated and LLM-generated\nquestions enriched for adversarial queries. Both our human assessment framework\nand dataset design process are grounded in an iterative participatory approach\nand review of possible biases in Med-PaLM 2 answers to adversarial queries.\nThrough our empirical study, we find that the use of a collection of datasets\ncurated through a variety of methodologies, coupled with a thorough evaluation\nprotocol that leverages multiple assessment rubric designs and diverse rater\ngroups, surfaces biases that may be missed via narrower evaluation approaches.\nOur experience underscores the importance of using diverse assessment\nmethodologies and involving raters of varying backgrounds and expertise. We\nemphasize that while our framework can identify specific forms of bias, it is\nnot sufficient to holistically assess whether the deployment of an AI system\npromotes equitable health outcomes. We hope the broader community leverages and\nbuilds on these tools and methods towards realizing a shared goal of LLMs that\npromote accessible and equitable healthcare for all.\n","authors":["Stephen R. Pfohl","Heather Cole-Lewis","Rory Sayres","Darlene Neal","Mercy Asiedu","Awa Dieng","Nenad Tomasev","Qazi Mamunur Rashid","Shekoofeh Azizi","Negar Rostamzadeh","Liam G. McCoy","Leo Anthony Celi","Yun Liu","Mike Schaekermann","Alanna Walton","Alicia Parrish","Chirag Nagpal","Preeti Singh","Akeiylah Dewitt","Philip Mansfield","Sushant Prakash","Katherine Heller","Alan Karthikesalingam","Christopher Semturs","Joelle Barral","Greg Corrado","Yossi Matias","Jamila Smith-Loud","Ivor Horn","Karan Singhal"],"pdf_url":"https://arxiv.org/pdf/2403.12025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12024v1","updated":"2024-03-18T17:56:13Z","published":"2024-03-18T17:56:13Z","title":"Enhancing Hokkien Dual Translation by Exploring and Standardizing of\n  Four Writing Systems","summary":"  Machine translation focuses mainly on high-resource languages (HRLs), while\nlow-resource languages (LRLs) like Taiwanese Hokkien are relatively\nunder-explored. This study aims to address this gap by developing a dual\ntranslation model between Taiwanese Hokkien and both Traditional Mandarin\nChinese and English. We employ a pre-trained LLaMA2-7B model specialized in\nTraditional Mandarin Chinese to leverage the orthographic similarities between\nTaiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive\nexperiments involve translation tasks across various writing systems of\nTaiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that\nthe use of a limited monolingual corpus also further improve the model's\nTaiwanese Hokkien capabilities. We then utilize our translation model to\nstandardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting\nin further performance improvements. Additionally, we introduce an evaluation\nmethod incorporating back-translation and GPT-4 to ensure reliable translation\nquality assessment even for LRLs. The study contributes to narrowing the\nresource gap for Taiwanese Hokkien and empirically investigates the advantages\nand limitations of pre-training and fine-tuning based on LLaMA 2.\n","authors":["Bo-Han Lu","Yi-Hsuan Lin","En-Shiun Annie Lee","Richard Tzong-Han Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.12024v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.12017v1","updated":"2024-03-18T17:52:57Z","published":"2024-03-18T17:52:57Z","title":"Supervised Fine-Tuning as Inverse Reinforcement Learning","summary":"  The prevailing approach to aligning Large Language Models (LLMs) typically\nrelies on human or AI feedback and assumes access to specific types of\npreference datasets. In our work, we question the efficacy of such datasets and\nexplore various scenarios where alignment with expert demonstrations proves\nmore realistic. We build a sequential decision-making framework to formulate\nthe problem of aligning LLMs using demonstration datasets. Drawing insights\nfrom inverse reinforcement learning and imitation learning, we introduce\nvarious approaches for divergence minimization in the LLM alignment tasks. Our\nanalysis highlights the mass-covering and mode-seeking behaviors of these\ndifferent approaches. Inclusively, we examine the pros and cons of the\nclassical supervised fine-tuning method, elaborating on scenarios where\ndifferent methods shine.\n","authors":["Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.12017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12014v1","updated":"2024-03-18T17:51:16Z","published":"2024-03-18T17:51:16Z","title":"EnvGen: Generating and Adapting Environments via LLMs for Training\n  Embodied Agents","summary":"  Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller embodied RL agents learn useful skills that they are weak at? We\npropose EnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand simulator objectives that the agents should learn and is then asked to\ngenerate a set of environment configurations (e.g., different terrains, items\ngiven to agents, etc.). Next, we train a small RL agent in a mixture of the\noriginal and LLM-generated environments. Then, we enable the LLM to\ncontinuously adapt the generated environments to progressively improve the\nskills that the agent is weak at, by providing feedback to the LLM in the form\nof the agent's performance. We demonstrate the usefulness of EnvGen with\ncomprehensive experiments in Crafter and Heist environments. We find that a\nsmall RL agent trained with EnvGen can outperform SOTA methods, including a\nGPT-4 agent, and learns long-horizon tasks significantly faster. We show\nqualitatively how the LLM adapts training environments to help improve RL\nagents' weaker skills over time. Additionally, EnvGen is substantially more\nefficient as it only uses a small number of LLM calls (e.g., 4 in total),\nwhereas LLM agents require thousands of LLM calls. Lastly, we present detailed\nablation studies for our design choices.\n","authors":["Abhay Zala","Jaemin Cho","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2403.12014v1.pdf","comment":"First two authors contributed equally; Project website:\n  https://envgen-llm.github.io/"},{"id":"http://arxiv.org/abs/2403.12766v1","updated":"2024-03-18T17:32:32Z","published":"2024-03-18T17:32:32Z","title":"NovelQA: A Benchmark for Long-Range Novel Question Answering","summary":"  The rapid advancement of Large Language Models (LLMs) has introduced a new\nfrontier in natural language processing, particularly in understanding and\nprocessing long-context information. However, the evaluation of these models'\nlong-context abilities remains a challenge due to the limitations of current\nbenchmarks. To address this gap, we introduce NovelQA, a benchmark specifically\ndesigned to test the capabilities of LLMs with extended texts. Constructed from\nEnglish novels, NovelQA offers a unique blend of complexity, length, and\nnarrative coherence, making it an ideal tool for assessing deep textual\nunderstanding in LLMs. This paper presents the design and construction of\nNovelQA, highlighting its manual annotation, and diverse question types. Our\nevaluation of Long-context LLMs on NovelQA reveals significant insights into\nthe models' performance, particularly emphasizing the challenges they face with\nmulti-hop reasoning, detail-oriented questions, and extremely long input with\nmore than 100,000 tokens. The results underscore the necessity for further\nadvancements in LLMs to improve their long-context comprehension and\ncomputational literary studies.\n","authors":["Cunxiang Wang","Ruoxi Ning","Boqi Pan","Tonghui Wu","Qipeng Guo","Cheng Deng","Guangsheng Bao","Qian Wang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11996v1","updated":"2024-03-18T17:30:27Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Using generative Artificial Intelligence (AI), we transformed a set of 1,000\nscientific papers in the area of biological materials into detailed ontological\nknowledge graphs, revealing their inherently scale-free nature. Using graph\ntraversal path detection between dissimilar concepts based on combinatorial\nranking of node similarity and betweenness centrality, we reveal deep insights\ninto unprecedented interdisciplinary relationships that can be used to answer\nqueries, identify gaps in knowledge, and propose never-before-seen material\ndesigns and their behaviors. One comparison revealed detailed structural\nparallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. The\nalgorithm further created an innovative hierarchical mycelium-based composite\nthat incorporates joint synthesis of graph sampling with principles extracted\nfrom Kandinsky's Composition VII painting, where the resulting composite\nreflects a balance of chaos and order, with features like adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across physical, biological, and artistic spheres,\nrevealing a nuanced ontology of immanence and material flux that resonates with\npostmodern philosophy, and positions these interconnections within a\nheterarchical framework. Our findings reveal the dynamic, context-dependent\ninterplay of entities beyond traditional hierarchical paradigms, emphasizing\nthe significant role of individual components and their fluctuative\nrelationships within the system. Our predictions achieve a far higher degree of\nnovelty, technical detail and explorative capacity than conventional generative\nAI methods. The approach establishes a widely useful framework for innovation\nby revealing hidden connections that facilitate discovery.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11984v1","updated":"2024-03-18T17:21:35Z","published":"2024-03-18T17:21:35Z","title":"Using Generative Text Models to Create Qualitative Codebooks for Student\n  Evaluations of Teaching","summary":"  Feedback is a critical aspect of improvement. Unfortunately, when there is a\nlot of feedback from multiple sources, it can be difficult to distill the\ninformation into actionable insights. Consider student evaluations of teaching\n(SETs), which are important sources of feedback for educators. They can give\ninstructors insights into what worked during a semester. A collection of SETs\ncan also be useful to administrators as signals for courses or entire programs.\nHowever, on a large scale as in high-enrollment courses or administrative\nrecords over several years, the volume of SETs can render them difficult to\nanalyze. In this paper, we discuss a novel method for analyzing SETs using\nnatural language processing (NLP) and large language models (LLMs). We\ndemonstrate the method by applying it to a corpus of 5,000 SETs from a large\npublic university. We show that the method can be used to extract, embed,\ncluster, and summarize the SETs to identify the themes they express. More\ngenerally, this work illustrates how to use the combination of NLP techniques\nand LLMs to generate a codebook for SETs. We conclude by discussing the\nimplications of this method for analyzing SETs and other types of student\nwriting in teaching and research settings.\n","authors":["Andrew Katz","Mitchell Gerhardt","Michelle Soledad"],"pdf_url":"https://arxiv.org/pdf/2403.11984v1.pdf","comment":"Natural language processing, large language models, generative AI,\n  student evaluations of teaching, codebook generation, qualitative data\n  analysis"},{"id":"http://arxiv.org/abs/2401.11972v2","updated":"2024-03-18T17:05:30Z","published":"2024-01-22T14:24:03Z","title":"Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid\n  Approaches to Natural Language Processing","summary":"  The advancement of machine learning and symbolic approaches have underscored\ntheir strengths and weaknesses in Natural Language Processing (NLP). While\nmachine learning approaches are powerful in identifying patterns in data, they\noften fall short in learning commonsense and the factual knowledge required for\nthe NLP tasks. Meanwhile, the symbolic methods excel in representing\nknowledge-rich data. However, they struggle to adapt dynamic data and\ngeneralize the knowledge. Bridging these two paradigms through hybrid\napproaches enables the alleviation of weaknesses in both while preserving their\nstrengths. Recent studies extol the virtues of this union, showcasing promising\nresults in a wide range of NLP tasks. In this paper, we present an overview of\nhybrid approaches used for NLP. Specifically, we delve into the\nstate-of-the-art hybrid approaches used for a broad spectrum of NLP tasks\nrequiring natural language understanding, generation, and reasoning.\nFurthermore, we discuss the existing resources available for hybrid approaches\nfor NLP along with the challenges and future directions, offering a roadmap for\nfuture research avenues.\n","authors":["Rrubaa Panchendrarajan","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2401.11972v2.pdf","comment":"Revised according to review comments"},{"id":"http://arxiv.org/abs/2403.11958v1","updated":"2024-03-18T16:52:54Z","published":"2024-03-18T16:52:54Z","title":"Language Evolution with Deep Learning","summary":"  Computational modeling plays an essential role in the study of language\nemergence. It aims to simulate the conditions and learning processes that could\ntrigger the emergence of a structured language within a simulated controlled\nenvironment. Several methods have been used to investigate the origin of our\nlanguage, including agent-based systems, Bayesian agents, genetic algorithms,\nand rule-based systems. This chapter explores another class of computational\nmodels that have recently revolutionized the field of machine learning: deep\nlearning models. The chapter introduces the basic concepts of deep and\nreinforcement learning methods and summarizes their helpfulness for simulating\nlanguage emergence. It also discusses the key findings, limitations, and recent\nattempts to build realistic simulations. This chapter targets linguists and\ncognitive scientists seeking an introduction to deep learning as a tool to\ninvestigate language evolution.\n","authors":["Mathieu Rita","Paul Michel","Rahma Chaabouni","Olivier Pietquin","Emmanuel Dupoux","Florian Strub"],"pdf_url":"https://arxiv.org/pdf/2403.11958v1.pdf","comment":"to appear in the Oxford Handbook of Approaches to Language Evolution"},{"id":"http://arxiv.org/abs/2401.11969v3","updated":"2024-03-18T16:49:59Z","published":"2024-01-22T14:17:03Z","title":"Claim Detection for Automated Fact-checking: A Survey on Monolingual,\n  Multilingual and Cross-Lingual Research","summary":"  Automated fact-checking has drawn considerable attention over the past few\ndecades due to the increase in the diffusion of misinformation on online\nplatforms. This is often carried out as a sequence of tasks comprising (i) the\ndetection of sentences circulating in online platforms which constitute claims\nneeding verification, followed by (ii) the verification process of those\nclaims. This survey focuses on the former, by discussing existing efforts\ntowards detecting claims needing fact-checking, with a particular focus on\nmultilingual data and methods. This is a challenging and fertile direction\nwhere existing methods are yet far from matching human performance due to the\nprofoundly challenging nature of the issue. Especially, the dissemination of\ninformation across multiple social platforms, articulated in multiple languages\nand modalities demands more generalized solutions for combating misinformation.\nFocusing on multilingual misinformation, we present a comprehensive survey of\nexisting multilingual claim detection research. We present state-of-the-art\nmultilingual claim detection research categorized into three key factors of the\nproblem, verifiability, priority, and similarity. Further, we present a\ndetailed overview of the existing multilingual datasets along with the\nchallenges and suggest possible future advancements.\n","authors":["Rrubaa Panchendrarajan","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2401.11969v3.pdf","comment":"Accepted revision"},{"id":"http://arxiv.org/abs/2306.16001v3","updated":"2024-03-18T16:22:16Z","published":"2023-06-28T08:20:35Z","title":"Streamlining Social Media Information Retrieval for COVID-19 Research\n  with Deep Learning","summary":"  Objective: Social media-based public health research is crucial for epidemic\nsurveillance, but most studies identify relevant corpora with keyword-matching.\nThis study develops a system to streamline the process of curating colloquial\nmedical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial\nsymptom dictionary from COVID-19-related tweets as proof of concept. Methods:\nCOVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The\npipeline includes three modules: a named entity recognition module to detect\nsymptoms in tweets; an entity normalization module to aggregate detected\nentities; and a mapping module that iteratively maps entities to Unified\nMedical Language System concepts. A random 500 entity sample were drawn from\nthe final dictionary for accuracy validation. Additionally, we conducted a\nsymptom frequency distribution analysis to compare our dictionary to a\npre-defined lexicon from previous research. Results: We identified 498,480\nunique symptom entity expressions from the tweets. Pre-processing reduces the\nnumber to 18,226. The final dictionary contains 38,175 unique expressions of\nsymptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom\ndistribution analysis found that our dictionary detects more symptoms and is\neffective at identifying psychiatric disorders like anxiety and depression,\noften missed by pre-defined lexicons. Conclusions: This study advances public\nhealth research by implementing a novel, systematic pipeline for curating\nsymptom lexicons from social media data. The final lexicon's high accuracy,\nvalidated by medical professionals, underscores the potential of this\nmethodology to reliably interpret and categorize vast amounts of unstructured\nsocial media data into actionable medical insights across diverse linguistic\nand regional landscapes.\n","authors":["Yining Hua","Jiageng Wu","Shixu Lin","Minghui Li","Yujie Zhang","Dinah Foer","Siwen Wang","Peilin Zhou","Jie Yang","Li Zhou"],"pdf_url":"https://arxiv.org/pdf/2306.16001v3.pdf","comment":"Updated full paper. Abstract presented at IEEE ICHI 2023 and AMIA\n  Annual Symposium 2023"},{"id":"http://arxiv.org/abs/2402.17527v2","updated":"2024-03-18T16:21:24Z","published":"2024-02-27T14:11:32Z","title":"Predict the Next Word: Humans exhibit uncertainty in this task and\n  language models _____","summary":"  Language models (LMs) are statistical models trained to assign probability to\nhuman-generated text. As such, it is reasonable to question whether they\napproximate linguistic variability exhibited by humans well. This form of\nstatistical assessment is difficult to perform at the passage level, for it\nrequires acceptability judgements (i.e., human evaluation) or a robust\nautomated proxy (which is non-trivial). At the word level, however, given some\ncontext, samples from an LM can be assessed via exact matching against a\nprerecorded dataset of alternative single-word continuations of the available\ncontext. We exploit this fact and evaluate the LM's ability to reproduce\nvariability that humans (in particular, a population of English speakers)\nexhibit in the 'next word prediction' task. This can be seen as assessing a\nform of calibration, which, in the context of text classification, Baan et al.\n(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and\nChatGPT and find that they exhibit fairly low calibration to human uncertainty.\nWe also verify the failure of expected calibration error (ECE) to reflect this,\nand as such, advise the community against relying on it in this setting.\n","authors":["Evgenia Ilia","Wilker Aziz"],"pdf_url":"https://arxiv.org/pdf/2402.17527v2.pdf","comment":"22 pages, EACL 2024"},{"id":"http://arxiv.org/abs/2403.11921v1","updated":"2024-03-18T16:19:41Z","published":"2024-03-18T16:19:41Z","title":"Adaptative Bilingual Aligning Using Multilingual Sentence Embedding","summary":"  In this paper, we present an adaptive bitextual alignment system called\nAIlign. This aligner relies on sentence embeddings to extract reliable anchor\npoints that can guide the alignment path, even for texts whose parallelism is\nfragmentary and not strictly monotonic. In an experiment on several datasets,\nwe show that AIlign achieves results equivalent to the state of the art, with\nquasi-linear complexity. In addition, AIlign is able to handle texts whose\nparallelism and monotonicity properties are only satisfied locally, unlike\nrecent systems such as Vecalign or Bertalign.\n","authors":["Olivier Kraif"],"pdf_url":"https://arxiv.org/pdf/2403.11921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11905v1","updated":"2024-03-18T16:06:30Z","published":"2024-03-18T16:06:30Z","title":"Tur[k]ingBench: A Challenge Benchmark for Web Agents","summary":"  Recent chatbots have demonstrated impressive ability to understand and\ncommunicate in raw-text form. However, there is more to the world than raw\ntext. For example, humans spend long hours of their time on web pages, where\ntext is intertwined with other modalities and tasks are accomplished in the\nform of various complex interactions. Can state-of-the-art multi-modal models\ngeneralize to such complex domains?\n  To address this question, we introduce TurkingBench, a benchmark of tasks\nformulated as web pages containing textual instructions with multi-modal\ncontext. Unlike existing work which employs artificially synthesized web pages,\nhere we use natural HTML pages that were originally designed for crowdsourcing\nworkers for various annotation purposes. The HTML instructions of each task are\nalso instantiated with various values (obtained from the crowdsourcing tasks)\nto form new instances of the task. This benchmark contains 32.2K instances\ndistributed across 158 tasks.\n  Additionally, to facilitate the evaluation on TurkingBench, we develop an\nevaluation framework that connects the responses of chatbots to modifications\non web pages (modifying a text box, checking a radio, etc.). We evaluate the\nperformance of state-of-the-art models, including language-only, vision-only,\nand layout-only models, and their combinations, on this benchmark. Our findings\nreveal that these models perform significantly better than random chance, yet\nconsiderable room exists for improvement. We hope this benchmark will help\nfacilitate the evaluation and development of web-based agents.\n","authors":["Kevin Xu","Yeganeh Kordi","Kate Sanders","Yizhong Wang","Adam Byerly","Jack Zhang","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2403.11905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11904v1","updated":"2024-03-18T16:04:55Z","published":"2024-03-18T16:04:55Z","title":"CICLe: Conformal In-Context Learning for Largescale Multi-Class Food\n  Risk Classification","summary":"  Contaminated or adulterated food poses a substantial risk to human health.\nGiven sets of labeled web texts for training, Machine Learning and Natural\nLanguage Processing can be applied to automatically detect such risks. We\npublish a dataset of 7,546 short texts describing public food recall\nannouncements. Each text is manually labeled, on two granularity levels (coarse\nand fine), for food products and hazards that the recall corresponds to. We\ndescribe the dataset and benchmark naive, traditional, and Transformer models.\nBased on our analysis, Logistic Regression based on a tf-idf representation\noutperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss\ndifferent prompting strategies and present an LLM-in-the-loop framework, based\non Conformal Prediction, which boosts the performance of the base classifier\nwhile reducing energy consumption compared to normal prompting.\n","authors":["Korbinian Randl","John Pavlopoulos","Aron Henriksson","Tony Lindgren"],"pdf_url":"https://arxiv.org/pdf/2403.11904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11903v1","updated":"2024-03-18T16:03:45Z","published":"2024-03-18T16:03:45Z","title":"A Closer Look at Claim Decomposition","summary":"  As generated text becomes more commonplace, it is increasingly important to\nevaluate how well-supported such text is by external knowledge sources. Many\napproaches for evaluating textual support rely on some method for decomposing\ntext into its individual subclaims which are scored against a trusted\nreference. We investigate how various methods of claim decomposition --\nespecially LLM-based methods -- affect the result of an evaluation approach\nsuch as the recently proposed FActScore, finding that it is sensitive to the\ndecomposition method used. This sensitivity arises because such metrics\nattribute overall textual support to the model that generated the text even\nthough error can also come from the metric's decomposition step. To measure\ndecomposition quality, we introduce an adaptation of FActScore, which we call\nDecompScore. We then propose an LLM-based approach to generating decompositions\ninspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian\nsemantics and demonstrate its improved decomposition quality over previous\nmethods.\n","authors":["Miriam Wanner","Seth Ebner","Zhengping Jiang","Mark Dredze","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2403.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01313v3","updated":"2024-03-18T16:02:10Z","published":"2023-08-02T17:57:25Z","title":"PerceptionCLIP: Visual Classification by Inferring and Conditioning on\n  Contexts","summary":"  Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninteroperability. Our code is available at\nhttps://github.com/umd-huang-lab/perceptionCLIP\n","authors":["Bang An","Sicheng Zhu","Michael-Andrei Panaitescu-Liess","Chaithanya Kumar Mummadi","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2308.01313v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.11896v1","updated":"2024-03-18T15:54:46Z","published":"2024-03-18T15:54:46Z","title":"Investigating Markers and Drivers of Gender Bias in Machine Translations","summary":"  Implicit gender bias in Large Language Models (LLMs) is a well-documented\nproblem, and implications of gender introduced into automatic translations can\nperpetuate real-world biases. However, some LLMs use heuristics or\npost-processing to mask such bias, making investigation difficult. Here, we\nexamine bias in LLMss via back-translation, using the DeepL translation API to\ninvestigate the bias evinced when repeatedly translating a set of 56 Software\nEngineering tasks used in a previous study. Each statement starts with 'she',\nand is translated first into a 'genderless' intermediate language then back\ninto English; we then examine pronoun-choice in the back-translated texts. We\nexpand prior research in the following ways: (1) by comparing results across\nfive intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and\nHungarian; (2) by proposing a novel metric for assessing the variation in\ngender implied in the repeated translations, avoiding the over-interpretation\nof individual pronouns, apparent in earlier work; (3) by investigating sentence\nfeatures that drive bias; (4) and by comparing results from three time-lapsed\ndatasets to establish the reproducibility of the approach. We found that some\nlanguages display similar patterns of pronoun use, falling into three loose\ngroups, but that patterns vary between groups; this underlines the need to work\nwith multiple languages. We also identify the main verb appearing in a sentence\nas a likely significant driver of implied gender in the translations. Moreover,\nwe see a good level of replicability in the results, and establish that our\nvariation metric proves robust despite an obvious change in the behaviour of\nthe DeepL translation API during the course of the study. These results show\nthat the back-translation method can provide further insights into bias in\nlanguage models.\n","authors":["Peter J Barclay","Ashkan Sami"],"pdf_url":"https://arxiv.org/pdf/2403.11896v1.pdf","comment":"Accepted to SANER 2024; see\n  https://conf.researchr.org/home/saner-2024"},{"id":"http://arxiv.org/abs/2403.11894v1","updated":"2024-03-18T15:53:33Z","published":"2024-03-18T15:53:33Z","title":"From explainable to interpretable deep learning for natural language\n  processing in healthcare: how far from reality?","summary":"  Deep learning (DL) has substantially enhanced healthcare research by\naddressing various natural language processing (NLP) tasks. Yet, the increasing\ncomplexity of DL-based NLP methods necessitates transparent model\ninterpretability, or at least explainability, for reliable decision-making.\nThis work presents a thorough scoping review on explainable and interpretable\nDL in healthcare NLP. The term \"XIAI\" (eXplainable and Interpretable Artificial\nIntelligence) was introduced to distinguish XAI from IAI. Methods were further\ncategorized based on their functionality (model-, input-, output-based) and\nscope (local, global). Our analysis shows that attention mechanisms were the\nmost dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The\nmajor challenges identified are that most XIAI do not explore \"global\" modeling\nprocesses, the lack of best practices, and the unmet need for systematic\nevaluation and benchmarks. Important opportunities were raised such as using\n\"attention\" to enhance multi-modal XIAI for personalized medicine and combine\nDL with causal reasoning. Our discussion encourages the integration of XIAI in\nLLMs and domain-specific smaller models. Our review can stimulate further\nresearch and benchmarks toward improving inherent IAI and engaging complex NLP\nin healthcare.\n","authors":["Guangming Huang","Yunfei Long","Yingya Li","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2403.11894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11886v1","updated":"2024-03-18T15:39:14Z","published":"2024-03-18T15:39:14Z","title":"QueryAgent: A Reliable and Efficient Reasoning Framework with\n  Environmental Feedback based Self-Correction","summary":"  Employing Large Language Models (LLMs) for semantic parsing has achieved\nremarkable success. However, we find existing methods fall short in terms of\nreliability and efficiency when hallucinations are encountered. In this paper,\nwe address these challenges with a framework called QueryAgent, which solves a\nquestion step-by-step and performs step-wise self-correction. We introduce an\nenvironmental feedback-based self-correction method called ERASER. Unlike\ntraditional approaches, ERASER leverages rich environmental feedback in the\nintermediate steps to perform selective and differentiated self-correction only\nwhen necessary. Experimental results demonstrate that QueryAgent notably\noutperforms all previous few-shot methods using only one example on GrailQA and\nGraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms\nof efficiency, including runtime, query overhead, and API invocation costs. By\nleveraging ERASER, we further improve another baseline (i.e., AgentBench) by\napproximately 10 points, revealing the strong transferability of our approach.\n","authors":["Xiang Huang","Sitao Cheng","Shanshan Huang","Jiayu Shen","Yong Xu","Chaoyun Zhang","Yuzhong Qu"],"pdf_url":"https://arxiv.org/pdf/2403.11886v1.pdf","comment":"ACL 2024 under review"},{"id":"http://arxiv.org/abs/2403.11873v1","updated":"2024-03-18T15:26:32Z","published":"2024-03-18T15:26:32Z","title":"CO3: Low-resource Contrastive Co-training for Generative Conversational\n  Query Rewrite","summary":"  Generative query rewrite generates reconstructed query rewrites using the\nconversation history while rely heavily on gold rewrite pairs that are\nexpensive to obtain. Recently, few-shot learning is gaining increasing\npopularity for this task, whereas these methods are sensitive to the inherent\nnoise due to limited data size. Besides, both attempts face performance\ndegradation when there exists language style shift between training and testing\ncases. To this end, we study low-resource generative conversational query\nrewrite that is robust to both noise and language style shift. The core idea is\nto utilize massive unlabeled data to make further improvements via a\ncontrastive co-training paradigm. Specifically, we co-train two dual models\n(namely Rewriter and Simplifier) such that each of them provides extra guidance\nthrough pseudo-labeling for enhancing the other in an iterative manner. We also\nleverage contrastive learning with data augmentation, which enables our model\npay more attention on the truly valuable information than the noise. Extensive\nexperiments demonstrate the superiority of our model under both few-shot and\nzero-shot scenarios. We also verify the better generalization ability of our\nmodel when encountering language style shift.\n","authors":["Yifei Yuan","Chen Shi","Runze Wang","Liyi Chen","Renjun Hu","Zengming Zhang","Feijun Jiang","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2403.11873v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2402.01030v2","updated":"2024-03-18T15:18:45Z","published":"2024-02-01T21:38:58Z","title":"Executable Code Actions Elicit Better LLM Agents","summary":"  Large Language Model (LLM) agents, capable of performing a broad range of\nactions, such as invoking tools and controlling robots, show great potential in\ntackling real-world challenges. LLM agents are typically prompted to produce\nactions by generating JSON or text in a pre-defined format, which is usually\nlimited by constrained action space (e.g., the scope of pre-defined tools) and\nrestricted flexibility (e.g., inability to compose multiple tools). This work\nproposes to use executable Python code to consolidate LLM agents' actions into\na unified action space (CodeAct). Integrated with a Python interpreter, CodeAct\ncan execute code actions and dynamically revise prior actions or emit new\nactions upon new observations through multi-turn interactions. Our extensive\nanalysis of 17 LLMs on API-Bank and a newly curated benchmark shows that\nCodeAct outperforms widely used alternatives (up to 20% higher success rate).\nThe encouraging performance of CodeAct motivates us to build an open-source LLM\nagent that interacts with environments by executing interpretable code and\ncollaborates with users using natural language. To this end, we collect an\ninstruction-tuning dataset CodeActInstruct that consists of 7k multi-turn\ninteractions using CodeAct. We show that it can be used with existing data to\nimprove models in agent-oriented tasks without compromising their general\ncapability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with\nPython interpreter and uniquely tailored to perform sophisticated tasks (e.g.,\nmodel training) using existing libraries and autonomously self-debug.\n","authors":["Xingyao Wang","Yangyi Chen","Lifan Yuan","Yizhe Zhang","Yunzhu Li","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2402.01030v2.pdf","comment":"Code, data, model, and demo are available at\n  https://github.com/xingyaoww/code-act"},{"id":"http://arxiv.org/abs/2401.12873v3","updated":"2024-03-18T15:16:16Z","published":"2024-01-23T16:07:43Z","title":"Improving Machine Translation with Human Feedback: An Exploration of\n  Quality Estimation as a Reward Model","summary":"  Insufficient modeling of human preferences within the reward model is a major\nobstacle for leveraging human feedback to improve translation quality.\nFortunately, quality estimation (QE), which predicts the quality of a given\ntranslation without reference, has achieved impressive alignment with human\nevaluations in the last two years. In this work, we investigate the potential\nof employing the QE model as the reward model to predict human preferences for\nfeedback training. We first identify the overoptimization problem during\nQE-based feedback training, manifested as an increase in reward while\ntranslation quality declines. We examine the problem and argue that the\nvulnerability of the QE model might lead to high rewards for incorrect\ntranslations, resulting in overoptimization and error propagation. To address\nthe problem, we adopt a simple yet effective method that uses heuristic rules\nto detect the incorrect translations and assigns a penalty term to the reward\nscores of them. Experimental results show that the proposed QE-based feedback\ntraining achieves consistent and significant improvements across various\nsettings, further verified through human preference studies. Our subsequent\nanalysis demonstrates the high data efficiency of the proposed QE-based\nfeedback training: it outperforms systems using larger parallel corpora by a\nsmall amount of monolingual data. Our code is available at:\nhttps://github.com/zwhe99/FeedbackMT\n","authors":["Zhiwei He","Xing Wang","Wenxiang Jiao","Zhuosheng Zhang","Rui Wang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2401.12873v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.11858v1","updated":"2024-03-18T15:08:01Z","published":"2024-03-18T15:08:01Z","title":"GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management\n  in Agriculture","summary":"  In the rapidly evolving field of artificial intelligence (AI), the\napplication of large language models (LLMs) in agriculture, particularly in\npest management, remains nascent. We aimed to prove the feasibility by\nevaluating the content of the pest management advice generated by LLMs,\nincluding the Generative Pre-trained Transformer (GPT) series from OpenAI and\nthe FLAN series from Google. Considering the context-specific properties of\nagricultural advice, automatically measuring or quantifying the quality of text\ngenerated by LLMs becomes a significant challenge. We proposed an innovative\napproach, using GPT-4 as an evaluator, to score the generated content on\nCoherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and\nExhaustiveness. Additionally, we integrated an expert system based on crop\nthreshold data as a baseline to obtain scores for Factual Accuracy on whether\npests found in crop fields should take management action. Each model's score\nwas weighted by percentage to obtain a final score. The results showed that\nGPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories.\nFurthermore, the use of instruction-based prompting containing domain-specific\nknowledge proved the feasibility of LLMs as an effective tool in agriculture,\nwith an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing\npest management suggestions.\n","authors":["Shanglong Yang","Zhipeng Yuan","Shunbao Li","Ruoling Peng","Kang Liu","Po Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01139v2","updated":"2024-03-18T14:55:46Z","published":"2024-03-02T08:53:40Z","title":"ParallelPARC: A Scalable Pipeline for Generating Natural-Language\n  Analogies","summary":"  Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.\n","authors":["Oren Sultan","Yonatan Bitton","Ron Yosef","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2403.01139v2.pdf","comment":"NAACL 2024 main"},{"id":"http://arxiv.org/abs/2403.11838v1","updated":"2024-03-18T14:48:29Z","published":"2024-03-18T14:48:29Z","title":"Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for\n  Language Models","summary":"  Large Language Models (LLMs) exhibit impressive capabilities but also present\nrisks such as biased content generation and privacy issues. One of the current\nalignment techniques includes principle-driven integration, but it faces\nchallenges arising from the imprecision of manually crafted rules and\ninadequate risk perception in models without safety training. To address these,\nwe introduce Guide-Align, a two-stage approach. Initially, a safety-trained\nmodel identifies potential risks and formulates specific guidelines for various\ninputs, thereby establishing a comprehensive library of guidelines and models\nfor input-guidelines retrieval. Subsequently, the retrieval model correlates\nnew inputs with pertinent guidelines, guiding LLMs in response generation to\nensure safe and high-quality outputs, thus aligning with human values. An\nadditional optional stage involves fine-tuning a model with new well-aligned\ndatasets generated through the process implemented in the second stage. Our\nmethod customizes guidelines to accommodate diverse inputs, thereby enhancing\nthe fine-grainedness and comprehensiveness of the guideline library.\nFurthermore, it incorporates safety expertise from a safety-trained LLM through\na lightweight retrieval model. We evaluated our approach on three benchmarks,\ndemonstrating significant improvements in LLM security and quality. Notably,\nour fine-tuned model, Labrador, even at 13 billion parameters, outperforms\nGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.\n","authors":["Yi Luo","Zhenghao Lin","Yuhao Zhang","Jiashuo Sun","Chen Lin","Chengjin Xu","Xiangdong Su","Yelong Shen","Jian Guo","Yeyun Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11838v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.11834v1","updated":"2024-03-18T14:45:52Z","published":"2024-03-18T14:45:52Z","title":"Towards Understanding the Relationship between In-context Learning and\n  Compositional Generalization","summary":"  According to the principle of compositional generalization, the meaning of a\ncomplex expression can be understood as a function of the meaning of its parts\nand of how they are combined. This principle is crucial for human language\nprocessing and also, arguably, for NLP models in the face of\nout-of-distribution data. However, many neural network models, including\nTransformers, have been shown to struggle with compositional generalization. In\nthis paper, we hypothesize that forcing models to in-context learn can provide\nan inductive bias to promote compositional generalization. To test this\nhypothesis, we train a causal Transformer in a setting that renders ordinary\nlearning very difficult: we present it with different orderings of the training\ninstance and shuffle instance labels. This corresponds to training the model on\nall possible few-shot learning problems attainable from the dataset. The model\ncan solve the task, however, by utilizing earlier examples to generalize to\nlater ones (i.e. in-context learning). In evaluations on the datasets, SCAN,\nCOGS, and GeoQuery, models trained in this manner indeed show improved\ncompositional generalization. This indicates the usefulness of in-context\nlearning problems as an inductive bias for generalization.\n","authors":["Sungjun Han","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2403.11834v1.pdf","comment":"To be published in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11833v1","updated":"2024-03-18T14:45:20Z","published":"2024-03-18T14:45:20Z","title":"SSCAE -- Semantic, Syntactic, and Context-aware natural language\n  Adversarial Examples generator","summary":"  Machine learning models are vulnerable to maliciously crafted Adversarial\nExamples (AEs). Training a machine learning model with AEs improves its\nrobustness and stability against adversarial attacks. It is essential to\ndevelop models that produce high-quality AEs. Developing such models has been\nmuch slower in natural language processing (NLP) than in areas such as computer\nvision. This paper introduces a practical and efficient adversarial attack\nmodel called SSCAE for \\textbf{S}emantic, \\textbf{S}yntactic, and\n\\textbf{C}ontext-aware natural language \\textbf{AE}s generator. SSCAE\nidentifies important words and uses a masked language model to generate an\nearly set of substitutions. Next, two well-known language models are employed\nto evaluate the initial set in terms of semantic and syntactic characteristics.\nWe introduce (1) a dynamic threshold to capture more efficient perturbations\nand (2) a local greedy search to generate high-quality AEs. As a black-box\nmethod, SSCAE generates humanly imperceptible and context-aware AEs that\npreserve semantic consistency and the source language's syntactical and\ngrammatical requirements. The effectiveness and superiority of the proposed\nSSCAE model are illustrated with fifteen comparative experiments and extensive\nsensitivity analysis for parameter optimization. SSCAE outperforms the existing\nmodels in all experiments while maintaining a higher semantic consistency with\na lower query number and a comparable perturbation rate.\n","authors":["Javad Rafiei Asl","Mohammad H. Rafiei","Manar Alohaly","Daniel Takabi"],"pdf_url":"https://arxiv.org/pdf/2403.11833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11810v1","updated":"2024-03-18T14:08:59Z","published":"2024-03-18T14:08:59Z","title":"Metaphor Understanding Challenge Dataset for LLMs","summary":"  Metaphors in natural language are a reflection of fundamental cognitive\nprocesses such as analogical reasoning and categorisation, and are deeply\nrooted in everyday communication. Metaphor understanding is therefore an\nessential task for large language models (LLMs). We release the Metaphor\nUnderstanding Challenge Dataset (MUNCH), designed to evaluate the metaphor\nunderstanding capabilities of LLMs. The dataset provides over 10k paraphrases\nfor sentences containing metaphor use, as well as 1.5k instances containing\ninapt paraphrases. The inapt paraphrases were carefully selected to serve as\ncontrol to determine whether the model indeed performs full metaphor\ninterpretation or rather resorts to lexical similarity. All apt and inapt\nparaphrases were manually annotated. The metaphorical sentences cover natural\nmetaphor uses across 4 genres (academic, news, fiction, and conversation), and\nthey exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5\ndemonstrate that MUNCH presents a challenging task for LLMs. The dataset is\nfreely accessible at\nhttps://github.com/xiaoyuisrain/metaphor-understanding-challenge.\n","authors":["Xiaoyu Tong","Rochelle Choenni","Martha Lewis","Ekaterina Shutova"],"pdf_url":"https://arxiv.org/pdf/2403.11810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11807v1","updated":"2024-03-18T14:04:47Z","published":"2024-03-18T14:04:47Z","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments","summary":"  Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates LLMs' decision-making capabilities through the lens of a\nwell-established field, Game Theory. We focus specifically on games that\nsupport the participation of more than two agents simultaneously. Subsequently,\nwe introduce our framework, GAMA-Bench, including eight classical multi-agent\ngames. We design a scoring scheme to assess a model's performance in these\ngames quantitatively. Through GAMA-Bench, we investigate LLMs' robustness,\ngeneralizability, and enhancement strategies. Results reveal that while GPT-3.5\nshows satisfying robustness, its generalizability is relatively limited.\nHowever, its performance can be improved through approaches such as\nChain-of-Thought. Additionally, we conduct evaluations across various LLMs and\nfind that GPT-4 outperforms other models on GAMA-Bench, achieving a score of\n72.5. Moreover, the increasingly higher scores across the three iterations of\nGPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's\nintelligence with each update. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench.\n","authors":["Jen-tse Huang","Eric John Li","Man Ho Lam","Tian Liang","Wenxuan Wang","Youliang Yuan","Wenxiang Jiao","Xing Wang","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.11807v1.pdf","comment":"16 pages, 15 figures, 9 tables. Working in Progress"},{"id":"http://arxiv.org/abs/2403.11802v1","updated":"2024-03-18T14:01:45Z","published":"2024-03-18T14:01:45Z","title":"Counting-Stars: A Simple, Efficient, and Reasonable Strategy for\n  Evaluating Long-Context Large Language Models","summary":"  While recent research endeavors have concentrated on developing Large\nLanguage Models (LLMs) with robust long-context capabilities, due to the lack\nof appropriate evaluation strategies, relatively little is known about how well\nthe long-context processing abilities and performance of leading LLMs (e.g.,\nChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and\nreasonable strategy for evaluating long-context LLMs as a new benchmark, named\nCounting-Stars. The Counting-Stars is designed to require LLMs to fully\nunderstand and capture long dependencies in long contexts and be able to\ncollect inter-dependency across multiple pieces of evidence spanning the entire\ncontext to finish the task. Based on the Counting-Stars, we conduct experiments\nto evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat.\nThe experimental results indicate that GPT-4 Turbo and Kimi Chat achieve\nsignificant performance in the long context from 4K to 128K. We further present\ntwo intriguing analyses regarding the behavior of LLMs processing long context.\n","authors":["Mingyang Song","Mao Zheng","Xuan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.11802v1.pdf","comment":"a technical report"},{"id":"http://arxiv.org/abs/2403.11793v1","updated":"2024-03-18T13:50:50Z","published":"2024-03-18T13:50:50Z","title":"Reasoning Abilities of Large Language Models: In-Depth Analysis on the\n  Abstraction and Reasoning Corpus","summary":"  The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstract and Reasoning\nCorpus (ARC) dataset to evaluate the inference and contextual understanding\nabilities of large language models in a process-centric manner. ARC demands\nrigorous logical structures for problem-solving, making it a benchmark that\nfacilitates the comparison of model inference abilities with humans.\nExperimental results confirm that while large language models possess weak\ninference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning.\n","authors":["Seungpil Lee","Woochang Sim","Donghyeon Shin","Sanha Hwang","Wongyu Seo","Jiwon Park","Seokki Lee","Sejin Kim","Sundong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.11793v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2403.11786v1","updated":"2024-03-18T13:44:48Z","published":"2024-03-18T13:44:48Z","title":"Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained\n  Large Language Models","summary":"  Extracting hyper-relations is crucial for constructing comprehensive\nknowledge graphs, but there are limited supervised methods available for this\ntask. To address this gap, we introduce a zero-shot prompt-based method using\nOpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.\nComparing our model with a baseline, we achieved promising results, with a\nrecall of 0.77. Although our precision is currently lower, a detailed analysis\nof the model outputs has uncovered potential pathways for future research in\nthis area.\n","authors":["Preetha Datta","Fedor Vitiugin","Anastasiia Chizhikova","Nitin Sawhney"],"pdf_url":"https://arxiv.org/pdf/2403.11786v1.pdf","comment":"5 pages + references"},{"id":"http://arxiv.org/abs/2403.11771v1","updated":"2024-03-18T13:30:03Z","published":"2024-03-18T13:30:03Z","title":"Modality-Agnostic fMRI Decoding of Vision and Language","summary":"  Previous studies have shown that it is possible to map brain activation data\nof subjects viewing images onto the feature representation space of not only\nvision models (modality-specific decoding) but also language models\n(cross-modal decoding). In this work, we introduce and use a new large-scale\nfMRI dataset (~8,500 trials per subject) of people watching both images and\ntext descriptions of such images. This novel dataset enables the development of\nmodality-agnostic decoders: a single decoder that can predict which stimulus a\nsubject is seeing, irrespective of the modality (image or text) in which the\nstimulus is presented. We train and evaluate such decoders to map brain signals\nonto stimulus representations from a large range of publicly available vision,\nlanguage and multimodal (vision+language) models. Our findings reveal that (1)\nmodality-agnostic decoders perform as well as (and sometimes even better than)\nmodality-specific decoders (2) modality-agnostic decoders mapping brain data\nonto representations from unimodal models perform as well as decoders relying\non multimodal representations (3) while language and low-level visual\n(occipital) brain regions are best at decoding text and image stimuli,\nrespectively, high-level visual (temporal) regions perform well on both\nstimulus types.\n","authors":["Mitja Nikolaus","Milad Mozafari","Nicholas Asher","Leila Reddy","Rufin VanRullen"],"pdf_url":"https://arxiv.org/pdf/2403.11771v1.pdf","comment":"To appear at ICLR 2024 workshop on Representational Alignment\n  (Re-Align)"},{"id":"http://arxiv.org/abs/2403.11752v1","updated":"2024-03-18T13:02:02Z","published":"2024-03-18T13:02:02Z","title":"Revisiting The Classics: A Study on Identifying and Rectifying Gender\n  Stereotypes in Rhymes and Poems","summary":"  Rhymes and poems are a powerful medium for transmitting cultural norms and\nsocietal roles. However, the pervasive existence of gender stereotypes in these\nworks perpetuates biased perceptions and limits the scope of individuals'\nidentities. Past works have shown that stereotyping and prejudice emerge in\nearly childhood, and developmental research on causal mechanisms is critical\nfor understanding and controlling stereotyping and prejudice. This work\ncontributes by gathering a dataset of rhymes and poems to identify gender\nstereotypes and propose a model with 97\\% accuracy to identify gender bias.\nGender stereotypes were rectified using a Large Language Model (LLM) and its\neffectiveness was evaluated in a comparative survey against human educator\nrectifications. To summarize, this work highlights the pervasive nature of\ngender stereotypes in literary works and reveals the potential of LLMs to\nrectify gender stereotypes. This study raises awareness and promotes\ninclusivity within artistic expressions, making a significant contribution to\nthe discourse on gender equality.\n","authors":["Aditya Narayan Sankaran","Vigneshwaran Shankaran","Sampath Lonka","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.11752v1.pdf","comment":"Accepted to appear at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11747v1","updated":"2024-03-18T12:58:16Z","published":"2024-03-18T12:58:16Z","title":"Embedded Named Entity Recognition using Probing Classifiers","summary":"  Extracting semantic information from generated text is a useful tool for\napplications such as automated fact checking or retrieval augmented generation.\nCurrently, this requires either separate models during inference, which\nincreases computational cost, or destructive fine-tuning of the language model.\nInstead, we propose directly embedding information extraction capabilities into\npre-trained language models using probing classifiers, enabling efficient\nsimultaneous text generation and information extraction. For this, we introduce\nan approach called EMBER and show that it enables named entity recognition in\ndecoder-only language models without fine-tuning them and while incurring\nminimal additional computational cost at inference time. Specifically, our\nexperiments using GPT-2 show that EMBER maintains high token generation rates\nduring streaming text generation, with only a negligible decrease in speed of\naround 1% compared to a 43.64% slowdown measured for a baseline using a\nseparate NER model. Code and data are available at\nhttps://github.com/nicpopovic/EMBER.\n","authors":["Nicholas Popovič","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2403.11747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09732v2","updated":"2024-03-18T12:45:41Z","published":"2024-03-13T02:32:41Z","title":"PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with\n  Cross-consistency","summary":"  Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.\n","authors":["Zhishuai Li","Xiang Wang","Jingjing Zhao","Sun Yang","Guoqing Du","Xiaoru Hu","Bin Zhang","Yuxiao Ye","Ziyue Li","Rui Zhao","Hangyu Mao"],"pdf_url":"https://arxiv.org/pdf/2403.09732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04325v2","updated":"2024-03-18T11:17:48Z","published":"2024-03-07T08:44:42Z","title":"Measuring Meaning Composition in the Human Brain with Composition Scores\n  from Large Language Models","summary":"  The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.\n","authors":["Changjiang Gao","Jixing Li","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2403.04325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03121v2","updated":"2024-03-18T11:04:44Z","published":"2024-03-05T17:04:05Z","title":"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes\n  in Emotion Attribution","summary":"  Large language models (LLMs) reflect societal norms and biases, especially\nabout gender. While societal biases and stereotypes have been extensively\nresearched in various NLP applications, there is a surprising gap for emotion\nanalysis. However, emotion and gender are closely linked in societal discourse.\nE.g., women are often thought of as more empathetic, while men's anger is more\nsocially accepted. To fill this gap, we present the first comprehensive study\nof gendered emotion attribution in five state-of-the-art LLMs (open- and\nclosed-source). We investigate whether emotions are gendered, and whether these\nvariations are based on societal stereotypes. We prompt the models to adopt a\ngendered persona and attribute emotions to an event like 'When I had a serious\nargument with a dear person'. We then analyze the emotions generated by the\nmodels in relation to the gender-event pairs. We find that all models\nconsistently exhibit gendered emotions, influenced by gender stereotypes. These\nfindings are in line with established research in psychology and gender\nstudies. Our study sheds light on the complex societal interplay between\nlanguage, gender, and emotion. The reproduction of emotion stereotypes in LLMs\nallows us to use those models to study the topic in detail, but raises\nquestions about the predictive use of those same LLMs for emotion applications.\n","authors":["Flor Miriam Plaza-del-Arco","Amanda Cercas Curry","Alba Curry","Gavin Abercrombie","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2403.03121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05472v2","updated":"2024-03-18T10:54:15Z","published":"2023-09-11T14:13:09Z","title":"LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for\n  Self-supervised Representations of French Speech","summary":"  Self-supervised learning (SSL) is at the origin of unprecedented improvements\nin many different domains including computer vision and natural language\nprocessing. Speech processing drastically benefitted from SSL as most of the\ncurrent domain-related tasks are now being approached with pre-trained models.\nThis work introduces LeBenchmark 2.0 an open-source framework for assessing and\nbuilding SSL-equipped French speech technologies. It includes documented,\nlarge-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous\nspeech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to\none billion learnable parameters shared with the community, and an evaluation\nprotocol made of six downstream tasks to complement existing benchmarks.\nLeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for\nspeech with the investigation of frozen versus fine-tuned downstream models,\ntask-agnostic versus task-specific pre-trained models as well as a discussion\non the carbon footprint of large-scale model training. Overall, the newly\nintroduced models trained on 14,000 hours of French speech outperform\nmultilingual and previous LeBenchmark SSL models across the benchmark but also\nrequired up to four times more energy for pre-training.\n","authors":["Titouan Parcollet","Ha Nguyen","Solene Evain","Marcely Zanon Boito","Adrien Pupier","Salima Mdhaffar","Hang Le","Sina Alisamir","Natalia Tomashenko","Marco Dinarelli","Shucong Zhang","Alexandre Allauzen","Maximin Coavoux","Yannick Esteve","Mickael Rouvier","Jerome Goulian","Benjamin Lecouteux","Francois Portet","Solange Rossato","Fabien Ringeval","Didier Schwab","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2309.05472v2.pdf","comment":"Published in Computer Science and Language. Preprint allowed"},{"id":"http://arxiv.org/abs/2403.01222v2","updated":"2024-03-18T10:51:14Z","published":"2024-03-02T14:38:03Z","title":"Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions","summary":"  Emotions are a central aspect of communication. Consequently, emotion\nanalysis (EA) is a rapidly growing field in natural language processing (NLP).\nHowever, there is no consensus on scope, direction, or methods. In this paper,\nwe conduct a thorough review of 154 relevant NLP publications from the last\ndecade. Based on this review, we address four different questions: (1) How are\nEA tasks defined in NLP? (2) What are the most prominent emotion frameworks and\nwhich emotions are modeled? (3) Is the subjectivity of emotions considered in\nterms of demographics and cultural factors? and (4) What are the primary NLP\napplications for EA? We take stock of trends in EA and tasks, emotion\nframeworks used, existing datasets, methods, and applications. We then discuss\nfour lacunae: (1) the absence of demographic and cultural aspects does not\naccount for the variation in how emotions are perceived, but instead assumes\nthey are universally experienced in the same manner; (2) the poor fit of\nemotion categories from the two main emotion theories to the task; (3) the lack\nof standardized EA terminology hinders gap identification, comparison, and\nfuture goals; and (4) the absence of interdisciplinary research isolates EA\nfrom insights in other fields. Our work will enable more focused research into\nEA and a more holistic approach to modeling emotions in NLP.\n","authors":["Flor Miriam Plaza-del-Arco","Alba Curry","Amanda Cercas Curry","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2403.01222v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.11044v2","updated":"2024-03-18T10:36:05Z","published":"2023-06-19T16:15:46Z","title":"Frequency effects in Linear Discriminative Learning","summary":"  Word frequency is a strong predictor in most lexical processing tasks. Thus,\nany model of word recognition needs to account for how word frequency effects\narise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)\nmodels lexical processing with linear mappings between words' forms and their\nmeanings. So far, the mappings can either be obtained incrementally via\nerror-driven learning, a computationally expensive process able to capture\nfrequency effects, or in an efficient, but frequency-agnostic solution\nmodelling the theoretical endstate of learning (EL) where all words are learned\noptimally. In this study we show how an efficient, yet frequency-informed\nmapping between form and meaning can be obtained (Frequency-informed learning;\nFIL). We find that FIL well approximates an incremental solution while being\ncomputationally much cheaper. FIL shows a relatively low type- and high\ntoken-accuracy, demonstrating that the model is able to process most word\ntokens encountered by speakers in daily life correctly. We use FIL to model\nreaction times in the Dutch Lexicon Project (Keuleers et al., 2010) and find\nthat FIL predicts well the S-shaped relationship between frequency and the mean\nof reaction times but underestimates the variance of reaction times for low\nfrequency words. FIL is also better able to account for priming effects in an\nauditory lexical decision task in Mandarin Chinese (Lee, 2007), compared to EL.\nFinally, we used ordered data from CHILDES (Brown, 1973; Demuth et al., 2006)\nto compare mappings obtained with FIL and incremental learning. The mappings\nare highly correlated, but with FIL some nuances based on word ordering effects\nare lost. Our results show how frequency effects in a learning model can be\nsimulated efficiently, and raise questions about how to best account for\nlow-frequency words in cognitive models.\n","authors":["Maria Heitmeier","Yu-Ying Chuang","Seth D. Axen","R. Harald Baayen"],"pdf_url":"https://arxiv.org/pdf/2306.11044v2.pdf","comment":"32 pages, 12 figures, 3 tables; revised version"},{"id":"http://arxiv.org/abs/2403.07440v2","updated":"2024-03-18T10:13:05Z","published":"2024-03-12T09:32:25Z","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning","summary":"  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.56% in the DART and\nWebNLG tasks, respectively.\n","authors":["Yao Liang","Yuwei Wang","Yang Li","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.07440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08339v4","updated":"2024-03-18T09:56:09Z","published":"2023-05-15T04:10:13Z","title":"Assessing the potential of AI-assisted pragmatic annotation: The case of\n  apologies","summary":"  Certain forms of linguistic annotation, like part of speech and semantic\ntagging, can be automated with high accuracy. However, manual annotation is\nstill necessary for complex pragmatic and discursive features that lack a\ndirect mapping to lexical forms. This manual process is time-consuming and\nerror-prone, limiting the scalability of function-to-form approaches in corpus\nlinguistics. To address this, our study explores automating pragma-discursive\ncorpus annotation using large language models (LLMs). We compare ChatGPT, the\nBing chatbot, and a human coder in annotating apology components in English\nbased on the local grammar framework. We find that the Bing chatbot\noutperformed ChatGPT, with accuracy approaching that of a human coder. These\nresults suggest that AI can be successfully deployed to aid pragma-discursive\ncorpus annotation, making the process more efficient and scalable. Keywords:\nlinguistic annotation, function-to-form approaches, large language models,\nlocal grammar analysis, Bing chatbot, ChatGPT\n","authors":["Danni Yu","Luyang Li","Hang Su","Matteo Fuoli"],"pdf_url":"https://arxiv.org/pdf/2305.08339v4.pdf","comment":"24 pages, 2 figures, 3 tablels"},{"id":"http://arxiv.org/abs/2403.11621v1","updated":"2024-03-18T09:55:01Z","published":"2024-03-18T09:55:01Z","title":"Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large\n  Language Model","summary":"  Large Language Models (LLMs) are composed of neurons that exhibit various\nbehaviors and roles, which become increasingly diversified as models scale.\nRecent studies have revealed that not all neurons are active across different\ndatasets, and this sparsity correlates positively with the task-specific\nability, leading to advancements in model pruning and training efficiency.\nTraditional fine-tuning methods engage all parameters of LLMs, which is\ncomputationally expensive and may not be necessary. In contrast,\nParameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of\ntrainable parameters, yet they still operate at a relatively macro scale (e.g.,\nlayer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach\nthat refines the granularity of parameter training down to the individual\nneuron, enabling more precise and computationally efficient model updates. The\nexperimental results show that NeFT not only exceeded the performance of\nfull-parameter fine-tuning and PEFT but also provided insights into the\nanalysis of neurons.\n","authors":["Haoyun Xu","Runzhe Zhan","Derek F. Wong","Lidia S. Chao"],"pdf_url":"https://arxiv.org/pdf/2403.11621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03411v2","updated":"2024-03-18T09:47:24Z","published":"2024-01-07T08:03:06Z","title":"GRAM: Global Reasoning for Multi-Page VQA","summary":"  The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document tokens, we propose a tailored bias adaptation\nmethod. For additional computational savings during decoding, we introduce an\noptional compression stage using our compression-transformer\n(C-Former),reducing the encoded sequence length, thereby allowing a tradeoff\nbetween quality and latency. Extensive experiments showcase GRAM's\nstate-of-the-art performance on the benchmarks for multi-page DocVQA,\ndemonstrating the effectiveness of our approach.\n","authors":["Tsachi Blau","Sharon Fogel","Roi Ronen","Alona Golts","Roy Ganz","Elad Ben Avraham","Aviad Aberdam","Shahar Tsiper","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2401.03411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11944v2","updated":"2024-03-18T09:02:03Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Wenhu Chen","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11585v1","updated":"2024-03-18T08:58:47Z","published":"2024-03-18T08:58:47Z","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines","summary":"  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n","authors":["Ekaterina Trofimova","Emil Sataev","Andrey E. Ustyuzhanin"],"pdf_url":"https://arxiv.org/pdf/2403.11585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16701v2","updated":"2024-03-18T08:55:36Z","published":"2023-08-15T17:38:55Z","title":"Is it Really Negative? Evaluating Natural Language Video Localization\n  Performance on Multiple Reliable Videos Pool","summary":"  With the explosion of multimedia content in recent years, Video Corpus Moment\nRetrieval (VCMR), which aims to detect a video moment that matches a given\nnatural language query from multiple videos, has become a critical problem.\nHowever, existing VCMR studies have a significant limitation since they have\nregarded all videos not paired with a specific query as negative, neglecting\nthe possibility of including false negatives when constructing the negative\nvideo set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)\ntask that aims to localize video frames within a massive video set, mitigating\nthe possibility of falsely distinguishing positive and negative videos. For\nthis task, we suggest an automatic dataset construction framework by employing\ntextual and visual semantic matching evaluation methods on the existing video\nmoment search datasets and introduce three MVMR datasets. To solve MVMR task,\nwe further propose a strong method, CroCs, which employs cross-directional\ncontrastive learning that selectively identifies the reliable and informative\nnegatives, enhancing the robustness of a model on MVMR task. Experimental\nresults on the introduced datasets reveal that existing video moment search\nmodels are easily distracted by negative video frames, whereas our model shows\nsignificant performance.\n","authors":["Nakyeong Yang","Minsung Kim","Seunghyun Yoon","Joongbo Shin","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2309.16701v2.pdf","comment":"15 pages, 10 figures"}]},"2024-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2403.13801v1","updated":"2024-03-20T17:58:12Z","published":"2024-03-20T17:58:12Z","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied\n  Control with LLMs","summary":"  We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.\n","authors":["Yusuke Mikami","Andrew Melnik","Jun Miura","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2403.13801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.07923v4","updated":"2024-03-20T17:55:48Z","published":"2023-10-11T22:35:18Z","title":"The Expressive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps, assuming a\nslight generalization to standard pre-norm, adds a clear new ability (under\nstandard complexity conjectures): recognizing all regular languages. Our\nresults also imply that linear steps keep transformer decoders within\ncontext-sensitive languages, and polynomial steps with generalized pre-norm\nmake them recognize exactly the class of polynomial-time solvable problems --\nthe first exact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v4.pdf","comment":"9-page preprint. Updated March 20 after ICLR acceptance"},{"id":"http://arxiv.org/abs/2403.13799v1","updated":"2024-03-20T17:55:35Z","published":"2024-03-20T17:55:35Z","title":"Reverse Training to Nurse the Reversal Curse","summary":"  Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.\n","authors":["Olga Golovneva","Zeyuan Allen-Zhu","Jason Weston","Sainbayar Sukhbaatar"],"pdf_url":"https://arxiv.org/pdf/2403.13799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13786v1","updated":"2024-03-20T17:47:49Z","published":"2024-03-20T17:47:49Z","title":"Chain-of-Interaction: Enhancing Large Language Models for Psychiatric\n  Behavior Understanding by Dyadic Contexts","summary":"  Automatic coding patient behaviors is essential to support decision making\nfor psychotherapists during the motivational interviewing (MI), a collaborative\ncommunication intervention approach to address psychiatric issues, such as\nalcohol and drug addiction. While the behavior coding task has rapidly adapted\nmachine learning to predict patient states during the MI sessions, lacking of\ndomain-specific knowledge and overlooking patient-therapist interactions are\nmajor challenges in developing and deploying those models in real practice. To\nencounter those challenges, we introduce the Chain-of-Interaction (CoI)\nprompting method aiming to contextualize large language models (LLMs) for\npsychiatric decision support by the dyadic interactions. The CoI prompting\napproach systematically breaks down the coding task into three key reasoning\nsteps, extract patient engagement, learn therapist question strategies, and\nintegrates dyadic interactions between patients and therapists. This approach\nenables large language models to leverage the coding scheme, patient state, and\ndomain knowledge for patient behavioral coding. Experiments on real-world\ndatasets can prove the effectiveness and flexibility of our prompting method\nwith multiple state-of-the-art LLMs over existing prompting baselines. We have\nconducted extensive ablation analysis and demonstrate the critical role of\ndyadic interactions in applying LLMs for psychotherapy behavior understanding.\n","authors":["Guangzeng Han","Weisi Liu","Xiaolei Huang","Brian Borsari"],"pdf_url":"https://arxiv.org/pdf/2403.13786v1.pdf","comment":"Accepted to IEEE ICHI 2024"},{"id":"http://arxiv.org/abs/2403.13780v1","updated":"2024-03-20T17:42:08Z","published":"2024-03-20T17:42:08Z","title":"Information-Theoretic Distillation for Reference-less Summarization","summary":"  The current winning recipe for automatic summarization is using proprietary\nlarge-scale language models (LLMs) such as ChatGPT as is, or imitation learning\nfrom them as teacher models. While increasingly ubiquitous dependence on such\nlarge-scale language models is convenient, there remains an important question\nof whether small-scale models could have achieved competitive results, if we\nwere to seek an alternative learning method -- that allows for a more\ncost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a\nnovel framework to distill a powerful summarizer based on the\ninformation-theoretic objective for summarization, without relying on either\nthe LLM's capability or human-written references. To achieve this, we first\npropose a novel formulation of the desiderata of summarization (saliency,\nfaithfulness and brevity) through the lens of mutual information between the\noriginal document and the summary. Based on this formulation, we start off from\nPythia-2.8B as the teacher model, which is not yet capable of summarization,\nthen self-train the model to optimize for the information-centric measures of\nideal summaries. Distilling from the improved teacher, we arrive at a compact\nbut powerful summarizer with only 568M parameters that performs competitively\nagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensive\nanalysis demonstrates that our approach outperforms in-domain supervised models\nin human evaluation, let alone state-of-the-art unsupervised methods, and wins\nover ChatGPT in controllable summarization.\n","authors":["Jaehun Jung","Ximing Lu","Liwei Jiang","Faeze Brahman","Peter West","Pang Wei Koh","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2403.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11085v2","updated":"2024-03-20T17:35:15Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14456v4","updated":"2024-03-20T17:16:37Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13754v1","updated":"2024-03-20T17:01:56Z","published":"2024-03-20T17:01:56Z","title":"Different Tokenization Schemes Lead to Comparable Performance in Spanish\n  Number Agreement","summary":"  The relationship between language model tokenization and performance is an\nopen area of research. Here, we investigate how different tokenization schemes\nimpact number agreement in Spanish plurals. We find that\nmorphologically-aligned tokenization performs similarly to other tokenization\nschemes, even when induced artificially for words that would not be tokenized\nthat way during training. We then present exploratory analyses demonstrating\nthat language model embeddings for different plural tokenizations have similar\ndistributions along the embedding space axis that maximally distinguishes\nsingular and plural nouns. Our results suggest that morphologically-aligned\ntokenization is a viable tokenization approach, and existing models already\ngeneralize some morphological patterns to new items. However, our results\nindicate that morphological tokenization is not strictly required for\nperformance.\n","authors":["Catherine Arnett","Pamela D. Rivière","Tyler A. Chang","Sean Trott"],"pdf_url":"https://arxiv.org/pdf/2403.13754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02706v5","updated":"2024-03-20T16:56:48Z","published":"2023-09-06T04:38:16Z","title":"HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models","summary":"  Large language models (LLMs) trained on massive corpora demonstrate\nimpressive capabilities in a wide range of tasks. While there are ongoing\nefforts to adapt these models to languages beyond English, the attention given\nto their evaluation methodologies remains limited. Current multilingual\nbenchmarks often rely on back translations or re-implementations of English\ntests, limiting their capacity to capture unique cultural and linguistic\nnuances. To bridge this gap for the Korean language, we introduce the HAE-RAE\nBench, a dataset curated to challenge models lacking Korean cultural and\ncontextual depth. The dataset encompasses six downstream tasks across four\ndomains: vocabulary, history, general knowledge, and reading comprehension.\nUnlike traditional evaluation suites focused on token and sequence\nclassification or mathematical and logical reasoning, the HAE-RAE Bench\nemphasizes a model's aptitude for recalling Korean-specific knowledge and\ncultural contexts. Comparative analysis with prior Korean benchmarks indicates\nthat the HAE-RAE Bench presents a greater challenge to non-Korean models by\ndisturbing abilities and knowledge learned from English being transferred.\n","authors":["Guijin Son","Hanwool Lee","Suwan Kim","Huiseo Kim","Jaecheol Lee","Je Won Yeom","Jihyu Jung","Jung Woo Kim","Songseong Kim"],"pdf_url":"https://arxiv.org/pdf/2309.02706v5.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13737v1","updated":"2024-03-20T16:43:42Z","published":"2024-03-20T16:43:42Z","title":"EthioLLM: Multilingual Large Language Models for Ethiopian Languages\n  with Task Evaluation","summary":"  Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.\n","authors":["Atnafu Lambebo Tonja","Israel Abebe Azime","Tadesse Destaw Belay","Mesay Gemeda Yigezu","Moges Ahmed Mehamed","Abinew Ali Ayele","Ebrahim Chekol Jibril","Michael Melese Woldeyohannis","Olga Kolesnikova","Philipp Slusallek","Dietrich Klakow","Shengwu Xiong","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2403.13737v1.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2308.08739v2","updated":"2024-03-20T16:41:11Z","published":"2023-08-17T02:26:30Z","title":"Enhancing Phrase Representation by Information Bottleneck Guided Text\n  Diffusion Process for Keyphrase Extraction","summary":"  Keyphrase extraction (KPE) is an important task in Natural Language\nProcessing for many scenarios, which aims to extract keyphrases that are\npresent in a given document. Many existing supervised methods treat KPE as\nsequential labeling, span-level classification, or generative tasks. However,\nthese methods lack the ability to utilize keyphrase information, which may\nresult in biased results. In this study, we propose Diff-KPE, which leverages\nthe supervised Variational Information Bottleneck (VIB) to guide the text\ndiffusion process for generating enhanced keyphrase representations. Diff-KPE\nfirst generates the desired keyphrase embeddings conditioned on the entire\ndocument and then injects the generated keyphrase embeddings into each phrase\nrepresentation. A ranking network and VIB are then optimized together with rank\nloss and classification loss, respectively. This design of Diff-KPE allows us\nto rank each candidate phrase by utilizing both the information of keyphrases\nand the document. Experiments show that Diff-KPE outperforms existing KPE\nmethods on a large open domain keyphrase extraction benchmark, OpenKP, and a\nscientific domain dataset, KP20K.\n","authors":["Yuanzhen Luo","Qingyu Zhou","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.08739v2.pdf","comment":"10 pages, 2 figures, accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.12963v3","updated":"2024-03-20T16:36:06Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present AutoMix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to AutoMix is a\nfew-shot self-verification mechanism, which estimates the reliability of its\nown outputs without requiring training. Given that verifications can be noisy,\nwe employ a meta-verifier in AutoMix to refine the accuracy of these\nassessments. Our experiments using LLAMA2-13B and GPT-4, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 86%.\nOur code and data are available at https://github.com/automix-llm/automix.\n","authors":["Aman Madaan","Pranjal Aggarwal","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay"," Mausam","Manaal Faruqui"],"pdf_url":"https://arxiv.org/pdf/2310.12963v3.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2309.07915v3","updated":"2024-03-20T16:17:02Z","published":"2023-09-14T17:59:17Z","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning","summary":"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Xiaojian Ma","Kaikai An","Liang Chen","Zixuan Liu","Sheng Wang","Wenjuan Han","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2309.07915v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2306.17447v3","updated":"2024-03-20T15:53:37Z","published":"2023-06-30T07:44:49Z","title":"Correct Like Humans: Progressive Learning Framework for Chinese Text\n  Error Correction","summary":"  Chinese Text Error Correction (CTEC) aims to detect and correct errors in the\ninput text, which benefits human daily life and various downstream tasks.\nRecent approaches mainly employ Pre-trained Language Models (PLMs) to resolve\nCTEC. Although PLMs have achieved remarkable success in CTEC, we argue that\nprevious studies still overlook the importance of human thinking patterns. To\nenhance the development of PLMs for CTEC, inspired by humans' daily\nerror-correcting behavior, we propose a novel model-agnostic progressive\nlearning framework, named ProTEC, which guides PLMs-based CTEC models to learn\nto correct like humans. During the training process, ProTEC guides the model to\nlearn text error correction by incorporating these sub-tasks into a progressive\nparadigm. During the inference process, the model completes these sub-tasks in\nturn to generate the correction results. Extensive experiments and detailed\nanalyses demonstrate the effectiveness and efficiency of our proposed\nmodel-agnostic ProTEC framework.\n","authors":["Yinghui Li","Shirong Ma","Shaoshen Chen","Haojing Huang","Shulin Huang","Yangning Li","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2306.17447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09098v2","updated":"2024-03-20T15:41:07Z","published":"2023-05-16T01:51:22Z","title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression","summary":"  Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions. The code\nis available at https://github.com/wutaiqiang/WID-NAACL2024.\n","authors":["Taiqiang Wu","Cheng Hou","Shanshan Lao","Jiayi Li","Ngai Wong","Zhe Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.09098v2.pdf","comment":"9 pages, 4 figures, NAACL2024 findings"},{"id":"http://arxiv.org/abs/2403.13681v1","updated":"2024-03-20T15:39:54Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned\n  Language Model for Indian Legal Case Documents","summary":"  In this paper, we present PARAMANU-AYN, a language model based exclusively on\ncase documents of the Supreme Court of India, the Constitution of India, and\nthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model is\npretrained from scratch at a context size of 8192. We evaluated our pretrained\nlegal model on perplexity metrics. We also instruction-tuned our pretrained\nmodel on a set of 10,763 instructions covering various legal tasks such as\nlegal reasoning, judgement explanation, legal clause generation, legal\ndrafting, legal contract drafting, case summarization, constitutional\nquestion-answering, etc. We also evaluated the responses of prompts for\ninstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,\nand legal reasoning metrics in a scale of 10. Our model can be run on CPU and\nachieved 42.46 tokens/sec CPU inference speed. We found that our models,\ndespite not being pretrained on legal books, various legal contracts, and legal\ndocuments, were able to learn the domain knowledge required for drafting\nvarious legal contracts and legal clauses, and generalize to draft legal\ncontracts and legal clauses with limited instruction tuning. Hence, we conclude\nthat for a strong domain-specialized generative language model (such as legal),\nvery large amounts of data are not required to develop models from scratch. We\nbelieve that this work is the first attempt to make a dedicated generative\nlegal language model from scratch for Indian Supreme Court jurisdiction or in\nlegal NLP overall. We plan to release our Paramanu-Ayn model at\nhttps://www.bharatgpts.com.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13679v1","updated":"2024-03-20T15:38:36Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13666v1","updated":"2024-03-20T15:20:30Z","published":"2024-03-20T15:20:30Z","title":"Grounding Spatial Relations in Text-Only Language Models","summary":"  This paper shows that text-only Language Models (LM) can learn to ground\nspatial relations like \"left of\" or \"below\" if they are provided with explicit\nlocation information of objects and they are properly trained to leverage those\nlocations. We perform experiments on a verbalized version of the Visual Spatial\nReasoning (VSR) dataset, where images are coupled with textual statements which\ncontain real or fake spatial relations between two objects of the image. We\nverbalize the images using an off-the-shelf object detector, adding location\ntokens to every object label to represent their bounding boxes in textual form.\nGiven the small size of VSR, we do not observe any improvement when using\nlocations, but pretraining the LM over a synthetic dataset automatically\nderived by us improves results significantly when using location tokens. We\nthus show that locations allow LMs to ground spatial relations, with our\ntext-only LMs outperforming Vision-and-Language Models and setting the new\nstate-of-the-art for the VSR dataset. Our analysis show that our text-only LMs\ncan generalize beyond the relations seen in the synthetic dataset to some\nextent, learning also more useful information than that encoded in the spatial\nrules we used to create the synthetic dataset itself.\n","authors":["Gorka Azkune","Ander Salaberria","Eneko Agirre"],"pdf_url":"https://arxiv.org/pdf/2403.13666v1.pdf","comment":"Accepted in Neural Networks"},{"id":"http://arxiv.org/abs/2403.13638v1","updated":"2024-03-20T14:41:01Z","published":"2024-03-20T14:41:01Z","title":"Do Not Worry if You Do Not Have Data: Building Pretrained Language\n  Models Using Translationese","summary":"  In this paper, we explore the utility of \\textit{Translationese} as synthetic\ndata created using machine translation for pre-training language models (LMs).\nPre-training requires vast amounts of monolingual data, which is mostly\nunavailable for languages other than English. Recently, there has been a\ngrowing interest in using synthetic data to address this data scarcity. We take\nthe case of English and Indic languages and translate web-crawled monolingual\ndocuments (clean) into the target language. Then, we train language models\ncontaining 28M and 85M parameters on this translationese data (synthetic). We\nshow that their performance on downstream natural language understanding and\ngenerative tasks is only 3.56\\% poorer on NLU tasks and 1.51\\% on NLG tasks\nthan LMs pre-trained on clean data. Further, we propose the use of lightweight\n\\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently\nwhich significantly improves the performance of our models. We also find that\nLMs trained on synthetic data strongly benefit from extended pretraining on a\ntiny fraction (10\\%) of clean data. We release the data we collected and\ncreated as a part of this work, \\textit{IndicMonoDoc}, the largest collection\nof monolingual document-level corpora, which we hope will help bridge the gap\nbetween English and non-English performance for large language models.\n","authors":["Meet Doshi","Raj Dabre","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2403.13638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09002v3","updated":"2024-03-20T14:08:39Z","published":"2024-01-17T06:42:44Z","title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models","summary":"  In our research, we pioneer a novel approach to evaluate the effectiveness of\njailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,\ndiverging from traditional robustness-focused binary evaluations. Our study\nintroduces two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework, using a scoring range from 0 to 1,\noffers a unique perspective, enabling a more comprehensive and nuanced\nevaluation of attack effectiveness and empowering attackers to refine their\nattack prompts with greater understanding. Furthermore, we have developed a\ncomprehensive ground truth dataset specifically tailored for jailbreak tasks.\nThis dataset not only serves as a crucial benchmark for our current study but\nalso establishes a foundational resource for future research, enabling\nconsistent and comparative analyses in this evolving field. Upon meticulous\ncomparison with traditional evaluation methods, we discovered that our\nevaluation aligns with the baseline's trend while offering a more profound and\ndetailed assessment. We believe that by accurately evaluating the effectiveness\nof attack prompts in the Jailbreak task, our work lays a solid foundation for\nassessing a wider array of similar or even more complex tasks in the realm of\nprompt injection, potentially revolutionizing this field.\n","authors":["Dong shu","Mingyu Jin","Suiyuan Zhu","Beichen Wang","Zihao Zhou","Chong Zhang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09002v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13592v1","updated":"2024-03-20T13:42:57Z","published":"2024-03-20T13:42:57Z","title":"Llama meets EU: Investigating the European Political Spectrum through\n  the Lens of LLMs","summary":"  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n","authors":["Ilias Chalkidis","Stephanie Brandl"],"pdf_url":"https://arxiv.org/pdf/2403.13592v1.pdf","comment":"accepted to NAACL 2024 as a short paper"},{"id":"http://arxiv.org/abs/2403.13590v1","updated":"2024-03-20T13:38:07Z","published":"2024-03-20T13:38:07Z","title":"Teacher-Student Training for Debiasing: General Permutation Debiasing\n  for Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive zero-shot\ncapabilities and versatility in NLP tasks, however they sometimes fail to\nmaintain crucial invariances for specific tasks. One example is permutation\nsensitivity, where LLMs' outputs may significantly vary depending on the order\nof the input options. While debiasing techniques can mitigate these issues, and\nyield better performance and reliability, they often come with a high\ncomputational cost at inference. This paper addresses this inefficiency at\ninference time. The aim is to distill the capabilities of a computationally\nintensive, debiased, teacher model into a more compact student model. We\nexplore two variants of student models: one based on pure distillation, and the\nother on an error-correction approach for more complex tasks, where the student\ncorrects a single biased decision from the teacher to achieve a debiased\noutput. Our approach is general and can be applied to both black-box and\nwhite-box LLMs. Furthermore, we demonstrate that our compact, encoder-only\nstudent models can outperform their larger, biased teacher counterparts,\nachieving better results with significantly fewer parameters.\n","authors":["Adian Liusie","Yassir Fathullah","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2403.13590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13588v1","updated":"2024-03-20T13:37:00Z","published":"2024-03-20T13:37:00Z","title":"Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language\n  Models","summary":"  As Pre-trained Language Models (PLMs), a popular approach for code\nintelligence, continue to grow in size, the computational cost of their usage\nhas become prohibitively expensive. Prompt learning, a recent development in\nthe field of natural language processing, emerges as a potential solution to\naddress this challenge. In this paper, we investigate the effectiveness of\nprompt learning in code intelligence tasks. We unveil its reliance on manually\ndesigned prompts, which often require significant human effort and expertise.\nMoreover, we discover existing automatic prompt design methods are very limited\nto code intelligence tasks due to factors including gradient dependence, high\ncomputational demands, and limited applicability. To effectively address both\nissues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate\ngenetic algorithm to automatically design prompts. With GenAP, non-experts can\neffortlessly generate superior prompts compared to meticulously manual-designed\nones. GenAP operates without the need for gradients or additional computational\ncosts, rendering it gradient-free and cost-effective. Moreover, GenAP supports\nboth understanding and generation types of code intelligence tasks, exhibiting\ngreat applicability. We conduct GenAP on three popular code intelligence PLMs\nwith three canonical code intelligence tasks including defect prediction, code\nsummarization, and code translation. The results suggest that GenAP can\neffectively automate the process of designing prompts. Specifically, GenAP\noutperforms all other methods across all three tasks (e.g., improving accuracy\nby an average of 2.13% for defect prediction). To the best of our knowledge,\nGenAP is the first work to automatically design prompts for code intelligence\nPLMs.\n","authors":["Chengzhe Feng","Yanan Sun","Ke Li","Pan Zhou","Jiancheng Lv","Aojun Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13583v1","updated":"2024-03-20T13:33:55Z","published":"2024-03-20T13:33:55Z","title":"CONLINE: Complex Code Generation and Refinement with Online Searching\n  and Correctness Testing","summary":"  Large Language Models (LLMs) have revolutionized code generation ability by\nconverting natural language descriptions into executable code. However,\ngenerating complex code within real-world scenarios remains challenging due to\nintricate structures, subtle bugs, understanding of advanced data types, and\nlack of supplementary contents. To address these challenges, we introduce the\nCONLINE framework, which enhances code generation by incorporating planned\nonline searches for information retrieval and automated correctness testing for\niterative refinement. CONLINE also serializes the complex inputs and outputs to\nimprove comprehension and generate test case to ensure the framework's\nadaptability for real-world applications. CONLINE is validated through rigorous\nexperiments on the DS-1000 and ClassEval datasets. It shows that CONLINE\nsubstantially improves the quality of complex code generation, highlighting its\npotential to enhance the practicality and reliability of LLMs in generating\nintricate code.\n","authors":["Xinyi He","Jiaru Zou","Yun Lin","Mengyu Zhou","Shi Han","Zejian Yuan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08015v2","updated":"2024-03-20T13:33:19Z","published":"2024-02-12T19:25:11Z","title":"Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and\n  Generative Datasets","summary":"  Large language models (LLMs) have received a lot of attention in natural\nlanguage processing (NLP) research because of their exceptional performance in\nunderstanding and generating human languages. However, low-resource languages\nare left behind due to the unavailability of resources. In this work, we focus\non enhancing the LLaMA-2-Amharic model by integrating task-specific and\ngenerative datasets to improve language model performance for Amharic. We\ncompile an Amharic instruction fine-tuning dataset and fine-tuned\nLLaMA-2-Amharic model. The fine-tuned model shows promising results in\ndifferent NLP tasks. We open-source our dataset creation pipeline, instruction\ndatasets, trained models, and evaluation outputs to promote language-specific\nstudies on these models.\n","authors":["Israel Abebe Azime","Atnafu Lambebo Tonja","Tadesse Destaw Belay","Mitiku Yohannes Fuge","Aman Kassahun Wassie","Eyasu Shiferaw Jada","Yonas Chanie","Walelign Tewabe Sewunetie","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2402.08015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13578v1","updated":"2024-03-20T13:24:41Z","published":"2024-03-20T13:24:41Z","title":"Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for\n  Counselor Reflection Generation","summary":"  In this paper, we study the problem of multi-reward reinforcement learning to\njointly optimize for multiple text qualities for natural language generation.\nWe focus on the task of counselor reflection generation, where we optimize the\ngenerators to simultaneously improve the fluency, coherence, and reflection\nquality of generated counselor responses. We introduce two novel bandit\nmethods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining\nrewards into a single value and optimizing them simultaneously. Specifically,\nwe employ non-contextual and contextual multi-arm bandits to dynamically adjust\nmultiple reward weights during training. Through automatic and manual\nevaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,\noutperform existing naive and bandit baselines, showcasing their potential for\nenhancing language models.\n","authors":["Do June Min","Veronica Perez-Rosas","Kenneth Resnicow","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.13578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18248v3","updated":"2024-03-20T13:12:48Z","published":"2023-05-29T17:12:03Z","title":"Do Language Models Know When They're Hallucinating References?","summary":"  State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.\n","authors":["Ayush Agrawal","Mirac Suzgun","Lester Mackey","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2305.18248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13560v1","updated":"2024-03-20T12:52:38Z","published":"2024-03-20T12:52:38Z","title":"eRST: A Signaled Graph Theory of Discourse Relations and Organization","summary":"  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, nonprojective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n","authors":["Amir Zeldes","Tatsuya Aoyama","Yang Janet Liu","Siyao Peng","Debopam Das","Luke Gessler"],"pdf_url":"https://arxiv.org/pdf/2403.13560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05113v3","updated":"2024-03-20T11:56:52Z","published":"2023-07-11T08:45:46Z","title":"Piecing Together Clues: A Benchmark for Evaluating the Detective Skills\n  of Large Language Models","summary":"  Detectives frequently engage in information detection and reasoning\nsimultaneously when making decisions across various cases, especially when\nconfronted with a vast amount of information. With the rapid development of\nlarge language models~(LLMs), evaluating how these models identify key\ninformation and reason to solve questions becomes increasingly relevant. We\nintroduces the DetectBench, a reading comprehension dataset designed to assess\na model's ability to jointly ability in key information detection and multi-hop\nreasoning when facing complex and implicit information. The DetectBench\ncomprises 3,928 questions, each paired with a paragraph averaging 190 tokens in\nlength. To enhance model's detective skills, we propose the Detective Thinking\nFramework. These methods encourage models to identify all possible clues within\nthe context before reasoning. Our experiments reveal that existing models\nperform poorly in both information detection and multi-hop reasoning. However,\nthe Detective Thinking Framework approach alleviates this issue.\n","authors":["Zhouhong Gu","Lin Zhang","Jiangjie Chen","Haoning Ye","Xiaoxuan Zhu","Zihan Li","Zheyu Ye","Yan Gao","Yao Hu","Yanghua Xiao","Hongwei Feng"],"pdf_url":"https://arxiv.org/pdf/2307.05113v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11117v5","updated":"2024-03-20T11:35:04Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: emotional inertia and contagion-driven dependency modeling\n  for emotion recognition in conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. In this paper, we propose an emotional\ninertia and contagion-driven dependency modeling approach (EmotionIC) for ERC\ntask. Our EmotionIC consists of three main components, i.e., Identity Masked\nMulti-Head Attention (IMMHA), Dialogue-based Gated Recurrent Unit (DiaGRU), and\nSkip-chain Conditional Random Field (SkipCRF). Compared to previous ERC models,\nEmotionIC can model a conversation more thoroughly at both the\nfeature-extraction and classification levels. The proposed model attempts to\nintegrate the advantages of attention- and recurrence-based methods at the\nfeature-extraction level. Specifically, IMMHA is applied to capture\nidentity-based global contextual dependencies, while DiaGRU is utilized to\nextract speaker- and temporal-aware local contextual information. At the\nclassification level, SkipCRF can explicitly mine complex emotional flows from\nhigher-order neighboring utterances in the conversation. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Yingjian Liu","Jiang Li","Xiaoping Wang","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.11117v5.pdf","comment":"Accepted by SCIENCE CHINA Information Sciences (SCIS)"},{"id":"http://arxiv.org/abs/2403.13514v1","updated":"2024-03-20T11:30:45Z","published":"2024-03-20T11:30:45Z","title":"How Gender Interacts with Political Values: A Case Study on Czech BERT\n  Models","summary":"  Neural language models, which reach state-of-the-art results on most natural\nlanguage processing tasks, are trained on large text corpora that inevitably\ncontain value-burdened content and often capture undesirable biases, which the\nmodels reflect. This case study focuses on the political biases of pre-trained\nencoders in Czech and compares them with a representative value survey. Because\nCzech is a gendered language, we also measure how the grammatical gender\ncoincides with responses to men and women in the survey. We introduce a novel\nmethod for measuring the model's perceived political values. We find that the\nmodels do not assign statement probability following value-driven reasoning,\nand there is no systematic difference between feminine and masculine sentences.\nWe conclude that BERT-sized models do not manifest systematic alignment with\npolitical values and that the biases observed in the models are rather due to\nsuperficial imitation of training data patterns than systematic value beliefs\nencoded in the models.\n","authors":["Adnan Al Ali","Jindřich Libovický"],"pdf_url":"https://arxiv.org/pdf/2403.13514v1.pdf","comment":"11 pages, 2 figures; LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13513v1","updated":"2024-03-20T11:27:20Z","published":"2024-03-20T11:27:20Z","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects\n  in Large Multimodal Models","summary":"  This paper presents a way of enhancing the reliability of Large Multimodal\nModels (LMMs) in addressing hallucination effects, where models generate\nincorrect or unrelated responses. Without additional instruction tuning\nparadigm, we introduce Counterfactual Inception, a novel method that implants\ncounterfactual thoughts into LMMs using carefully chosen, misaligned\ncounterfactual keywords. This method is grounded in the concept of\ncounterfactual thinking, a cognitive process where humans consider alternative\nrealities and outcomes. By applying this human-like reasoning mechanism to\nLMMs, we aim to reduce hallucination effects and improve the models'\ntrustworthiness. We also propose Dual-modality Verification Process (DVP), a\nrigorous framework for selecting optimal counterfactual keywords to trigger\ncounterfactual thinking into LMMs, concurrently considering visual and\nlinguistic context. Our extensive experiments across various LMMs, including\nboth open-source and proprietary models, corroborate that our method\nsignificantly mitigates hallucination phenomena across different datasets.\n","authors":["Junho Kim","Yeon Ju Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.13513v1.pdf","comment":"under review, code available:\n  https://github.com/IVY-LVLM/Counterfactual-Inception"},{"id":"http://arxiv.org/abs/2310.20204v3","updated":"2024-03-20T10:52:03Z","published":"2023-10-31T06:04:18Z","title":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History","summary":"  Developing clinical prediction models (e.g., mortality prediction) based on\nelectronic health records (EHRs) typically relies on expert opinion for feature\nselection and adjusting observation window size. This burdens experts and\ncreates a bottleneck in the development process. We propose Retrieval-Enhanced\nMedical prediction model (REMed) to address such challenges. REMed can\nessentially evaluate an unlimited number of clinical events, select the\nrelevant ones, and make predictions. This approach effectively eliminates the\nneed for manual feature selection and enables an unrestricted observation\nwindow. We verified these properties through experiments on 27 clinical tasks\nand two independent cohorts from publicly available EHR datasets, where REMed\noutperformed other contemporary architectures that aim to handle as many events\nas possible. Notably, we found that the preferences of REMed align closely with\nthose of medical experts. We expect our approach to significantly expedite the\ndevelopment of EHR prediction models by minimizing clinicians' need for manual\ninvolvement.\n","authors":["Junu Kim","Chaeeun Shim","Bosco Seong Kyu Yang","Chami Im","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2310.20204v3.pdf","comment":"The source codes corresponding to this paper are available at:\n  https://github.com/starmpcc/REMed"},{"id":"http://arxiv.org/abs/2403.13485v1","updated":"2024-03-20T10:40:01Z","published":"2024-03-20T10:40:01Z","title":"An Entropy-based Text Watermarking Detection Method","summary":"  Currently, text watermarking algorithms for large language models (LLMs) can\nembed hidden features to texts generated by LLMs to facilitate subsequent\ndetection, thus alleviating the problem of misuse of LLMs. Although the current\ntext watermarking algorithms perform well in most high-entropy scenarios, its\nperformance in low-entropy scenarios still needs to be improved. In this work,\nwe proposed that the influence of token entropy should be fully considered in\nthe watermark detection process, that is, the weight of each token should be\nadjusted according to its entropy during watermark detection, rather than\nsetting the weight of all tokens to the same value as in previous methods.\nSpecifically, we proposed an Entropy-based Watermark Detection (EWD) that gives\nhigher-entropy tokens higher weights during watermark detection, so as to\nbetter reflect the degree of watermarking. Furthermore, the proposed detection\nprocess is training-free and fully automated. %In actual detection, we use a\nproxy-LLM to calculate the entropy of each token, without the need to use the\noriginal LLM. In the experiment, we found that our method can achieve better\ndetection performance in low-entropy scenarios, and our method is also general\nand can be applied to texts with different entropy distributions. Our code and\ndata will be available online.\n","authors":["Yijian Lu","Aiwei Liu","Dianzhi Yu","Jingjing Li","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2403.13485v1.pdf","comment":"8 pages, 5 figures, submitted to ARR Feb 2024"},{"id":"http://arxiv.org/abs/2309.09783v2","updated":"2024-03-20T10:33:24Z","published":"2023-09-18T14:01:06Z","title":"The ParlaSent Multilingual Training Dataset for Sentiment Identification\n  in Parliamentary Proceedings","summary":"  The paper presents a new training dataset of sentences in 7 languages,\nmanually annotated for sentiment, which are used in a series of experiments\nfocused on training a robust sentiment identifier for parliamentary\nproceedings. The paper additionally introduces the first domain-specific\nmultilingual transformer language model for political science applications,\nwhich was additionally pre-trained on 1.72 billion words from parliamentary\nproceedings of 27 European parliaments. We present experiments demonstrating\nhow the additional pre-training on parliamentary data can significantly improve\nthe model downstream performance, in our case, sentiment identification in\nparliamentary proceedings. We further show that our multilingual model performs\nvery well on languages not seen during fine-tuning, and that additional\nfine-tuning data from other languages significantly improves the target\nparliament's results. The paper makes an important contribution to multiple\ndisciplines inside the social sciences, and bridges them with computer science\nand computational linguistics. Lastly, the resulting fine-tuned language model\nsets up a more robust approach to sentiment analysis of political texts across\nlanguages, which allows scholars to study political sentiment from a\ncomparative perspective using standardized tools and techniques.\n","authors":["Michal Mochtak","Peter Rupnik","Nikola Ljubešić"],"pdf_url":"https://arxiv.org/pdf/2309.09783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06832v2","updated":"2024-03-20T10:02:54Z","published":"2024-03-11T15:48:43Z","title":"The Power of Noise: Toward a Unified Multi-modal Knowledge Graph\n  Representation Framework","summary":"  The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.\n","authors":["Zhuo Chen","Yin Fang","Yichi Zhang","Lingbing Guo","Jiaoyan Chen","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06832v2.pdf","comment":"Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at\n  https://github.com/zjukg/SNAG"},{"id":"http://arxiv.org/abs/2403.02889v2","updated":"2024-03-20T09:53:17Z","published":"2024-03-05T11:50:01Z","title":"In Search of Truth: An Interrogation Approach to Hallucination Detection","summary":"  Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.\n","authors":["Yakir Yehuda","Itzik Malkiel","Oren Barkan","Jonathan Weill","Royi Ronen","Noam Koenigstein"],"pdf_url":"https://arxiv.org/pdf/2403.02889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13447v1","updated":"2024-03-20T09:42:43Z","published":"2024-03-20T09:42:43Z","title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models","summary":"  Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.\n","authors":["Wenqiao Zhang","Tianwei Lin","Jiang Liu","Fangxun Shu","Haoyuan Li","Lei Zhang","He Wanggui","Hao Zhou","Zheqi Lv","Hao Jiang","Juncheng Li","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.13447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07726v2","updated":"2024-03-20T09:36:13Z","published":"2024-03-12T15:06:22Z","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes","summary":"  This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.\n","authors":["Timothee Mickus","Elaine Zosa","Raúl Vázquez","Teemu Vahtola","Jörg Tiedemann","Vincent Segonne","Alessandro Raganato","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2403.07726v2.pdf","comment":"SemEval 2024 shared task. Pre-review version"},{"id":"http://arxiv.org/abs/2403.13433v1","updated":"2024-03-20T09:21:32Z","published":"2024-03-20T09:21:32Z","title":"Agent Group Chat: An Interactive Group Chat Simulacra For Better\n  Eliciting Collective Emergent Behavior","summary":"  To investigate the role of language in human collective behaviors, we\ndeveloped the Agent Group Chat simulation to simulate linguistic interactions\namong multi-agent in different settings. Agents are asked to free chat in this\nsimulation for their own purposes based on their character setting, aiming to\nsee agents exhibit emergent behaviours that are both unforeseen and\nsignificant. Four narrative scenarios, Inheritance Disputes, Law Court Debates,\nPhilosophical Discourses, Movie Casting Contention, are integrated into Agent\nGroup Chat to evaluate its support for diverse storylines. By configuring\nspecific environmental settings within Agent Group Chat, we are able to assess\nwhether agents exhibit behaviors that align with human expectations. We\nevaluate the disorder within the environment by computing the n-gram Shannon\nentropy of all the content speak by characters. Our findings reveal that under\nthe premise of agents possessing substantial alignment with human expectations,\nfacilitating more extensive information exchange within the simulation ensures\ngreater orderliness amidst diversity, which leads to the emergence of more\nunexpected and meaningful emergent behaviors. The code is open source in\nhttps://github.com/MikeGu721/AgentGroup, and online platform will be open soon.\n","authors":["Zhouhong Gu","Xiaoxuan Zhu","Haoran Guo","Lin Zhang","Yin Cai","Hao Shen","Jiangjie Chen","Zheyu Ye","Yifei Dai","Yan Gao","Yao Hu","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.13433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14166v3","updated":"2024-03-20T08:52:42Z","published":"2024-01-25T13:20:47Z","title":"BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on\n  Few-shot Inference via Debiased Domain Abstraction","summary":"  As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.\n","authors":["Jiangmeng Li","Fei Song","Yifan Jin","Wenwen Qiang","Changwen Zheng","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.14166v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2305.13888v2","updated":"2024-03-20T08:37:42Z","published":"2023-05-23T10:11:56Z","title":"PaD: Program-aided Distillation Can Teach Small Models Reasoning Better\n  than Chain-of-thought Fine-tuning","summary":"  While large language models (LLMs) excel in various natural language\nprocessing tasks, their huge size and the inaccessibility of parameters present\nchallenges for practical deployment. Previous studies try to distill\ntask-specific ability from LLMs to smaller models, using data synthesis and\nchain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains\nfaulty reasoning, which deteriorates the quality of distillation, especially in\nreasoning capabilities. In this work, we propose Program-aided Distillation\n(PaD), which introduces reasoning programs to suppress the errors in distilled\ndata, and thus achieves better distillation quality for reasoning tasks. In\nPaD, we utilize the reasoning program to substitute the CoT, allowing automated\nerror checking of synthetic data. Further, through error injecting and further\ntraining, the small distilling model could iteratively self-refine the\nreasoning. Moreover, we conduct a step-wise beam search by step-by-step\nverifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic\nreasoning, symbolic reasoning, and general ability. Experimental results\ndemonstrate that smaller models using PaD can not only outperform certain\nLLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines\nwith a significantly smaller scale of parameters and data. The source code is\npublicly available at https://github.com/Xuekai-Zhu/pad.\n","authors":["Xuekai Zhu","Biqing Qi","Kaiyan Zhang","Xinwei Long","Zhouhan Lin","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.13888v2.pdf","comment":"NAACL 2024 Long Paper; Code and data are available at\n  https://github.com/Xuekai-Zhu/pad"},{"id":"http://arxiv.org/abs/2309.13243v2","updated":"2024-03-20T08:16:14Z","published":"2023-09-23T03:28:25Z","title":"ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education","summary":"  The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale, real-world interactions between students and AI\nsystems still remain limited. In this study, we present ChEDDAR, ChatGPT & EFL\nLearner's Dialogue Dataset As Revising an essay, which is collected from a\nsemester-long longitudinal experiment involving 212 college students enrolled\nin English as Foreign Langauge (EFL) writing courses. The students were asked\nto revise their essays through dialogues with ChatGPT. ChEDDAR includes a\nconversation log, utterance-level essay edit history, self-rated satisfaction,\nand students' intent, in addition to session-level pre-and-post surveys\ndocumenting their objectives and overall experiences. We analyze students'\nusage patterns and perceptions regarding generative AI with respect to their\nintent and satisfaction. As a foundational step, we establish baseline results\nfor two pivotal tasks in task-oriented dialogue systems within educational\ncontexts: intent detection and satisfaction estimation. We finally suggest\nfurther research to refine the integration of generative AI into education\nsettings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly\navailable at https://github.com/zeunie/ChEDDAR.\n","authors":["Jieun Han","Haneul Yoo","Junho Myung","Minsun Kim","Tak Yeon Lee","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2309.13243v2.pdf","comment":"The new version of this paper is on arXiv as arXiv:2403.08272"},{"id":"http://arxiv.org/abs/2403.13372v1","updated":"2024-03-20T08:08:54Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13372v1.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2403.13369v1","updated":"2024-03-20T08:01:33Z","published":"2024-03-20T08:01:33Z","title":"Clinical information extraction for Low-resource languages with Few-shot\n  learning using Pre-trained language models and Prompting","summary":"  Automatic extraction of medical information from clinical documents poses\nseveral challenges: high costs of required clinical expertise, limited\ninterpretability of model predictions, restricted computational resources and\nprivacy regulations. Recent advances in domain-adaptation and prompting methods\nshowed promising results with minimal training data using lightweight masked\nlanguage models, which are suited for well-established interpretability\nmethods. We are first to present a systematic evaluation of these methods in a\nlow-resource setting, by performing multi-class section classification on\nGerman doctor's letters. We conduct extensive class-wise evaluations supported\nby Shapley values, to validate the quality of our small training data set and\nto ensure the interpretability of model predictions. We demonstrate that a\nlightweight, domain-adapted pretrained model, prompted with just 20 shots,\noutperforms a traditional classification model by 30.5% accuracy. Our results\nserve as a process-oriented guideline for clinical information extraction\nprojects working with low-resource.\n","authors":["Phillip Richter-Pechanski","Philipp Wiesenbach","Dominic M. Schwab","Christina Kiriakou","Nicolas Geis","Christoph Dieterich","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2403.13369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13368v1","updated":"2024-03-20T08:01:22Z","published":"2024-03-20T08:01:22Z","title":"Computational Models to Study Language Processing in the Human Brain: A\n  Survey","summary":"  Despite differing from the human language processing mechanism in\nimplementation and algorithms, current language models demonstrate remarkable\nhuman-like or surpassing language capabilities. Should computational language\nmodels be employed in studying the brain, and if so, when and how? To delve\ninto this topic, this paper reviews efforts in using computational models for\nbrain research, highlighting emerging trends. To ensure a fair comparison, the\npaper evaluates various computational models using consistent metrics on the\nsame dataset. Our analysis reveals that no single model outperforms others on\nall datasets, underscoring the need for rich testing datasets and rigid\nexperimental control to draw robust conclusions in studies involving\ncomputational models.\n","authors":["Shaonan Wang","Jingyuan Sun","Yunhao Zhang","Nan Lin","Marie-Francine Moens","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2403.13368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13362v1","updated":"2024-03-20T07:44:06Z","published":"2024-03-20T07:44:06Z","title":"Incentivizing News Consumption on Social Media Platforms Using Large\n  Language Models and Realistic Bot Accounts","summary":"  Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.\n","authors":["Hadi Askari","Anshuman Chhabra","Bernhard Clemm von Hohenberg","Michael Heseltine","Magdalena Wojcieszak"],"pdf_url":"https://arxiv.org/pdf/2403.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03512v3","updated":"2024-03-20T07:39:48Z","published":"2024-01-07T15:00:36Z","title":"CharPoet: A Chinese Classical Poetry Generation System Based on\n  Token-free LLM","summary":"  Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.\n","authors":["Chengyue Yu","Lei Zang","Jiaotuan Wang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2401.03512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00758v3","updated":"2024-03-20T07:37:24Z","published":"2024-03-01T18:55:20Z","title":"Mitigating Reversal Curse in Large Language Models via Semantic-aware\n  Permutation Training","summary":"  While large language models (LLMs) have achieved impressive performance\nacross diverse tasks, recent studies showcase that causal LLMs suffer from the\n\"reversal curse\". It is a typical example that the model knows \"A's father is\nB\", but is unable to reason \"B's child is A\". This limitation poses a challenge\nto the advancement of artificial general intelligence (AGI), as it suggests a\ngap in the models' ability to comprehend and apply bidirectional reasoning. In\nthis paper, we first conduct substantial evaluation and identify that the root\ncause of the reversal curse lies in the different word order between the\ntraining and inference stage, namely, the poor ability of causal language\nmodels to predict antecedent words within the training data. Accordingly,\npermutation on the training data is considered as a potential solution, since\nthis can make the model predict antecedent words or tokens. However, previous\npermutation methods may disrupt complete phrases or entities, thereby posing\nchallenges for the model to comprehend and learn from training data. To address\nthis issue, we propose Semantic-aware Permutation Training (SPT), which\naddresses this issue by segmenting the training sentences into semantic units\n(i.e., entities or phrases) with an assistant language model and permuting\nthese units before feeding into the model. Extensive experiments demonstrate\nthat SPT effectively mitigates the reversal curse since the performance on\nreversed questions approximates that on the forward ones, and significantly\nadvances the performance of existing works.\n","authors":["Qingyan Guo","Rui Wang","Junliang Guo","Xu Tan","Jiang Bian","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2403.00758v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11467v2","updated":"2024-03-20T07:08:22Z","published":"2024-01-21T11:42:18Z","title":"Over-Reasoning and Redundant Calculation of Large Language Models","summary":"  Large language models (LLMs) can solve problems step-by-step. While this\nchain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if\nLLMs \\textit{know} when to use CoT and whether those CoT are always necessary\nto answer the question. This paper shows that LLMs tend to generate redundant\ncalculations and reasoning on a manually constructed math QA dataset,\nGSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered\nwithout any calculations, but LLMs, including Llama-2 models and Claude-2, tend\nto generate lengthy and unnecessary calculations to answer the questions. We\nalso conduct experiments to explain why LLMs generate redundant calculations\nand reasonings. GSM8K-Zero is publicly available at\nhttps://github.com/d223302/Over-Reasoning-of-LLMs and\nhttps://huggingface.co/datasets/dcml0714/GSM8K-Zero.\n","authors":["Cheng-Han Chiang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11467v2.pdf","comment":"EACL 2024 main conference paper. Camera-ready version"},{"id":"http://arxiv.org/abs/2403.13344v1","updated":"2024-03-20T07:05:19Z","published":"2024-03-20T07:05:19Z","title":"USE: Dynamic User Modeling with Stateful Sequence Models","summary":"  User embeddings play a crucial role in user engagement forecasting and\npersonalized services. Recent advances in sequence modeling have sparked\ninterest in learning user embeddings from behavioral data. Yet behavior-based\nuser embedding learning faces the unique challenge of dynamic user modeling. As\nusers continuously interact with the apps, user embeddings should be\nperiodically updated to account for users' recent and long-term behavior\npatterns. Existing methods highly rely on stateless sequence models that lack\nmemory of historical behavior. They have to either discard historical data and\nuse only the most recent data or reprocess the old and new data jointly. Both\ncases incur substantial computational overhead. To address this limitation, we\nintroduce User Stateful Embedding (USE). USE generates user embeddings and\nreflects users' evolving behaviors without the need for exhaustive reprocessing\nby storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future W-behavior\nprediction to transcend the limitations of next-token prediction by forecasting\na broader horizon of upcoming user behaviors. By combining it with the Same\nUser Prediction, a contrastive learning-based objective that predicts whether\ndifferent segments of behavior sequences belong to the same user, we further\nimprove the embeddings' distinctiveness and representativeness. We conducted\nexperiments on 8 downstream tasks using Snapchat users' behavioral logs in both\nstatic (i.e., fixed user behavior sequences) and dynamic (i.e., periodically\nupdated user behavior sequences) settings. We demonstrate USE's superior\nperformance over established baselines. The results underscore USE's\neffectiveness and efficiency in integrating historical and recent user behavior\nsequences into user embeddings in dynamic user modeling.\n","authors":["Zhihan Zhou","Qixiang Fang","Leonardo Neves","Francesco Barbieri","Yozen Liu","Han Liu","Maarten W. Bos","Ron Dotsch"],"pdf_url":"https://arxiv.org/pdf/2403.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13334v1","updated":"2024-03-20T06:37:59Z","published":"2024-03-20T06:37:59Z","title":"Hyacinth6B: A large language model for Traditional Chinese","summary":"  This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.\n","authors":["Chih-Wei Song","Yin-Te Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.13334v1.pdf","comment":"14pages"},{"id":"http://arxiv.org/abs/2402.15506v3","updated":"2024-03-20T06:00:14Z","published":"2024-02-23T18:56:26Z","title":"AgentOhana: Design Unified Data and Training Pipeline for Effective\n  Agent Learning","summary":"  Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.\n","authors":["Jianguo Zhang","Tian Lan","Rithesh Murthy","Zhiwei Liu","Weiran Yao","Juntao Tan","Thai Hoang","Liangwei Yang","Yihao Feng","Zuxin Liu","Tulika Awalgaonkar","Juan Carlos Niebles","Silvio Savarese","Shelby Heinecke","Huan Wang","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.15506v3.pdf","comment":"Add GitHub repo link at\n  \\url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link\n  at \\url{https://huggingface.co/Salesforce/xLAM-v0.1-r}"},{"id":"http://arxiv.org/abs/2403.13313v1","updated":"2024-03-20T05:34:03Z","published":"2024-03-20T05:34:03Z","title":"Polaris: A Safety-focused LLM Constellation Architecture for Healthcare","summary":"  We develop Polaris, the first safety-focused LLM constellation for real-time\npatient-AI healthcare conversations. Unlike prior LLM works in healthcare\nfocusing on tasks like question answering, our work specifically focuses on\nlong multi-turn voice conversations. Our one-trillion parameter constellation\nsystem is composed of several multibillion parameter LLMs as co-operative\nagents: a stateful primary agent that focuses on driving an engaging\nconversation and several specialist support agents focused on healthcare tasks\nperformed by nurses to increase safety and reduce hallucinations. We develop a\nsophisticated training protocol for iterative co-training of the agents that\noptimize for diverse objectives. We train our models on proprietary data,\nclinical care plans, healthcare regulatory documents, medical manuals, and\nother medical reasoning documents. We align our models to speak like medical\nprofessionals, using organic healthcare conversations and simulated ones\nbetween patient actors and experienced nurses. This allows our system to\nexpress unique capabilities such as rapport building, trust building, empathy\nand bedside manner. Finally, we present the first comprehensive clinician\nevaluation of an LLM system for healthcare. We recruited over 1100 U.S.\nlicensed nurses and over 130 U.S. licensed physicians to perform end-to-end\nconversational evaluations of our system by posing as patients and rating the\nsystem on several measures. We demonstrate Polaris performs on par with human\nnurses on aggregate across dimensions such as medical safety, clinical\nreadiness, conversational quality, and bedside manner. Additionally, we conduct\na challenging task-based evaluation of the individual specialist support\nagents, where we demonstrate our LLM agents significantly outperform a much\nlarger general-purpose LLM (GPT-4) as well as from its own medium-size class\n(LLaMA-2 70B).\n","authors":["Subhabrata Mukherjee","Paul Gamble","Markel Sanz Ausin","Neel Kant","Kriti Aggarwal","Neha Manjunath","Debajyoti Datta","Zhengliang Liu","Jiayuan Ding","Sophia Busacca","Cezanne Bianco","Swapnil Sharma","Rae Lasko","Michelle Voisard","Sanchay Harneja","Darya Filippova","Gerry Meixiong","Kevin Cha","Amir Youssefi","Meyhaa Buvanesh","Howard Weingram","Sebastian Bierman-Lytle","Harpreet Singh Mangat","Kim Parikh","Saad Godil","Alex Miller"],"pdf_url":"https://arxiv.org/pdf/2403.13313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13312v1","updated":"2024-03-20T05:29:06Z","published":"2024-03-20T05:29:06Z","title":"LeanReasoner: Boosting Complex Logical Reasoning with Lean","summary":"  Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.\n","authors":["Dongwei Jiang","Marcio Fonseca","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2403.13312v1.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2311.07838v2","updated":"2024-03-20T05:04:06Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v2.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.13301v1","updated":"2024-03-20T04:57:32Z","published":"2024-03-20T04:57:32Z","title":"Reading Users' Minds from What They Say: An Investigation into LLM-based\n  Empathic Mental Inference","summary":"  In human-centered design, developing a comprehensive and in-depth\nunderstanding of user experiences, i.e., empathic understanding, is paramount\nfor designing products that truly meet human needs. Nevertheless, accurately\ncomprehending the real underlying mental states of a large human population\nremains a significant challenge today. This difficulty mainly arises from the\ntrade-off between depth and scale of user experience research: gaining in-depth\ninsights from a small group of users does not easily scale to a larger\npopulation, and vice versa. This paper investigates the use of Large Language\nModels (LLMs) for performing mental inference tasks, specifically inferring\nusers' underlying goals and fundamental psychological needs (FPNs). Baseline\nand benchmark datasets were collected from human users and designers to develop\nan empathic accuracy metric for measuring the mental inference performance of\nLLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with\nvaried zero-shot prompt engineering techniques are experimented against that of\nhuman designers. Experimental results suggest that LLMs can infer and\nunderstand the underlying goals and FPNs of users with performance comparable\nto that of human designers, suggesting a promising avenue for enhancing the\nscalability of empathic design approaches through the integration of advanced\nartificial intelligence technologies. This work has the potential to\nsignificantly augment the toolkit available to designers during human-centered\ndesign, enabling the development of both large-scale and in-depth understanding\nof users' experiences.\n","authors":["Qihao Zhu","Leah Chong","Maria Yang","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13301v1.pdf","comment":"Submitted to IDETC-CIE2024"},{"id":"http://arxiv.org/abs/2306.12245v4","updated":"2024-03-20T03:51:23Z","published":"2023-06-21T13:04:30Z","title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for\n  Entity Linking","summary":"  Entity Linking (EL) is a fundamental task for Information Extraction and\nKnowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first\nfind mentions in the given input document and then link the mentions to\ncorresponding entities in a specific knowledge base. Recently, the paradigm of\nretriever-reader promotes the progress of end-to-end EL, benefiting from the\nadvantages of dense entity retrieval and machine reading comprehension.\nHowever, the existing study only trains the retriever and the reader separately\nin a pipeline manner, which ignores the benefit that the interaction between\nthe retriever and the reader can bring to the task. To advance the\nretriever-reader paradigm to perform more perfectly on end-to-end EL, we\npropose BEER$^2$, a Bidirectional End-to-End training framework for Retriever\nand Reader. Through our designed bidirectional end-to-end training, BEER$^2$\nguides the retriever and the reader to learn from each other, make progress\ntogether, and ultimately improve EL performance. Extensive experiments on\nbenchmarks of multiple domains demonstrate the effectiveness of our proposed\nBEER$^2$.\n","authors":["Yinghui Li","Yong Jiang","Yangning Li","Xingyu Lu","Pengjun Xie","Ying Shen","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2306.12245v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01931v2","updated":"2024-03-20T03:33:32Z","published":"2023-06-02T22:12:05Z","title":"Exploring semantic information in disease: Simple Data Augmentation\n  Techniques for Chinese Disease Normalization","summary":"  Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. While data augmentation is a powerful approach for addressing data\nscarcity, our findings reveal that conventional data augmentation techniques\noften impede task performance, primarily due to the multi-axis and\nmulti-granularity nature of disease names. Consequently, we introduce a set of\ncustomized data augmentation techniques designed to leverage the semantic\ninformation inherent in disease names. These techniques aim to enhance the\nmodel's understanding of the semantic intricacies and classification structure\nof disease names. Through extensive experimentation, we illustrate that our\nproposed plug-and-play methods not only surpass general data augmentation\ntechniques but also exhibit significant performance improvements across various\nbaseline models and training objectives, particularly in scenarios with limited\ntraining data. This underscores its potential for widespread application in\nmedical language processing tasks.\n","authors":["Wenqian Cui","Xiangling Fu","Shaohui Liu","Mingjun Gu","Xien Liu","Ji Wu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2306.01931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00415v3","updated":"2024-03-20T03:23:11Z","published":"2022-05-01T07:51:22Z","title":"Don't Blame the Annotator: Bias Already Starts in the Annotation\n  Instructions","summary":"  In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write many similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n","authors":["Mihir Parmar","Swaroop Mishra","Mor Geva","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2205.00415v3.pdf","comment":"EACL 2023 (Outstanding Paper Award)"},{"id":"http://arxiv.org/abs/2403.13272v1","updated":"2024-03-20T03:14:54Z","published":"2024-03-20T03:14:54Z","title":"Community Needs and Assets: A Computational Analysis of Community\n  Conversations","summary":"  A community needs assessment is a tool used by non-profits and government\nagencies to quantify the strengths and issues of a community, allowing them to\nallocate their resources better. Such approaches are transitioning towards\nleveraging social media conversations to analyze the needs of communities and\nthe assets already present within them. However, manual analysis of\nexponentially increasing social media conversations is challenging. There is a\ngap in the present literature in computationally analyzing how community\nmembers discuss the strengths and needs of the community. To address this gap,\nwe introduce the task of identifying, extracting, and categorizing community\nneeds and assets from conversational data using sophisticated natural language\nprocessing methods. To facilitate this task, we introduce the first dataset\nabout community needs and assets consisting of 3,511 conversations from Reddit,\nannotated using crowdsourced workers. Using this dataset, we evaluate an\nutterance-level classification model compared to sentiment classification and a\npopular large language model (in a zero-shot setting), where we find that our\nmodel outperforms both baselines at an F1 score of 94% compared to 49% and 61%\nrespectively. Furthermore, we observe through our study that conversations\nabout needs have negative sentiments and emotions, while conversations about\nassets focus on location and entities. The dataset is available at\nhttps://github.com/towhidabsar/CommunityNeeds.\n","authors":["Md Towhidul Absar Chowdhury","Naveen Sharma","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2403.13272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13269v1","updated":"2024-03-20T03:07:50Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.03963v2","updated":"2024-03-20T03:06:16Z","published":"2022-10-08T08:07:47Z","title":"SDA: Simple Discrete Augmentation for Contrastive Sentence\n  Representation Learning","summary":"  Contrastive learning has recently achieved compelling performance in\nunsupervised sentence representation. As an essential element, data\naugmentation protocols, however, have not been well explored. The pioneering\nwork SimCSE resorting to a simple dropout mechanism (viewed as continuous\naugmentation) surprisingly dominates discrete augmentations such as cropping,\nword deletion, and synonym replacement as reported. To understand the\nunderlying rationales, we revisit existing approaches and attempt to\nhypothesize the desiderata of reasonable data augmentation methods: balance of\nsemantic consistency and expression diversity. We then develop three simple yet\neffective discrete sentence augmentation schemes: punctuation insertion, modal\nverbs, and double negation. They act as minimal noises at lexical level to\nproduce diverse forms of sentences. Furthermore, standard negation is\ncapitalized on to generate negative samples for alleviating feature suppression\ninvolved in contrastive learning. We experimented extensively with semantic\ntextual similarity on diverse datasets. The results support the superiority of\nthe proposed methods consistently.\n","authors":["Dongsheng Zhu","Zhenyu Mao","Jinghui Lu","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2210.03963v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.07081v2","updated":"2024-03-20T03:04:47Z","published":"2023-09-13T16:46:27Z","title":"Can Whisper perform speech-based in-context learning?","summary":"  This paper investigates the in-context learning abilities of the Whisper\nautomatic speech recognition (ASR) models released by OpenAI. A novel\nspeech-based in-context learning (SICL) approach is proposed for test-time\nadaptation, which can reduce the word error rates (WERs) with only a small\nnumber of labelled speech samples without gradient descent. Language-level\nadaptation experiments using Chinese dialects showed that when applying SICL to\nisolated word ASR, consistent and considerable relative WER reductions can be\nachieved using Whisper models of any size on two dialects, which is on average\n32.3%. A k-nearest-neighbours-based in-context example selection technique can\nbe applied to further improve the efficiency of SICL, which can increase the\naverage relative WER reduction to 36.4%. The findings are verified using\nspeaker adaptation or continuous speech recognition tasks, and both achieved\nconsiderable relative WER reductions. Detailed quantitative analyses are also\nprovided to shed light on SICL's adaptability to phonological variances and\ndialect-specific lexical nuances.\n","authors":["Siyin Wang","Chao-Han Huck Yang","Ji Wu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07081v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.10943v2","updated":"2024-03-20T02:52:42Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v2.pdf","comment":"Published in ICLR 2024; The abstract is slightly modified due to the\n  length limitation"},{"id":"http://arxiv.org/abs/2403.13257v1","updated":"2024-03-20T02:38:01Z","published":"2024-03-20T02:38:01Z","title":"Arcee's MergeKit: A Toolkit for Merging Large Language Models","summary":"  The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npre-trained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multi-task learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.\n","authors":["Charles Goddard","Shamane Siriwardhana","Malikeh Ehghaghi","Luke Meyers","Vlad Karpukhin","Brian Benedict","Mark McQuade","Jacob Solawetz"],"pdf_url":"https://arxiv.org/pdf/2403.13257v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13253v1","updated":"2024-03-20T02:32:24Z","published":"2024-03-20T02:32:24Z","title":"Document Author Classification Using Parsed Language Structure","summary":"  Over the years there has been ongoing interest in detecting authorship of a\ntext based on statistical properties of the text, such as by using occurrence\nrates of noncontextual words. In previous work, these techniques have been\nused, for example, to determine authorship of all of \\emph{The Federalist\nPapers}. Such methods may be useful in more modern times to detect fake or AI\nauthorship. Progress in statistical natural language parsers introduces the\npossibility of using grammatical structure to detect authorship. In this paper\nwe explore a new possibility for detecting authorship using grammatical\nstructural information extracted using a statistical natural language parser.\nThis paper provides a proof of concept, testing author classification based on\ngrammatical structure on a set of \"proof texts,\" The Federalist Papers and\nSanditon which have been as test cases in previous authorship detection\nstudies. Several features extracted from the statistical natural language\nparser were explored: all subtrees of some depth from any level; rooted\nsubtrees of some depth, part of speech, and part of speech by level in the\nparse tree. It was found to be helpful to project the features into a lower\ndimensional space. Statistical experiments on these documents demonstrate that\ninformation from a statistical parser can, in fact, assist in distinguishing\nauthors.\n","authors":["Todd K Moon","Jacob H. Gunther"],"pdf_url":"https://arxiv.org/pdf/2403.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13250v1","updated":"2024-03-20T02:29:09Z","published":"2024-03-20T02:29:09Z","title":"Facilitating Pornographic Text Detection for Open-Domain Dialogue\n  Systems via Knowledge Distillation of Large Language Models","summary":"  Pornographic content occurring in human-machine interaction dialogues can\ncause severe side effects for users in open-domain dialogue systems. However,\nresearch on detecting pornographic language within human-machine interaction\ndialogues is an important subject that is rarely studied. To advance in this\ndirection, we introduce CensorChat, a dialogue monitoring dataset aimed at\ndetecting whether the dialogue session contains pornographic content. To this\nend, we collect real-life human-machine interaction dialogues in the wild and\nbreak them down into single utterances and single-turn dialogues, with the last\nutterance spoken by the chatbot. We propose utilizing knowledge distillation of\nlarge language models to annotate the dataset. Specifically, first, the raw\ndataset is annotated by four open-source large language models, with the\nmajority vote determining the label. Second, we use ChatGPT to update the empty\nlabel from the first step. Third, to ensure the quality of the validation and\ntest sets, we utilize GPT-4 for label calibration. If the current label does\nnot match the one generated by GPT-4, we employ a self-criticism strategy to\nverify its correctness. Finally, to facilitate the detection of pornographic\ntext, we develop a series of text classifiers using a pseudo-labeled dataset.\nDetailed data analysis demonstrates that leveraging knowledge distillation\ntechniques with large language models provides a practical and cost-efficient\nmethod for developing pornographic text detectors.\n","authors":["Huachuan Qiu","Shuai Zhang","Hongliang He","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2403.13250v1.pdf","comment":"Accepted to CSCWD 2024 (27th International Conference on Computer\n  Supported Cooperative Work in Design). arXiv admin note: text overlap with\n  arXiv:2309.09749"},{"id":"http://arxiv.org/abs/2311.14648v3","updated":"2024-03-20T02:21:20Z","published":"2023-11-24T18:29:50Z","title":"Calibrated Language Models Must Hallucinate","summary":"  Recent language models generate false but plausible-sounding text with\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\nlanguage-based AI systems and can harm people who rely upon their outputs. This\nwork shows that there is an inherent statistical lower-bound on the rate that\npretrained language models hallucinate certain types of facts, having nothing\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\nfacts whose veracity cannot be determined from the training data, we show that\nhallucinations must occur at a certain rate for language models that satisfy a\nstatistical calibration condition appropriate for generative language models.\nSpecifically, if the maximum probability of any fact is bounded, we show that\nthe probability of generating a hallucination is close to the fraction of facts\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\nassuming ideal training data without errors.\n  One conclusion is that models pretrained to be sufficiently good predictors\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\ntype of arbitrary facts that tend to appear once in the training set. However,\nour analysis also suggests that there is no statistical reason that pretraining\nwill lead to hallucination on facts that tend to appear more than once in the\ntraining data (like references to publications such as articles and books,\nwhose hallucinations have been particularly notable and problematic) or on\nsystematic facts (like arithmetic calculations). Therefore, different\narchitectures and learning algorithms may mitigate these latter types of\nhallucinations.\n","authors":["Adam Tauman Kalai","Santosh S. Vempala"],"pdf_url":"https://arxiv.org/pdf/2311.14648v3.pdf","comment":"In Proceedings of the 56th Annual ACM Symposium on Theory of\n  Computing (STOC) 2024"},{"id":"http://arxiv.org/abs/2309.08150v2","updated":"2024-03-20T02:17:16Z","published":"2023-09-15T04:34:40Z","title":"Unimodal Aggregation for CTC-based Speech Recognition","summary":"  This paper works on non-autoregressive automatic speech recognition. A\nunimodal aggregation (UMA) is proposed to segment and integrate the feature\nframes that belong to the same text token, and thus to learn better feature\nrepresentations for text tokens. The frame-wise features and weights are both\nderived from an encoder. Then, the feature frames with unimodal weights are\nintegrated and further processed by a decoder. Connectionist temporal\nclassification (CTC) loss is applied for training. Compared to the regular CTC,\nthe proposed method learns better feature representations and shortens the\nsequence length, resulting in lower recognition error and computational\ncomplexity. Experiments on three Mandarin datasets show that UMA demonstrates\nsuperior or comparable performance to other advanced non-autoregressive\nmethods, such as self-conditioned CTC. Moreover, by integrating\nself-conditioned CTC into the proposed framework, the performance can be\nfurther noticeably improved.\n","authors":["Ying Fang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08150v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13244v1","updated":"2024-03-20T02:15:55Z","published":"2024-03-20T02:15:55Z","title":"Instruction Multi-Constraint Molecular Generation Using a\n  Teacher-Student Large Language Model","summary":"  While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 88.08%, 65.27%, and\n61.44%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science. Code is available at\nhttps://github.com/HHW-zhou/TSMMG.\n","authors":["Peng Zhou","Jianmin Wang","Chunyan Li","Zixu Wang","Yiping Liu","Siqi Sun","Jianxin Lin","Longyue Wang","Xiangxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.13244v1.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13240v1","updated":"2024-03-20T02:04:42Z","published":"2024-03-20T02:04:42Z","title":"SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual\n  Summarization","summary":"  Cross-lingual summarization (XLS) generates summaries in a language different\nfrom that of the input documents (e.g., English to Spanish), allowing speakers\nof the target language to gain a concise view of their content. In the present\nday, the predominant approach to this task is to take a performing, pretrained\nmultilingual language model (LM) and fine-tune it for XLS on the language pairs\nof interest. However, the scarcity of fine-tuning samples makes this approach\nchallenging in some cases. For this reason, in this paper we propose revisiting\nthe summarize-and-translate pipeline, where the summarization and translation\ntasks are performed in a sequence. This approach allows reusing the many,\npublicly-available resources for monolingual summarization and translation,\nobtaining a very competitive zero-shot performance. In addition, the proposed\npipeline is completely differentiable end-to-end, allowing it to take advantage\nof few-shot fine-tuning, where available. Experiments over two contemporary and\nwidely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable\nzero-shot performance of the proposed approach, and also its strong few-shot\nperformance compared to an equivalent multilingual LM baseline, that the\nproposed approach has been able to outperform in many languages with only 10%\nof the fine-tuning samples.\n","authors":["Jacob Parnell","Inigo Jauregi Unanue","Massimo Piccardi"],"pdf_url":"https://arxiv.org/pdf/2403.13240v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2312.04262v2","updated":"2024-03-20T01:59:39Z","published":"2023-12-07T12:40:00Z","title":"PsyChat: A Client-Centric Dialogue System for Mental Health Support","summary":"  Dialogue systems are increasingly integrated into mental health support to\nhelp clients facilitate exploration, gain insight, take action, and ultimately\nheal themselves. A practical and user-friendly dialogue system should be\nclient-centric, focusing on the client's behaviors. However, existing dialogue\nsystems publicly available for mental health support often concentrate solely\non the counselor's strategies rather than the behaviors expressed by clients.\nThis can lead to unreasonable or inappropriate counseling strategies and\ncorresponding responses generated by the dialogue system. To address this\nissue, we propose PsyChat, a client-centric dialogue system that provides\npsychological support through online chat. The client-centric dialogue system\ncomprises five modules: client behavior recognition, counselor strategy\nselection, input packer, response generator, and response selection. Both\nautomatic and human evaluations demonstrate the effectiveness and practicality\nof our proposed dialogue system for real-life mental health support.\nFurthermore, the case study demonstrates that the dialogue system can predict\nthe client's behaviors, select appropriate counselor strategies, and generate\naccurate and suitable responses.\n","authors":["Huachuan Qiu","Anqi Li","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2312.04262v2.pdf","comment":"Accepted to CSCWD 2024 (27th International Conference on Computer\n  Supported Cooperative Work in Design)"},{"id":"http://arxiv.org/abs/2403.13233v1","updated":"2024-03-20T01:46:06Z","published":"2024-03-20T01:46:06Z","title":"Technical Report: Competition Solution For BetterMixture","summary":"  In the era of flourishing large-scale models, the challenge of selecting and\noptimizing datasets from the vast and complex sea of data, to enhance the\nperformance of large language models within the constraints of limited\ncomputational resources, has become paramount. This paper details our solution\nfor the BetterMixture challenge, which focuses on the fine-tuning data mixing\nfor large language models. Our approach, which secured third place,\nincorporates data deduplication, low-level and high-level quality filtering,\nand diversity selection. The foundation of our solution is Ke-Data-Juicer, an\nextension of Data-Juicer, demonstrating its robust capabilities in handling and\noptimizing data for large language models.\n","authors":["Shuaijiang Zhao","Xiaoquan Fang"],"pdf_url":"https://arxiv.org/pdf/2403.13233v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2306.12725v4","updated":"2024-03-20T01:30:41Z","published":"2023-06-22T07:57:19Z","title":"Generative Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask. Our code is available at https://github.com/HITsz-TMG/GEMEL.\n","authors":["Senbao Shi","Zhenran Xu","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.12725v4.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.00221v3","updated":"2024-03-20T01:04:11Z","published":"2023-08-01T01:27:40Z","title":"Advancing Beyond Identification: Multi-bit Watermark for Large Language\n  Models","summary":"  We show the viability of tackling misuses of large language models beyond the\nidentification of machine-generated text. While existing zero-bit watermark\nmethods focus on detection only, some malicious misuses demand tracing the\nadversary user for counteracting them. To address this, we propose Multi-bit\nWatermark via Position Allocation, embedding traceable multi-bit information\nduring language model generation. Through allocating tokens onto different\nparts of the messages, we embed longer messages in high corruption settings\nwithout added latency. By independently embedding sub-units of messages, the\nproposed method outperforms the existing works in terms of robustness and\nlatency. Leveraging the benefits of zero-bit watermarking, our method enables\nrobust extraction of the watermark without any model access, embedding and\nextraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining\ntext quality, while allowing zero-bit detection all at the same time. Code is\nreleased here: https://github.com/bangawayoo/mb-lm-watermarking\n","authors":["KiYoon Yoo","Wonhyuk Ahn","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2308.00221v3.pdf","comment":"NAACL 2024 main. 9 pages and appendix"},{"id":"http://arxiv.org/abs/2403.13213v1","updated":"2024-03-20T00:22:38Z","published":"2024-03-20T00:22:38Z","title":"From Representational Harms to Quality-of-Service Harms: A Case Study on\n  Llama 2 Safety Safeguards","summary":"  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n","authors":["Khaoula Chehbouni","Megha Roshan","Emmanuel Ma","Futian Andrew Wei","Afaf Taïk","Jackie CK Cheung","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2403.13213v1.pdf","comment":"9 pages, 4 figures, submitted to the 62nd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2312.04302v2","updated":"2024-03-20T23:32:08Z","published":"2023-12-07T13:53:29Z","title":"Prompt Highlighter: Interactive Control for Multi-Modal LLMs","summary":"  This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/\n","authors":["Yuechen Zhang","Shengju Qian","Bohao Peng","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2312.04302v2.pdf","comment":"CVPR 2024; Project Page:\n  https://julianjuaner.github.io/projects/PromptHighlighter"},{"id":"http://arxiv.org/abs/2403.08002v2","updated":"2024-03-20T23:31:22Z","published":"2024-03-12T18:12:02Z","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A\n  Case Study in Radiology Imaging","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14037v1","updated":"2024-03-20T23:21:35Z","published":"2024-03-20T23:21:35Z","title":"Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection","summary":"  Misinformation can seriously impact society, affecting anything from public\nopinion to institutional confidence and the political horizon of a state. Fake\nNews (FN) proliferation on online websites and Online Social Networks (OSNs)\nhas increased profusely. Various fact-checking websites include news in English\nand barely provide information about FN in regional languages. Thus the Urdu FN\npurveyors cannot be discerned using factchecking portals. SOTA approaches for\nFake News Detection (FND) count upon appropriately labelled and large datasets.\nFND in regional and resource-constrained languages lags due to the lack of\nlimited-sized datasets and legitimate lexical resources. The previous datasets\nfor Urdu FND are limited-sized, domain-restricted, publicly unavailable and not\nmanually verified where the news is translated from English into Urdu. In this\npaper, we curate and contribute the first largest publicly available dataset\nfor Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations\nof existing Urdu datasets in the literature. It constitutes 10,083 fake and\nreal news on fifteen domains collected from leading and authentic Urdu\nnewspapers and news channel websites in Pakistan and India. FN for the\nAx-to-Grind dataset is collected from websites and crowdsourcing. The dataset\ncontains news items in Urdu from the year 2017 to the year 2023. Expert\njournalists annotated the dataset. We benchmark the dataset with an ensemble\nmodel of mBERT,XLNet, and XLM RoBERTa. The selected models are originally\ntrained on multilingual large corpora. The results of the proposed model are\nbased on performance metrics, F1-score, accuracy, precision, recall and MCC\nvalue.\n","authors":["Sheetal Harris","Jinshuo Liu","Hassan Jalil Hadi","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2403.14037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14009v1","updated":"2024-03-20T22:14:39Z","published":"2024-03-20T22:14:39Z","title":"A New Massive Multilingual Dataset for High-Performance Language\n  Technologies","summary":"  We present the HPLT (High Performance Language Technologies) language\nresources, a new massive multilingual dataset including both monolingual and\nbilingual corpora extracted from CommonCrawl and previously unused web crawls\nfrom the Internet Archive. We describe our methods for data acquisition,\nmanagement and processing of large corpora, which rely on open-source software\ntools and high-performance computing. Our monolingual collection focuses on\nlow- to medium-resourced languages and covers 75 languages and a total of ~5.6\ntrillion word tokens de-duplicated on the document level. Our English-centric\nparallel corpus is derived from its monolingual counterpart and covers 18\nlanguage pairs and more than 96 million aligned sentence pairs with roughly 1.4\nbillion English tokens. The HPLT language resources are one of the largest open\ntext corpora ever released, providing a great resource for language modeling\nand machine translation training. We publicly release the corpora, the\nsoftware, and the tools used in this work.\n","authors":["Ona de Gibert","Graeme Nail","Nikolay Arefyev","Marta Bañón","Jelmer van der Linde","Shaoxiong Ji","Jaume Zaragoza-Bernabeu","Mikko Aulamo","Gema Ramírez-Sánchez","Andrey Kutuzov","Sampo Pyysalo","Stephan Oepen","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2403.14009v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14006v1","updated":"2024-03-20T22:11:01Z","published":"2024-03-20T22:11:01Z","title":"On Prompt Sensitivity of ChatGPT in Affective Computing","summary":"  Recent studies have demonstrated the emerging capabilities of foundation\nmodels like ChatGPT in several fields, including affective computing. However,\naccessing these emerging capabilities is facilitated through prompt\nengineering. Despite the existence of some prompting techniques, the field is\nstill rapidly evolving and many prompting ideas still require investigation. In\nthis work, we introduce a method to evaluate and investigate the sensitivity of\nthe performance of foundation models based on different prompts or generation\nparameters. We perform our evaluation on ChatGPT within the scope of affective\ncomputing on three major problems, namely sentiment analysis, toxicity\ndetection, and sarcasm detection. First, we carry out a sensitivity analysis on\npivotal parameters in auto-regressive text generation, specifically the\ntemperature parameter $T$ and the top-$p$ parameter in Nucleus sampling,\ndictating how conservative or creative the model should be during generation.\nFurthermore, we explore the efficacy of several prompting ideas, where we\nexplore how giving different incentives or structures affect the performance.\nOur evaluation takes into consideration performance measures on the affective\ncomputing tasks, and the effectiveness of the model to follow the stated\ninstructions, hence generating easy-to-parse responses to be smoothly used in\ndownstream applications.\n","authors":["Mostafa M. Amin","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2403.14006v1.pdf","comment":"2 Tables, 1 Figure, preprint submission to ACII 2024"},{"id":"http://arxiv.org/abs/2403.14001v1","updated":"2024-03-20T21:58:32Z","published":"2024-03-20T21:58:32Z","title":"Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained\n  Sentence Embeddings","summary":"  Sentence embeddings produced by Pretrained Language Models (PLMs) have\nreceived wide attention from the NLP community due to their superior\nperformance when representing texts in numerous downstream applications.\nHowever, the high dimensionality of the sentence embeddings produced by PLMs is\nproblematic when representing large numbers of sentences in memory- or\ncompute-constrained devices. As a solution, we evaluate unsupervised\ndimensionality reduction methods to reduce the dimensionality of sentence\nembeddings produced by PLMs. Our experimental results show that simple methods\nsuch as Principal Component Analysis (PCA) can reduce the dimensionality of\nsentence embeddings by almost $50\\%$, without incurring a significant loss in\nperformance in multiple downstream tasks. Surprisingly, reducing the\ndimensionality further improves performance over the original high-dimensional\nversions for the sentence embeddings produced by some PLMs in some tasks.\n","authors":["Gaifan Zhang","Yi Zhou","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2403.14001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04451v2","updated":"2024-03-20T21:34:56Z","published":"2023-10-03T19:44:37Z","title":"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language\n  Models","summary":"  The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.\n","authors":["Xiaogeng Liu","Nan Xu","Muhao Chen","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.04451v2.pdf","comment":"Published as a conference paper at ICLR 2024. Code is available at\n  https://github.com/SheltonLiu-N/AutoDAN"},{"id":"http://arxiv.org/abs/2310.19268v2","updated":"2024-03-20T21:24:33Z","published":"2023-10-30T05:03:26Z","title":"Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via\n  Social Commonsense and Linguistic Signals","summary":"  Machine ethics ensures ethical conduct in Artificial Intelligence (AI) models\nand agents. Examining real-life applications benefit learning practical ethics\nin many situations, offering valuable data to grasp the complexities of human\nethics in diverse contexts. In this paper, we examine social media platforms\nfor understanding real-life ethical scenarios and human moral judgments. We\nexamine posts from a popular Reddit subreddit (i.e., a subcommunity) called\nr/AmITheAsshole, where authors and commenters share their moral judgments on\nwho is blameworthy. We employ computational techniques to investigate the\nunderlying reasoning influencing moral judgments. We focus on excerpts-which we\nterm moral sparks-from original posts that commenters include to indicate what\nmotivates their judgments. To this end, we examine how (1) events activating\nsocial commonsense and (2) linguistic signals affect moral sparks assignment\nand their subsequent judgments. By examining over 24 672 posts and 175988\ncomments, we find that event-related negative character traits (e.g., immature\nand rude) attract attention and stimulate blame, implying a dependent\nrelationship between character traits and moral values. Specially, we focus on\ncausal graph involving events (c-events) that activate social commonsense. We\nobserve that c-events are perceived with varying levels of informativeness,\ninfluencing moral spark and judgment assignment in distinct ways. This\nobservation is reinforced by examining linguistic features describing\nsemantically similar c-events. Moreover, language influencing commenters'\ncognitive processes enhances the probability of an excerpt becoming a moral\nspark, while factual and concrete descriptions tend to inhibit this effect.\n","authors":["Ruijie Xi","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2310.19268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03378v3","updated":"2024-03-20T20:57:51Z","published":"2023-09-06T21:56:24Z","title":"RoDia: A New Dataset for Romanian Dialect Identification from Speech","summary":"  We introduce RoDia, the first dataset for Romanian dialect identification\nfrom speech. The RoDia dataset includes a varied compilation of speech samples\nfrom five distinct regions of Romania, covering both urban and rural\nenvironments, totaling 2 hours of manually annotated speech data. Along with\nour dataset, we introduce a set of competitive models to be used as baselines\nfor future research. The top scoring model achieves a macro F1 score of 59.83%\nand a micro F1 score of 62.08%, indicating that the task is challenging. We\nthus believe that RoDia is a valuable resource that will stimulate research\naiming to address the challenges of Romanian dialect identification. We release\nour dataset at https://github.com/codrut2/RoDia.\n","authors":["Codrut Rotaru","Nicolae-Catalin Ristea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2309.03378v3.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.05020v2","updated":"2024-03-20T20:44:17Z","published":"2024-03-08T03:49:17Z","title":"Is this the real life? Is this just fantasy? The Misleading Success of\n  Simulating Social Interactions With LLMs","summary":"  Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena with LLM-based\nagents. However, most work has used an omniscient perspective on these\nsimulations (e.g., single LLM to generate all interlocutors), which is\nfundamentally at odds with the non-omniscient, information asymmetric\ninteractions that humans have. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that interlocutors\nsimulated omnisciently are much more successful at accomplishing social goals\ncompared to non-omniscient agents, despite the latter being the more realistic\nsetting. Furthermore, we demonstrate that learning from omniscient simulations\nimproves the apparent naturalness of interactions but scarcely enhances goal\nachievement in cooperative scenarios. Our findings indicate that addressing\ninformation asymmetry remains a fundamental challenge for LLM-based agents.\n","authors":["Xuhui Zhou","Zhe Su","Tiwalayo Eisape","Hyunwoo Kim","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2403.05020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05342v4","updated":"2024-03-20T20:37:17Z","published":"2023-08-10T05:10:17Z","title":"Metacognitive Prompting Improves Understanding in Large Language Models","summary":"  In Large Language Models (LLMs), there have been consistent advancements in\ntask-specific performance, largely influenced by effective prompt design.\nRecent advancements in prompting have enhanced reasoning in logic-intensive\ntasks for LLMs, yet the nuanced understanding abilities of these models,\ncrucial for processing and interpreting complex information, remain\nunderexplored. In this study, we introduce Metacognitive Prompting (MP), a\nstrategy inspired by human introspective reasoning processes. Using MP, LLMs\nundergo a systematic series of structured, self-aware evaluations, drawing on\nboth their vast inherent knowledge and new insights. We conduct extensive\nexperiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across\nten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE,\nand LexGLUE benchmarks. Additionally, we compare our method with\nchain-of-thought prompting and its advanced versions. The results show that\nGPT-4 consistently excels across all tasks, while other models have shown\nsignificant progress in some tasks when used in conjunction with MP.\nFurthermore, MP consistently outperforms existing prompting methods in both\ngeneral and domain-specific NLU tasks. This study underscores the potential to\namplify the understanding abilities of LLMs and highlights the benefits of\nmirroring human introspective reasoning in NLU tasks.\n","authors":["Yuqing Wang","Yun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.05342v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2312.17244v2","updated":"2024-03-20T20:21:58Z","published":"2023-12-28T18:59:09Z","title":"The LLM Surgeon","summary":"  State-of-the-art language models are becoming increasingly large in an effort\nto achieve the highest performance on large corpora of available textual data.\nHowever, the sheer size of the Transformer architectures makes it difficult to\ndeploy models within computational, environmental or device-specific\nconstraints. We explore data-driven compression of existing pretrained models\nas an alternative to training smaller models from scratch. To do so, we scale\nKronecker-factored curvature approximations of the target loss landscape to\nlarge language models. In doing so, we can compute both the dynamic allocation\nof structures that can be removed as well as updates of remaining weights that\naccount for the removal. We provide a general framework for unstructured,\nsemi-structured and structured pruning and improve upon weight updates to\ncapture more correlations between weights, while remaining computationally\nefficient. Experimentally, our method can prune rows and columns from a range\nof OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,\nand achieve state-of-the-art results in unstructured and semi-structured\npruning of large language models.\n","authors":["Tycho F. A. van der Ouderaa","Markus Nagel","Mart van Baalen","Yuki M. Asano","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2312.17244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10735v2","updated":"2024-03-20T19:14:54Z","published":"2024-02-16T14:52:05Z","title":"Assessing the Reasoning Abilities of ChatGPT in the Context of Claim\n  Verification","summary":"  The reasoning capabilities of LLMs are currently hotly debated. We examine\nthe issue from the perspective of claim/rumour verification. We propose the\nfirst logical reasoning framework designed to break down any claim or rumour\npaired with evidence into the atomic reasoning steps necessary for\nverification. Based on our framework, we curate two annotated collections of\nsuch claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world\nset stemming from rumours circulating on Twitter. We use them to evaluate the\nreasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as\nChatGPT) within the context of our framework, providing a thorough analysis.\nOur results show that ChatGPT struggles in abductive reasoning, although this\ncan be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to\nZero-Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body\nof research suggesting that ChatGPT's reasoning processes are unlikely to\nmirror human-like reasoning, and that LLMs need to be more rigorously evaluated\nto distinguish between hype and actual capabilities, especially in high-stakes\nreal-world tasks such as claim verification.\n","authors":["John Dougrez-Lewis","Mahmud Elahi Akhter","Yulan He","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2402.10735v2.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2209.00568v2","updated":"2024-03-20T19:09:57Z","published":"2022-09-01T16:19:22Z","title":"Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal\n  Relation Extraction","summary":"  Event Temporal Relation Extraction (ETRE) is a crucial yet challenging\nproblem. Event pairs are situated within a discourse at different distances,\nwhich we refer to as proximity bands. The temporal ordering communicated about\nevent pairs situated at more remote (i.e., ``long'') or less remote (i.e.,\n``short'') proximity bands is encoded differently. SOTA ETRE models have tended\nto perform well on events situated at either short or long proximity bands, but\nnot both. Yet, real-world, natural texts contain all types of temporal\nevent-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge\nCo-Distillation, a fusion approach that shares knowledge across multiple event\npair proximity bands in order to improve performance on all types of temporal\ndatasets. Our experimental results show that MulCo successfully integrates\nlinguistic cues pertaining to temporal reasoning across both short and long\nproximity bands and achieves new state-of-the-art results on several ETRE\nbenchmark datasets.\n","authors":["Hao-Ren Yao","Luke Breitfeller","Aakanksha Naik","Chunxiao Zhou","Carolyn Rose"],"pdf_url":"https://arxiv.org/pdf/2209.00568v2.pdf","comment":"update"},{"id":"http://arxiv.org/abs/2307.03296v2","updated":"2024-03-20T19:08:06Z","published":"2023-07-06T21:10:50Z","title":"Gammatonegram Representation for End-to-End Dysarthric Speech Processing\n  Tasks: Speech Recognition, Speaker Identification, and Intelligibility\n  Assessment","summary":"  Dysarthria is a disability that causes a disturbance in the human speech\nsystem and reduces the quality and intelligibility of a person's speech.\nBecause of this effect, the normal speech processing systems can not work\nproperly on impaired speech. This disability is usually associated with\nphysical disabilities. Therefore, designing a system that can perform some\ntasks by receiving voice commands in the smart home can be a significant\nachievement. In this work, we introduce gammatonegram as an effective method to\nrepresent audio files with discriminative details, which is used as input for\nthe convolutional neural network. On the other word, we convert each speech\nfile into an image and propose image recognition system to classify speech in\ndifferent scenarios. Proposed CNN is based on the transfer learning method on\nthe pre-trained Alexnet. In this research, the efficiency of the proposed\nsystem for speech recognition, speaker identification, and intelligibility\nassessment is evaluated. According to the results on the UA dataset, the\nproposed speech recognition system achieved 91.29% accuracy in\nspeaker-dependent mode, the speaker identification system acquired 87.74%\naccuracy in text-dependent mode, and the intelligibility assessment system\nachieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network\nspeech recognition system that works fully automatically. This system is\nlocated in a cascade arrangement with the two-class intelligibility assessment\nsystem, and the output of this system activates each one of the speech\nrecognition networks. This architecture achieves an accuracy of 92.3% WRR. The\nsource code of this paper is available.\n","authors":["Aref Farhadipour","Hadi Veisi"],"pdf_url":"https://arxiv.org/pdf/2307.03296v2.pdf","comment":"12 pages, 8 figures. Iran J Comput Sci (2024)"},{"id":"http://arxiv.org/abs/2403.13925v1","updated":"2024-03-20T18:59:18Z","published":"2024-03-20T18:59:18Z","title":"Reducing Large Language Model Bias with Emphasis on 'Restricted\n  Industries': Automated Dataset Augmentation and Prejudice Quantification","summary":"  Despite the growing capabilities of large language models, there exists\nconcerns about the biases they develop. In this paper, we propose a novel,\nautomated mechanism for debiasing through specified dataset augmentation in the\nlens of bias producers and in the context of 'restricted industries' with\nlimited data. We additionally create two new additional metrics, the mb-index\nand db-index, to quantify bias, considering the idea that bias occurs due to\nboth intrinsic model architecture and dataset.\n","authors":["Devam Mondal","Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2403.13925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13922v1","updated":"2024-03-20T18:49:59Z","published":"2024-03-20T18:49:59Z","title":"Visually Grounded Speech Models have a Mutual Exclusivity Bias","summary":"  When children learn new words, they employ constraints such as the mutual\nexclusivity (ME) bias: a novel word is mapped to a novel object rather than a\nfamiliar one. This bias has been studied computationally, but only in models\nthat use discrete word representations as input, ignoring the high variability\nof spoken words. We investigate the ME bias in the context of visually grounded\nspeech models that learn from natural images and continuous speech audio.\nConcretely, we train a model on familiar words and test its ME bias by asking\nit to select between a novel and a familiar object when queried with a novel\nword. To simulate prior acoustic and visual knowledge, we experiment with\nseveral initialisation strategies using pretrained speech and vision networks.\nOur findings reveal the ME bias across the different initialisation approaches,\nwith a stronger bias in models with more prior (in particular, visual)\nknowledge. Additional tests confirm the robustness of our results, even when\ndifferent loss functions are considered.\n","authors":["Leanne Nortje","Dan Oneaţă","Yevgen Matusevych","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2403.13922v1.pdf","comment":"Accepted to TACL, pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2403.13903v1","updated":"2024-03-20T18:18:48Z","published":"2024-03-20T18:18:48Z","title":"Leveraging Linguistically Enhanced Embeddings for Open Information\n  Extraction","summary":"  Open Information Extraction (OIE) is a structured prediction (SP) task in\nNatural Language Processing (NLP) that aims to extract structured $n$-ary\ntuples - usually subject-relation-object triples - from free text. The word\nembeddings in the input text can be enhanced with linguistic features, usually\nPart-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However,\npast enhancement techniques cannot leverage the power of pretrained language\nmodels (PLMs), which themselves have been hardly used for OIE. To bridge this\ngap, we are the first to leverage linguistic features with a Seq2Seq PLM for\nOIE. We do so by introducing two methods - Weighted Addition and Linearized\nConcatenation. Our work can give any neural OIE architecture the key\nperformance boost from both PLMs and linguistic features in one go. In our\nsettings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on\nPrecision, Recall and F1 scores respectively over the baseline. Beyond this, we\naddress other important challenges in the field: to reduce compute overheads\nwith the features, we are the first ones to exploit Semantic Dependency Parse\n(SemDP) tags; to address flaws in current datasets, we create a clean synthetic\ndataset; finally, we contribute the first known study of OIE behaviour in SP\nmodels.\n","authors":["Fauzan Farooqui","Thanmay Jayakumar","Pulkit Mathur","Mansi Radke"],"pdf_url":"https://arxiv.org/pdf/2403.13903v1.pdf","comment":"Accepted at LREC-COLING 2024 Main Conference, Long Paper"},{"id":"http://arxiv.org/abs/2403.13901v1","updated":"2024-03-20T18:13:17Z","published":"2024-03-20T18:13:17Z","title":"Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases","summary":"  Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of tongue-twisters - a form of language that\nis required to be conditioned on a phoneme level to maximize sound overlap,\nwhilst maintaining semantic consistency with an input topic and still being\ngrammatically correct. We present TwisterLister, a pipeline for generating\nphonologically informed tongue-twisters from Large Language Models (LLMs) that\nwe use to generate TwistList 2.0, the largest annotated dataset of\ntongue-twisters to date, consisting of 17K+ examples from a combination of\nhuman and LLM authors. Our generation pipeline involves the use of a\nphonologically constrained vocabulary alongside LLM prompting to generate\nnovel, non-derivative tongue-twister examples. We additionally present the\nresults of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding\nmodule (PACD) that can be integrated into any causal language model and\ndemonstrate that this method generates good quality tongue-twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue-twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue-twisters based on Phonemic Edit Distance (PED).\n","authors":["Tyler Loakman","Chen Tang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13901v1.pdf","comment":"Submitted to Computational Linguistics"},{"id":"http://arxiv.org/abs/2402.17128v3","updated":"2024-03-20T14:49:16Z","published":"2024-02-27T01:48:19Z","title":"OSCaR: Object State Captioning and State Change Representation","summary":"  The capability of intelligent models to extrapolate and comprehend changes in\nobject states is a crucial yet demanding aspect of AI research, particularly\nthrough the lens of human interaction in real-world settings. This task\ninvolves describing complex visual environments, identifying active objects,\nand interpreting their changes as conveyed through language. Traditional\nmethods, which isolate object captioning and state change detection, offer a\nlimited view of dynamic environments. Moreover, relying on a small set of\nsymbolic words to represent changes has restricted the expressiveness of the\nlanguage. To address these challenges, in this paper, we introduce the Object\nState Captioning and State Change Representation (OSCaR) dataset and benchmark.\nOSCaR consists of 14,084 annotated video segments with nearly 1,000 unique\nobjects from various egocentric video collections. It sets a new testbed for\nevaluating multimodal large language models (MLLMs). Our experiments\ndemonstrate that while MLLMs show some skill, they lack a full understanding of\nobject state changes. The benchmark includes a fine-tuned model that, despite\ninitial capabilities, requires significant improvements in accuracy and\ngeneralization ability for effective understanding of these changes. Our code\nand dataset are available at https://github.com/nguyennm1024/OSCaR.\n","authors":["Nguyen Nguyen","Jing Bi","Ali Vosoughi","Yapeng Tian","Pooyan Fazli","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17128v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14003v1","updated":"2024-03-20T22:05:18Z","published":"2024-03-20T22:05:18Z","title":"Multi-Modal Hallucination Control by Visual Information Grounding","summary":"  Generative Vision-Language Models (VLMs) are prone to generate\nplausible-sounding textual answers that, however, are not always grounded in\nthe input image. We investigate this phenomenon, usually referred to as\n\"hallucination\" and show that it stems from an excessive reliance on the\nlanguage prior. In particular, we show that as more tokens are generated, the\nreliance on the visual prompt decreases, and this behavior strongly correlates\nwith the emergence of hallucinations. To reduce hallucinations, we introduce\nMulti-Modal Mutual-Information Decoding (M3ID), a new sampling method for\nprompt amplification. M3ID amplifies the influence of the reference image over\nthe language prior, hence favoring the generation of tokens with higher mutual\ninformation with the visual prompt. M3ID can be applied to any pre-trained\nautoregressive VLM at inference time without necessitating further training and\nwith minimal computational overhead. If training is an option, we show that\nM3ID can be paired with Direct Preference Optimization (DPO) to improve the\nmodel's reliance on the prompt image without requiring any labels. Our\nempirical findings show that our algorithms maintain the fluency and linguistic\ncapabilities of pre-trained VLMs while reducing hallucinations by mitigating\nvisually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and\nM3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by\n25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as\nPOPE by 21% and 24%.\n","authors":["Alessandro Favero","Luca Zancato","Matthew Trager","Siddharth Choudhary","Pramuditha Perera","Alessandro Achille","Ashwin Swaminathan","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2403.14003v1.pdf","comment":null}]},"2024-03-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2311.09206v2","updated":"2024-03-21T17:56:37Z","published":"2023-11-15T18:47:52Z","title":"TableLlama: Towards Open Large Generalist Models for Tables","summary":"  Semi-structured tables are ubiquitous. There has been a variety of tasks that\naim to automatically interpret, augment, and query tables. Current methods\noften require pretraining on tables or special model architecture design, are\nrestricted to specific table types, or have simplifying assumptions about\ntables and tasks. This paper makes the first step towards developing\nopen-source large language models (LLMs) as generalists for a diversity of\ntable-based tasks. Towards that end, we construct TableInstruct, a new dataset\nwith a variety of realistic tables and tasks, for instruction tuning and\nevaluating LLMs. We further develop the first open-source generalist model for\ntables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the\nlong context challenge. We experiment under both in-domain setting and\nout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves\ncomparable or better performance than the SOTA for each task, despite the\nlatter often has task-specific design. On 6 out-of-domain datasets, it achieves\n5-44 absolute point gains compared with the base model, showing that training\non TableInstruct enhances the model's generalizability. We open-source our\ndataset and trained model to boost future work on developing open generalist\nmodels for tables.\n","authors":["Tianshu Zhang","Xiang Yue","Yifei Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09206v2.pdf","comment":"NAACL 2024 long paper"},{"id":"http://arxiv.org/abs/2403.14589v1","updated":"2024-03-21T17:43:44Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14582v1","updated":"2024-03-21T17:36:08Z","published":"2024-03-21T17:36:08Z","title":"Large Language Models for Multi-Choice Question Classification of\n  Medical Subjects","summary":"  The aim of this paper is to evaluate whether large language models trained on\nmulti-choice question data can be used to discriminate between medical\nsubjects. This is an important and challenging task for automatic question\nanswering. To achieve this goal, we train deep neural networks for multi-class\nclassification of questions into the inferred medical subjects. Using our\nMulti-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art\nresults on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their\ndevelopment and test sets, respectively. In this sense, we show the capability\nof AI and LLMs in particular for multi-classification tasks in the Healthcare\ndomain.\n","authors":["Víctor Ponce-López"],"pdf_url":"https://arxiv.org/pdf/2403.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11085v3","updated":"2024-03-21T17:25:23Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14565v1","updated":"2024-03-21T17:09:08Z","published":"2024-03-21T17:09:08Z","title":"A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'\n  Formative Assessment Responses in Science","summary":"  This paper explores the use of large language models (LLMs) to score and\nexplain short-answer assessments in K-12 science. While existing methods can\nscore more structured math and computer science assessments, they often do not\nprovide explanations for the scores. Our study focuses on employing GPT-4 for\nautomated assessment in middle school Earth Science, combining few-shot and\nactive learning with chain-of-thought reasoning. Using a human-in-the-loop\napproach, we successfully score and provide meaningful explanations for\nformative assessment responses. A systematic analysis of our method's pros and\ncons sheds light on the potential for human-in-the-loop techniques to enhance\nautomated grading for open-ended science assessments.\n","authors":["Clayton Cohn","Nicole Hutchins","Tuan Le","Gautam Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.14565v1.pdf","comment":"In press at EAAI-24: The 14th Symposium on Educational Advances in\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2403.06563v2","updated":"2024-03-21T17:08:43Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14562v1","updated":"2024-03-21T17:06:17Z","published":"2024-03-21T17:06:17Z","title":"The Era of Semantic Decoding","summary":"  Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.\n","authors":["Maxime Peyrard","Martin Josifoski","Robert West"],"pdf_url":"https://arxiv.org/pdf/2403.14562v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14551v1","updated":"2024-03-21T16:52:01Z","published":"2024-03-21T16:52:01Z","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","summary":"  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2403.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14541v1","updated":"2024-03-21T16:41:12Z","published":"2024-03-21T16:41:12Z","title":"EDT: Improving Large Language Models' Generation by Entropy-based\n  Dynamic Temperature Sampling","summary":"  Recently, Large Language Models (LLMs) have demonstrated outstanding\nperformance across a wide range of downstream language tasks. Temperature\nsampling is a commonly used decoding strategy for LLMs' generation process.\nHowever, a fixed temperature parameter is used in most cases, which may not\nalways be an optimal choice for balancing generation quality and diversity. In\nthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)\nSampling method, to achieve a more balanced performance in terms of both\ngeneration quality and diversity by dynamically selecting the temperature\nparameter. Additionally, we also show model performance and comprehensive\nanalyses for 4 different generation benchmarks. Our experiments show that EDT\nsignificantly outperforms the existing strategies across different tasks.\n","authors":["Shimao Zhang","Yu Bao","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2403.14541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14515v1","updated":"2024-03-21T16:11:44Z","published":"2024-03-21T16:11:44Z","title":"Building a Language-Learning Game for Brazilian Indigenous Languages: A\n  Case of Study","summary":"  In this paper we discuss a first attempt to build a language learning game\nfor brazilian indigenous languages and the challenges around it. We present a\ndesign for the tool with gamification aspects. Then we describe a process to\nautomatically generate language exercises and questions from a dependency\ntreebank and a lexical database for Tupian languages. We discuss the\nlimitations of our prototype highlighting ethical and practical implementation\nconcerns. Finally, we conclude that new data gathering processes should be\nestablished in partnership with indigenous communities and oriented for\neducational purposes.\n","authors":["Gustavo Polleti"],"pdf_url":"https://arxiv.org/pdf/2403.14515v1.pdf","comment":"First Workshop on NLP for Indigenous Languages of Lusophone\n  Countries, 16th International Conference on Computational Processing of\n  Portuguese (PROPOR 2024)"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2309.11566v2","updated":"2024-03-21T15:31:45Z","published":"2023-09-20T18:08:28Z","title":"SignBank+: Preparing a Multilingual Sign Language Dataset for Machine\n  Translation Using Large Language Models","summary":"  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n","authors":["Amit Moryossef","Zifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2309.11566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2403.14469v1","updated":"2024-03-21T15:16:50Z","published":"2024-03-21T15:16:50Z","title":"ChatGPT Alternative Solutions: Large Language Models Survey","summary":"  In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.\n","authors":["Hanieh Alipour","Nick Pendar","Kohinoor Roy"],"pdf_url":"https://arxiv.org/pdf/2403.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14467v1","updated":"2024-03-21T15:14:25Z","published":"2024-03-21T15:14:25Z","title":"Recourse for reclamation: Chatting with generative language models","summary":"  Researchers and developers increasingly rely on toxicity scoring to moderate\ngenerative language model outputs, in settings such as customer service,\ninformation retrieval, and content generation. However, toxicity scoring may\nrender pertinent information inaccessible, rigidify or \"value-lock\" cultural\nnorms, and prevent language reclamation processes, particularly for\nmarginalized people. In this work, we extend the concept of algorithmic\nrecourse to generative language models: we provide users a novel mechanism to\nachieve their desired prediction by dynamically setting thresholds for toxicity\nfiltering. Users thereby exercise increased agency relative to interactions\nwith the baseline system. A pilot study ($n = 30$) supports the potential of\nour proposed recourse mechanism, indicating improvements in usability compared\nto fixed-threshold toxicity-filtering of model outputs. Future work should\nexplore the intersection of toxicity scoring, model controllability, user\nagency, and language reclamation processes -- particularly with regard to the\nbias that many communities encounter when interacting with generative language\nmodels.\n","authors":["Jennifer Chien","Kevin R. McKee","Jackie Kay","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2403.14467v1.pdf","comment":"Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA 2024)"},{"id":"http://arxiv.org/abs/2403.14460v1","updated":"2024-03-21T15:07:57Z","published":"2024-03-21T15:07:57Z","title":"Towards Single-System Illusion in Software-Defined Vehicles --\n  Automated, AI-Powered Workflow","summary":"  We propose a novel model- and feature-based approach to development of\nvehicle software systems, where the end architecture is not explicitly defined.\nInstead, it emerges from an iterative process of search and optimization given\ncertain constraints, requirements and hardware architecture, while retaining\nthe property of single-system illusion, where applications run in a logically\nuniform environment. One of the key points of the presented approach is the\ninclusion of modern generative AI, specifically Large Language Models (LLMs),\nin the loop. With the recent advances in the field, we expect that the LLMs\nwill be able to assist in processing of requirements, generation of formal\nsystem models, as well as generation of software deployment specification and\ntest code. The resulting pipeline is automated to a large extent, with feedback\nbeing generated at each step.\n","authors":["Krzysztof Lebioda","Viktor Vorobev","Nenad Petrovic","Fengjunjie Pan","Vahid Zolfaghari","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.14460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14459v1","updated":"2024-03-21T15:06:14Z","published":"2024-03-21T15:06:14Z","title":"Multi-Level Explanations for Generative Language Models","summary":"  Perturbation-based explanation methods such as LIME and SHAP are commonly\napplied to text classification. This work focuses on their extension to\ngenerative language models. To address the challenges of text as output and\nlong text inputs, we propose a general framework called MExGen that can be\ninstantiated with different attribution algorithms. To handle text output, we\nintroduce the notion of scalarizers for mapping text to real numbers and\ninvestigate multiple possibilities. To handle long inputs, we take a\nmulti-level approach, proceeding from coarser levels of granularity to finer\nones, and focus on algorithms with linear scaling in model queries. We conduct\na systematic evaluation, both automated and human, of perturbation-based\nattribution methods for summarization and context-grounded question answering.\nThe results show that our framework can provide more locally faithful\nexplanations of generated outputs.\n","authors":["Lucas Monteiro Paes","Dennis Wei","Hyo Jin Do","Hendrik Strobelt","Ronny Luss","Amit Dhurandhar","Manish Nagireddy","Karthikeyan Natesan Ramamurthy","Prasanna Sattigeri","Werner Geyer","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.14459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.14454v1","updated":"2024-03-21T15:02:03Z","published":"2024-03-21T15:02:03Z","title":"Prediction of Translation Techniques for the Translation Process","summary":"  Machine translation (MT) encompasses a variety of methodologies aimed at\nenhancing the accuracy of translations. In contrast, the process of\nhuman-generated translation relies on a wide range of translation techniques,\nwhich are crucial for ensuring linguistic adequacy and fluency. This study\nsuggests that these translation techniques could further optimize machine\ntranslation if they are automatically identified before being applied to guide\nthe translation process effectively. The study differentiates between two\nscenarios of the translation process: from-scratch translation and\npost-editing. For each scenario, a specific set of experiments has been\ndesigned to forecast the most appropriate translation techniques. The findings\nindicate that the predictive accuracy for from-scratch translation reaches 82%,\nwhile the post-editing process exhibits even greater potential, achieving an\naccuracy rate of 93%.\n","authors":["Fan Zhou","Vincent Vandeghinste"],"pdf_url":"https://arxiv.org/pdf/2403.14454v1.pdf","comment":"11 pages, 6 figures, conference"},{"id":"http://arxiv.org/abs/2403.14444v1","updated":"2024-03-21T14:51:51Z","published":"2024-03-21T14:51:51Z","title":"More than Just Statistical Recurrence: Human and Machine Unsupervised\n  Learning of Māori Word Segmentation across Morphological Processes","summary":"  Non-M\\=aori-speaking New Zealanders (NMS)are able to segment M\\=aori words in\na highlysimilar way to fluent speakers (Panther et al.,2024). This ability is\nassumed to derive through the identification and extraction of statistically\nrecurrent forms. We examine this assumption by asking how NMS segmentations\ncompare to those produced by Morfessor, an unsupervised machine learning model\nthat operates based on statistical recurrence, across words formed by a variety\nof morphological processes. Both NMS and Morfessor succeed in segmenting words\nformed by concatenative processes (compounding and affixation without\nallomorphy), but NMS also succeed for words that invoke templates\n(reduplication and allomorphy) and other cues to morphological structure,\nimplying that their learning process is sensitive to more than just statistical\nrecurrence.\n","authors":["Ashvini Varatharaj","Simon Todd"],"pdf_url":"https://arxiv.org/pdf/2403.14444v1.pdf","comment":"10 pages, 1 Figure, 2 tables"},{"id":"http://arxiv.org/abs/2403.10882v2","updated":"2024-03-21T14:50:18Z","published":"2024-03-16T10:26:38Z","title":"Optimizing Language Augmentation for Multilingual Large Language Models:\n  A Case Study on Korean","summary":"  Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.\n","authors":["ChangSu Choi","Yongbin Jeong","Seoyoon Park","InHo Won","HyeonSeok Lim","SangMin Kim","Yejee Kang","Chanhyuk Yoon","Jaewan Park","Yiseul Lee","HyeJin Lee","Younggyun Hahm","Hansaem Kim","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.10882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14438v1","updated":"2024-03-21T14:44:03Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wager","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.14427v1","updated":"2024-03-21T14:33:34Z","published":"2024-03-21T14:33:34Z","title":"Emergent communication and learning pressures in language models: a\n  language evolution perspective","summary":"  Language models and humans are two types of learning systems. Finding or\nfacilitating commonalities could enable major breakthroughs in our\nunderstanding of the acquisition and evolution of language. Many theories of\nlanguage evolution rely heavily on learning biases and learning pressures. Yet\ndue to substantial differences in learning pressures, it is questionable\nwhether the similarity between humans and machines is sufficient for insights\nto carry over and to be worth testing with human participants. Here, we review\nthe emergent communication literature, a subfield of multi-agent reinforcement\nlearning, from a language evolution perspective. We find that the emergent\ncommunication literature excels at designing and adapting models to recover\ninitially absent linguistic phenomena of natural languages. Based on a short\nliterature review, we identify key pressures that have recovered initially\nabsent human patterns in emergent communication models: communicative success,\nefficiency, learnability, and other psycho-/sociolinguistic factors. We argue\nthat this may serve as inspiration for how to design language models for\nlanguage acquisition and language evolution research.\n","authors":["Lukas Galke","Limor Raviv"],"pdf_url":"https://arxiv.org/pdf/2403.14427v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.14409v1","updated":"2024-03-21T13:57:43Z","published":"2024-03-21T13:57:43Z","title":"Locating and Mitigating Gender Bias in Large Language Models","summary":"  Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14409v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.14403v1","updated":"2024-03-21T13:52:30Z","published":"2024-03-21T13:52:30Z","title":"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n  Models through Question Complexity","summary":"  Retrieval-Augmented Large Language Models (LLMs), which incorporate the\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\nas a promising approach to enhancing response accuracy in several tasks, such\nas Question-Answering (QA). However, even though there are various approaches\ndealing with queries of different complexities, they either handle simple\nqueries with unnecessary computational overhead or fail to adequately address\ncomplex multi-step queries; yet, not all user requests fall into only one of\nthe simple or complex categories. In this work, we propose a novel adaptive QA\nframework, that can dynamically select the most suitable strategy for\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\nbased on the query complexity. Also, this selection process is operationalized\nwith a classifier, which is a smaller LM trained to predict the complexity\nlevel of incoming queries with automatically collected labels, obtained from\nactual predicted outcomes of models and inherent inductive biases in datasets.\nThis approach offers a balanced strategy, seamlessly adapting between the\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\nmethods, in response to a range of query complexities. We validate our model on\na set of open-domain QA datasets, covering multiple query complexities, and\nshow that ours enhances the overall efficiency and accuracy of QA systems,\ncompared to relevant baselines including the adaptive retrieval approaches.\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\n","authors":["Soyeong Jeong","Jinheon Baek","Sukmin Cho","Sung Ju Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2403.14403v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14402v1","updated":"2024-03-21T13:52:17Z","published":"2024-03-21T13:52:17Z","title":"XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for\n  Noise-Robust Speech Perception","summary":"  Speech recognition and translation systems perform poorly on noisy inputs,\nwhich are frequent in realistic environments. Augmenting these systems with\nvisual signals has the potential to improve robustness to noise. However,\naudio-visual (AV) data is only available in limited amounts and for fewer\nlanguages than audio-only resources. To address this gap, we present XLAVS-R, a\ncross-lingual audio-visual speech representation model for noise-robust speech\nrecognition and translation in over 100 languages. It is designed to maximize\nthe benefits of limited multilingual AV pre-training data, by building on top\nof audio-only multilingual pre-training and simplifying existing pre-training\nschemes. Extensive evaluation on the MuAViC benchmark shows the strength of\nXLAVS-R on downstream audio-visual speech recognition and translation tasks,\nwhere it outperforms the previous state of the art by up to 18.5% WER and 4.7\nBLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability\nwith audio-only fine-tuning.\n","authors":["HyoJung Han","Mohamed Anwar","Juan Pino","Wei-Ning Hsu","Marine Carpuat","Bowen Shi","Changhan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14399v1","updated":"2024-03-21T13:47:40Z","published":"2024-03-21T13:47:40Z","title":"Building Accurate Translation-Tailored LLMs with Language Aware\n  Instruction Tuning","summary":"  Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.\n","authors":["Changtong Zan","Liang Ding","Li Shen","Yibing Zhen","Weifeng Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11259v2","updated":"2024-03-21T13:41:35Z","published":"2023-09-20T12:35:19Z","title":"Sequence-to-Sequence Spanish Pre-trained Language Models","summary":"  In recent years, significant advancements in pre-trained language models have\ndriven the creation of numerous non-English language variants, with a\nparticular emphasis on encoder-only and decoder-only architectures. While\nSpanish language models based on BERT and GPT have demonstrated proficiency in\nnatural language understanding and generation, there remains a noticeable\nscarcity of encoder-decoder models explicitly designed for sequence-to-sequence\ntasks, which aim to map input sequences to generate output sequences\nconditionally. This paper breaks new ground by introducing the implementation\nand evaluation of renowned encoder-decoder architectures exclusively\npre-trained on Spanish corpora. Specifically, we present Spanish versions of\nBART, T5, and BERT2BERT-style models and subject them to a comprehensive\nassessment across various sequence-to-sequence tasks, including summarization,\nquestion answering, split-and-rephrase, dialogue, and translation. Our findings\nunderscore the competitive performance of all models, with the BART- and\nT5-based models emerging as top performers across all tasks. We have made all\nmodels publicly available to the research community to foster future\nexplorations and advancements in Spanish NLP:\nhttps://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.\n","authors":["Vladimir Araujo","Maria Mihaela Trusca","Rodrigo Tufiño","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2309.11259v2.pdf","comment":"Accepted paper at LREC-Coling2024"},{"id":"http://arxiv.org/abs/2306.00618v2","updated":"2024-03-21T13:37:23Z","published":"2023-06-01T12:44:33Z","title":"Effective Structured Prompting by Meta-Learning and Representative\n  Verbalizer","summary":"  Prompt tuning for pre-trained masked language models (MLM) has shown\npromising performance in natural language processing tasks with few labeled\nexamples. It tunes a prompt for the downstream task, and a verbalizer is used\nto bridge the predicted token and label prediction. Due to the limited training\ndata, prompt initialization is crucial for prompt tuning. Recently,\nMetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared\ninitialization for all task-specific prompts. However, a single initialization\nis insufficient to obtain good prompts for all tasks and samples when the tasks\nare complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a\nheavy burden on computation and memory as the MLM is usually large. To address\nthese issues, we use a prompt pool to extract more task knowledge and construct\ninstance-dependent prompts via attention. We further propose a novel soft\nverbalizer (RepVerb) which constructs label embedding from feature embeddings\ndirectly. Combining meta-learning the prompt pool and RepVerb, we propose\nMetaPrompter for effective structured prompting. MetaPrompter is\nparameter-efficient as only the pool is required to be tuned. Experimental\nresults demonstrate that MetaPrompter performs better than the recent\nstate-of-the-arts and RepVerb outperforms existing soft verbalizers.\n","authors":["Weisen Jiang","Yu Zhang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2306.00618v2.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2403.14390v1","updated":"2024-03-21T13:29:54Z","published":"2024-03-21T13:29:54Z","title":"From Large to Tiny: Distilling and Refining Mathematical Expertise for\n  Math Word Problems with Weakly Supervision","summary":"  Addressing the challenge of high annotation costs in solving Math Word\nProblems (MWPs) through full supervision with intermediate equations, recent\nworks have proposed weakly supervised task settings that rely solely on the\nfinal answer as a supervised signal. Existing leading approaches typically\nemploy various search techniques to infer intermediate equations, but cannot\nensure their semantic consistency with natural language descriptions. The rise\nof Large Language Models (LLMs) like ChatGPT has opened up new possibilities\nfor addressing MWPs directly. However, the computational demands of LLMs make\nthem less than ideal for use in settings where resources are tight. In light of\nthese challenges, we introduce an innovative two-stage framework that adeptly\ntransfers mathematical Expertise from large to tiny language models. In\n\\emph{Distillation Stage}, we propose a series of extraction processes that\nsatisfy the properties of MWPs to distill mathematical knowledge from LLMs to\nconstruct problem-equation pairs required for supervised training. In\n\\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee\nthe full utilization of all data, we further utilize the unsuccessfully\nsearched data effectively by Knowledge Refine method. Finally, We train a small\nmodel using distilled data generated through two-stage methods. As our method\nfully leverages the semantic understanding capabilities during the searching\n'problem-equation' pair, it demonstrates significantly improved performance on\nthe Math23K and Weak12K datasets compared to existing small model methods,\nwhile maintaining a much lower computational cost than ChatGPT.\n","authors":["Qingwen Lin","Boyan Xu","Zhengting Huang","Ruichu Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14381v1","updated":"2024-03-21T13:15:25Z","published":"2024-03-21T13:15:25Z","title":"Editing Knowledge Representation of Language Lodel via Rephrased Prefix\n  Prompts","summary":"  Neural language models (LMs) have been extensively trained on vast corpora to\nstore factual knowledge about various aspects of the world described in texts.\nCurrent technologies typically employ knowledge editing methods or specific\nprompts to modify LM outputs. However, existing knowledge editing methods are\ncostly and inefficient, struggling to produce appropriate text. Additionally,\nprompt engineering is opaque and requires significant effort to find suitable\nprompts. To address these issues, we introduce a new method called PSPEM\n(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just\none training. It resolves the inefficiencies and generalizability issues in\nknowledge editing methods and overcomes the opacity of prompt engineering by\nautomatically seeking optimal soft prompts. Specifically, PSPEM utilizes a\nprompt encoder and an encoding converter to refine key information in prompts\nand uses prompt alignment techniques to guide model generation, ensuring text\nconsistency and adherence to the intended structure and content, thereby\nmaintaining an optimal balance between efficiency and accuracy. We have\nvalidated the effectiveness of PSPEM through knowledge editing and attribute\ninserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing\naccuracy and demonstrated the highest level of fluency. We further analyzed the\nsimilarities between PSPEM and original prompts and their impact on the model's\ninternals. The results indicate that PSPEM can serve as an alternative to\noriginal prompts, supporting the model in effective editing.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14381v1.pdf","comment":"19pages,3figures"},{"id":"http://arxiv.org/abs/2403.14374v1","updated":"2024-03-21T13:05:18Z","published":"2024-03-21T13:05:18Z","title":"FIT-RAG: Black-Box RAG with Factual Information and Token Reduction","summary":"  Due to the extraordinarily large number of parameters, fine-tuning Large\nLanguage Models (LLMs) to update long-tail or out-of-date knowledge is\nimpractical in lots of applications. To avoid fine-tuning, we can alternatively\ntreat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment\nit with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.\nRecently, black-box RAG has achieved success in knowledge-intensive tasks and\nhas gained much attention. Existing black-box RAG methods typically fine-tune\nthe retriever to cater to LLMs' preferences and concatenate all the retrieved\ndocuments as the input, which suffers from two issues: (1) Ignorance of Factual\nInformation. The LLM preferred documents may not contain the factual\ninformation for the given question, which can mislead the retriever and hurt\nthe effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating\nall the retrieved documents brings large amounts of unnecessary tokens for\nLLMs, which degenerates the efficiency of black-box RAG. To address these\nissues, this paper proposes a novel black-box RAG framework which utilizes the\nfactual information in the retrieval and reduces the number of tokens for\naugmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by\nconstructing a bi-label document scorer. Besides, it reduces the tokens by\nintroducing a self-knowledge recognizer and a sub-document-level token reducer.\nFIT-RAG achieves both superior effectiveness and efficiency, which is validated\nby extensive experiments across three open-domain question-answering datasets:\nTriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of\nLlama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA,\nrespectively. Furthermore, it can save approximately half of the tokens on\naverage across the three datasets.\n","authors":["Yuren Mao","Xuemei Dong","Wenyi Xu","Yunjun Gao","Bin Wei","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14364v1","updated":"2024-03-21T12:45:12Z","published":"2024-03-21T12:45:12Z","title":"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for\n  Atomic Factual Knowledge Update in Causal Language Models","summary":"  The factuality of large language model (LLMs) tends to decay over time since\nevents posterior to their training are \"unknown\" to them. One way to keep\nmodels up-to-date could be factual update: the task of inserting, replacing, or\nremoving certain simple (atomic) facts within the model. To study this task, we\npresent WikiFactDiff, a dataset that describes the evolution of factual\nknowledge between two dates as a collection of simple facts divided into three\ncategories: new, obsolete, and static. We describe several update scenarios\narising from various combinations of these three types of basic update. The\nfacts are represented by subject-relation-object triples; indeed, WikiFactDiff\nwas constructed by comparing the state of the Wikidata knowledge base at 4\nJanuary 2021 and 27 February 2023. Those fact are accompanied by verbalization\ntemplates and cloze tests that enable running update algorithms and their\nevaluation metrics. Contrary to other datasets, such as zsRE and CounterFact,\nWikiFactDiff constitutes a realistic update setting that involves various\nupdate scenarios, including replacements, archival, and new entity insertions.\nWe also present an evaluation of existing update algorithms on WikiFactDiff.\n","authors":["Hichem Ammar Khodja","Frédéric Béchet","Quentin Brabant","Alexis Nasr","Gwénolé Lecorvé"],"pdf_url":"https://arxiv.org/pdf/2403.14364v1.pdf","comment":"Accepted for publication at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14341v1","updated":"2024-03-21T12:17:59Z","published":"2024-03-21T12:17:59Z","title":"Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial\n  Narratives","summary":"  In this paper, we introduce the Financial-STS task, a financial\ndomain-specific NLP task designed to measure the nuanced semantic similarity\nbetween pairs of financial narratives. These narratives originate from the\nfinancial statements of the same company but correspond to different periods,\nsuch as year-over-year comparisons. Measuring the subtle semantic differences\nbetween these paired narratives enables market stakeholders to gauge changes\nover time in the company's financial and operational situations, which is\ncritical for financial decision-making. We find that existing pretrained\nembedding models and LLM embeddings fall short in discerning these subtle\nfinancial narrative shifts. To address this gap, we propose an LLM-augmented\npipeline specifically designed for the Financial-STS task. Evaluation on a\nhuman-annotated dataset demonstrates that our proposed method outperforms\nexisting methods trained on classic STS tasks and generic LLM embeddings.\n","authors":["Jiaxin Liu","Yi Yang","Kar Yan Tam"],"pdf_url":"https://arxiv.org/pdf/2403.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14339v1","updated":"2024-03-21T12:11:26Z","published":"2024-03-21T12:11:26Z","title":"$\\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning","summary":"  Machine Unlearning, the process of selectively eliminating the influence of\ncertain data examples used during a model's training, has gained significant\nattention as a means for practitioners to comply with recent data protection\nregulations. However, existing unlearning methods face critical drawbacks,\nincluding their prohibitively high cost, often associated with a large number\nof hyperparameters, and the limitation of forgetting only relatively small data\nportions. This often makes retraining the model from scratch a quicker and more\neffective solution. In this study, we introduce Gradient-based and\nTask-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework\ndesigned to remove the influence of a subset of training data efficiently. It\napplies adaptive gradient ascent to the data to be forgotten while using\nstandard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple\nbenefits over existing approaches. It enables the unlearning of large sections\nof the training dataset (up to 30%). It is versatile, supporting various\nunlearning tasks (such as subset forgetting or class removal) and applicable\nacross different domains (images, text, etc.). Importantly, $\\nabla \\tau$\nrequires no hyperparameter adjustments, making it a more appealing option than\nretraining the model from scratch. We evaluate our framework's effectiveness\nusing a set of well-established Membership Inference Attack metrics,\ndemonstrating up to 10% enhancements in performance compared to\nstate-of-the-art methods without compromising the original model's accuracy.\n","authors":["Daniel Trippa","Cesare Campagnano","Maria Sofia Bucarelli","Gabriele Tolomei","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2403.14339v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.14312v1","updated":"2024-03-21T11:34:26Z","published":"2024-03-21T11:34:26Z","title":"ChainLM: Empowering Large Language Models with Improved Chain-of-Thought\n  Prompting","summary":"  Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of\nlarge language models (LLMs), establishing itself as a primary approach to\nsolving complex reasoning tasks. Existing CoT synthesis approaches usually\nfocus on simpler reasoning tasks and thus result in low-quality and\ninconsistent CoT prompts. In response to this challenge, we present an\nempirical investigation of CoT prompting and introduce CoTGenius, a novel\nframework designed for the automatic generation of superior CoT prompts.\nCoTGenius is developed based on three major evolution strategies, i.e.,\ncomplicate, diversify, and specify-alongside two filtering mechanisms:\nevolutionary success judgement and correctness verification. We further employ\nCoTGenius to create an extensive CoT dataset, and subsequently fine-tune the\nLlama 2-Chat 7B and 13B models on this dataset. We call the resulting model\nChainLM. To deal with the cumulative error issue in reasoning steps, we propose\na step-level debating method, wherein multiple debaters discuss each reasoning\nstep to arrive at the correct answer. Extensive experiments demonstrate that\nour ChainLM models exhibit enhanced proficiency in addressing a spectrum of\ncomplex reasoning problems compared to existing models. In addition, we conduct\nan in-depth analysis of the impact of data categories within CoTGenius on the\nmodel performance. We release our dataset and code at\nhttps://github.com/RUCAIBox/ChainLM.\n","authors":["Xiaoxue Cheng","Junyi Li","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.14312v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.02622v2","updated":"2024-03-21T10:57:40Z","published":"2024-02-04T21:44:09Z","title":"DenseFormer: Enhancing Information Flow in Transformers via Depth\n  Weighted Averaging","summary":"  The transformer architecture by Vaswani et al. (2017) is now ubiquitous\nacross application domains, from natural language processing to speech\nprocessing and image understanding. We propose DenseFormer, a simple\nmodification to the standard architecture that improves the perplexity of the\nmodel without increasing its size -- adding a few thousand parameters for\nlarge-scale models in the 100B parameters range. Our approach relies on an\nadditional averaging step after each transformer block, which computes a\nweighted average of current and past representations -- we refer to this\noperation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit\ncoherent patterns of information flow, revealing the strong and structured\nreuse of activations from distant layers. Experiments demonstrate that\nDenseFormer is more data efficient, reaching the same perplexity of much deeper\ntransformer models, and that for the same perplexity, these new models\noutperform transformer baselines in terms of memory efficiency and inference\ntime.\n","authors":["Matteo Pagliardini","Amirkeivan Mohtashami","Francois Fleuret","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2402.02622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17918v2","updated":"2024-03-21T10:57:23Z","published":"2023-10-27T06:22:14Z","title":"Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection\n  Method","summary":"  Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.\n","authors":["Yukun Zhao","Lingyong Yan","Weiwei Sun","Guoliang Xing","Chong Meng","Shuaiqiang Wang","Zhicong Cheng","Zhaochun Ren","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.17918v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14275v1","updated":"2024-03-21T10:31:11Z","published":"2024-03-21T10:31:11Z","title":"Is Reference Necessary in the Evaluation of NLG Systems? When and Where?","summary":"  The majority of automatic metrics for evaluating NLG systems are\nreference-based. However, the challenge of collecting human annotation results\nin a lack of reliable references in numerous application scenarios. Despite\nrecent advancements in reference-free metrics, it has not been well understood\nwhen and where they can be used as an alternative to reference-based metrics.\nIn this study, by employing diverse analytical approaches, we comprehensively\nassess the performance of both metrics across a wide range of NLG tasks,\nencompassing eight datasets and eight evaluation models. Based on solid\nexperiments, the results show that reference-free metrics exhibit a higher\ncorrelation with human judgment and greater sensitivity to deficiencies in\nlanguage quality. However, their effectiveness varies across tasks and is\ninfluenced by the quality of candidate texts. Therefore, it's important to\nassess the performance of reference-free metrics before applying them to a new\ntask, especially when inputs are in uncommon form or when the answer space is\nhighly variable. Our study can provide insight into the appropriate application\nof automatic metrics and the impact of metric choice on evaluation performance.\n","authors":["Shuqian Sheng","Yi Xu","Luoyi Fu","Jiaxin Ding","Lei Zhou","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00862v2","updated":"2024-03-21T10:14:09Z","published":"2024-02-29T21:05:14Z","title":"NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and\n  Safety Adherence in Chinese Journalistic Editorial Applications","summary":"  This study presents NewsBench, a novel benchmark framework developed to\nevaluate the capability of Large Language Models (LLMs) in Chinese Journalistic\nWriting Proficiency (JWP) and their Safety Adherence (SA), addressing the gap\nbetween journalistic ethics and the risks associated with AI utilization.\nComprising 1,267 tasks across 5 editorial applications, 7 aspects (including\nsafety and journalistic writing with 4 detailed facets), and spanning 24 news\ntopics domains, NewsBench employs two GPT-4 based automatic evaluation\nprotocols validated by human assessment. Our comprehensive analysis of 10 LLMs\nhighlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative\ndeficiency in journalistic ethic adherence during creative writing tasks. These\nfindings underscore the need for enhanced ethical guidance in AI-generated\njournalistic content, marking a step forward in aligning AI capabilities with\njournalistic standards and safety considerations.\n","authors":["Miao Li","Ming-Bin Chen","Bo Tang","Shengbin Hou","Pengyu Wang","Haiying Deng","Zhiyu Li","Feiyu Xiong","Keming Mao","Peng Cheng","Yi Luo"],"pdf_url":"https://arxiv.org/pdf/2403.00862v2.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2307.06029v3","updated":"2024-03-21T09:46:31Z","published":"2023-07-12T09:23:41Z","title":"Pluggable Neural Machine Translation Models via Memory-augmented\n  Adapters","summary":"  Although neural machine translation (NMT) models perform well in the general\ndomain, it remains rather challenging to control their generation behavior to\nsatisfy the requirement of different users. Given the expensive training cost\nand the data scarcity challenge of learning a new model from scratch for each\nuser requirement, we propose a memory-augmented adapter to steer pretrained NMT\nmodels in a pluggable manner. Specifically, we construct a multi-granular\nmemory based on the user-provided text samples and propose a new adapter\narchitecture to combine the model representations and the retrieved results. We\nalso propose a training strategy using memory dropout to reduce spurious\ndependencies between the NMT model and the memory. We validate our approach on\nboth style- and domain-specific experiments and the results indicate that our\nmethod can outperform several representative pluggable baselines.\n","authors":["Yuzhuang Xu","Shuo Wang","Peng Li","Xuebo Liu","Xiaolong Wang","Weidong Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2307.06029v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14258v1","updated":"2024-03-21T09:36:36Z","published":"2024-03-21T09:36:36Z","title":"LLM-based Extraction of Contradictions from Patents","summary":"  Already since the 1950s TRIZ shows that patents and the technical\ncontradictions they solve are an important source of inspiration for the\ndevelopment of innovative products. However, TRIZ is a heuristic based on a\nhistoric patent analysis and does not make use of the ever-increasing number of\nlatest technological solutions in current patents. Because of the huge number\nof patents, their length, and, last but not least, their complexity there is a\nneed for modern patent retrieval and patent analysis to go beyond\nkeyword-oriented methods. Recent advances in patent retrieval and analysis\nmainly focus on dense vectors based on neural AI Transformer language models\nlike Google BERT. They are, for example, used for dense retrieval, question\nanswering or summarization and key concept extraction. A research focus within\nthe methods for patent summarization and key concept extraction are generic\ninventive concepts respectively TRIZ concepts like problems, solutions,\nadvantage of invention, parameters, and contradictions. Succeeding rule-based\napproaches, finetuned BERT-like language models for sentence-wise\nclassification represent the state-of-the-art of inventive concept extraction.\nWhile they work comparatively well for basic concepts like problems or\nsolutions, contradictions - as a more complex abstraction - remain a challenge\nfor these models. This paper goes one step further, as it presents a method to\nextract TRIZ contradictions from patent texts based on Prompt Engineering using\na generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction\ndetection, sentence extraction, contradiction summarization, parameter\nextraction and assignment to the 39 abstract TRIZ engineering parameters are\nall performed in a single prompt using the LangChain framework. Our results\nshow that \"off-the-shelf\" GPT-4 is a serious alternative to existing\napproaches.\n","authors":["Stefan Trapp","Joachim Warschat"],"pdf_url":"https://arxiv.org/pdf/2403.14258v1.pdf","comment":"10 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.14255v1","updated":"2024-03-21T09:28:38Z","published":"2024-03-21T09:28:38Z","title":"ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion\n  Classification","summary":"  Improving the accessibility of psychotherapy with the aid of Large Language\nModels (LLMs) is garnering a significant attention in recent years. Recognizing\ncognitive distortions from the interviewee's utterances can be an essential\npart of psychotherapy, especially for cognitive behavioral therapy. In this\npaper, we propose ERD, which improves LLM-based cognitive distortion\nclassification performance with the aid of additional modules of (1) extracting\nthe parts related to cognitive distortion, and (2) debating the reasoning steps\nby multiple agents. Our experimental results on a public dataset show that ERD\nimproves the multi-class F1 score as well as binary specificity score.\nRegarding the latter score, it turns out that our method is effective in\ndebiasing the baseline method which has high false positive rate, especially\nwhen the summary of multi-agent debate is provided to LLMs.\n","authors":["Sehee Lim","Yejin Kim","Chi-Hyun Choi","Jy-yong Sohn","Byung-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14253v1","updated":"2024-03-21T09:26:04Z","published":"2024-03-21T09:26:04Z","title":"K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional\n  Expression","summary":"  In many literary texts, emotions are indirectly conveyed through descriptions\nof actions, facial expressions, and appearances, necessitating emotion\ninference for narrative understanding. In this paper, we introduce K-Act2Emo, a\nKorean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional\nexpressions and the emotions inferable from them. We categorize reasoning types\ninto inferences in positive situations, inferences in negative situations, and\ninferences when expressions do not serve as emotional cues. Unlike existing\nCSKGs, K-Act2Emo specializes in emotional contexts, and experimental results\nvalidate its effectiveness for training emotion inference models.\nSignificantly, the BART-based knowledge model fine-tuned with K-Act2Emo\noutperforms various existing Korean large language models, achieving\nperformance levels comparable to GPT-4 Turbo.\n","authors":["Kyuhee Kim","Surin Lee","Sangah Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14253v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.14252v1","updated":"2024-03-21T09:25:24Z","published":"2024-03-21T09:25:24Z","title":"LayoutLLM: Large Language Model Instruction Tuning for Visually Rich\n  Document Understanding","summary":"  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n","authors":["Masato Fujitake"],"pdf_url":"https://arxiv.org/pdf/2403.14252v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2109.01048v3","updated":"2024-03-21T09:23:38Z","published":"2021-09-02T16:05:24Z","title":"Pre-training Language Model Incorporating Domain-specific Heterogeneous\n  Knowledge into A Unified Representation","summary":"  Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities, and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of\ntext, including unstructured text, semi-structured text, and well-structured\ntext. To capture the corresponding relations among these multi-format\nknowledge, our approach uses masked language model objective to learn word\nknowledge, uses triple classification objective and title matching objective to\nlearn entity knowledge and topic knowledge respectively. To obtain the\naforementioned multi-format text, we construct a corpus in the tourism domain\nand conduct experiments on 5 tourism NLP datasets. The results show that our\napproach outperforms the pre-training of plain text using only 1/4 of the data.\nWe further pre-train the domain-agnostic HKLM and achieve performance gains on\nthe XNLI dataset.\n","authors":["Hongyin Zhu","Hao Peng","Zhiheng Lyu","Lei Hou","Juanzi Li","Jinghui Xiao"],"pdf_url":"https://arxiv.org/pdf/2109.01048v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12022v2","updated":"2024-03-21T09:11:22Z","published":"2023-08-23T09:29:29Z","title":"Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by\n  List-Context Information","summary":"  Passage reranking is a critical task in various applications, particularly\nwhen dealing with large volumes of documents. Existing neural architectures\nhave limitations in retrieving the most relevant passage for a given question\nbecause the semantics of the segmented passages are often incomplete, and they\ntypically match the question to each passage individually, rarely considering\ncontextual information from other passages that could provide comparative and\nreference information. This paper presents a list-context attention mechanism\nto augment the passage representation by incorporating the list-context\ninformation from other candidates. The proposed coarse-to-fine (C2F) neural\nretriever addresses the out-of-memory limitation of the passage attention\nmechanism by dividing the list-context modeling process into two sub-processes\nwith a cache policy learning algorithm, enabling the efficient encoding of\ncontext information from a large number of candidate answers. This method can\nbe generally used to encode context information from any number of candidate\nanswers in one pass. Different from most multi-stage information retrieval\narchitectures, this model integrates the coarse and fine rankers into the joint\noptimization process, allowing for feedback between the two layers to update\nthe model simultaneously. Experiments demonstrate the effectiveness of the\nproposed approach.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2308.12022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08403v2","updated":"2024-03-21T09:02:26Z","published":"2024-02-13T12:04:43Z","title":"LLMs and the Human Condition","summary":"  This paper presents three established theories of human decision-making and\ndescribes how they can be integrated to provide a model of purposive human\naction. Taking seriously the idea of language as action the model is then\napplied to the conversational user interfaces. Theory based AI research has had\na hard time recently and the aim here is to revitalise interest in\nunderstanding what LLMs are actually doing other than running poorly understood\nmachine learning routines over all the data the relevant Big Tech company can\nhoover up. When a raspberry pi computer for under 50USD is up to 400 times\nfaster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech\ncan get really close to having an infinite number of monkeys typing at random\nand producing text, some of which will make sense. By understanding where\nChatGPT's apparent intelligence comes from, perhaps we can perform the magic\nwith fewer resources and at the same time gain some understanding about our\nrelationship with our world.\n","authors":["Peter Wallis"],"pdf_url":"https://arxiv.org/pdf/2402.08403v2.pdf","comment":"A 2nd draft with a better abstract and introduction. target is IVA in\n  2024"},{"id":"http://arxiv.org/abs/2403.14243v1","updated":"2024-03-21T09:02:17Z","published":"2024-03-21T09:02:17Z","title":"Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large\n  Language Models with Machine Learning in tele-dermatology","summary":"  The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.\n","authors":["Dimitrios P. Panagoulias","Evridiki Tsoureli-Nikita","Maria Virvou","George A. Tsihrintzis"],"pdf_url":"https://arxiv.org/pdf/2403.14243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14238v1","updated":"2024-03-21T08:57:27Z","published":"2024-03-21T08:57:27Z","title":"Reinforcement Learning from Reflective Feedback (RLRF): Aligning and\n  Improving LLMs via Fine-Grained Self-Reflection","summary":"  Despite the promise of RLHF in aligning LLMs with human preferences, it often\nleads to superficial alignment, prioritizing stylistic changes over improving\ndownstream performance of LLMs. Underspecified preferences could obscure\ndirections to align the models. Lacking exploration restricts identification of\ndesirable outputs to improve the models. To overcome these challenges, we\npropose a novel framework: Reinforcement Learning from Reflective Feedback\n(RLRF), which leverages fine-grained feedback based on detailed criteria to\nimprove the core capabilities of LLMs. RLRF employs a self-reflection mechanism\nto systematically explore and refine LLM responses, then fine-tuning the models\nvia a RL algorithm along with promising responses. Our experiments across\nJust-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and\ntransformative potential of RLRF beyond superficial surface-level adjustment.\n","authors":["Kyungjae Lee","Dasol Hwang","Sunghyun Park","Youngsoo Jang","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14238v1.pdf","comment":"22 pages, 5 figures, Submitted to ACL 2024"},{"id":"http://arxiv.org/abs/2403.14236v1","updated":"2024-03-21T08:54:24Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  Model editing is a growing area focused on updating the knowledge embedded\nwithin models. Among the various methodologies, ROME and MEMIT stand out as\nleading \"locate-and-edit\" model editing techniques. While MEMIT enables batched\nediting of memories, ROME is limited to changing one fact at a time. This paper\nintroduces a unifying framework that brings ROME and MEMIT under a single\nconceptual umbrella, optimizing for the same goal, which we call the\n\"preservation-memorization\" objective. This objective aims to preserve the\nrepresentations of certain selected vectors while memorizing the\nrepresentations of new factual information. Specifically, ROME optimizes this\nobjective using an equality constraint, whereas MEMIT employs a more flexible\nleast-square constraint. In addition to making batched edits, MEMIT also edits\nthe model at multiple layers. We disentangle the distribution of edits to\nmultiple layers from the optimization objective of MEMIT and show that these\nedit-distribution algorithms should be considered separate entities worthy of\ntheir own line of research.\n  Finally, we present EMMET - an Equality-constrained Mass Model Editing\nalgorithm for Transformers, a new batched memory-editing algorithm. With EMMET,\nwe present a closed form solution for the equality-constrained version of the\npreservation-memorization objective. We show that EMMET is able to perform\nbatched-edits on par with MEMIT up to a batch-size of 256 and discuss the\nchallenges in stabilizing EMMET. By articulating the \"locate-and-edit\" model\nediting algorithms under a simple conceptual framework of\n\"preservation-memorization\", we aim to bridge the gap between intuition and\nmathematics and hope to simplify the journey for future researchers in model\nediting.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14974v2","updated":"2024-03-21T08:50:44Z","published":"2023-09-25T09:21:25Z","title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts","summary":"  In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n","authors":["Thibault Clérice"],"pdf_url":"https://arxiv.org/pdf/2309.14974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v2","updated":"2024-03-21T08:36:39Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v2.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2310.16343v2","updated":"2024-03-21T08:29:35Z","published":"2023-10-25T03:58:49Z","title":"Evaluating, Understanding, and Improving Constrained Text Generation for\n  Large Language Models","summary":"  Advancements in natural language generation (NLG) and large language models\n(LLMs) have led to proficient text generation in various tasks. However,\nintegrating intricate constraints into neural text generation, due to LLMs'\nopacity, remains challenging. This study investigates constrained text\ngeneration for LLMs, where predefined constraints are applied during LLM's\ngeneration process. Our research mainly focuses on mainstream open-source LLMs,\ncategorizing constraints into lexical, structural, and relation-based types. We\nalso present various benchmarks to facilitate fair evaluation. The study\naddresses some key research questions, including evaluating, understanding and\nimproving constrained text generation for LLMs. Results illuminate LLMs'\ncapacity and deficiency to incorporate constraints and provide insights for\nfuture developments in constrained text generation. Codes and datasets will be\nreleased upon acceptance.\n","authors":["Xiang Chen","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2310.16343v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.14222v1","updated":"2024-03-21T08:22:44Z","published":"2024-03-21T08:22:44Z","title":"Large-Scale Label Interpretation Learning for Few-Shot Named Entity\n  Recognition","summary":"  Few-shot named entity recognition (NER) detects named entities within text\nusing only a few annotated examples. One promising line of research is to\nleverage natural language descriptions of each entity type: the common label\nPER might, for example, be verbalized as ''person entity.'' In an initial label\ninterpretation learning phase, the model learns to interpret such verbalized\ndescriptions of entity types. In a subsequent few-shot tagset extension phase,\nthis model is then given a description of a previously unseen entity type (such\nas ''music album'') and optionally a few training examples to perform few-shot\nNER for this type. In this paper, we systematically explore the impact of a\nstrong semantic prior to interpret verbalizations of new entity types by\nmassively scaling up the number and granularity of entity types used for label\ninterpretation learning. To this end, we leverage an entity linking benchmark\nto create a dataset with orders of magnitude of more distinct entity types and\ndescriptions as currently used datasets. We find that this increased signal\nyields strong results in zero- and few-shot NER in in-domain, cross-domain, and\neven cross-lingual settings. Our findings indicate significant potential for\nimproving few-shot NER through heuristical data-based optimization.\n","authors":["Jonas Golde","Felix Hamborg","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.14222v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.14221v1","updated":"2024-03-21T08:21:12Z","published":"2024-03-21T08:21:12Z","title":"Improving the Robustness of Large Language Models via Consistency\n  Alignment","summary":"  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n","authors":["Zhao Yukun","Yan Lingyong","Sun Weiwei","Xing Guoliang","Wang Shuaiqiang","Meng Chong","Cheng Zhicong","Ren Zhaochun","Yin Dawei"],"pdf_url":"https://arxiv.org/pdf/2403.14221v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14208v1","updated":"2024-03-21T08:00:05Z","published":"2024-03-21T08:00:05Z","title":"Automatic Annotation of Grammaticality in Child-Caregiver Conversations","summary":"  The acquisition of grammar has been a central question to adjudicate between\ntheories of language acquisition. In order to conduct faster, more\nreproducible, and larger-scale corpus studies on grammaticality in\nchild-caregiver conversations, tools for automatic annotation can offer an\neffective alternative to tedious manual annotation. We propose a coding scheme\nfor context-dependent grammaticality in child-caregiver conversations and\nannotate more than 4,000 utterances from a large corpus of transcribed\nconversations. Based on these annotations, we train and evaluate a range of NLP\nmodels. Our results show that fine-tuned Transformer-based models perform best,\nachieving human inter-annotation agreement levels.As a first application and\nsanity check of this tool, we use the trained models to annotate a corpus\nalmost two orders of magnitude larger than the manually annotated data and\nverify that children's grammaticality shows a steady increase with age.This\nwork contributes to the growing literature on applying state-of-the-art NLP\nmethods to help study child language acquisition at scale.\n","authors":["Mitja Nikolaus","Abhishek Agrawal","Petros Kaklamanis","Alex Warstadt","Abdellah Fourtassi"],"pdf_url":"https://arxiv.org/pdf/2403.14208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14197v1","updated":"2024-03-21T07:47:57Z","published":"2024-03-21T07:47:57Z","title":"Context Quality Matters in Training Fusion-in-Decoder for Extractive\n  Open-Domain Question Answering","summary":"  Retrieval-augmented generation models augment knowledge encoded in a language\nmodel by providing additional relevant external knowledge (context) during\ngeneration. Although it has been shown that the quantity and quality of context\nimpact the performance of retrieval-augmented generation models during\ninference, limited research explores how these characteristics affect model\ntraining. This paper explores how context quantity and quality during model\ntraining affect the performance of Fusion-in-Decoder (FiD), the\nstate-of-the-art retrieval-augmented generation model, in extractive\nopen-domain question answering tasks. Experimental results suggest that FiD\nmodels overfit to context quality during training and show suboptimal\nperformance when evaluated on different context quality. Through the\nexperimental results, we also reveal FiD models trained with different context\nquality have different cross-attention distribution patterns. Specifically, as\ncontext quality during training increases, FiD models tend to attend more\nuniformly to each passage in context. Finally, based on these observations, we\npropose a method to mitigate overfitting to specific context quality by\nintroducing bias to the cross-attention distribution, which we demonstrate to\nbe effective in improving the performance of FiD models on different context\nquality.\n","authors":["Kosuke Akimoto","Kunihiro Takeoka","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2403.14197v1.pdf","comment":"EMNLP Findings 2023"},{"id":"http://arxiv.org/abs/2312.04877v3","updated":"2024-03-21T07:22:54Z","published":"2023-12-08T07:27:26Z","title":"Generating Explanations to Understand and Repair Embedding-based Entity\n  Alignment","summary":"  Entity alignment (EA) seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based EA has gained marked success in\nrecent years, it lacks explanations for alignment decisions. In this paper, we\npresent the first framework that can generate explanations for understanding\nand repairing embedding-based EA results. Given an EA pair produced by an\nembedding model, we first compare its neighbor entities and relations to build\na matching subgraph as a local explanation. We then construct an alignment\ndependency graph to understand the pair from an abstract perspective. Finally,\nwe repair the pair by resolving three types of alignment conflicts based on\ndependency graphs. Experiments on a variety of EA datasets demonstrate the\neffectiveness, generalization, and robustness of our framework in explaining\nand repairing embedding-based EA results.\n","authors":["Xiaobin Tian","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2312.04877v3.pdf","comment":"Accepted in the 40th IEEE International Conference on Data\n  Engineering (ICDE 2024)"},{"id":"http://arxiv.org/abs/2403.09559v2","updated":"2024-03-21T06:51:16Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14171v1","updated":"2024-03-21T06:47:28Z","published":"2024-03-21T06:47:28Z","title":"MMIDR: Teaching Large Language Model to Interpret Multimodal\n  Misinformation via Knowledge Distillation","summary":"  Automatic detection of multimodal misinformation has gained a widespread\nattention recently. However, the potential of powerful Large Language Models\n(LLMs) for multimodal misinformation detection remains underexplored. Besides,\nhow to teach LLMs to interpret multimodal misinformation in cost-effective and\naccessible way is still an open question. To address that, we propose MMIDR, a\nframework designed to teach LLMs in providing fluent and high-quality textual\nexplanations for their decision-making process of multimodal misinformation. To\nconvert multimodal misinformation into an appropriate instruction-following\nformat, we present a data augmentation perspective and pipeline. This pipeline\nconsists of a visual information processing module and an evidence retrieval\nmodule. Subsequently, we prompt the proprietary LLMs with processed contents to\nextract rationales for interpreting the authenticity of multimodal\nmisinformation. Furthermore, we design an efficient knowledge distillation\napproach to distill the capability of proprietary LLMs in explaining multimodal\nmisinformation into open-source LLMs. To explore several research questions\nregarding the performance of LLMs in multimodal misinformation detection tasks,\nwe construct an instruction-following multimodal misinformation dataset and\nconduct comprehensive experiments. The experimental findings reveal that our\nMMIDR exhibits sufficient detection performance and possesses the capacity to\nprovide compelling rationales to support its assessments.\n","authors":["Longzheng Wang","Xiaohan Xu","Lei Zhang","Jiarui Lu","Yongxiu Xu","Hongbo Xu","Chuang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14171v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14168v1","updated":"2024-03-21T06:43:59Z","published":"2024-03-21T06:43:59Z","title":"M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual\n  Academic Lecture Dataset","summary":"  Publishing open-source academic video recordings is an emergent and prevalent\napproach to sharing knowledge online. Such videos carry rich multimodal\ninformation including speech, the facial and body movements of the speakers, as\nwell as the texts and pictures in the slides and possibly even the papers.\nAlthough multiple academic video datasets have been constructed and released,\nfew of them support both multimodal content recognition and understanding\ntasks, which is partially due to the lack of high-quality human annotations. In\nthis paper, we propose a novel multimodal, multigenre, and multipurpose\naudio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of\nvideos from five sources covering computer science, mathematics, and medical\nand biology topics. With high-quality human annotations of the spoken and\nwritten words, in particular high-valued name entities, the dataset can be used\nfor multiple audio-visual recognition and understanding tasks. Evaluations\nperformed on contextual speech recognition, speech synthesis, and slide and\nscript generation tasks demonstrate that the diversity of M$^3$AV makes it a\nchallenging dataset.\n","authors":["Zhe Chen","Heyang Liu","Wenyi Yu","Guangzhi Sun","Hongcheng Liu","Ji Wu","Chao Zhang","Yu Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11145v2","updated":"2024-03-21T06:22:56Z","published":"2024-03-17T08:51:01Z","title":"A Challenge Dataset and Effective Models for Conversational Stance\n  Detection","summary":"  Previous stance detection studies typically concentrate on evaluating stances\nwithin individual instances, thereby exhibiting limitations in effectively\nmodeling multi-party discussions concerning the same specific topic, as\nnaturally transpire in authentic social media interactions. This constraint\narises primarily due to the scarcity of datasets that authentically replicate\nreal social media contexts, hindering the research progress of conversational\nstance detection. In this paper, we introduce a new multi-turn conversation\nstance detection dataset (called \\textbf{MT-CSD}), which encompasses multiple\ntargets for conversational stance detection. To derive stances from this\nchallenging dataset, we propose a global-local attention network\n(\\textbf{GLAN}) to address both long and short-range dependencies inherent in\nconversational data. Notably, even state-of-the-art stance detection methods,\nexemplified by GLAN, exhibit an accuracy of only 50.47\\%, highlighting the\npersistent challenges in conversational stance detection. Furthermore, our\nMT-CSD dataset serves as a valuable resource to catalyze advancements in\ncross-domain stance detection, where a classifier is adapted from a different\nyet related target. We believe that MT-CSD will contribute to advancing\nreal-world applications of stance detection research. Our source code, data,\nand models are available at \\url{https://github.com/nfq729/MT-CSD}.\n","authors":["Fuqiang Niu","Min Yang","Ang Li","Baoquan Zhang","Xiaojiang Peng","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12373v2","updated":"2024-03-21T06:01:48Z","published":"2024-03-19T02:34:18Z","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners","summary":"  Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Traditional\napproaches to mitigate these errors involve human or tool-based feedback, such\nas employing task-specific verifiers or aggregating multiple reasoning paths.\nThese methods, however, either depend heavily on human input or struggle with\ninconsistent responses. To overcome these limitations, we present RankPrompt,\nan innovative prompting strategy that empowers LLMs to autonomously rank their\nresponses without needing extra resources. RankPrompt simplifies the ranking\nchallenge into comparative evaluations among different responses, leveraging\nLLMs' innate ability to generate comparative examples within context. Our\nexperiments across 11 arithmetic and commonsense reasoning tasks show that\nRankPrompt significantly enhances the reasoning performance of ChatGPT and\nGPT-4, with improvements of up to 13%. Furthermore, RankPrompt shows\nexceptional performance in LLM-based automatic evaluations for open-ended\ntasks, matching human judgments 74% of the time in the AlpacaEval dataset. It\nalso proves to be robust against changes in response order and inconsistency.\nOverall, our findings endorse RankPrompt as an effective method for extracting\nhigh-quality feedback directly from language models.\n","authors":["Chi Hu","Yuan Ge","Xiangnan Ma","Hang Cao","Qiang Li","Yonghua Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12373v2.pdf","comment":"LREC-Coling 2024 Long Paper"},{"id":"http://arxiv.org/abs/2402.03848v3","updated":"2024-03-21T05:58:10Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, 6 different GLLMs and 3\ndifferent prompting methods using the ANLS* metric is also provided,\ndemonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,\nSFT outperforms other techniques and improves the state-of-the-art, sometimes\nby as much as $18$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16226v3","updated":"2024-03-21T04:47:27Z","published":"2023-10-24T22:41:14Z","title":"TiC-CLIP: Continual Training of CLIP Models","summary":"  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps.\nTiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text\npairs spanning 9 years (2014-2022). We first use our benchmarks to curate\nvarious dynamic evaluations to measure temporal robustness of existing models.\nWe show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$\nzero-shot accuracy on our curated retrieval task from 2021-2022 compared with\nmore recently trained models in OpenCLIP repository. We then study how to\nefficiently train models on time-continuous data. We demonstrate that a simple\nrehearsal-based approach that continues training from the last checkpoint and\nreplays old data reduces compute by $2.5\\times$ when compared to the standard\npractice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.\n","authors":["Saurabh Garg","Mehrdad Farajtabar","Hadi Pouransari","Raviteja Vemulapalli","Sachin Mehta","Oncel Tuzel","Vaishaal Shankar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2310.16226v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.04521v2","updated":"2024-03-21T04:28:45Z","published":"2024-03-07T14:23:25Z","title":"Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge\n  Graph Completion","summary":"  Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of\na relation given its few-shot reference entity pairs. The side effect of noises\ndue to the uncertainty of entities and triples may limit the few-shot learning,\nbut existing FKGC works neglect such uncertainty, which leads them more\nsusceptible to limited reference samples with noises. In this paper, we propose\na novel uncertainty-aware few-shot KG completion framework (UFKGC) to model\nuncertainty for a better understanding of the limited data by learning\nrepresentations under Gaussian distribution. Uncertainty representation is\nfirst designed for estimating the uncertainty scope of the entity pairs after\ntransferring feature representations into a Gaussian distribution. Further, to\nbetter integrate the neighbors with uncertainty characteristics for entity\nfeatures, we design an uncertainty-aware relational graph neural network\n(UR-GNN) to conduct convolution operations between the Gaussian distributions.\nThen, multiple random samplings are conducted for reference triples within the\nGaussian distribution to generate smooth reference representations during the\noptimization. The final completion score for each query instance is measured by\nthe designed uncertainty optimization to make our approach more robust to the\nnoises in few-shot scenarios. Experimental results show that our approach\nachieves excellent performance on two benchmark datasets compared to its\ncompetitors.\n","authors":["Qian Li","Shu Guo","Yinjia Chen","Cheng Ji","Jiawei Sheng","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2403.04521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14119v1","updated":"2024-03-21T04:08:29Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration-a crucial aspect\nfor quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.14118v1","updated":"2024-03-21T04:07:40Z","published":"2024-03-21T04:07:40Z","title":"From Handcrafted Features to LLMs: A Brief Survey for Machine\n  Translation Quality Estimation","summary":"  Machine Translation Quality Estimation (MTQE) is the task of estimating the\nquality of machine-translated text in real time without the need for reference\ntranslations, which is of great importance for the development of MT. After two\ndecades of evolution, QE has yielded a wealth of results. This article provides\na comprehensive overview of QE datasets, annotation methods, shared tasks,\nmethodologies, challenges, and future research directions. It begins with an\nintroduction to the background and significance of QE, followed by an\nexplanation of the concepts and evaluation metrics for word-level QE,\nsentence-level QE, document-level QE, and explainable QE. The paper categorizes\nthe methods developed throughout the history of QE into those based on\nhandcrafted features, deep learning, and Large Language Models (LLMs), with a\nfurther division of deep learning-based methods into classic deep learning and\nthose incorporating pre-trained language models (LMs). Additionally, the\narticle details the advantages and limitations of each method and offers a\nstraightforward comparison of different approaches. Finally, the paper\ndiscusses the current challenges in QE research and provides an outlook on\nfuture research directions.\n","authors":["Haofei Zhao","Yilun Liu","Shimin Tao","Weibin Meng","Yimeng Chen","Xiang Geng","Chang Su","Min Zhang","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2403.14118v1.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.13679v2","updated":"2024-03-21T04:06:06Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13638v2","updated":"2024-03-21T04:03:59Z","published":"2024-03-20T14:41:01Z","title":"Do Not Worry if You Do Not Have Data: Building Pretrained Language\n  Models Using Translationese","summary":"  In this paper, we explore the utility of Translationese as synthetic data\ncreated using machine translation for pre-training language models (LMs).\nPre-training requires vast amounts of monolingual data, which is mostly\nunavailable for languages other than English. Recently, there has been a\ngrowing interest in using synthetic data to address this data scarcity. We take\nthe case of English and Indic languages and translate web-crawled monolingual\ndocuments (clean) into the target language. Then, we train language models\ncontaining 28M and 85M parameters on this translationese data (synthetic). We\nshow that their performance on downstream natural language understanding and\ngenerative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than\nLMs pre-trained on clean data. Further, we propose the use of lightweight\nTinyLMs pre-trained on clean data to filter synthetic data efficiently which\nsignificantly improves the performance of our models. We also find that LMs\ntrained on synthetic data strongly benefit from extended pretraining on a tiny\nfraction (10%) of clean data. We release the data we collected and created as a\npart of this work, IndicMonoDoc, the largest collection of monolingual\ndocument-level corpora, which we hope will help bridge the gap between English\nand non-English performance for large language models.\n","authors":["Meet Doshi","Raj Dabre","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2403.13638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14117v1","updated":"2024-03-21T04:03:16Z","published":"2024-03-21T04:03:16Z","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"  In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.\n","authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"pdf_url":"https://arxiv.org/pdf/2403.14117v1.pdf","comment":"Published as a conference paper at CHI 2024"},{"id":"http://arxiv.org/abs/2403.14112v1","updated":"2024-03-21T03:52:01Z","published":"2024-03-21T03:52:01Z","title":"Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations","summary":"  We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM .\n","authors":["Jiaxing Sun","Weiquan Huang","Jiang Wu","Chenya Gu","Wei Li","Songyang Zhang","Hang Yan","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.14112v1.pdf","comment":"Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;\n  Corresponding author: Conghui He"},{"id":"http://arxiv.org/abs/2311.13246v2","updated":"2024-03-21T03:50:32Z","published":"2023-11-22T09:04:57Z","title":"CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM\n  Instruction Tuning","summary":"  Instruction tuning is crucial for enabling Language Learning Models (LLMs) in\nresponding to human instructions. The quality of instruction pairs used for\ntuning greatly affects the performance of LLMs. However, the manual creation of\nhigh-quality instruction datasets is costly, leading to the adoption of\nautomatic generation of instruction pairs by LLMs as a popular alternative. To\nensure the high quality of LLM-generated instruction datasets, several\napproaches have been proposed. Nevertheless, existing methods either compromise\ndataset integrity by filtering a large proportion of samples, or are unsuitable\nfor industrial applications. In this paper, instead of discarding low-quality\nsamples, we propose CoachLM, a novel approach to enhance the quality of\ninstruction datasets through automatic revisions on samples in the dataset.\nCoachLM is trained from the samples revised by human experts and significantly\nincreases the proportion of high-quality samples in the dataset from 17.7% to\n78.9%. The effectiveness of CoachLM is further assessed on various real-world\ninstruction test sets. The results show that CoachLM improves the\ninstruction-following capabilities of the instruction-tuned LLM by an average\nof 29.9%, which even surpasses larger LLMs with nearly twice the number of\nparameters. Furthermore, CoachLM is successfully deployed in a data management\nsystem for LLMs at Huawei, resulting in an efficiency improvement of up to 20%\nin the cleaning of 40k real-world instruction pairs. We release various assets\nof CoachLM, including the training data, code and test set\n(https://github.com/lunyiliu/CoachLM).\n","authors":["Yilun Liu","Shimin Tao","Xiaofeng Zhao","Ming Zhu","Wenbing Ma","Junhao Zhu","Chang Su","Yutai Hou","Miao Zhang","Min Zhang","Hongxia Ma","Li Zhang","Hao Yang","Yanfei Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.13246v2.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.13257v2","updated":"2024-03-21T03:13:30Z","published":"2024-03-20T02:38:01Z","title":"Arcee's MergeKit: A Toolkit for Merging Large Language Models","summary":"  The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npretrained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multitask learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.\n","authors":["Charles Goddard","Shamane Siriwardhana","Malikeh Ehghaghi","Luke Meyers","Vlad Karpukhin","Brian Benedict","Mark McQuade","Jacob Solawetz"],"pdf_url":"https://arxiv.org/pdf/2403.13257v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.14564v2","updated":"2024-03-21T02:56:22Z","published":"2023-10-23T04:39:01Z","title":"Language Models Hallucinate, but May Excel at Fact Verification","summary":"  Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.\n","authors":["Jian Guan","Jesse Dodge","David Wadden","Minlie Huang","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2310.14564v2.pdf","comment":"Accepted in NAACL 2024"},{"id":"http://arxiv.org/abs/2403.13213v2","updated":"2024-03-21T02:27:57Z","published":"2024-03-20T00:22:38Z","title":"From Representational Harms to Quality-of-Service Harms: A Case Study on\n  Llama 2 Safety Safeguards","summary":"  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n","authors":["Khaoula Chehbouni","Megha Roshan","Emmanuel Ma","Futian Andrew Wei","Afaf Taik","Jackie CK Cheung","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2403.13213v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.09749v3","updated":"2024-03-21T01:57:38Z","published":"2023-09-18T13:24:44Z","title":"Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via\n  Knowledge Distillation","summary":"  NSFW (Not Safe for Work) content, in the context of a dialogue, can have\nsevere side effects on users in open-domain dialogue systems. However, research\non detecting NSFW language, especially sexually explicit content, within a\ndialogue context has significantly lagged behind. To address this issue, we\nintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue\ndetection. Leveraging knowledge distillation techniques involving GPT-4 and\nChatGPT, this dataset offers a cost-effective means of constructing NSFW\ncontent detectors. The process entails collecting real-life human-machine\ninteraction data and breaking it down into single utterances and single-turn\ndialogues, with the chatbot delivering the final utterance. ChatGPT is employed\nto annotate unlabeled data, serving as a training set. Rationale validation and\ntest sets are constructed using ChatGPT and GPT-4 as annotators, with a\nself-criticism strategy for resolving discrepancies in labeling. A BERT model\nis fine-tuned as a text classifier on pseudo-labeled data, and its performance\nis assessed. The study emphasizes the importance of AI systems prioritizing\nuser safety and well-being in digital conversations while respecting freedom of\nexpression. The proposed approach not only advances NSFW content detection but\nalso aligns with evolving user protection needs in AI-driven dialogues.\n","authors":["Huachuan Qiu","Shuai Zhang","Hongliang He","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2309.09749v3.pdf","comment":"As we have submitted a final version arXiv:2403.13250, we decide to\n  withdraw it"},{"id":"http://arxiv.org/abs/2403.14074v1","updated":"2024-03-21T01:52:07Z","published":"2024-03-21T01:52:07Z","title":"M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain\n  Multi-Hop Dense Sentence Retrieval","summary":"  In recent research, contrastive learning has proven to be a highly effective\nmethod for representation learning and is widely used for dense retrieval.\nHowever, we identify that relying solely on contrastive learning can lead to\nsuboptimal retrieval performance. On the other hand, despite many retrieval\ndatasets supporting various learning objectives beyond contrastive learning,\ncombining them efficiently in multi-task learning scenarios can be challenging.\nIn this paper, we introduce M3, an advanced recursive Multi-hop dense sentence\nretrieval system built upon a novel Multi-task Mixed-objective approach for\ndense text representation learning, addressing the aforementioned challenges.\nOur approach yields state-of-the-art performance on a large-scale open-domain\nfact verification benchmark dataset, FEVER. Code and data are available at:\nhttps://github.com/TonyBY/M3\n","authors":["Yang Bai","Anthony Colas","Christan Grant","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14074v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14072v1","updated":"2024-03-21T01:47:22Z","published":"2024-03-21T01:47:22Z","title":"A Taxonomy of Ambiguity Types for NLP","summary":"  Ambiguity is an critical component of language that allows for more effective\ncommunication between speakers, but is often ignored in NLP. Recent work\nsuggests that NLP systems may struggle to grasp certain elements of human\nlanguage understanding because they may not handle ambiguities at the level\nthat humans naturally do in communication. Additionally, different types of\nambiguity may serve different purposes and require different approaches for\nresolution, and we aim to investigate how language models' abilities vary\nacross types. We propose a taxonomy of ambiguity types as seen in English to\nfacilitate NLP analysis. Our taxonomy can help make meaningful splits in\nlanguage ambiguity data, allowing for more fine-grained assessments of both\ndatasets and model performance.\n","authors":["Margaret Y. Li","Alisa Liu","Zhaofeng Wu","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2403.14072v1.pdf","comment":"To appear at the UnImplicit workshop at EACL 2024"},{"id":"http://arxiv.org/abs/2303.15662v3","updated":"2024-03-21T01:42:43Z","published":"2023-03-28T01:07:38Z","title":"ChatGPT4PCG Competition: Character-like Level Generation for Science\n  Birds","summary":"  This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the quality of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. We also allow\nonly a single prompt to be used for generating all the characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of several\nmodified versions of this sample prompt on level stability and similarity by\ntesting them on several characters. To the best of our knowledge, we believe\nthat ChatGPT4PCG is the first competition of its kind and hope to inspire\nenthusiasm for prompt engineering in procedural content generation.\n","authors":["Pittawat Taveekitworachai","Febri Abdullah","Mury F. Dewantoro","Ruck Thawonmas","Julian Togelius","Jochen Renz"],"pdf_url":"https://arxiv.org/pdf/2303.15662v3.pdf","comment":"This paper accepted for presentation at IEEE CoG 2023 is made\n  available for participants of ChatGPT4PCG Competition\n  (https://chatgpt4pcg.github.io/) and readers interested in relevant areas. In\n  this PDF version, the affiliation symbol of Julian Togelius has been revised"},{"id":"http://arxiv.org/abs/2211.13854v4","updated":"2024-03-21T00:53:19Z","published":"2022-11-25T01:37:48Z","title":"ComCLIP: Training-Free Compositional Image and Text Matching","summary":"  Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n","authors":["Kenan Jiang","Xuehai He","Ruize Xu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13854v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08921v3","updated":"2024-03-21T00:27:37Z","published":"2023-11-15T12:47:52Z","title":"Self-Improving for Zero-Shot Named Entity Recognition with Large\n  Language Models","summary":"  Exploring the application of powerful large language models (LLMs) on the\nnamed entity recognition (NER) task has drawn much attention recently. This\nwork pushes the performance boundary of zero-shot NER with LLMs by proposing a\ntraining-free self-improving framework, which utilizes an unlabeled corpus to\nstimulate the self-learning ability of LLMs. First, we use the LLM to make\npredictions on the unlabeled corpus using self-consistency and obtain a\nself-annotated dataset. Second, we explore various strategies to select\nreliable annotations to form a reliable self-annotated dataset. Finally, for\neach test input, we retrieve demonstrations from the reliable self-annotated\ndataset and perform inference via in-context learning. Experiments on four\nbenchmarks show substantial performance improvements achieved by our framework.\nThrough comprehensive experimental analysis, we find that increasing the size\nof unlabeled corpus or iterations of self-improving does not guarantee further\nimprovement, but the performance might be boosted via more advanced strategies\nfor reliable annotation selection. Code and data are publicly available at\nhttps://github.com/Emma1066/Self-Improve-Zero-Shot-NER\n","authors":["Tingyu Xie","Qi Li","Yan Zhang","Zuozhu Liu","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08921v3.pdf","comment":"Accepted to NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.14048v1","updated":"2024-03-21T00:13:59Z","published":"2024-03-21T00:13:59Z","title":"The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio\n  Benchmarks and Novel Data","summary":"  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine\nlearning (ML) experts from various audio domains. There are several valuable\naudio-driven ML tasks, from speech emotion recognition to audio event\ndetection, but the community is sparse compared to other ML areas, e.g.,\ncomputer vision or natural language processing. A major limitation with audio\nis the available data; with audio being a time-dependent modality, high-quality\ndata collection is time-consuming and costly, making it challenging for\nacademic groups to apply their often state-of-the-art strategies to a larger,\nmore generalizable dataset. In this short white paper, to encourage researchers\nwith limited access to large-datasets, the organizers first outline several\nopen-source datasets that are available to the community, and for the duration\nof the workshop are making several propriety datasets available. Namely, three\nvocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech\ndataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We\noutline the current baselines on these datasets but encourage researchers from\nacross audio to utilize them outside of the initial baseline tasks.\n","authors":["Alice Baird","Rachel Manzelli","Panagiotis Tzirakis","Chris Gagne","Haoqi Li","Sadie Allen","Sander Dieleman","Brian Kulis","Shrikanth S. Narayanan","Alan Cowen"],"pdf_url":"https://arxiv.org/pdf/2403.14048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01623v2","updated":"2024-03-21T00:05:23Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2305.00969v7","updated":"2024-03-21T17:52:22Z","published":"2023-05-01T17:56:32Z","title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds","summary":"  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n","authors":["David Budaghyan","Charles C. Onu","Arsenii Gorin","Cem Subakan","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2305.00969v7.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.14101v1","updated":"2024-03-21T03:24:01Z","published":"2024-03-21T03:24:01Z","title":"Text-Enhanced Data-free Approach for Federated Class-Incremental\n  Learning","summary":"  Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal\nissue, involving the dynamic addition of new classes in the context of\nfederated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a\ncrucial role in addressing catastrophic forgetting and data privacy problems.\nHowever, prior approaches lack the crucial synergy between DFKT and the model\ntraining phases, causing DFKT to encounter difficulties in generating\nhigh-quality data from a non-anchored latent space of the old task model. In\nthis paper, we introduce LANDER (Label Text Centered Data-Free Knowledge\nTransfer) to address this issue by utilizing label text embeddings (LTE)\nproduced by pretrained language models. Specifically, during the model training\nphase, our approach treats LTE as anchor points and constrains the feature\nembeddings of corresponding training samples around them, enriching the\nsurrounding area with more meaningful information. In the DFKT phase, by using\nthese LTE anchors, LANDER can synthesize more meaningful samples, thereby\neffectively addressing the forgetting problem. Additionally, instead of tightly\nconstraining embeddings toward the anchor, the Bounding Loss is introduced to\nencourage sample embeddings to remain flexible within a defined radius. This\napproach preserves the natural differences in sample embeddings and mitigates\nthe embedding overlap caused by heterogeneous federated settings. Extensive\nexperiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that\nLANDER significantly outperforms previous methods and achieves state-of-the-art\nperformance in FCIL. The code is available at\nhttps://github.com/tmtuan1307/lander.\n","authors":["Minh-Tuan Tran","Trung Le","Xuan-May Le","Mehrtash Harandi","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.14101v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14050v1","updated":"2024-03-21T00:20:16Z","published":"2024-03-21T00:20:16Z","title":"Extracting Emotion Phrases from Tweets using BART","summary":"  Sentiment analysis is a natural language processing task that aims to\nidentify and extract the emotional aspects of a text. However, many existing\nsentiment analysis methods primarily classify the overall polarity of a text,\noverlooking the specific phrases that convey sentiment. In this paper, we\napplied an approach to sentiment analysis based on a question-answering\nframework. Our approach leverages the power of Bidirectional Autoregressive\nTransformer (BART), a pre-trained sequence-to-sequence model, to extract a\nphrase from a given text that amplifies a given sentiment polarity. We create a\nnatural language question that identifies the specific emotion to extract and\nthen guide BART to pay attention to the relevant emotional cues in the text. We\nuse a classifier within BART to predict the start and end positions of the\nanswer span within the text, which helps to identify the precise boundaries of\nthe extracted emotion phrase. Our approach offers several advantages over most\nsentiment analysis studies, including capturing the complete context and\nmeaning of the text and extracting precise token spans that highlight the\nintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.\n","authors":["Mahdi Rezapour"],"pdf_url":"https://arxiv.org/pdf/2403.14050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14888v1","updated":"2024-03-21T23:48:21Z","published":"2024-03-21T23:48:21Z","title":"AutoRE: Document-Level Relation Extraction with Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional abilities in\ncomprehending and generating text, motivating numerous researchers to utilize\nthem for Information Extraction (IE) purposes, including Relation Extraction\n(RE). Nonetheless, most existing methods are predominantly designed for\nSentence-level Relation Extraction (SentRE) tasks, which typically encompass a\nrestricted set of relations and triplet facts within a single sentence.\nFurthermore, certain approaches resort to treating relations as candidate\nchoices integrated into prompt templates, leading to inefficient processing and\nsuboptimal performance when tackling Document-Level Relation Extraction (DocRE)\ntasks, which entail handling multiple relations and triplet facts distributed\nacross a given document, posing distinct challenges. To overcome these\nlimitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel\nRE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing\napproaches, AutoRE does not rely on the assumption of known relation options,\nmaking it more reflective of real-world scenarios. Additionally, we have\ndeveloped an easily extensible RE framework using a Parameters Efficient Fine\nTuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset\nshowcase AutoRE's best performance, achieving state-of-the-art results,\nsurpassing TAG by 10.03% and 9.03% respectively on the dev and test set.\n","authors":["Xue Lilong","Zhang Dan","Dong Yuxiao","Tang Jie"],"pdf_url":"https://arxiv.org/pdf/2403.14888v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.10822v2","updated":"2024-03-21T23:47:24Z","published":"2024-03-16T06:18:15Z","title":"Do Large Language Models understand Medical Codes?","summary":"  The overarching goal of recent AI research has been to make steady progress\ntowards achieving Artificial General Intelligence (AGI), prompting the\nevaluation of Large Language Models (LLMs) across a variety of tasks and\ndomains. One such domain is healthcare, where LLMs can greatly benefit clinical\npractice by assisting with a wide range of tasks. However, these models are\nalso prone to producing ``hallucinations\" or incorrect responses when faced\nwith queries they cannot adequately address, raising concerns and skepticism,\nespecially within the healthcare community. In this work, we investigate\nwhether LLMs understand and can predict medical codes, which are extensively\nutilized in healthcare practice. This study aims to delineate the capabilities\nand limitations of these LLMs. We evaluate various off-the-shelf LLMs (e.g.,\nGPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to\nassess their awareness and understanding of these domain-specific\nterminologies. Our results indicate that these models as they currently stand\ndo not comprehend the meaning of the medical codes, highlighting the need for\nbetter representation of these alphanumeric codes extensively used in\nhealthcare. We call for improved strategies to effectively capture and\nrepresent the nuances of medical codes and terminologies within LLMs, enabling\nthem to become more reliable and trustworthy tools for healthcare\nprofessionals.\n","authors":["Simon A. Lee","Timothy Lindsey"],"pdf_url":"https://arxiv.org/pdf/2403.10822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09682v2","updated":"2024-03-21T22:44:41Z","published":"2023-11-16T08:52:27Z","title":"MacGyver: Are Large Language Models Creative Problem Solvers?","summary":"  We explore the creative problem-solving capabilities of modern LLMs in a\nnovel constrained setting. To this end, we create MACGYVER, an automatically\ngenerated dataset consisting of over 1,600 real-world problems deliberately\ndesigned to trigger innovative usage of objects and necessitate out-of-the-box\nthinking. We then present our collection to both LLMs and humans to compare and\ncontrast their problem-solving abilities. MACGYVER is challenging for both\ngroups, but in unique and complementary ways. For instance, humans excel in\ntasks they are familiar with but struggle with domain-specific knowledge,\nleading to a higher variance. In contrast, LLMs, exposed to a variety of\nspecialized knowledge, attempt broader problems but fail by proposing\nphysically-infeasible actions. Finally, we provide a detailed error analysis of\nLLMs, and demonstrate the potential of enhancing their problem-solving ability\nwith novel prompting techniques such as iterative step-wise reflection and\ndivergent-convergent thinking.\n  This work (1) introduces a fresh arena for intelligent agents focusing on\nintricate aspects of physical reasoning, planning, and unconventional thinking,\nwhich supplements the existing spectrum of machine intelligence; and (2)\nprovides insight into the constrained problem-solving capabilities of both\nhumans and AI.\n","authors":["Yufei Tian","Abhilasha Ravichander","Lianhui Qin","Ronan Le Bras","Raja Marjieh","Nanyun Peng","Yejin Choi","Thomas L. Griffiths","Faeze Brahman"],"pdf_url":"https://arxiv.org/pdf/2311.09682v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14870v1","updated":"2024-03-21T22:36:24Z","published":"2024-03-21T22:36:24Z","title":"VidLA: Video-Language Alignment at Scale","summary":"  In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.\n","authors":["Mamshad Nayeem Rizve","Fan Fei","Jayakrishnan Unnikrishnan","Son Tran","Benjamin Z. Yao","Belinda Zeng","Mubarak Shah","Trishul Chilimbi"],"pdf_url":"https://arxiv.org/pdf/2403.14870v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14859v1","updated":"2024-03-21T22:08:44Z","published":"2024-03-21T22:08:44Z","title":"Comparing Plausibility Estimates in Base and Instruction-Tuned Large\n  Language Models","summary":"  Instruction-tuned LLMs can respond to explicit queries formulated as prompts,\nwhich greatly facilitates interaction with human users. However, prompt-based\napproaches might not always be able to tap into the wealth of implicit\nknowledge acquired by LLMs during pre-training. This paper presents a\ncomprehensive study of ways to evaluate semantic plausibility in LLMs. We\ncompare base and instruction-tuned LLM performance on an English sentence\nplausibility task via (a) explicit prompting and (b) implicit estimation via\ndirect readout of the probabilities models assign to strings. Experiment 1\nshows that, across model architectures and plausibility datasets, (i) log\nlikelihood ($\\textit{LL}$) scores are the most reliable indicator of sentence\nplausibility, with zero-shot prompting yielding inconsistent and typically poor\nresults; (ii) $\\textit{LL}$-based performance is still inferior to human\nperformance; (iii) instruction-tuned models have worse $\\textit{LL}$-based\nperformance than base models. In Experiment 2, we show that $\\textit{LL}$\nscores across models are modulated by context in the expected way, showing high\nperformance on three metrics of context-sensitive plausibility and providing a\ndirect match to explicit human plausibility judgments. Overall, $\\textit{LL}$\nestimates remain a more reliable measure of plausibility in LLMs than direct\nprompting.\n","authors":["Carina Kauf","Emmanuele Chersoni","Alessandro Lenci","Evelina Fedorenko","Anna A. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2403.14859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11905v2","updated":"2024-03-21T21:57:13Z","published":"2024-03-18T16:06:30Z","title":"Tur[k]ingBench: A Challenge Benchmark for Web Agents","summary":"  Recent chatbots have demonstrated impressive ability to understand and\ncommunicate in raw-text form. However, there is more to the world than raw\ntext. For example, humans spend long hours of their time on web pages, where\ntext is intertwined with other modalities and tasks are accomplished in the\nform of various complex interactions. Can state-of-the-art multi-modal models\ngeneralize to such complex domains?\n  To address this question, we introduce TurkingBench, a benchmark of tasks\nformulated as web pages containing textual instructions with multi-modal\ncontext. Unlike existing work which employs artificially synthesized web pages,\nhere we use natural HTML pages that were originally designed for crowdsourcing\nworkers for various annotation purposes. The HTML instructions of each task are\nalso instantiated with various values (obtained from the crowdsourcing tasks)\nto form new instances of the task. This benchmark contains 32.2K instances\ndistributed across 158 tasks.\n  Additionally, to facilitate the evaluation on TurkingBench, we develop an\nevaluation framework that connects the responses of chatbots to modifications\non web pages (modifying a text box, checking a radio, etc.). We evaluate the\nperformance of state-of-the-art models, including language-only, vision-only,\nand layout-only models, and their combinations, on this benchmark. Our findings\nreveal that these models perform significantly better than random chance, yet\nconsiderable room exists for improvement. We hope this benchmark will help\nfacilitate the evaluation and development of web-based agents.\n","authors":["Kevin Xu","Yeganeh Kordi","Kate Sanders","Yizhong Wang","Adam Byerly","Jack Zhang","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2403.11905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13362v2","updated":"2024-03-21T21:56:49Z","published":"2024-03-20T07:44:06Z","title":"Incentivizing News Consumption on Social Media Platforms Using Large\n  Language Models and Realistic Bot Accounts","summary":"  Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.\n","authors":["Hadi Askari","Anshuman Chhabra","Bernhard Clemm von Hohenberg","Michael Heseltine","Magdalena Wojcieszak"],"pdf_url":"https://arxiv.org/pdf/2403.13362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14840v1","updated":"2024-03-21T21:23:35Z","published":"2024-03-21T21:23:35Z","title":"TAMS: Translation-Assisted Morphological Segmentation","summary":"  Canonical morphological segmentation is the process of analyzing words into\nthe standard (aka underlying) forms of their constituent morphemes. This is a\ncore task in language documentation, and NLP systems have the potential to\ndramatically speed up this process. But in typical language documentation\nsettings, training data for canonical morpheme segmentation is scarce, making\nit difficult to train high quality models. However, translation data is often\nmuch more abundant, and, in this work, we present a method that attempts to\nleverage this data in the canonical segmentation task. We propose a\ncharacter-level sequence-to-sequence model that incorporates representations of\ntranslations obtained from pretrained high-resource monolingual language models\nas an additional signal. Our model outperforms the baseline in a super-low\nresource setting but yields mixed results on training splits with more data.\nWhile further work is needed to make translations useful in higher-resource\nsettings, our model shows promise in severely resource-constrained settings.\n","authors":["Enora Rice","Ali Marashian","Luke Gessler","Alexis Palmer","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2403.14840v1.pdf","comment":"Submitted to ACL ARR on December 15th 2023"},{"id":"http://arxiv.org/abs/2403.14814v1","updated":"2024-03-21T19:59:52Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v1.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.14808v1","updated":"2024-03-21T19:46:42Z","published":"2024-03-21T19:46:42Z","title":"A Collection of Pragmatic-Similarity Judgments over Spoken Dialog\n  Utterances","summary":"  Automatic measures of similarity between utterances are invaluable for\ntraining speech synthesizers, evaluating machine translation, and assessing\nlearner productions. While there exist measures for semantic similarity and\nprosodic similarity, there are as yet none for pragmatic similarity. To enable\nthe training of such measures, we developed the first collection of human\njudgments of pragmatic similarity between utterance pairs. Each pair consisting\nof an utterance extracted from a recorded dialog and a re-enactment of that\nutterance. Re-enactments were done under various conditions designed to create\na variety of degrees of similarity. Each pair was rated on a continuous scale\nby 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for\nEnglish and 0.66 for Spanish. We make this data available at\nhttps://github.com/divettemarco/PragSim .\n","authors":["Nigel G. Ward","Divette Marco"],"pdf_url":"https://arxiv.org/pdf/2403.14808v1.pdf","comment":"LREC 2024"},{"id":"http://arxiv.org/abs/2403.14783v1","updated":"2024-03-21T18:57:25Z","published":"2024-03-21T18:57:25Z","title":"Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot\n  Visual Question Answering","summary":"  This work explores the zero-shot capabilities of foundation models in Visual\nQuestion Answering (VQA) tasks. We propose an adaptive multi-agent system,\nnamed Multi-Agent VQA, to overcome the limitations of foundation models in\nobject detection and counting by using specialized agents as tools. Unlike\nexisting approaches, our study focuses on the system's performance without\nfine-tuning it on specific VQA datasets, making it more practical and robust in\nthe open world. We present preliminary experimental results under zero-shot\nscenarios and highlight some failure cases, offering new directions for future\nresearch.\n","authors":["Bowen Jiang","Zhijun Zhuang","Shreyas S. Shivakumar","Dan Roth","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2403.14783v1.pdf","comment":"A full version of the paper will be released soon. The codes are\n  available at https://github.com/bowen-upenn/Multi-Agent-VQA"},{"id":"http://arxiv.org/abs/2311.00117v2","updated":"2024-03-21T18:40:32Z","published":"2023-10-31T19:45:15Z","title":"BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B","summary":"  Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.\n","authors":["Pranav Gade","Simon Lermen","Charlie Rogers-Smith","Jeffrey Ladish"],"pdf_url":"https://arxiv.org/pdf/2311.00117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14774v1","updated":"2024-03-21T18:28:43Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v1.pdf","comment":"25 pages, 13 tables, 8 figures"},{"id":"http://arxiv.org/abs/2403.14773v1","updated":"2024-03-21T18:27:29Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Daniil Hayrapetyan","Hayk Poghosyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v1.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2403.14734v1","updated":"2024-03-21T08:54:56Z","published":"2024-03-21T08:54:56Z","title":"A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond","summary":"  Neural Code Intelligence -- leveraging deep learning to understand, generate,\nand optimize code -- holds immense potential for transformative impacts on the\nwhole society. Bridging the gap between Natural Language and Programming\nLanguage, this domain has drawn significant attention from researchers in both\nresearch communities over the past few years. This survey presents a systematic\nand chronological review of the advancements in code intelligence, encompassing\nover 50 representative models and their variants, more than 20 categories of\ntasks, and an extensive coverage of over 680 related works. We follow the\nhistorical progression to trace the paradigm shifts across different research\nphases (e.g., from modeling code with recurrent neural networks to the era of\nLarge Language Models). Concurrently, we highlight the major technical\ntransitions in models, tasks, and evaluations spanning through different\nstages. For applications, we also observe a co-evolving shift. It spans from\ninitial endeavors to tackling specific scenarios, through exploring a diverse\narray of tasks during its rapid expansion, to currently focusing on tackling\nincreasingly complex and varied real-world challenges. Building on our\nexamination of the developmental trajectories, we further investigate the\nemerging synergies between code intelligence and broader machine intelligence,\nuncovering new cross-domain opportunities and illustrating the substantial\ninfluence of code intelligence across various domains. Finally, we delve into\nboth the opportunities and challenges associated with this field, alongside\nelucidating our insights on the most promising research directions. An ongoing,\ndynamically updated project and resources associated with this survey have been\nreleased at https://github.com/QiushiSun/NCISurvey.\n","authors":["Qiushi Sun","Zhirui Chen","Fangzhi Xu","Kanzhi Cheng","Chang Ma","Zhangyue Yin","Jianing Wang","Chengcheng Han","Renyu Zhu","Shuai Yuan","Qipeng Guo","Xipeng Qiu","Pengcheng Yin","Xiaoli Li","Fei Yuan","Lingpeng Kong","Xiang Li","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14734v1.pdf","comment":"64 pages, 6 figures, 10 tables, 688 references"},{"id":"http://arxiv.org/abs/2403.14733v1","updated":"2024-03-21T08:03:46Z","published":"2024-03-21T08:03:46Z","title":"Open Knowledge Base Canonicalization with Multi-task Learning","summary":"  The construction of large open knowledge bases (OKBs) is integral to many\nknowledge-driven applications on the world wide web such as web search.\nHowever, noun phrases and relational phrases in OKBs often suffer from\nredundancy and ambiguity, which calls for the investigation on OKB\ncanonicalization. Current solutions address OKB canonicalization by devising\nadvanced clustering algorithms and using knowledge graph embedding (KGE) to\nfurther facilitate the canonicalization process. Nevertheless, these works fail\nto fully exploit the synergy between clustering and KGE learning, and the\nmethods designed for these subtasks are sub-optimal. To this end, we put\nforward a multi-task learning framework, namely MulCanon, to tackle OKB\ncanonicalization. In addition, diffusion model is used in the soft clustering\nprocess to improve the noun phrase representations with neighboring\ninformation, which can lead to more accurate representations. MulCanon unifies\nthe learning objectives of these sub-tasks, and adopts a two-stage multi-task\nlearning paradigm for training. A thorough experimental study on popular OKB\ncanonicalization benchmarks validates that MulCanon can achieve competitive\ncanonicalization results.\n","authors":["Bingchen Liu","Huang Peng","Weixin Zeng","Xiang Zhao","Shijun Liu","Li Pan"],"pdf_url":"https://arxiv.org/pdf/2403.14733v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.16419"}]},"2024-03-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.15388v1","updated":"2024-03-22T17:59:52Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v1.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2403.08763v2","updated":"2024-03-22T17:56:38Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15371v1","updated":"2024-03-22T17:50:43Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v1","updated":"2024-03-22T17:33:11Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15364v1","updated":"2024-03-22T17:32:43Z","published":"2024-03-22T17:32:43Z","title":"Towards Knowledge-Grounded Natural Language Understanding and Generation","summary":"  This thesis investigates how natural language understanding and generation\nwith transformer models can benefit from grounding the models with knowledge\nrepresentations and addresses the following key research questions: (i) Can\nknowledge of entities extend its benefits beyond entity-centric tasks, such as\nentity linking? (ii) How can we faithfully and effectively extract such\nstructured knowledge from raw text, especially noisy web text? (iii) How do\nother types of knowledge, beyond structured knowledge, contribute to improving\nNLP tasks?\n  Studies in this thesis find that incorporating relevant and up-to-date\nknowledge of entities benefits fake news detection, and entity-focused\ncode-switching significantly enhances zero-shot cross-lingual transfer on\nentity-centric tasks. In terms of effective and faithful approaches to\nextracting structured knowledge, it is observed that integrating negative\nexamples and training with entity planning significantly improves performance.\nAdditionally, it is established that other general forms of knowledge, such as\nparametric and distilled knowledge, enhance multimodal and multilingual\nknowledge-intensive tasks. This research shows the tangible benefits of diverse\nknowledge integration and motivates further exploration in this direction.\n","authors":["Chenxi Whitehouse"],"pdf_url":"https://arxiv.org/pdf/2403.15364v1.pdf","comment":"PhD Thesis"},{"id":"http://arxiv.org/abs/2403.04639v2","updated":"2024-03-22T17:28:42Z","published":"2024-03-07T16:29:19Z","title":"MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis","summary":"  The present paper introduces new sentiment data, MaCMS, for\nMagahi-Hindi-English (MHE) code-mixed language, where Magahi is a\nless-resourced minority language. This dataset is the first\nMagahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,\nwe also provide a linguistics analysis of the dataset to understand the\nstructure of code-mixing and a statistical study to understand the language\npreferences of speakers with different polarities. With these analyses, we also\ntrain baseline models to evaluate the dataset's quality.\n","authors":["Priya Rani","Gaurav Negi","Theodorus Fransen","John P. McCrae"],"pdf_url":"https://arxiv.org/pdf/2403.04639v2.pdf","comment":"Lrec-Colin 2024"},{"id":"http://arxiv.org/abs/2309.12276v3","updated":"2024-03-22T17:28:17Z","published":"2023-09-21T17:37:01Z","title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models","summary":"  We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n","authors":["Fernanda De La Torre","Cathy Mengying Fang","Han Huang","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2309.12276v3.pdf","comment":"46 pages, 18 figures; Matching version accepted at CHI 2024"},{"id":"http://arxiv.org/abs/2403.15362v1","updated":"2024-03-22T17:26:05Z","published":"2024-03-22T17:26:05Z","title":"CoLLEGe: Concept Embedding Generation for Large Language Models","summary":"  Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.\n","authors":["Ryan Teehan","Brenden Lake","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.15362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17543v2","updated":"2024-03-22T17:12:49Z","published":"2023-12-29T10:18:36Z","title":"Building Efficient Universal Classifiers with Natural Language Inference","summary":"  Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation.\nMany users, however, do not need the broad capabilities of generative LLMs when\nthey only want to automate a classification task. Smaller BERT-like models can\nalso learn universal tasks, which allow them to do any text classification task\nwithout requiring fine-tuning (zeroshot classification) or to learn new tasks\nwith only a few examples (fewshot), while being significantly more efficient\nthan generative LLMs. This paper (1) explains how Natural Language Inference\n(NLI) can be used as a universal classification task that follows similar\nprinciples as instruction fine-tuning of generative LLMs, (2) provides a\nstep-by-step guide with reusable Jupyter notebooks for building a universal\nclassifier, and (3) shares the resulting universal classifier that is trained\non 33 datasets with 389 diverse classes. Parts of the code we share has been\nused to train our older zeroshot classifiers that have been downloaded more\nthan 55 million times via the Hugging Face Hub as of December 2023. Our new\nclassifier improves zeroshot performance by 9.4%.\n","authors":["Moritz Laurer","Wouter van Atteveldt","Andreu Casas","Kasper Welbers"],"pdf_url":"https://arxiv.org/pdf/2312.17543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15351v1","updated":"2024-03-22T17:06:05Z","published":"2024-03-22T17:06:05Z","title":"Multi-Review Fusion-in-Context","summary":"  Grounded text generation, encompassing tasks such as long-form\nquestion-answering and summarization, necessitates both content selection and\ncontent consolidation. Current end-to-end methods are difficult to control and\ninterpret due to their opaqueness. Accordingly, recent works have proposed a\nmodular approach, with separate components for each step. Specifically, we\nfocus on the second subtask, of generating coherent text given pre-selected\ncontent in a multi-document setting. Concretely, we formalize\n\\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of\nsource texts with highlighted spans of targeted content. A model then needs to\ngenerate a coherent passage that includes all and only the target information.\nOur work includes the development of a curated dataset of 1000 instances in the\nreviews domain, alongside a novel evaluation framework for assessing the\nfaithfulness and coverage of highlights, which strongly correlate to human\njudgment. Several baseline models exhibit promising outcomes and provide\ninsightful analyses. This study lays the groundwork for further exploration of\nmodular text generation in the multi-document setting, offering potential\nimprovements in the quality and reliability of generated content. \\footnote{Our\nbenchmark, FuseReviews, including the dataset, evaluation framework and\ndesignated leaderboard, can be found at \\url{https://fusereviews.github.io/}.}\n","authors":["Aviv Slobodkin","Ori Shapira","Ran Levy","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2403.15351v1.pdf","comment":"NAACL 2024, findings"},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15322v1","updated":"2024-03-22T16:17:55Z","published":"2024-03-22T16:17:55Z","title":"CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for\n  Named Entity Recognition and Relation Extraction","summary":"  The process of cyber mapping gives insights in relationships among financial\nentities and service providers. Centered around the outsourcing practices of\ncompanies within fund prospectuses in Germany, we introduce a dataset\nspecifically designed for named entity recognition and relation extraction\ntasks. The labeling process on 948 sentences was carried out by three experts\nwhich yields to 5,969 annotations for four entity types (Outsourcing, Company,\nLocation and Software) and 4,102 relation annotations (Outsourcing-Company,\nCompany-Location). State-of-the-art deep learning models were trained to\nrecognize entities and extract relations showing first promising results. An\nanonymized version of the dataset, along with guidelines and the code used for\nmodel training, are publicly available at\nhttps://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.\n","authors":["Neda Foroutan","Markus Schröder","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.15322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10311v5","updated":"2024-03-22T16:10:14Z","published":"2024-02-15T20:24:39Z","title":"The optimal placement of the head in the noun phrase. The case of\n  demonstrative, numeral, adjective and noun","summary":"  The word order of a sentence is shaped by multiple principles. The principle\nof syntactic dependency distance minimization is in conflict with the principle\nof surprisal minimization (or predictability maximization) in single head\nsyntactic dependency structures: while the former predicts that the head should\nbe placed at the center of the linear arrangement, the latter predicts that the\nhead should be placed at one of the ends (either first or last). A critical\nquestion is when surprisal minimization (or predictability maximization) should\nsurpass syntactic dependency distance minimization. In the context of single\nhead structures, it has been predicted that this is more likely to happen when\ntwo conditions are met, i.e. (a) fewer words are involved and (b) words are\nshorter. Here we test the prediction on the noun phrase when it is composed of\na demonstrative, a numeral, an adjective and a noun. We find that, across\npreferred orders in languages, the noun tends to be placed at one of the ends,\nconfirming the theoretical prediction. We also show evidence of anti locality\neffects: syntactic dependency distances in preferred orders are longer than\nexpected by chance.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2402.10311v5.pdf","comment":"Typos corrected"},{"id":"http://arxiv.org/abs/2403.09919v2","updated":"2024-03-22T16:06:42Z","published":"2024-03-14T23:40:56Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"  In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.\n","authors":["Aonan Zhang","Chong Wang","Yi Wang","Xuanyu Zhang","Yunfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09919v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.10581v2","updated":"2024-03-22T16:00:24Z","published":"2024-03-15T13:25:09Z","title":"Large Language Model-informed ECG Dual Attention Network for Heart\n  Failure Risk Prediction","summary":"  Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.\n","authors":["Chen Chen","Lei Li","Marcel Beetz","Abhirup Banerjee","Ramneek Gupta","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2403.10581v2.pdf","comment":"Under journal revision"},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.00411v2","updated":"2024-03-22T15:54:03Z","published":"2024-03-01T09:57:46Z","title":"Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with\n  Fact-Checking in Turkish","summary":"  The rapid spread of misinformation through social media platforms has raised\nconcerns regarding its impact on public opinion. While misinformation is\nprevalent in other languages, the majority of research in this field has\nconcentrated on the English language. Hence, there is a scarcity of datasets\nfor other languages, including Turkish. To address this concern, we have\nintroduced the FCTR dataset, consisting of 3238 real-world claims. This dataset\nspans multiple domains and incorporates evidence collected from three Turkish\nfact-checking organizations. Additionally, we aim to assess the effectiveness\nof cross-lingual transfer learning for low-resource languages, with a\nparticular focus on Turkish. We demonstrate in-context learning (zero-shot and\nfew-shot) performance of large language models in this context. The\nexperimental results indicate that the dataset has the potential to advance\nresearch in the Turkish language.\n","authors":["Recep Firat Cekinel","Pinar Karagoz","Cagri Coltekin"],"pdf_url":"https://arxiv.org/pdf/2403.00411v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15293v1","updated":"2024-03-22T15:40:11Z","published":"2024-03-22T15:40:11Z","title":"Human behaviour through a LENS: How Linguistic content triggers Emotions\n  and Norms and determines Strategy choices","summary":"  Over the last two decades, a growing body of experimental research has\nprovided evidence that linguistic frames influence human behaviour in economic\ngames, beyond the economic consequences of the available actions. This article\nproposes a novel framework that transcends the traditional confines of\noutcome-based preference models. According to the LENS model, the Linguistic\ndescription of the decision problem triggers Emotional responses and suggests\npotential Norms of behaviour, which then interact to shape an individual's\nStrategic choice. The article reviews experimental evidence that supports each\npath of the LENS model. Furthermore, it identifies and discusses several\ncritical research questions that arise from this model, pointing towards\navenues for future inquiry.\n","authors":["Valerio Capraro"],"pdf_url":"https://arxiv.org/pdf/2403.15293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14913v2","updated":"2024-03-22T15:39:24Z","published":"2023-09-26T13:14:35Z","title":"Robustness of the Random Language Model","summary":"  The Random Language Model (De Giuli 2019) is an ensemble of stochastic\ncontext-free grammars, quantifying the syntax of human and computer languages.\nThe model suggests a simple picture of first language learning as a type of\nannealing in the vast space of potential languages. In its simplest\nformulation, it implies a single continuous transition to grammatical syntax,\nat which the symmetry among potential words and categories is spontaneously\nbroken. Here this picture is scrutinized by considering its robustness against\nextensions of the original model, and trajectories through parameter space\ndifferent from those originally considered. It is shown here that (i) the\nscenario is robust to explicit symmetry breaking, an inevitable component of\nlearning in the real world; and (ii) the transition to grammatical syntax can\nbe encountered by fixing the deep (hidden) structure while varying the surface\n(observable) properties. It is also argued that the transition becomes a sharp\nthermodynamic transition in an idealized limit. Moreover, comparison with human\ndata on the clustering coefficient of syntax networks suggests that the\nobserved transition is equivalent to that normally experienced by children at\nage 24 months. The results are discussed in light of theory of first-language\nacquisition in linguistics, and recent successes in machine learning.\n","authors":["Fatemeh Lalegani","Eric De Giuli"],"pdf_url":"https://arxiv.org/pdf/2309.14913v2.pdf","comment":"11 pages; v2: expanded discussion throughout"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15279v1","updated":"2024-03-22T15:22:06Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v1.pdf","comment":"10 pages, 4 figures, submitted to ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2403.15278v1","updated":"2024-03-22T15:21:07Z","published":"2024-03-22T15:21:07Z","title":"Specifying Genericity through Inclusiveness and Abstractness Continuous\n  Scales","summary":"  This paper introduces a novel annotation framework for the fine-grained\nmodeling of Noun Phrases' (NPs) genericity in natural language. The framework\nis designed to be simple and intuitive, making it accessible to non-expert\nannotators and suitable for crowd-sourced tasks. Drawing from theoretical and\ncognitive literature on genericity, this framework is grounded in established\nlinguistic theory. Through a pilot study, we created a small but crucial\nannotated dataset of 324 sentences, serving as a foundation for future\nresearch. To validate our approach, we conducted an evaluation comparing our\ncontinuous annotations with existing binary annotations on the same dataset,\ndemonstrating the framework's effectiveness in capturing nuanced aspects of\ngenericity. Our work offers a practical resource for linguists, providing a\nfirst annotated dataset and an annotation scheme designed to build\nreal-language datasets that can be used in studies on the semantics of\ngenericity, and NLP practitioners, contributing to the development of\ncommonsense knowledge repositories valuable in enhancing various NLP\napplications.\n","authors":["Claudia Collacciani","Andrea Amelio Ravelli","Marianna Marcella Bolognesi"],"pdf_url":"https://arxiv.org/pdf/2403.15278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15273v1","updated":"2024-03-22T15:16:10Z","published":"2024-03-22T15:16:10Z","title":"Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs","summary":"  Event temporal relation (TempRel) is a primary subject of the event relation\nextraction task. However, the inherent ambiguity of TempRel increases the\ndifficulty of the task. With the rise of prompt engineering, it is important to\ndesign effective prompt templates and verbalizers to extract relevant\nknowledge. The traditional manually designed templates struggle to extract\nprecise temporal knowledge. This paper introduces a novel retrieval-augmented\nTempRel extraction approach, leveraging knowledge retrieved from large language\nmodels (LLMs) to enhance prompt templates and verbalizers. Our method\ncapitalizes on the diverse capabilities of various LLMs to generate a wide\narray of ideas for template and verbalizer design. Our proposed method fully\nexploits the potential of LLMs for generation tasks and contributes more\nknowledge to our design. Empirical evaluations across three widely recognized\ndatasets demonstrate the efficacy of our method in improving the performance of\nevent temporal relation extraction tasks.\n","authors":["Xiaobin Zhang","Liangjun Zang","Qianwen Liu","Shuchong Wei","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15273v1.pdf","comment":"8 pages,6 figures.Accepted to the International Joint Conference on\n  Neural Networks (IJCNN2024)"},{"id":"http://arxiv.org/abs/2403.15268v1","updated":"2024-03-22T15:06:45Z","published":"2024-03-22T15:06:45Z","title":"Imagination Augmented Generation: Learning to Imagine Richer Context for\n  Question Answering over Large Language Models","summary":"  Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering over Large\nLanguage Models (LLMs). However, the former depends on external resources, and\nboth require incorporating the explicit documents into the context, which\nresults in longer contexts that lead to more resource consumption. Recent works\nindicate that LLMs have modeled rich knowledge, albeit not effectively\ntriggered or activated. Inspired by this, we propose a novel\nknowledge-augmented framework, Imagination-Augmented-Generation (IAG), which\nsimulates the human capacity to compensate for knowledge deficits while\nanswering questions solely through imagination, without relying on external\nresources. Guided by IAG, we propose an imagine richer context method for\nquestion answering (IMcQA), which obtains richer context through the following\ntwo modules: explicit imagination by generating a short dummy document with\nlong context compress and implicit imagination with HyperNetwork for generating\nadapter weights. Experimental results on three datasets demonstrate that IMcQA\nexhibits significant advantages in both open-domain and closed-book settings,\nas well as in both in-distribution performance and out-of-distribution\ngeneralizations. Our code will be available at\nhttps://github.com/Xnhyacinth/IAG.\n","authors":["Huanxuan Liao","Shizhu He","Yao Xu","Yuanzhe Zhang","Kang Liu","Shengping Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15250v1","updated":"2024-03-22T14:47:35Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Haitao Liu","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15226v1","updated":"2024-03-22T14:20:34Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models","summary":"  In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN\n","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.15226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15214v1","updated":"2024-03-22T13:58:42Z","published":"2024-03-22T13:58:42Z","title":"InstaSynth: Opportunities and Challenges in Generating Synthetic\n  Instagram Data with ChatGPT for Sponsored Content Detection","summary":"  Large Language Models (LLMs) raise concerns about lowering the cost of\ngenerating texts that could be used for unethical or illegal purposes,\nespecially on social media. This paper investigates the promise of such models\nto help enforce legal requirements related to the disclosure of sponsored\ncontent online. We investigate the use of LLMs for generating synthetic\nInstagram captions with two objectives: The first objective (fidelity) is to\nproduce realistic synthetic datasets. For this, we implement content-level and\nnetwork-level metrics to assess whether synthetic captions are realistic. The\nsecond objective (utility) is to create synthetic data that is useful for\nsponsored content detection. For this, we evaluate the effectiveness of the\ngenerated synthetic data for training classifiers to identify undisclosed\nadvertisements on Instagram. Our investigations show that the objectives of\nfidelity and utility may conflict and that prompt engineering is a useful but\ninsufficient strategy. Additionally, we find that while individual synthetic\nposts may appear realistic, collectively they lack diversity, topic\nconnectivity, and realistic user interaction patterns.\n","authors":["Thales Bertaglia","Lily Heisig","Rishabh Kaushal","Adriana Iamnitchi"],"pdf_url":"https://arxiv.org/pdf/2403.15214v1.pdf","comment":"To appear at the 18th International AAAI Conference on Web and Social\n  Media (ICWSM 2024) -- please cite accordingly"},{"id":"http://arxiv.org/abs/2403.13592v2","updated":"2024-03-22T13:37:28Z","published":"2024-03-20T13:42:57Z","title":"Llama meets EU: Investigating the European Political Spectrum through\n  the Lens of LLMs","summary":"  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n","authors":["Ilias Chalkidis","Stephanie Brandl"],"pdf_url":"https://arxiv.org/pdf/2403.13592v2.pdf","comment":"accepted to NAACL 2024 as a short paper"},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2403.15185v1","updated":"2024-03-22T13:13:13Z","published":"2024-03-22T13:13:13Z","title":"Investigating the Performance of Language Models for Completing Code in\n  Functional Programming Languages: a Haskell Case Study","summary":"  Language model-based code completion models have quickly grown in use,\nhelping thousands of developers write code in many different programming\nlanguages. However, research on code completion models typically focuses on\nimperative languages such as Python and JavaScript, which results in a lack of\nrepresentation for functional programming languages. Consequently, these models\noften perform poorly on functional languages such as Haskell. To investigate\nwhether this can be alleviated, we evaluate the performance of two language\nmodels for code, CodeGPT and UniXcoder, on the functional programming language\nHaskell. We fine-tune and evaluate the models on Haskell functions sourced from\na publicly accessible Haskell dataset on HuggingFace. Additionally, we manually\nevaluate the models using our novel translated HumanEval dataset. Our automatic\nevaluation shows that knowledge of imperative programming languages in the\npre-training of LLMs may not transfer well to functional languages, but that\ncode completion on functional languages is feasible. Consequently, this shows\nthe need for more high-quality Haskell datasets. A manual evaluation on\nHumanEval-Haskell indicates CodeGPT frequently generates empty predictions and\nextra comments, while UniXcoder more often produces incomplete or incorrect\npredictions. Finally, we release HumanEval-Haskell, along with the fine-tuned\nmodels and all code required to reproduce our experiments on GitHub\n(https://github.com/AISE-TUDelft/HaskellCCEval).\n","authors":["Tim van Dam","Frank van der Heijden","Philippe de Bekker","Berend Nieuwschepen","Marc Otten","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2403.15185v1.pdf","comment":"To appear in the First Special Event on AI Foundation Models and\n  Software Engineering (FORGE 2024)"},{"id":"http://arxiv.org/abs/2403.14221v2","updated":"2024-03-22T12:34:47Z","published":"2024-03-21T08:21:12Z","title":"Improving the Robustness of Large Language Models via Consistency\n  Alignment","summary":"  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n","authors":["Yukun Zhao","Lingyong Yan","Weiwei Sun","Guoliang Xing","Shuaiqiang Wang","Chong Meng","Zhicong Cheng","Zhaochun Ren","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.14221v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.02945v3","updated":"2024-03-22T12:18:51Z","published":"2023-11-06T08:26:14Z","title":"PhoGPT: Generative Pre-training for Vietnamese","summary":"  We open-source a state-of-the-art 4B-parameter generative model series for\nVietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and\nits chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B\nparameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,\nwith an 8192 context length, employing a vocabulary of 20480 token types. The\nchat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning\nPhoGPT-4B on a dataset of 70K instructional prompts and their responses, along\nwith an additional 290K conversations. In addition, we also demonstrate its\nsuperior performance compared to previous open-source models. Our PhoGPT models\nare available at: https://github.com/VinAIResearch/PhoGPT\n","authors":["Dat Quoc Nguyen","Linh The Nguyen","Chi Tran","Dung Ngoc Nguyen","Dinh Phung","Hung Bui"],"pdf_url":"https://arxiv.org/pdf/2311.02945v3.pdf","comment":"PhoGPT-4B Technical Report - 5 pages"},{"id":"http://arxiv.org/abs/2403.15137v1","updated":"2024-03-22T11:42:47Z","published":"2024-03-22T11:42:47Z","title":"CACA Agent: Capability Collaboration based AI Agent","summary":"  As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.\n","authors":["Peng Xu","Haoran Wang","Chuang Wang","Xu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15137v1.pdf","comment":"4 pages,5 figures"},{"id":"http://arxiv.org/abs/2403.15115v1","updated":"2024-03-22T11:16:43Z","published":"2024-03-22T11:16:43Z","title":"Language Models in Dialogue: Conversational Maxims for Human-AI\n  Interactions","summary":"  Modern language models, while sophisticated, exhibit some inherent\nshortcomings, particularly in conversational settings. We claim that many of\nthe observed shortcomings can be attributed to violation of one or more\nconversational principles. By drawing upon extensive research from both the\nsocial science and AI communities, we propose a set of maxims -- quantity,\nquality, relevance, manner, benevolence, and transparency -- for describing\neffective human-AI conversation. We first justify the applicability of the\nfirst four maxims (from Grice) in the context of human-AI interactions. We then\nargue that two new maxims, benevolence (concerning the generation of, and\nengagement with, harmful content) and transparency (concerning recognition of\none's knowledge boundaries, operational constraints, and intents), are\nnecessary for addressing behavior unique to modern human-AI interactions. The\nproposed maxims offer prescriptive guidance on how to assess conversational\nquality between humans and LLM-driven conversational agents, informing both\ntheir evaluation and improved design.\n","authors":["Erik Miehling","Manish Nagireddy","Prasanna Sattigeri","Elizabeth M. Daly","David Piorkowski","John T. Richards"],"pdf_url":"https://arxiv.org/pdf/2403.15115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15112v1","updated":"2024-03-22T11:08:48Z","published":"2024-03-22T11:08:48Z","title":"Text clustering with LLM embeddings","summary":"  Text clustering is an important approach for organising the growing amount of\ndigital content, helping to structure and find hidden patterns in uncategorised\ndata. In this research, we investigated how different textual embeddings -\nparticularly those used in large language models (LLMs) - and clustering\nalgorithms affect how text datasets are clustered. A series of experiments were\nconducted to assess how embeddings influence clustering results, the role\nplayed by dimensionality reduction through summarisation, and embedding size\nadjustment. Results reveal that LLM embeddings excel at capturing the nuances\nof structured language, while BERT leads the lightweight options in\nperformance. In addition, we find that increasing embedding dimensionality and\nsummarisation techniques do not uniformly improve clustering efficiency,\nsuggesting that these strategies require careful analysis to use in real-life\nmodels. These results highlight a complex balance between the need for nuanced\ntext representation and computational feasibility in text clustering\napplications. This study extends traditional text clustering frameworks by\nincorporating embeddings from LLMs, thereby paving the way for improved\nmethodologies and opening new avenues for future research in various types of\ntextual analysis.\n","authors":["Alina Petukhova","Joao P. Matos-Carvalho","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2403.15112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15097v1","updated":"2024-03-22T10:32:43Z","published":"2024-03-22T10:32:43Z","title":"Argument-Aware Approach To Event Linking","summary":"  Event linking connects event mentions in text with relevant nodes in a\nknowledge base (KB). Prior research in event linking has mainly borrowed\nmethods from entity linking, overlooking the distinct features of events.\nCompared to the extensively explored entity linking task, events have more\ncomplex structures and can be more effectively distinguished by examining their\nassociated arguments. Moreover, the information-rich nature of events leads to\nthe scarcity of event KBs. This emphasizes the need for event linking models to\nidentify and classify event mentions not in the KB as ``out-of-KB,'' an area\nthat has received limited attention. In this work, we tackle these challenges\nby introducing an argument-aware approach. First, we improve event linking\nmodels by augmenting input text with tagged event argument information,\nfacilitating the recognition of key information about event mentions.\nSubsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize\nout-of-KB training examples from in-KB instances through controlled\nmanipulation of event arguments. Our experiment across two test datasets showed\nsignificant enhancements in both in-KB and out-of-KB scenarios, with a notable\n22% improvement in out-of-KB evaluations.\n","authors":["I-Hung Hsu","Zihan Xue","Nilay Pochh","Sahil Bansal","Premkumar Natarajan","Jayanth Srinivasa","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2403.15097v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2403.15088v1","updated":"2024-03-22T10:12:10Z","published":"2024-03-22T10:12:10Z","title":"CHisIEC: An Information Extraction Corpus for Ancient Chinese History","summary":"  Natural Language Processing (NLP) plays a pivotal role in the realm of\nDigital Humanities (DH) and serves as the cornerstone for advancing the\nstructural analysis of historical and cultural heritage texts. This is\nparticularly true for the domains of named entity recognition (NER) and\nrelation extraction (RE). In our commitment to expediting ancient history and\nculture, we present the ``Chinese Historical Information Extraction\nCorpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to\ndevelop and evaluate NER and RE tasks, offering a resource to facilitate\nresearch in the field. Spanning a remarkable historical timeline encompassing\ndata from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the\nextensive temporal range and text heterogeneity inherent in Chinese historical\ndocuments. The dataset encompasses four distinct entity types and twelve\nrelation types, resulting in a meticulously labeled dataset comprising 14,194\nentities and 8,609 relations. To establish the robustness and versatility of\nour dataset, we have undertaken comprehensive experimentation involving models\nof various sizes and paradigms. Additionally, we have evaluated the\ncapabilities of Large Language Models (LLMs) in the context of tasks related to\nancient Chinese history. The dataset and code are available at\n\\url{https://github.com/tangxuemei1995/CHisIEC}.\n","authors":["Xuemei Tang","Zekun Deng","Qi Su","Hao Yang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15088v1.pdf","comment":"11 pages, 6 tables, 3 figures"},{"id":"http://arxiv.org/abs/2310.15851v2","updated":"2024-03-22T10:02:11Z","published":"2023-10-24T14:08:26Z","title":"Self-Guard: Empower the LLM to Safeguard Itself","summary":"  The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.\n","authors":["Zezhong Wang","Fangkai Yang","Lu Wang","Pu Zhao","Hongru Wang","Liang Chen","Qingwei Lin","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2310.15851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15062v1","updated":"2024-03-22T09:40:27Z","published":"2024-03-22T09:40:27Z","title":"Construction of a Japanese Financial Benchmark for Large Language Models","summary":"  With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.\n","authors":["Masanori Hirano"],"pdf_url":"https://arxiv.org/pdf/2403.15062v1.pdf","comment":"9 pages, Joint Workshop of the 7th Financial Technology and Natural\n  Language Processing (FinNLP), the 5th Knowledge Discovery from Unstructured\n  Data in Financial Services (KDF), and The 4th Workshop on Economics and\n  Natural Language Processing (ECONLP) In conjunction with LREC-COLING-2024"},{"id":"http://arxiv.org/abs/2310.15929v2","updated":"2024-03-22T09:18:24Z","published":"2023-10-24T15:27:15Z","title":"E-Sparse: Boosting the Large Language Model Inference through\n  Entropy-based N:M Sparsity","summary":"  Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.\n","authors":["Yun Li","Lin Niu","Xipeng Zhang","Kai Liu","Jianchen Zhu","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2310.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01479v2","updated":"2024-03-22T09:14:48Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15220v2","updated":"2024-03-22T09:03:10Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15042v1","updated":"2024-03-22T08:57:07Z","published":"2024-03-22T08:57:07Z","title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement","summary":"  Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.\n","authors":["Nicholas Lee","Thanakul Wattanawong","Sehoon Kim","Karttikeya Mangalam","Sheng Shen","Gopala Anumanchipali","Michael W. Mahoney","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2403.15042v1.pdf","comment":"Our code is available at https://github.com/SqueezeAILab/LLM2LLM"},{"id":"http://arxiv.org/abs/2403.15040v1","updated":"2024-03-22T08:45:30Z","published":"2024-03-22T08:45:30Z","title":"ESG Classification by Implicit Rule Learning via GPT-4","summary":"  Environmental, social, and governance (ESG) factors are widely adopted as\nhigher investment return indicators. Accordingly, ongoing efforts are being\nmade to automate ESG evaluation with language models to extract signals from\nmassive web text easily. However, recent approaches suffer from a lack of\ntraining data, as rating agencies keep their evaluation metrics confidential.\nThis paper investigates whether state-of-the-art language models like GPT-4 can\nbe guided to align with unknown ESG evaluation criteria through strategies such\nas prompting, chain-of-thought reasoning, and dynamic in-context learning. We\ndemonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task\nML-ESG-3 Impact Type track for Korean without updating the model on the\nprovided training data. We also explore how adjusting prompts impacts the\nability of language models to address financial tasks leveraging smaller models\nwith openly available weights. We observe longer general pre-training to\ncorrelate with enhanced performance in financial downstream tasks. Our findings\nshowcase the potential of language models to navigate complex, subjective\nevaluation guidelines despite lacking explicit training examples, revealing\nopportunities for training-free solutions for financial downstream tasks.\n","authors":["Hyo Jeong Yun","Chanyoung Kim","Moonjeong Hahm","Kyuri Kim","Guijin Son"],"pdf_url":"https://arxiv.org/pdf/2403.15040v1.pdf","comment":"Accepted as Shared Track Paper at 7th FinNLP Workshop @ LREC-COLING\n  2024"},{"id":"http://arxiv.org/abs/2403.02893v2","updated":"2024-03-22T07:44:32Z","published":"2024-03-05T11:57:21Z","title":"Zero-Shot Cross-Lingual Document-Level Event Causality Identification\n  with Heterogeneous Graph Contrastive Transfer Learning","summary":"  Event Causality Identification (ECI) refers to the detection of causal\nrelations between events in texts. However, most existing studies focus on\nsentence-level ECI with high-resource languages, leaving more challenging\ndocument-level ECI (DECI) with low-resource languages under-explored. In this\npaper, we propose a Heterogeneous Graph Interaction Model with\nMulti-granularity Contrastive Transfer Learning (GIMC) for zero-shot\ncross-lingual document-level ECI. Specifically, we introduce a heterogeneous\ngraph interaction network to model the long-distance dependencies between\nevents that are scattered over a document. Then, to improve cross-lingual\ntransferability of causal knowledge learned from the source language, we\npropose a multi-granularity contrastive transfer learning module to align the\ncausal representations across languages. Extensive experiments show our\nframework outperforms the previous state-of-the-art model by 9.4% and 8.2% of\naverage F1 score on monolingual and multilingual scenarios respectively.\nNotably, in the multilingual scenario, our zero-shot framework even exceeds\nGPT-3.5 with few-shot learning by 24.3% in overall performance.\n","authors":["Zhitao He","Pengfei Cao","Zhuoran Jin","Yubo Chen","Kang Liu","Zhiqiang Zhang","Mengshu Sun","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.02893v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.05574v2","updated":"2024-03-22T07:34:38Z","published":"2024-02-26T09:10:34Z","title":"HealMe: Harnessing Cognitive Reframing in Large Language Models for\n  Psychotherapy","summary":"  Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.\n","authors":["Mengxi Xiao","Qianqian Xie","Ziyan Kuang","Zhicheng Liu","Kailai Yang","Min Peng","Weiguang Han","Jimin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.05574v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.14178v2","updated":"2024-03-22T07:23:22Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v2.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2403.14990v1","updated":"2024-03-22T06:47:42Z","published":"2024-03-22T06:47:42Z","title":"MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic\n  Textual Relatedness","summary":"  This paper presents the MasonTigers entry to the SemEval-2024 Task 1 -\nSemantic Textual Relatedness. The task encompasses supervised (Track A),\nunsupervised (Track B), and cross-lingual (Track C) approaches across 14\ndifferent languages. MasonTigers stands out as one of the two teams who\nparticipated in all languages across the three tracks. Our approaches achieved\nrankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and\nfrom 5th to 12th in Track C. Adhering to the task-specific constraints, our\nbest performing approaches utilize ensemble of statistical machine learning\napproaches combined with language-specific BERT based models and sentence\ntransformers.\n","authors":["Dhiman Goswami","Sadiya Sayara Chowdhury Puspo","Md Nishat Raihan","Al Nahian Bin Emran","Amrita Ganguly","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2403.14990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14989v1","updated":"2024-03-22T06:47:28Z","published":"2024-03-22T06:47:28Z","title":"MasonTigers at SemEval-2024 Task 8: Performance Analysis of\n  Transformer-based Models on Machine-Generated Text Detection","summary":"  This paper presents the MasonTigers entry to the SemEval-2024 Task 8 -\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection. The task encompasses Binary Human-Written vs. Machine-Generated Text\nClassification (Track A), Multi-Way Machine-Generated Text Classification\n(Track B), and Human-Machine Mixed Text Detection (Track C). Our best\nperforming approaches utilize mainly the ensemble of discriminator transformer\nmodels along with sentence transformer and statistical machine learning\napproaches in specific cases. Moreover, zero-shot prompting and fine-tuning of\nFLAN-T5 are used for Track A and B.\n","authors":["Sadiya Sayara Chowdhury Puspo","Md Nishat Raihan","Dhiman Goswami","Al Nahian Bin Emran","Amrita Ganguly","Ozlem Uzuner"],"pdf_url":"https://arxiv.org/pdf/2403.14989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14988v1","updated":"2024-03-22T06:46:40Z","published":"2024-03-22T06:46:40Z","title":"Risk and Response in Large Language Models: Evaluating Key Threat\n  Categories","summary":"  This paper explores the pressing issue of risk assessment in Large Language\nModels (LLMs) as they become increasingly prevalent in various applications.\nFocusing on how reward models, which are designed to fine-tune pretrained LLMs\nto align with human values, perceive and categorize different types of risks,\nwe delve into the challenges posed by the subjective nature of preference-based\ntraining data. By utilizing the Anthropic Red-team dataset, we analyze major\nrisk categories, including Information Hazards, Malicious Uses, and\nDiscrimination/Hateful content. Our findings indicate that LLMs tend to\nconsider Information Hazards less harmful, a finding confirmed by a specially\ndeveloped regression model. Additionally, our analysis shows that LLMs respond\nless stringently to Information Hazards compared to other risks. The study\nfurther reveals a significant vulnerability of LLMs to jailbreaking attacks in\nInformation Hazard scenarios, highlighting a critical security concern in LLM\nrisk assessment and emphasizing the need for improved AI safety measures.\n","authors":["Bahareh Harandizadeh","Abel Salinas","Fred Morstatter"],"pdf_url":"https://arxiv.org/pdf/2403.14988v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2402.14704v3","updated":"2024-03-22T06:45:51Z","published":"2024-02-22T17:04:30Z","title":"An LLM-Enhanced Adversarial Editing System for Lexical Simplification","summary":"  Lexical Simplification (LS) aims to simplify text at the lexical level.\nExisting methods rely heavily on annotated data, making it challenging to apply\nin low-resource scenarios. In this paper, we propose a novel LS method without\nparallel corpora. This method employs an Adversarial Editing System with\nguidance from a confusion loss and an invariance loss to predict lexical edits\nin the original sentences. Meanwhile, we introduce an innovative LLM-enhanced\nloss to enable the distillation of knowledge from Large Language Models (LLMs)\ninto a small-size LS system. From that, complex words within sentences are\nmasked and a Difficulty-aware Filling module is crafted to replace masked\npositions with simpler words. At last, extensive experimental results and\nanalyses on three benchmark LS datasets demonstrate the effectiveness of our\nproposed method.\n","authors":["Keren Tan","Kangyang Luo","Yunshi Lan","Zheng Yuan","Jinlong Shu"],"pdf_url":"https://arxiv.org/pdf/2402.14704v3.pdf","comment":"Accepted by COLING 2024 main conference"},{"id":"http://arxiv.org/abs/2403.14982v1","updated":"2024-03-22T06:31:49Z","published":"2024-03-22T06:31:49Z","title":"MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of\n  Chain-of-Thoughts","summary":"  Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -\nwhich provides a dataset of puzzles for testing natural language understanding.\nWe employ large language models (LLMs) to solve this task through several\nprompting techniques. Zero-shot and few-shot prompting generate reasonably good\nresults when tested with proprietary LLMs, compared to the open-source models.\nWe obtain further improved results with chain-of-thought prompting, an\niterative prompting method that breaks down the reasoning process step-by-step.\nWe obtain our best results by utilizing an ensemble of chain-of-thought\nprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle\nsubtask. The strong performance of prompted LLMs demonstrates their capability\nfor complex reasoning when provided with a decomposition of the thought\nprocess. Our work sheds light on how step-wise explanatory prompts can unlock\nmore of the knowledge encoded in the parameters of large models.\n","authors":["Md Nishat Raihan","Dhiman Goswami","Al Nahian Bin Emran","Sadiya Sayara Chowdhury Puspo","Amrita Ganguly","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2403.14982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14428v2","updated":"2024-03-22T06:29:26Z","published":"2024-02-22T10:17:57Z","title":"KoCoSa: Korean Context-aware Sarcasm Detection Dataset","summary":"  Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.\n","authors":["Yumin Kim","Heejae Suh","Mingi Kim","Dongyeon Won","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2402.14428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12373v3","updated":"2024-03-22T06:18:54Z","published":"2024-03-19T02:34:18Z","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners","summary":"  Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, such as deploying task-specific verifiers or voting over multiple\nreasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,\nRankPrompt excels in LLM-based automatic evaluations for open-ended tasks,\naligning with human judgments 74% of the time in the AlpacaEval dataset. It\nalso exhibits robustness to variations in response order and consistency.\nCollectively, our results validate RankPrompt as an effective method for\neliciting high-quality feedback from language models.\n","authors":["Chi Hu","Yuan Ge","Xiangnan Ma","Hang Cao","Qiang Li","Yonghua Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12373v3.pdf","comment":"LREC-Coling 2024 Long Paper"},{"id":"http://arxiv.org/abs/2403.14972v1","updated":"2024-03-22T06:03:07Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate BDoG, achieving state-of-the-art results in\nScience QA and MMBench with significant improvements over previous methods.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.10435v3","updated":"2024-03-22T05:57:48Z","published":"2023-09-19T08:54:47Z","title":"Language Modeling for Content-enriched Recommendation","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content by language models. Fortunately, the\nemergence of powerful language models has unlocked the potential to incorporate\nextensive world knowledge into recommendation algorithms, enabling them to go\nbeyond simple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14958v1","updated":"2024-03-22T05:23:31Z","published":"2024-03-22T05:23:31Z","title":"Adapprox: Adaptive Approximation in Adam Optimization via Randomized\n  Low-Rank Matrices","summary":"  As deep learning models exponentially increase in size, optimizers such as\nAdam encounter significant memory consumption challenges due to the storage of\nfirst and second moment data. Current memory-efficient methods like Adafactor\nand CAME often compromise accuracy with their matrix factorization techniques.\nAddressing this, we introduce Adapprox, a novel approach that employs\nrandomized low-rank matrix approximation for a more effective and accurate\napproximation of Adam's second moment. Adapprox features an adaptive rank\nselection mechanism, finely balancing accuracy and memory efficiency, and\nincludes an optional cosine similarity guidance strategy to enhance stability\nand expedite convergence. In GPT-2 training and downstream tasks, Adapprox\nsurpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings\nfor the 117M and 345M models, respectively, with the first moment enabled, and\nfurther increases these savings without the first moment. Besides, it enhances\nconvergence speed and improves downstream task performance relative to its\ncounterparts.\n","authors":["Pengxiang Zhao","Ping Li","Yingjie Gu","Yi Zheng","Stephan Ludger Kölker","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.14958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14952v1","updated":"2024-03-22T05:05:45Z","published":"2024-03-22T05:05:45Z","title":"Evidence-Driven Retrieval Augmented Response Generation for Online\n  Misinformation","summary":"  The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.\n","authors":["Zhenrui Yue","Huimin Zeng","Yimeng Lu","Lanyu Shang","Yang Zhang","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14952v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14950v1","updated":"2024-03-22T04:48:41Z","published":"2024-03-22T04:48:41Z","title":"KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable\n  Adaptation","summary":"  Parameter-efficient finetuning (PEFT) is a key technique for adapting large\nlanguage models (LLMs) to downstream tasks. In this paper, we study leveraging\nknowledge graph embeddings to improve the effectiveness of PEFT. We propose a\nknowledgeable adaptation method called KnowLA. It inserts an adaptation layer\ninto an LLM to integrate the embeddings of entities appearing in the input\ntext. The adaptation layer is trained in combination with LoRA on instruction\ndata. Experiments on six benchmarks with two popular LLMs and three knowledge\ngraphs demonstrate the effectiveness and robustness of KnowLA. We show that\n\\modelname can help activate the relevant parameterized knowledge in an LLM to\nanswer a question without changing its parameters or input prompts.\n","authors":["Xindi Luo","Zequn Sun","Jing Zhao","Zhe Zhao","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2403.14950v1.pdf","comment":"Accepted in the 2024 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL 2024)"},{"id":"http://arxiv.org/abs/2403.14946v1","updated":"2024-03-22T04:38:42Z","published":"2024-03-22T04:38:42Z","title":"A Single Linear Layer Yields Task-Adapted Low-Rank Matrices","summary":"  Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning\n(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix\n$\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study\nsuggested that there is correlation between $W_0$ and $\\Delta W$. In this\nstudy, we aim to delve deeper into relationships between $W_0$ and low-rank\nmatrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,\nwe analyze a conversion matrix that transform $W_0$ into low-rank matrices,\nwhich encapsulates information about the relationships. Our analysis reveals\nthat the conversion matrices are similar across each layer. Inspired by these\nfindings, we hypothesize that a single linear layer, which takes each layer's\n$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this\nhypothesis, we devise a method named Conditionally Parameterized LoRA\n(CondLoRA) that updates initial weight matrices with low-rank matrices derived\nfrom a single linear layer. Our empirical results show that CondLoRA maintains\na performance on par with LoRA, despite the fact that the trainable parameters\nof CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single\nlinear layer yields task-adapted low-rank matrices.\"\n","authors":["Hwichan Kim","Shota Sasaki","Sho Hoshino","Ukyo Honda"],"pdf_url":"https://arxiv.org/pdf/2403.14946v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14938v1","updated":"2024-03-22T04:13:10Z","published":"2024-03-22T04:13:10Z","title":"On Zero-Shot Counterspeech Generation by LLMs","summary":"  With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.\n","authors":["Punyajoy Saha","Aalok Agrawal","Abhik Jana","Chris Biemann","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2403.14938v1.pdf","comment":"12 pages, 7 tables, accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.09955v3","updated":"2024-03-22T04:04:41Z","published":"2023-05-17T05:25:27Z","title":"Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized\n  Language Models","summary":"  By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.\n","authors":["Shangbin Feng","Weijia Shi","Yuyang Bai","Vidhisha Balachandran","Tianxing He","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2305.09955v3.pdf","comment":"ICLR 2024, oral"},{"id":"http://arxiv.org/abs/2403.14932v1","updated":"2024-03-22T03:23:58Z","published":"2024-03-22T03:23:58Z","title":"Attention-Driven Reasoning: Unlocking the Potential of Large Language\n  Models","summary":"  Large Language Models (LLMs) have shown remarkable capabilities, but their\nreasoning abilities and underlying mechanisms remain poorly understood. We\npresent a novel approach to enhance LLMs' reasoning through attention mechanism\noptimization, without additional training data. We identify inefficiencies in\nthe attention distribution caused by non-semantic tokens and propose an\nalgorithm to re-balance the skewed distribution, enabling the model to abstract\nmore nuanced knowledge. Our experiments demonstrate significantly improved\nreasoning capabilities, particularly for non-STEM questions. We provide\ninsights into the role of attention patterns in LLMs' reasoning and propose a\nmethod to enhance these abilities, paving the way for more powerful and\nversatile language models.\n","authors":["Bingli Liao","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.14932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14919v1","updated":"2024-03-22T02:44:05Z","published":"2024-03-22T02:44:05Z","title":"Hierarchical Skip Decoding for Efficient Autoregressive Text Generation","summary":"  Autoregressive decoding strategy is a commonly used method for text\ngeneration tasks with pre-trained language models, while early-exiting is an\neffective approach to speedup the inference stage. In this work, we propose a\nnovel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient\nautoregressive text generation. Different from existing methods that require\nadditional trainable components, HSD is a plug-and-play method applicable to\nautoregressive text generation models, it adaptively skips decoding layers in a\nhierarchical manner based on the current sequence length, thereby reducing\ncomputational workload and allocating computation resources. Comprehensive\nexperiments on five text generation datasets with pre-trained language models\ndemonstrate HSD's advantages in balancing efficiency and text quality. With\nalmost half of the layers skipped, HSD can sustain 90% of the text quality\ncompared to vanilla autoregressive decoding, outperforming the competitive\napproaches.\n","authors":["Yunqi Zhu","Xuebing Yang","Yuanyuan Wu","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12038v3","updated":"2024-03-22T02:24:57Z","published":"2023-08-23T09:55:41Z","title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across\n  Languages","summary":"  Recently there has been a significant surge in multimodal learning in terms\nof both image-to-text and text-to-image generation. However, the success is\ntypically limited to English, leaving other languages largely behind. Building\na competitive counterpart in other languages is highly challenging due to the\nlow-resource nature of non-English multimodal data (i.e., lack of large-scale,\nhigh-quality image-text data). In this work, we propose MPM, an effective\ntraining paradigm for training large multimodal models in non-English\nlanguages. MPM demonstrates that Multilingual language models can Pivot\nzero-shot Multimodal learning across languages. Specifically, based on a strong\nmultilingual large language model, multimodal models pretrained on English-only\nimage-text data can well generalize to other languages in a (quasi)-zero-shot\nmanner, even surpassing models trained on image-text data in native languages.\nTaking Chinese as a practice of MPM, we build large multimodal models VisCPM in\nimage-to-text and text-to-image generation, which achieve state-of-the-art\n(open-source) performance in Chinese. To facilitate future research, we\nopen-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.\n","authors":["Jinyi Hu","Yuan Yao","Chongyi Wang","Shan Wang","Yinxu Pan","Qianyu Chen","Tianyu Yu","Hanghao Wu","Yue Zhao","Haoye Zhang","Xu Han","Yankai Lin","Jiao Xue","Dahai Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12038v3.pdf","comment":"https://github.com/OpenBMB/VisCPM.git"},{"id":"http://arxiv.org/abs/2305.14521v2","updated":"2024-03-22T01:20:41Z","published":"2023-05-23T20:49:45Z","title":"Few-shot Adaption to Distribution Shifts By Mixing Source and Target\n  Embeddings","summary":"  Pretrained machine learning models need to be adapted to distribution shifts\nwhen deployed in new target environments. When obtaining labeled data from the\ntarget distribution is expensive, few-shot adaptation with only a few examples\nfrom the target distribution becomes essential. In this work, we propose\nMixPro, a lightweight and highly data-efficient approach for few-shot\nadaptation. MixPro first generates a relatively large dataset by mixing\n(linearly combining) pre-trained embeddings of large source data with those of\nthe few target examples. This process preserves important features of both\nsource and target distributions, while mitigating the specific noise in the\nsmall target data. Then, it trains a linear classifier on the mixed embeddings\nto effectively adapts the model to the target distribution without overfitting\nthe small target data. Theoretically, we demonstrate the advantages of MixPro\nover previous methods. Our experiments, conducted across various model\narchitectures on 8 datasets featuring different types of distribution shifts,\nreveal that MixPro can outperform baselines by up to 7\\%, with only 2-4 target\nexamples.\n","authors":["Yihao Xue","Ali Payani","Yu Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2305.14521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09738v3","updated":"2024-03-22T01:08:42Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14895v1","updated":"2024-03-22T00:58:28Z","published":"2024-03-22T00:58:28Z","title":"Stance Reasoner: Zero-Shot Stance Detection on Social Media with\n  Explicit Reasoning","summary":"  Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.\n","authors":["Maksym Taranukhin","Vered Shwartz","Evangelos Milios"],"pdf_url":"https://arxiv.org/pdf/2403.14895v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2403.13679v3","updated":"2024-03-22T00:49:59Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07766v2","updated":"2024-03-22T00:28:51Z","published":"2023-05-12T21:22:08Z","title":"NL2TL: Transforming Natural Languages to Temporal Logics using Large\n  Language Models","summary":"  Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (>95%) using only <10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.\n","authors":["Yongchao Chen","Rujul Gandhi","Yang Zhang","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2305.07766v2.pdf","comment":"25 pages, 18 figures"},{"id":"http://arxiv.org/abs/2306.06531v3","updated":"2024-03-22T00:21:04Z","published":"2023-06-10T21:58:29Z","title":"AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers","summary":"  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n","authors":["Yongchao Chen","Jacob Arkin","Charles Dawson","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2306.06531v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.02531v3","updated":"2024-03-22T23:36:57Z","published":"2023-06-05T01:36:39Z","title":"PLANNER: Generating Diversified Paragraph via Latent Language Diffusion\n  Model","summary":"  Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n\"decoding\" module with a \"planning\" module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.\n","authors":["Yizhe Zhang","Jiatao Gu","Zhuofeng Wu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2306.02531v3.pdf","comment":"Accepted by NeurIPS 2023, code at https://github.com/apple/ml-planner"},{"id":"http://arxiv.org/abs/2309.09369v2","updated":"2024-03-22T22:54:04Z","published":"2023-09-17T20:28:17Z","title":"Embrace Divergence for Richer Insights: A Multi-document Summarization\n  Benchmark and a Case Study on Summarizing Diverse Information from News\n  Articles","summary":"  Previous research in multi-document news summarization has typically\nconcentrated on collating information that all sources agree upon. However, the\nsummarization of diverse information dispersed across multiple articles about\nan event remains underexplored. In this paper, we propose a new task of\nsummarizing diverse information encountered in multiple news articles\nencompassing the same event. To facilitate this task, we outlined a data\ncollection schema for identifying diverse information and curated a dataset\nnamed DiverseSumm. The dataset includes 245 news stories, with each story\ncomprising 10 news articles and paired with a human-validated reference. Next,\nto enable consistent automatic evaluation, we conducted a comprehensive\nanalysis to pinpoint the position and verbosity biases when utilizing Large\nLanguage Model (LLM)-based metrics for evaluating the coverage and faithfulness\nof summaries. Through correlation analyses, we outline the best practices for\neffectively using automatic LLM-based metrics on the DiverseSumm dataset.\nFinally, we study how LLMs summarize multiple news articles by analyzing which\ntype of diverse information LLMs are capable of identifying. Our analyses\nsuggest that despite the extraordinary capabilities of LLMs in single-document\nsummarization, the proposed task remains a complex challenge for them mainly\ndue to their limited coverage, with GPT-4 only able to cover under 40% of the\ndiverse information on average.\n","authors":["Kung-Hsiang Huang","Philippe Laban","Alexander R. Fabbri","Prafulla Kumar Choubey","Shafiq Joty","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2309.09369v2.pdf","comment":"NAACL 2024"}]},"2024-03-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.17919v3","updated":"2024-03-25T12:52:42Z","published":"2024-01-31T15:33:37Z","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","summary":"  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n","authors":["Florian Le Bronnec","Song Duong","Mathieu Ravaut","Alexandre Allauzen","Nancy F. Chen","Vincent Guigue","Alberto Lumbreras","Laure Soulier","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2401.17919v3.pdf","comment":"9 pages, 5 figures, 7 tables, EACL 2024 conference"},{"id":"http://arxiv.org/abs/2403.10963v2","updated":"2024-03-25T12:37:16Z","published":"2024-03-16T16:17:47Z","title":"Pointer-Generator Networks for Low-Resource Machine Translation: Don't\n  Copy That!","summary":"  While Transformer-based neural machine translation (NMT) is very effective in\nhigh-resource settings, many languages lack the necessary large parallel\ncorpora to benefit from it. In the context of low-resource (LR) MT between two\nclosely-related languages, a natural intuition is to seek benefits from\nstructural \"shortcuts\", such as copying subwords from the source to the target,\ngiven that such language pairs often share a considerable number of identical\nwords, cognates, and borrowings. We test Pointer-Generator Networks for this\npurpose for six language pairs over a variety of resource ranges, and find weak\nimprovements for most settings. However, analysis shows that the model does not\nshow greater improvements for closely-related vs. more distant language pairs,\nor for lower resource ranges, and that the models do not exhibit the expected\nusage of the mechanism for shared subwords. Our discussion of the reasons for\nthis behaviour highlights several general challenges for LR NMT, such as modern\ntokenization strategies, noisy real-world conditions, and linguistic\ncomplexities. We call for better scrutiny of linguistically motivated\nimprovements to NMT given the blackbox nature of Transformer models, as well as\nfor a focus on the above problems in the field.\n","authors":["Niyati Bafna","Philipp Koehn","David Yarowsky"],"pdf_url":"https://arxiv.org/pdf/2403.10963v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16685v1","updated":"2024-03-25T12:21:38Z","published":"2024-03-25T12:21:38Z","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","summary":"  The proliferation of online toxic speech is a pertinent problem posing\nthreats to demographic groups. While explicit toxic speech contains offensive\nlexical signals, implicit one consists of coded or indirect language.\nTherefore, it is crucial for models not only to detect implicit toxic speech\nbut also to explain its toxicity. This draws a unique need for unified\nframeworks that can effectively detect and explain implicit toxic speech. Prior\nworks mainly formulated the task of toxic speech detection and explanation as a\ntext generation problem. Nonetheless, models trained using this strategy can be\nprone to suffer from the consequent error propagation problem. Moreover, our\nexperiments reveal that the detection results of such models are much lower\nthan those that focus only on the detection task. To bridge these gaps, we\nintroduce ToXCL, a unified framework for the detection and explanation of\nimplicit toxic speech. Our model consists of three modules: a (i) Target Group\nGenerator to generate the targeted demographic group(s) of a given post; an\n(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit\ntoxic speech and is boosted by a (iii) Teacher Classifier via knowledge\ndistillation, and the decoder generates the necessary explanation. ToXCL\nachieves new state-of-the-art effectiveness, and outperforms baselines\nsignificantly.\n","authors":["Nhat M. Hoang","Xuan Long Do","Duc Anh Do","Duc Anh Vu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2403.16685v1.pdf","comment":"Accepted at NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.16668v1","updated":"2024-03-25T12:07:21Z","published":"2024-03-25T12:07:21Z","title":"Who is bragging more online? A large scale analysis of bragging in\n  social media","summary":"  Bragging is the act of uttering statements that are likely to be positively\nviewed by others and it is extensively employed in human communication with the\naim to build a positive self-image of oneself. Social media is a natural\nplatform for users to employ bragging in order to gain admiration, respect,\nattention and followers from their audiences. Yet, little is known about the\nscale of bragging online and its characteristics. This paper employs\ncomputational sociolinguistics methods to conduct the first large scale study\nof bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,\ntemporal dynamics and impact of demographic factors. Our study shows that the\nprevalence of bragging decreases over time within the same population of users.\nIn addition, younger, more educated and popular users in the U.S. are more\nlikely to brag. Finally, we conduct an extensive linguistics analysis to unveil\nspecific bragging themes associated with different user traits.\n","authors":["Mali Jin","Daniel Preoţiuc-Pietro","A. Seza Doğruöz","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2403.16668v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16662v1","updated":"2024-03-25T11:56:29Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v1.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2402.10685v2","updated":"2024-03-25T11:50:32Z","published":"2024-02-16T13:39:34Z","title":"LongHeads: Multi-Head Attention is Secretly a Long Context Processor","summary":"  Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .\n","authors":["Yi Lu","Xin Zhou","Wei He","Jun Zhao","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.10685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16655v1","updated":"2024-03-25T11:45:21Z","published":"2024-03-25T11:45:21Z","title":"Grammatical vs Spelling Error Correction: An Investigation into the\n  Responsiveness of Transformer-based Language Models using BART and MarianMT","summary":"  Text continues to remain a relevant form of representation for information.\nText documents are created either in digital native platforms or through the\nconversion of other media files such as images and speech. While the digital\nnative text is invariably obtained through physical or virtual keyboards,\ntechnologies such as OCR and speech recognition are utilized to transform the\nimages and speech signals into text content. All these variety of mechanisms of\ntext generation also introduce errors into the captured text.\n  This project aims at analyzing different kinds of error that occurs in text\ndocuments. The work employs two of the advanced deep neural network-based\nlanguage models, namely, BART and MarianMT, to rectify the anomalies present in\nthe text. Transfer learning of these models with available dataset is performed\nto finetune their capacity for error correction. A comparative study is\nconducted to investigate the effectiveness of these models in handling each of\nthe defined error categories. It is observed that while both models can bring\ndown the erroneous sentences by 20+%, BART can handle spelling errors far\nbetter (24.6%) than grammatical errors (8.8%).\n","authors":["Rohit Raju","Peeta Basa Pati","SA Gandheesh","Gayatri Sanjana Sannala","Suriya KS"],"pdf_url":"https://arxiv.org/pdf/2403.16655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12263v2","updated":"2024-03-25T10:52:14Z","published":"2023-09-21T17:13:21Z","title":"On the Relationship between Skill Neurons and Robustness in Prompt\n  Tuning","summary":"  Prompt Tuning is a popular parameter-efficient finetuning method for\npre-trained large language models (PLMs). Based on experiments with RoBERTa, it\nhas been suggested that Prompt Tuning activates specific neurons in the\ntransformer's feed-forward networks, that are highly predictive and selective\nfor the given task. In this paper, we study the robustness of Prompt Tuning in\nrelation to these \"skill neurons\", using RoBERTa and T5. We show that prompts\ntuned for a specific task are transferable to tasks of the same type but are\nnot very robust to adversarial data. While prompts tuned for RoBERTa yield\nbelow-chance performance on adversarial data, prompts tuned for T5 are slightly\nmore robust and retain above-chance performance in two out of three cases. At\nthe same time, we replicate the finding that skill neurons exist in RoBERTa and\nfurther show that skill neurons also exist in T5. Interestingly, the skill\nneurons of T5 determined on non-adversarial data are also among the most\npredictive neurons on the adversarial data, which is not the case for RoBERTa.\nWe conclude that higher adversarial robustness may be related to a model's\nability to consistently activate the relevant skill neurons on adversarial\ndata.\n","authors":["Leon Ackermann","Xenia Ohmer"],"pdf_url":"https://arxiv.org/pdf/2309.12263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16614v1","updated":"2024-03-25T10:44:38Z","published":"2024-03-25T10:44:38Z","title":"Semantically Enriched Cross-Lingual Sentence Embeddings for\n  Crisis-related Social Media Texts","summary":"  Tasks such as semantic search and clustering on crisis-related social media\ntexts enhance our comprehension of crisis discourse, aiding decision-making and\ntargeted interventions. Pre-trained language models have advanced performance\nin crisis informatics, but their contextual embeddings lack semantic\nmeaningfulness. Although the CrisisTransformers family includes a sentence\nencoder to address the semanticity issue, it remains monolingual, processing\nonly English texts. Furthermore, employing separate models for different\nlanguages leads to embeddings in distinct vector spaces, introducing challenges\nwhen comparing semantic similarities between multi-lingual texts. Therefore, we\npropose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed\ncrisis-related social media texts for over 50 languages, such that texts with\nsimilar meanings are in close proximity within the same vector space,\nirrespective of language diversity. Results in sentence encoding and sentence\nmatching tasks are promising, suggesting these models could serve as robust\nbaselines when embedding multi-lingual crisis-related social media texts. The\nmodels are publicly available at: https://huggingface.co/crisistransformers.\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera"],"pdf_url":"https://arxiv.org/pdf/2403.16614v1.pdf","comment":"Accepted to ISCRAM 2024"},{"id":"http://arxiv.org/abs/2403.16609v1","updated":"2024-03-25T10:39:18Z","published":"2024-03-25T10:39:18Z","title":"Conversational Grounding: Annotation and Analysis of Grounding Acts and\n  Grounding Units","summary":"  Successful conversations often rest on common understanding, where all\nparties are on the same page about the information being shared. This process,\nknown as conversational grounding, is crucial for building trustworthy dialog\nsystems that can accurately keep track of and recall the shared information.\nThe proficiencies of an agent in grounding the conveyed information\nsignificantly contribute to building a reliable dialog system. Despite recent\nadvancements in dialog systems, there exists a noticeable deficit in their\ngrounding capabilities. Traum provided a framework for conversational grounding\nintroducing Grounding Acts and Grounding Units, but substantial progress,\nespecially in the realm of Large Language Models, remains lacking. To bridge\nthis gap, we present the annotation of two dialog corpora employing Grounding\nActs, Grounding Units, and a measure of their degree of grounding. We discuss\nour key findings during the annotation and also provide a baseline model to\ntest the performance of current Language Models in categorizing the grounding\nacts of the dialogs. Our work aims to provide a useful resource for further\nresearch in making conversations with machines better understood and more\nreliable in natural day-to-day collaborative dialogs.\n","authors":["Biswesh Mohapatra","Seemab Hassan","Laurent Romary","Justine Cassell"],"pdf_url":"https://arxiv.org/pdf/2403.16609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16592v1","updated":"2024-03-25T10:09:03Z","published":"2024-03-25T10:09:03Z","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain\n  Machine Generated Text Detection Techniques","summary":"  The Large Language Models (LLMs) exhibit remarkable ability to generate\nfluent content across a wide spectrum of user queries. However, this capability\nhas raised concerns regarding misinformation and personal information leakage.\nIn this paper, we present our methods for the SemEval2024 Task8, aiming to\ndetect machine-generated text across various domains in both mono-lingual and\nmulti-lingual contexts. Our study comprehensively analyzes various methods to\ndetect machine-generated text, including statistical, neural, and pre-trained\nmodel approaches. We also detail our experimental setup and perform a in-depth\nerror analysis to evaluate the effectiveness of these methods. Our methods\nobtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for\nsubtask-B. Furthermore, we also highlight the challenges and essential factors\nfor consideration in future studies.\n","authors":["Ashok Urlana","Aditya Saibewar","Bala Mallikarjunarao Garlapati","Charaka Vinayak Kumar","Ajeet Kumar Singh","Srinivasa Rao Chalamala"],"pdf_url":"https://arxiv.org/pdf/2403.16592v1.pdf","comment":"8 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2403.16584v1","updated":"2024-03-25T09:51:54Z","published":"2024-03-25T09:51:54Z","title":"Can Large Language Models (or Humans) Distill Text?","summary":"  We investigate the potential of large language models (LLMs) to distill text:\nto remove the textual traces of an undesired forbidden variable. We employ a\nrange of LLMs with varying architectures and training approaches to distill\ntext by identifying and removing information about the target variable while\npreserving other relevant signals. Our findings shed light on the strengths and\nlimitations of LLMs in addressing the distillation and provide insights into\nthe strategies for leveraging these models in computational social science\ninvestigations involving text data. In particular, we show that in the strong\ntest of removing sentiment, the statistical association between the processed\ntext and sentiment is still clearly detectable to machine learning classifiers\npost-LLM-distillation. Furthermore, we find that human annotators also struggle\nto distill sentiment while preserving other semantic content. This suggests\nthere may be limited separability between concept variables in some text\ncontexts, highlighting limitations of methods relying on text-level\ntransformations and also raising questions about the robustness of distillation\nmethods that achieve statistical independence in representation space if this\nis difficult for human coders operating on raw text to attain.\n","authors":["Nicolas Audinet de Pieuchon","Adel Daoud","Connor Thomas Jerzak","Moa Johansson","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15248v2","updated":"2024-03-25T09:36:54Z","published":"2024-02-23T10:27:42Z","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues","summary":"  During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2402.15248v2.pdf","comment":"Accepted @ LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16554v1","updated":"2024-03-25T09:04:14Z","published":"2024-03-25T09:04:14Z","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","summary":"  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincar\\'e\nExplanation (PE), for modeling feature interactions using hyperbolic spaces in\nan $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a\nframework to project the embeddings into hyperbolic spaces, which exhibit\nbetter inductive biases for syntax and semantic hierarchical structures.\nEventually, we prove that the hierarchical clustering process in the projected\nspace could be viewed as building a minimum spanning tree and propose a time\nefficient algorithm. Experimental results demonstrate the effectiveness of our\napproach.\n","authors":["Qian Chen","Xiaofeng He","Hongzhao Li","Hongyu Yi"],"pdf_url":"https://arxiv.org/pdf/2403.16554v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.19531v6","updated":"2024-03-25T08:46:58Z","published":"2023-10-30T13:33:21Z","title":"MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties\n  in Generative Language Models","summary":"  Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Xue Bai","Zijia Lin","Hui Chen","Guiguang Ding","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2310.19531v6.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01479v3","updated":"2024-03-25T08:46:15Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v3.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2307.06708v2","updated":"2024-03-25T08:44:53Z","published":"2023-07-13T12:06:48Z","title":"To share or not to share: What risks would laypeople accept to give\n  sensitive data to differentially-private NLP systems?","summary":"  Although the NLP community has adopted central differential privacy as a\ngo-to framework for privacy-preserving model training or data sharing, the\nchoice and interpretation of the key parameter, privacy budget $\\varepsilon$\nthat governs the strength of privacy protection, remains largely arbitrary. We\nargue that determining the $\\varepsilon$ value should not be solely in the\nhands of researchers or system developers, but must also take into account the\nactual people who share their potentially sensitive data. In other words: Would\nyou share your instant messages for $\\varepsilon$ of 10? We address this\nresearch gap by designing, implementing, and conducting a behavioral experiment\n(311 lay participants) to study the behavior of people in uncertain\ndecision-making situations with respect to privacy-threatening situations.\nFraming the risk perception in terms of two realistic NLP scenarios and using a\nvignette behavioral study help us determine what $\\varepsilon$ thresholds would\nlead lay people to be willing to share sensitive textual data - to our\nknowledge, the first study of its kind.\n","authors":["Christopher Weiss","Frauke Kreuter","Ivan Habernal"],"pdf_url":"https://arxiv.org/pdf/2307.06708v2.pdf","comment":"Accepted at LREC-COLING 2024; final camera-ready version"},{"id":"http://arxiv.org/abs/2403.16543v1","updated":"2024-03-25T08:36:06Z","published":"2024-03-25T08:36:06Z","title":"Efficient Information Extraction in Few-Shot Relation Classification\n  through Contrastive Representation Learning","summary":"  Differentiating relationships between entity pairs with limited labeled\ninstances poses a significant challenge in few-shot relation classification.\nRepresentations of textual data extract rich information spanning the domain,\nentities, and relations. In this paper, we introduce a novel approach to\nenhance information extraction combining multiple sentence representations and\ncontrastive learning. While representations in relation classification are\ncommonly extracted using entity marker tokens, we argue that substantial\ninformation within the internal model representations remains untapped. To\naddress this, we propose aligning multiple sentence representations, such as\nthe [CLS] token, the [MASK] token used in prompting, and entity marker tokens.\nOur method employs contrastive learning to extract complementary discriminative\ninformation from these individual representations. This is particularly\nrelevant in low-resource settings where information is scarce. Leveraging\nmultiple sentence representations is especially effective in distilling\ndiscriminative information for relation classification when additional\ninformation, like relation descriptions, are not available. We validate the\nadaptability of our approach, maintaining robust performance in scenarios that\ninclude relation descriptions, and showcasing its flexibility to adapt to\ndifferent resource constraints.\n","authors":["Philipp Borchert","Jochen De Weerdt","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.16543v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2309.08503v2","updated":"2024-03-25T08:33:37Z","published":"2023-09-15T16:05:48Z","title":"HealthFC: Verifying Health Claims with Evidence-Based Medical\n  Fact-Checking","summary":"  In the digital age, seeking health advice on the Internet has become a common\npractice. At the same time, determining the trustworthiness of online medical\ncontent is increasingly challenging. Fact-checking has emerged as an approach\nto assess the veracity of factual claims using evidence from credible knowledge\nsources. To help advance automated Natural Language Processing (NLP) solutions\nfor this task, in this paper we introduce a novel dataset HealthFC. It consists\nof 750 health-related claims in German and English, labeled for veracity by\nmedical experts and backed with evidence from systematic reviews and clinical\ntrials. We provide an analysis of the dataset, highlighting its characteristics\nand challenges. The dataset can be used for NLP tasks related to automated\nfact-checking, such as evidence retrieval, claim verification, or explanation\ngeneration. For testing purposes, we provide baseline systems based on\ndifferent approaches, examine their performance, and discuss the findings. We\nshow that the dataset is a challenging test bed with a high potential for\nfuture use.\n","authors":["Juraj Vladika","Phillip Schneider","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2309.08503v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.11504v2","updated":"2024-03-25T08:16:06Z","published":"2024-01-21T14:28:41Z","title":"With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation","summary":"  Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.\n","authors":["Y. Wang","D. Ma","D. Cai"],"pdf_url":"https://arxiv.org/pdf/2401.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16512v1","updated":"2024-03-25T07:55:29Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16483v1","updated":"2024-03-25T07:08:13Z","published":"2024-03-25T07:08:13Z","title":"Automatic Construction of a Large-Scale Corpus for Geoparsing Using\n  Wikipedia Hyperlinks","summary":"  Geoparsing is the task of estimating the latitude and longitude (coordinates)\nof location expressions in texts. Geoparsing must deal with the ambiguity of\nthe expressions that indicate multiple locations with the same notation. For\nevaluating geoparsing systems, several corpora have been proposed in previous\nwork. However, these corpora are small-scale and suffer from the coverage of\nlocation expressions on general domains. In this paper, we propose Wikipedia\nHyperlink-based Location Linking (WHLL), a novel method to construct a\nlarge-scale corpus for geoparsing from Wikipedia articles. WHLL leverages\nhyperlinks in Wikipedia to annotate multiple location expressions with\ncoordinates. With this method, we constructed the WHLL corpus, a new\nlarge-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,\neach containing about 7.8 unique location expressions. 45.6% of location\nexpressions are ambiguous and refer to more than one location with the same\nnotation. In each article, location expressions of the article title and those\nhyperlinks to other articles are assigned with coordinates. By utilizing\nhyperlinks, we can accurately assign location expressions with coordinates even\nwith ambiguous location expressions in the texts. Experimental results show\nthat there remains room for improvement by disambiguating location expressions.\n","authors":["Keyaki Ohno","Hirotaka Kameko","Keisuke Shirai","Taichi Nishimura","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2403.16483v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.04357v4","updated":"2024-03-25T06:54:10Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v4.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2309.13182v2","updated":"2024-03-25T06:49:16Z","published":"2023-09-22T21:15:28Z","title":"Effective Distillation of Table-based Reasoning Ability from LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their enormous\nparameter size and extremely high requirements for compute power pose\nchallenges for their practical deployment. Recent research has revealed that\nspecific capabilities of LLMs, such as numerical reasoning, can be transferred\nto smaller models through distillation. Some studies explore the potential of\nleveraging LLMs to perform table-based reasoning. However, there has been no\nprior work focusing on table reasoning skills in smaller models specifically\ntailored for scientific table-to-text generation tasks. In this paper, we\npropose a novel table-based reasoning distillation approach, with the aim of\ndistilling LLMs into tailored smaller models. Our experimental results have\nshown that a 220 million parameter model (Flan-T5-base) fine-tuned using\ndistilled data, not only achieves a significant improvement compared to\ntraditionally fine-tuned baselines, but also surpasses specific LLMs on a\nscientific table-to-text generation dataset. Our code is available at\nhttps://github.com/Bernard-Yang/DistillTableCoT.\n","authors":["Bohao Yang","Chen Tang","Kun Zhao","Chenghao Xiao","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2309.13182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16463v1","updated":"2024-03-25T06:45:09Z","published":"2024-03-25T06:45:09Z","title":"Few-shot Named Entity Recognition via Superposition Concept\n  Discrimination","summary":"  Few-shot NER aims to identify entities of target types with only limited\nnumber of illustrative instances. Unfortunately, few-shot NER is severely\nchallenged by the intrinsic precise generalization problem, i.e., it is hard to\naccurately determine the desired target type due to the ambiguity stemming from\ninformation deficiency. In this paper, we propose Superposition Concept\nDiscriminator (SuperCD), which resolves the above challenge via an active\nlearning paradigm. Specifically, a concept extractor is first introduced to\nidentify superposition concepts from illustrative instances, with each concept\ncorresponding to a possible generalization boundary. Then a superposition\ninstance retriever is applied to retrieve corresponding instances of these\nsuperposition concepts from large-scale text corpus. Finally, annotators are\nasked to annotate the retrieved instances and these annotated instances\ntogether with original illustrative instances are used to learn FS-NER models.\nTo this end, we learn a universal concept extractor and superposition instance\nretriever using a large-scale openly available knowledge bases. Experiments\nshow that SuperCD can effectively identify superposition concepts from\nillustrative instances, retrieve superposition instances from large-scale\ncorpus, and significantly improve the few-shot NER performance with minimal\nadditional efforts.\n","authors":["Jiawei Chen","Hongyu Lin","Xianpei Han","Yaojie Lu","Shanshan Jiang","Bin Dong","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.16463v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16447v1","updated":"2024-03-25T06:18:18Z","published":"2024-03-25T06:18:18Z","title":"A Study on How Attention Scores in the BERT Model are Aware of Lexical\n  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark","summary":"  This study examines whether the attention scores between tokens in the BERT\nmodel significantly vary based on lexical categories during the fine-tuning\nprocess for downstream tasks. Drawing inspiration from the notion that in human\nlanguage processing, syntactic and semantic information is parsed differently,\nwe categorize tokens in sentences according to their lexical categories and\nfocus on changes in attention scores among these categories. Our hypothesis\nposits that in downstream tasks that prioritize semantic information, attention\nscores centered on content words are enhanced, while in cases emphasizing\nsyntactic information, attention scores centered on function words are\nintensified. Through experimentation conducted on six tasks from the GLUE\nbenchmark dataset, we substantiate our hypothesis regarding the fine-tuning\nprocess. Furthermore, our additional investigations reveal the presence of BERT\nlayers that consistently assign more bias to specific lexical categories,\nirrespective of the task, highlighting the existence of task-agnostic lexical\ncategory preferences.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16446v1","updated":"2024-03-25T06:17:54Z","published":"2024-03-25T06:17:54Z","title":"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,\n  Data, and Algorithm","summary":"  Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.\n","authors":["Lei Liu","Xiaoyan Yang","Fangzhou Li","Chenfei Chi","Yue Shen","Shiwei Lyu Ming Zhang","Xiaowei Ma","Xiangguo Lyu","Liya Ma","Zhiqiang Zhang","Wei Xue","Yiran Huang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16444v1","updated":"2024-03-25T06:15:21Z","published":"2024-03-25T06:15:21Z","title":"KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for\n  Fine-Tuning Korean Large Language Models","summary":"  Instruction Tuning on Large Language Models is an essential process for model\nto function well and achieve high performance in specific tasks. Accordingly,\nin mainstream languages such as English, instruction-based datasets are being\nconstructed and made publicly available. In the case of Korean, publicly\navailable models and datasets all rely on using the output of ChatGPT or\ntranslating datasets built in English. In this paper, We introduce\n\\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.\n\\textit{KIT-19} is a dataset created in an instruction format, comprising 19\nexisting open-source datasets for Korean NLP tasks. In this paper, we train a\nKorean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness.\nThe experimental results show that the model trained on \\textit{KIT-19}\nsignificantly outperforms existing Korean LLMs. Based on the its quality and\nempirical results, this paper proposes that \\textit{KIT-19} has the potential\nto make a substantial contribution to the future improvement of Korean LLMs'\nperformance.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyemi Jo","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16443v1","updated":"2024-03-25T06:09:55Z","published":"2024-03-25T06:09:55Z","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","summary":"  The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.\n","authors":["Daoguang Zan","Ailun Yu","Wei Liu","Dong Chen","Bo Shen","Wei Li","Yafen Yao","Yongshun Gong","Xiaolin Chen","Bei Guan","Zhiguang Yang","Yongji Wang","Qianxiang Wang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16443v1.pdf","comment":"https://github.com/NL2Code/CodeS"},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.08298v2","updated":"2024-03-25T06:01:49Z","published":"2023-11-14T16:43:29Z","title":"A Survey of Confidence Estimation and Calibration in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.\n","authors":["Jiahui Geng","Fengyu Cai","Yuxia Wang","Heinz Koeppl","Preslav Nakov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.08298v2.pdf","comment":"16 pages, 1 page, 1 table"},{"id":"http://arxiv.org/abs/2403.16437v1","updated":"2024-03-25T05:37:16Z","published":"2024-03-25T05:37:16Z","title":"Evaluating Large Language Models with Runtime Behavior of Program\n  Execution","summary":"  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs.\n","authors":["Junkai Chen","Zhiyuan Pan","Xing Hu","Zhenhao Li","Ge Li","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2403.16437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14643v2","updated":"2024-03-25T05:35:12Z","published":"2024-02-21T16:44:35Z","title":"Exploring ChatGPT and its Impact on Society","summary":"  Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.\n","authors":["Md. Asraful Haque","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2403.14643v2.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16432v1","updated":"2024-03-25T05:27:35Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n\\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v1.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2310.01777v2","updated":"2024-03-25T04:04:05Z","published":"2023-10-03T03:56:26Z","title":"SEA: Sparse Linear Attention with Estimated Attention Mask","summary":"  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n","authors":["Heejun Lee","Jina Kim","Jeffrey Willette","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.01777v2.pdf","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2308.14115v2","updated":"2024-03-25T03:54:48Z","published":"2023-08-27T14:14:28Z","title":"Situated Natural Language Explanations","summary":"  Natural language is among the most accessible tools for explaining decisions\nto humans, and large pretrained language models (PLMs) have demonstrated\nimpressive abilities to generate coherent natural language explanations (NLE).\nThe existing NLE research perspectives do not take the audience into account.\nAn NLE can have high textual quality, but it might not accommodate audiences'\nneeds and preference. To address this limitation, we propose an alternative\nperspective, \\textit{situated} NLE. On the evaluation side, we set up automated\nevaluation scores. These scores describe the properties of NLEs in lexical,\nsemantic, and pragmatic categories. On the generation side, we identify three\nprompt engineering techniques and assess their applicability on the situations.\nSituated NLE provides a perspective and facilitates further research on the\ngeneration and evaluation of explanations.\n","authors":["Zining Zhu","Haoming Jiang","Jingfeng Yang","Sreyashi Nag","Chao Zhang","Jie Huang","Yifan Gao","Frank Rudzicz","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2308.14115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16396v1","updated":"2024-03-25T03:19:20Z","published":"2024-03-25T03:19:20Z","title":"Is There a One-Model-Fits-All Approach to Information Extraction?\n  Revisiting Task Definition Biases","summary":"  Definition bias is a negative phenomenon that can mislead models. Definition\nbias in information extraction appears not only across datasets from different\ndomains but also within datasets sharing the same domain. We identify two types\nof definition bias in IE: bias among information extraction datasets and bias\nbetween information extraction datasets and instruction tuning datasets. To\nsystematically investigate definition bias, we conduct three probing\nexperiments to quantitatively analyze it and discover the limitations of\nunified information extraction and large language models in solving definition\nbias. To mitigate definition bias in information extraction, we propose a\nmulti-stage framework consisting of definition bias measurement, bias-aware\nfine-tuning, and task-specific bias mitigation. Experimental results\ndemonstrate the effectiveness of our framework in addressing definition bias.\nResources of this paper can be found at\nhttps://github.com/EZ-hwh/definition-bias\n","authors":["Wenhao Huang","Qianyu He","Zhixu Li","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16396v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16394v1","updated":"2024-03-25T03:18:39Z","published":"2024-03-25T03:18:39Z","title":"Skews in the Phenomenon Space Hinder Generalization in Text-to-Image\n  Generation","summary":"  The literature on text-to-image generation is plagued by issues of faithfully\ncomposing entities with relations. But there lacks a formal understanding of\nhow entity-relation compositions can be effectively learned. Moreover, the\nunderlying phenomenon space that meaningfully reflects the problem structure is\nnot well-defined, leading to an arms race for larger quantities of data in the\nhope that generalization emerges out of large-scale pretraining. We hypothesize\nthat the underlying phenomenological coverage has not been proportionally\nscaled up, leading to a skew of the presented phenomenon which harms\ngeneralization. We introduce statistical metrics that quantify both the\nlinguistic and visual skew of a dataset for relational learning, and show that\ngeneralization failures of text-to-image generation are a direct result of\nincomplete or unbalanced phenomenological coverage. We first perform\nexperiments in a synthetic domain and demonstrate that systematically\ncontrolled metrics are strongly predictive of generalization performance. Then\nwe move to natural images and show that simple distribution perturbations in\nlight of our theories boost generalization without enlarging the absolute data\nsize. This work informs an important direction towards quality-enhancing the\ndata diversity or balance orthogonal to scaling up the absolute size. Our\ndiscussions point out important open questions on 1) Evaluation of generated\nentity-relation compositions, and 2) Better models for reasoning with abstract\nrelations.\n","authors":["Yingshan Chang","Yasi Zhang","Zhiyuan Fang","Yingnian Wu","Yonatan Bisk","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.16394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v2","updated":"2024-03-25T03:06:08Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16385v1","updated":"2024-03-25T03:02:27Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Li Zhuowan","Jasani Bhavan","Tang Peng","Ghadar Shabnam"],"pdf_url":"https://arxiv.org/pdf/2403.16385v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2308.11432v4","updated":"2024-03-25T02:56:58Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v4.pdf","comment":"35 pages, 5 figures, 3 tables, has been accepted by frontiers of\n  computer science (FCS), doi={10.1007/s11704-024-40231-1}"},{"id":"http://arxiv.org/abs/2402.10670v2","updated":"2024-03-25T02:52:43Z","published":"2024-02-16T13:21:33Z","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models","summary":"  Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.\n","authors":["Yuxuan Kuang","Hai Lin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.10670v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"}]},"2024-03-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.01623v3","updated":"2024-03-24T23:13:06Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v3.pdf","comment":"MLSys'24"},{"id":"http://arxiv.org/abs/2311.11202v2","updated":"2024-03-24T22:02:47Z","published":"2023-11-19T02:34:12Z","title":"Unmasking and Improving Data Credibility: A Study with Datasets for\n  Training Harmless Language Models","summary":"  Language models have shown promise in various tasks but can be affected by\nundesired data during training, fine-tuning, or alignment. For example, if some\nunsafe conversations are wrongly annotated as safe ones, the model fine-tuned\non these samples may be harmful. Therefore, the correctness of annotations,\ni.e., the credibility of the dataset, is important. This study focuses on the\ncredibility of real-world datasets, including the popular benchmarks Jigsaw\nCivil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\ncan be used for training a harmless language model. Given the cost and\ndifficulty of cleaning these datasets by humans, we introduce a systematic\nframework for evaluating the credibility of datasets, identifying label errors,\nand evaluating the influence of noisy labels in the curated language data,\nspecifically focusing on unsafe comments and conversation classification. With\nthe framework, we find and fix an average of 6.16% label errors in 11 datasets\nconstructed from the above benchmarks. The data credibility and downstream\nlearning performance can be remarkably improved by directly fixing label\nerrors, indicating the significance of cleaning existing real-world datasets.\nWe provide an open-source tool, Docta, for data cleaning at\nhttps://github.com/Docta-ai/docta.\n","authors":["Zhaowei Zhu","Jialu Wang","Hao Cheng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11202v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16303v1","updated":"2024-03-24T21:29:39Z","published":"2024-03-24T21:29:39Z","title":"Large Language Models in Biomedical and Health Informatics: A\n  Bibliometric Review","summary":"  Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This bibliometric review aims to provide\na panoramic view of how LLMs have been used in BHI by examining research\narticles and collaboration networks from 2022 to 2023. It further explores how\nLLMs can improve Natural Language Processing (NLP) applications in various BHI\nareas like medical diagnosis, patient engagement, electronic health record\nmanagement, and personalized medicine. To do this, our bibliometric review\nidentifies key trends, maps out research networks, and highlights major\ndevelopments in this fast-moving field. Lastly, it discusses the ethical\nconcerns and practical challenges of using LLMs in BHI, such as data privacy\nand reliable medical recommendations. Looking ahead, we consider how LLMs could\nfurther transform biomedical research as well as healthcare delivery and\npatient outcomes. This comprehensive review serves as a resource for\nstakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.\n","authors":["Huizi Yu","Lizhou Fan","Lingyao Li","Jiayan Zhou","Zihui Ma","Lu Xian","Wenyue Hua","Sijia He","Mingyu Jin","Yongfeng Zhang","Ashvin Gandhi","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16303v1.pdf","comment":"50 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.16295v1","updated":"2024-03-24T21:02:35Z","published":"2024-03-24T21:02:35Z","title":"LexDrafter: Terminology Drafting for Legislative Documents using\n  Retrieval Augmented Generation","summary":"  With the increase in legislative documents at the EU, the number of new terms\nand their definitions is increasing as well. As per the Joint Practical Guide\nof the European Parliament, the Council and the Commission, terms used in legal\ndocuments shall be consistent, and identical concepts shall be expressed\nwithout departing from their meaning in ordinary, legal, or technical language.\nThus, while drafting a new legislative document, having a framework that\nprovides insights about existing definitions and helps define new terms based\non a document's context will support such harmonized legal definitions across\ndifferent regulations and thus avoid ambiguities. In this paper, we present\nLexDrafter, a framework that assists in drafting Definitions articles for\nlegislative documents using retrieval augmented generation (RAG) and existing\nterm definitions present in different legislative documents. For this,\ndefinition elements are built by extracting definitions from existing\ndocuments. Using definition elements and RAG, a Definitions article can be\nsuggested on demand for a legislative document that is being drafted. We\ndemonstrate and evaluate the functionality of LexDrafter using a collection of\nEU documents from the energy domain. The code for LexDrafter framework is\navailable at https://github.com/achouhan93/LexDrafter.\n","authors":["Ashish Chouhan","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2403.16295v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02017v7","updated":"2024-03-24T19:15:22Z","published":"2023-03-27T21:27:58Z","title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its\n  Applications, Advantages, Limitations, and Future Directions in Natural\n  Language Processing","summary":"  Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n","authors":["Walid Hariri"],"pdf_url":"https://arxiv.org/pdf/2304.02017v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16265v1","updated":"2024-03-24T18:59:38Z","published":"2024-03-24T18:59:38Z","title":"Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved\n  Phrase Graphs","summary":"  We study the patent phrase similarity inference task, which measures the\nsemantic similarity between two patent phrases. As patent documents employ\nlegal and highly technical language, existing semantic textual similarity\nmethods that use localized contextual information do not perform satisfactorily\nin inferring patent phrase similarity. To address this, we introduce a\ngraph-augmented approach to amplify the global contextual information of the\npatent phrases. For each patent phrase, we construct a phrase graph that links\nto its focal patents and a list of patents that are either cited by or cite\nthese focal patents. The augmented phrase embedding is then derived from\ncombining its localized contextual embedding with its global embedding within\nthe phrase graph. We further propose a self-supervised learning objective that\ncapitalizes on the retrieved topology to refine both the contextualized\nembedding and the graph parameters in an end-to-end manner. Experimental\nresults from a unique patent phrase similarity dataset demonstrate that our\napproach significantly enhances the representation of patent phrases, resulting\nin marked improvements in similarity inference in a self-supervised fashion.\nSubstantial improvements are also observed in the supervised setting,\nunderscoring the potential benefits of leveraging retrieved phrase graph\naugmentation.\n","authors":["Zhuoyi Peng","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16265v1.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2305.14310v3","updated":"2024-03-24T18:03:10Z","published":"2023-05-23T17:48:21Z","title":"Navigating Prompt Complexity for Zero-Shot Classification: A Study of\n  Large Language Models in Computational Social Science","summary":"  Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.\n","authors":["Yida Mu","Ben P. Wu","William Thorne","Ambrose Robinson","Nikolaos Aletras","Carolina Scarton","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2305.14310v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16248v1","updated":"2024-03-24T17:39:51Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16247v1","updated":"2024-03-24T17:39:36Z","published":"2024-03-24T17:39:36Z","title":"Improving Sequence-to-Sequence Models for Abstractive Text Summarization\n  Using Meta Heuristic Approaches","summary":"  As human society transitions into the information age, reduction in our\nattention span is a contingency, and people who spend time reading lengthy news\narticles are decreasing rapidly and the need for succinct information is higher\nthan ever before. Therefore, it is essential to provide a quick overview of\nimportant news by concisely summarizing the top news article and the most\nintuitive headline. When humans try to make summaries, they extract the\nessential information from the source and add useful phrases and grammatical\nannotations from the original extract. Humans have a unique ability to create\nabstractions. However, automatic summarization is a complicated problem to\nsolve. The use of sequence-to-sequence (seq2seq) models for neural abstractive\ntext summarization has been ascending as far as prevalence. Numerous innovative\nstrategies have been proposed to develop the current seq2seq models further,\npermitting them to handle different issues like saliency, familiarity, and\nhuman lucidness and create excellent synopses. In this article, we aimed toward\nenhancing the present architectures and models for abstractive text\nsummarization. The modifications have been aimed at fine-tuning\nhyper-parameters, attempting specific encoder-decoder combinations. We examined\nmany experiments on an extensively used CNN/DailyMail dataset to check the\neffectiveness of various models.\n","authors":["Aditya Saxena","Ashutosh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.16247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14119v2","updated":"2024-03-24T17:16:53Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2310.16450v3","updated":"2024-03-24T17:14:11Z","published":"2023-10-25T08:13:02Z","title":"CLEX: Continuous Length Extrapolation for Large Language Models","summary":"  Transformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position\nEmbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their\nextrapolation abilities or sacrificing partial performance within the context\nwindow. Length extrapolation methods, although theoretically capable of\nextending the context window beyond the training sequence length, often\nunderperform in practical long-context applications. To address these\nchallenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We\ngeneralise the PE scaling approaches to model the continuous dynamics by\nordinary differential equations over the length scaling factor, thereby\novercoming the constraints of current PE scaling methods designed for specific\nlengths. Moreover, by extending the dynamics to desired context lengths beyond\nthe training sequence length, CLEX facilitates the length extrapolation with\nimpressive performance in practical tasks. We demonstrate that CLEX can be\nseamlessly incorporated into LLMs equipped with Rotary Position Embedding, such\nas LLaMA and GPT-NeoX, with negligible impact on training and inference\nlatency. Experimental results reveal that CLEX can effectively extend the\ncontext window to over 4x or almost 8x training length, with no deterioration\nin performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to\n32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.\n","authors":["Guanzheng Chen","Xin Li","Zaiqiao Meng","Shangsong Liang","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2310.16450v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2304.04806v3","updated":"2024-03-24T16:46:06Z","published":"2023-04-10T18:31:26Z","title":"Examining Temporalities on Stance Detection towards COVID-19 Vaccination","summary":"  Previous studies have highlighted the importance of vaccination as an\neffective strategy to control the transmission of the COVID-19 virus. It is\ncrucial for policymakers to have a comprehensive understanding of the public's\nstance towards vaccination on a large scale. However, attitudes towards\nCOVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved\nover time on social media. Thus, it is necessary to account for possible\ntemporal shifts when analysing these stances. This study aims to examine the\nimpact of temporal concept drift on stance detection towards COVID-19\nvaccination on Twitter. To this end, we evaluate a range of transformer-based\nmodels using chronological (splitting the training, validation, and test sets\nin order of time) and random splits (randomly splitting these three sets) of\nsocial media data. Our findings reveal significant discrepancies in model\nperformance between random and chronological splits in several existing\nCOVID-19-related datasets; specifically, chronological splits significantly\nreduce the accuracy of stance classification. Therefore, real-world stance\ndetection approaches need to be further refined to incorporate temporal factors\nas a key consideration.\n","authors":["Yida Mu","Mali Jin","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2304.04806v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16204v1","updated":"2024-03-24T15:57:24Z","published":"2024-03-24T15:57:24Z","title":"SQL-Encoder: Improving NL2SQL In-Context Learning Through a\n  Context-Aware Encoder","summary":"  Detecting structural similarity between queries is essential for selecting\nexamples in in-context learning models. However, assessing structural\nsimilarity based solely on the natural language expressions of queries, without\nconsidering SQL queries, presents a significant challenge. This paper explores\nthe significance of this similarity metric and proposes a model for accurately\nestimating it. To achieve this, we leverage a dataset comprising 170k question\npairs, meticulously curated to train a similarity prediction model. Our\ncomprehensive evaluation demonstrates that the proposed model adeptly captures\nthe structural similarity between questions, as evidenced by improvements in\nKendall-Tau distance and precision@k metrics. Notably, our model outperforms\nstrong competitive embedding models from OpenAI and Cohere. Furthermore,\ncompared to these competitive models, our proposed encoder enhances the\ndownstream performance of NL2SQL models in 1-shot in-context learning scenarios\nby 1-2\\% for GPT-3.5-turbo, 4-8\\% for CodeLlama-7B, and 2-3\\% for\nCodeLlama-13B.\n","authors":["Mohammadreza Pourreza","Davood Rafiei","Yuxi Feng","Raymond Li","Zhenan Fan","Weiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16694v2","updated":"2024-03-24T15:41:21Z","published":"2024-02-26T16:09:00Z","title":"HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual\n  Natural Language Generalization","summary":"  Large language models (LLMs) have made significant progress in generating\ncodes from textual prompts. However, existing benchmarks have mainly\nconcentrated on translating English prompts to multilingual codes or have been\nconstrained to very limited natural languages (NLs). These benchmarks have\noverlooked the vast landscape of massively multilingual NL to multilingual\ncode, leaving a critical gap in the evaluation of multilingual LLMs. In\nresponse, we introduce HumanEval-XL, a massively multilingual code generation\nbenchmark specifically crafted to address this deficiency. HumanEval-XL\nestablishes connections between 23 NLs and 12 programming languages (PLs), and\ncomprises of a collection of 22,080 prompts with an average of 8.33 test cases.\nBy ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a\ncomprehensive evaluation platform for multilingual LLMs, allowing the\nassessment of the understanding of different NLs. Our work serves as a\npioneering step towards filling the void in evaluating NL generalization in the\narea of multilingual code generation. We make our evaluation code and data\npublicly available at \\url{https://github.com/FloatAI/humaneval-xl}.\n","authors":["Qiwei Peng","Yekun Chai","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2402.16694v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16187v1","updated":"2024-03-24T15:09:55Z","published":"2024-03-24T15:09:55Z","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language\n  Models","summary":"  Parameter-efficient fine-tuning (PEFT) is widely studied for its\neffectiveness and efficiency in the era of large language models. Low-rank\nadaptation (LoRA) has demonstrated commendable performance as a popular and\nrepresentative method. However, it is implemented with a fixed intrinsic rank\nthat might not be the ideal setting for the downstream tasks. Recognizing the\nneed for more flexible downstream task adaptation, we extend the methodology of\nLoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)\nthat enables dynamic adjustments to the intrinsic rank during the adaptation\nprocess. First, we propose a novel method, AB-LoRA, that can effectively\nestimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we\ngradually prune abundant and negatively impacting LoRA ranks and allocate the\npruned LoRA budgets to important Transformer modules needing higher ranks. We\nhave conducted experiments on various tasks, and the experimental results\ndemonstrate that our ALoRA method can outperform the recent baselines with\ncomparable tunable parameters.\n","authors":["Zequan Liu","Jiawen Lyn","Wei Zhu","Xing Tian","Yvette Graham"],"pdf_url":"https://arxiv.org/pdf/2403.16187v1.pdf","comment":"Accepted by NAACL-2024"},{"id":"http://arxiv.org/abs/2403.16176v1","updated":"2024-03-24T14:35:44Z","published":"2024-03-24T14:35:44Z","title":"Subspace Defense: Discarding Adversarial Perturbations by Learning a\n  Subspace for Clean Signals","summary":"  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks\nthat place carefully crafted perturbations on normal examples to fool DNNs. To\nbetter understand such attacks, a characterization of the features carried by\nadversarial examples is needed. In this paper, we tackle this challenge by\ninspecting the subspaces of sample features through spectral analysis. We first\nempirically show that the features of either clean signals or adversarial\nperturbations are redundant and span in low-dimensional linear subspaces\nrespectively with minimal overlap, and the classical low-dimensional subspace\nprojection can suppress perturbation features out of the subspace of clean\nsignals. This makes it possible for DNNs to learn a subspace where only\nfeatures of clean signals exist while those of perturbations are discarded,\nwhich can facilitate the distinction of adversarial examples. To prevent the\nresidual perturbations that is inevitable in subspace learning, we propose an\nindependence criterion to disentangle clean signals from perturbations.\nExperimental results show that the proposed strategy enables the model to\ninherently suppress adversaries, which not only boosts model robustness but\nalso motivates new directions of effective adversarial defense.\n","authors":["Rui Zheng","Yuhao Zhou","Zhiheng Xi","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16176v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2302.14534v2","updated":"2024-03-24T14:34:53Z","published":"2023-02-28T12:44:10Z","title":"Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face","summary":"  We present Spacerini, a tool that integrates the Pyserini toolkit for\nreproducible information retrieval research with Hugging Face to enable the\nseamless construction and deployment of interactive search engines. Spacerini\nmakes state-of-the-art sparse and dense retrieval models more accessible to\nnon-IR practitioners while minimizing deployment effort. This is useful for NLP\nresearchers who want to better understand and validate their research by\nperforming qualitative analyses of training corpora, for IR researchers who\nwant to demonstrate new retrieval models integrated into the growing Pyserini\necosystem, and for third parties reproducing the work of other researchers.\nSpacerini is open source and includes utilities for loading, preprocessing,\nindexing, and deploying search engines locally and remotely. We demonstrate a\nportfolio of 13 search engines created with Spacerini for different use cases.\n","authors":["Christopher Akiki","Odunayo Ogundepo","Aleksandra Piktus","Xinyu Zhang","Akintunde Oladipo","Jimmy Lin","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2302.14534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v1","updated":"2024-03-24T14:21:06Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16158v1","updated":"2024-03-24T13:51:05Z","published":"2024-03-24T13:51:05Z","title":"Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition","summary":"  Named Entity Recognition (NER) plays a pivotal role in medical Natural\nLanguage Processing (NLP). Yet, there has not been an open-source medical NER\ndataset specifically for the Korean language. To address this, we utilized\nChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which\nwe are now presenting to the public. With the KBMC dataset, we noticed an\nimpressive 20% increase in medical NER performance compared to models trained\non general Korean NER datasets. This research underscores the significant\nbenefits and importance of using specialized tools and datasets, like ChatGPT,\nto enhance language processing in specialized fields such as healthcare.\n","authors":["Sungjoo Byun","Jiseung Hong","Sumin Park","Dongjun Jang","Jean Seo","Minseok Kim","Chaeyoung Oh","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02541v3","updated":"2024-03-24T13:39:45Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v3.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16142v1","updated":"2024-03-24T13:28:27Z","published":"2024-03-24T13:28:27Z","title":"What Happens to a Dataset Transformed by a Projection-based Concept\n  Removal Method?","summary":"  We investigate the behavior of methods that use linear projections to remove\ninformation about a concept from a language representation, and we consider the\nquestion of what happens to a dataset transformed by such a method. A\ntheoretical analysis and experiments on real-world and synthetic data show that\nthese methods inject strong statistical dependencies into the transformed\ndatasets. After applying such a method, the representation space is highly\nstructured: in the transformed space, an instance tends to be located near\ninstances of the opposite label. As a consequence, the original labeling can in\nsome cases be reconstructed by applying an anti-clustering method.\n","authors":["Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16139v1","updated":"2024-03-24T13:21:58Z","published":"2024-03-24T13:21:58Z","title":"A Little Leak Will Sink a Great Ship: Survey of Transparency for Large\n  Language Models from Start to Finish","summary":"  Large Language Models (LLMs) are trained on massive web-crawled corpora. This\nposes risks of leakage, including personal information, copyrighted texts, and\nbenchmark datasets. Such leakage leads to undermining human trust in AI due to\npotential unauthorized generation of content or overestimation of performance.\nWe establish the following three criteria concerning the leakage issues: (1)\nleakage rate: the proportion of leaked data in training data, (2) output rate:\nthe ease of generating leaked data, and (3) detection rate: the detection\nperformance of leaked versus non-leaked data. Despite the leakage rate being\nthe origin of data leakage issues, it is not understood how it affects the\noutput rate and detection rate. In this paper, we conduct an experimental\nsurvey to elucidate the relationship between the leakage rate and both the\noutput rate and detection rate for personal information, copyrighted texts, and\nbenchmark data. Additionally, we propose a self-detection approach that uses\nfew-shot learning in which LLMs detect whether instances are present or absent\nin their training data, in contrast to previous methods that do not employ\nexplicit learning. To explore the ease of generating leaked information, we\ncreate a dataset of prompts designed to elicit personal information,\ncopyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs\nproduce leaked information in most cases despite less such data in their\ntraining set. This indicates even small amounts of leaked data can greatly\naffect outputs. Our self-detection method showed superior performance compared\nto existing detection methods.\n","authors":["Masahiro Kaneko","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2403.16139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16129v1","updated":"2024-03-24T12:58:48Z","published":"2024-03-24T12:58:48Z","title":"A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation","summary":"  This paper explores techniques that focus on understanding and resolving\nambiguity in language within the field of natural language processing (NLP),\nhighlighting the complexity of linguistic phenomena such as polysemy and\nhomonymy and their implications for computational models. Focusing extensively\non Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from\ndeep learning techniques to leveraging lexical resources and knowledge graphs\nlike WordNet. The paper introduces cutting-edge methodologies like word sense\nextension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy\nby predicting new word senses. It examines specific applications in biomedical\ndisambiguation and language specific optimisation and discusses the\nsignificance of cognitive metaphors in discourse analysis. The research\nidentifies persistent challenges in the field, such as the scarcity of sense\nannotated corpora and the complexity of informal clinical texts. It concludes\nby suggesting future directions, including using large language models, visual\nWSD, and multilingual WSD systems, emphasising the ongoing evolution in\naddressing lexical complexities in NLP. This thinking perspective highlights\nthe advancement in this field to enable computers to understand language more\naccurately.\n","authors":["Miuru Abeysiriwardana","Deshan Sumanathilaka"],"pdf_url":"https://arxiv.org/pdf/2403.16129v1.pdf","comment":"6 pages, 5 figures, 3 tables, Accepted by 20th IEEE International\n  Colloquium on Signal Processing & its Applications (CSPA 2024)"},{"id":"http://arxiv.org/abs/2403.16127v1","updated":"2024-03-24T12:49:30Z","published":"2024-03-24T12:49:30Z","title":"WangchanLion and WangchanX MRC Eval","summary":"  This technical report describes the development of WangchanLion, an\ninstruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in\nthe Thai language. Our model is based on SEA-LION and a collection of\ninstruction following datasets. To promote open research and reproducibility,\nwe publically release all training data, code, and the final model weights\nunder the Apache-2 license. To assess the contextual understanding capability,\nwe conducted extensive experimental studies using two Thai MRC datasets, XQuAD\nand Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to\ncomprehend the context and produce an answer faithful to the reference one in\n0-shot and 1-shot settings. In addition, our evaluation goes beyond the\ntraditional MRC. We propose a new evaluation scheme assessing the answer's\ncorrectness, helpfulness, conciseness, and contextuality. Evaluation results\nprovide insight into how we can improve our model in the future. Our code is\npublic at https://github.com/vistec-AI/WangchanLion.\n","authors":["Wannaphong Phatthiyaphaibun","Surapon Nonesung","Patomporn Payoungkhamdee","Peerat Limkonchotiwat","Can Udomcharoenchaikit","Ekapol Chuangsuwanich","Sarana Nutanong"],"pdf_url":"https://arxiv.org/pdf/2403.16127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16099v1","updated":"2024-03-24T11:29:55Z","published":"2024-03-24T11:29:55Z","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","summary":"  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of\nFrench press considered unreliable by expert agencies, annotated using 11\nlabels by 8 annotators. By collecting more labels than usual, by more\nannotators than is typically done, we can identify features that humans\nconsider as characteristic of fake news, and compare them to the predictions of\nautomated classifiers. We present a topic and genre analysis using Gate Cloud,\nindicative of the prevalence of satire-like text in the corpus. We then use the\nsubjectivity analyzer VAGO, and a neural version of it, to clarify the link\nbetween ascriptions of the label Subjective and ascriptions of the label Fake\nNews. The annotated dataset is available online at the following url:\nhttps://github.com/obs-info/obsinfox\n  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,\nExaggeration, French Press\n","authors":["Benjamin Icard","François Maine","Morgane Casanova","Géraud Faye","Julien Chanson","Guillaume Gadek","Ghislain Atemezing","François Bancilhon","Paul Égré"],"pdf_url":"https://arxiv.org/pdf/2403.16099v1.pdf","comment":"Paper to appear in the Proceedings of the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.16087v1","updated":"2024-03-24T10:57:08Z","published":"2024-03-24T10:57:08Z","title":"LLMs as Compiler for Arabic Programming Language","summary":"  In this paper we introduce APL (Arabic Programming Language) that uses Large\nlanguage models (LLM) as semi-compiler to covert Arabic text code to python\ncode then run the code. Designing a full pipeline from the structure of the APL\ntext then a prompt (using prompt engineering) then running the prodcued python\ncode using PyRunner. This project has a three parts first python library, a\nplayground with simple interface and this research paper.\n","authors":["Serry Sibaee","Omar Najar","Lahouri Ghouti","Anis Koubaa"],"pdf_url":"https://arxiv.org/pdf/2403.16087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16084v1","updated":"2024-03-24T10:43:21Z","published":"2024-03-24T10:43:21Z","title":"Argument Quality Assessment in the Age of Instruction-Following Large\n  Language Models","summary":"  The computational treatment of arguments on controversial issues has been\nsubject to extensive NLP research, due to its envisioned impact on opinion\nformation, decision making, writing education, and the like. A critical task in\nany such application is the assessment of an argument's quality - but it is\nalso particularly challenging. In this position paper, we start from a brief\nsurvey of argument quality research, where we identify the diversity of quality\nnotions and the subjectiveness of their perception as the main hurdles towards\nsubstantial progress on argument quality assessment. We argue that the\ncapabilities of instruction-following large language models (LLMs) to leverage\nknowledge across contexts enable a much more reliable assessment. Rather than\njust fine-tuning LLMs towards leaderboard chasing on assessment tasks, they\nneed to be instructed systematically with argumentation theories and scenarios\nas well as with ways to solve argument-related problems. We discuss the\nreal-world opportunities and ethical issues emerging thereby.\n","authors":["Henning Wachsmuth","Gabriella Lapesa","Elena Cabrio","Anne Lauscher","Joonsuk Park","Eva Maria Vecchi","Serena Villata","Timon Ziegenbein"],"pdf_url":"https://arxiv.org/pdf/2403.16084v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.04369v3","updated":"2024-03-24T10:08:13Z","published":"2024-03-07T09:57:42Z","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge\n  Prediction","summary":"  Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.\n","authors":["Ang Li","Qiangchao Chen","Yiquan Wu","Ming Cai","Xiang Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.04369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18340v2","updated":"2024-03-24T09:09:00Z","published":"2023-10-22T02:32:53Z","title":"UrbanCLIP: Learning Text-enhanced Urban Region Profiling with\n  Contrastive Language-Image Pretraining from the Web","summary":"  Urban region profiling from web-sourced data is of utmost importance for\nurban planning and sustainable development. We are witnessing a rising trend of\nLLMs for various fields, especially dealing with multi-modal data research such\nas vision-language learning, where the text modality serves as a supplement\ninformation for the image. Since textual modality has never been introduced\ninto modality combinations in urban region profiling, we aim to answer two\nfundamental questions in this paper: i) Can textual modality enhance urban\nregion profiling? ii) and if so, in what ways and with regard to which aspects?\nTo answer the questions, we leverage the power of Large Language Models (LLMs)\nand introduce the first-ever LLM-enhanced framework that integrates the\nknowledge of textual modality into urban imagery profiling, named LLM-enhanced\nUrban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).\nSpecifically, it first generates a detailed textual description for each\nsatellite image by an open-source Image-to-Text LLM. Then, the model is trained\non the image-text pairs, seamlessly unifying natural language supervision for\nurban visual representation learning, jointly with contrastive loss and\nlanguage modeling loss. Results on predicting three urban indicators in four\nmajor Chinese metropolises demonstrate its superior performance, with an\naverage improvement of 6.1% on R^2 compared to the state-of-the-art methods.\nOur code and the image-language dataset will be released upon paper\nnotification.\n","authors":["Yibo Yan","Haomin Wen","Siru Zhong","Wei Chen","Haodong Chen","Qingsong Wen","Roger Zimmermann","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18340v2.pdf","comment":"Accepted by The Web Conference 2024"},{"id":"http://arxiv.org/abs/2403.16056v1","updated":"2024-03-24T07:48:05Z","published":"2024-03-24T07:48:05Z","title":"Qibo: A Large Language Model for Traditional Chinese Medicine","summary":"  In the field of Artificial Intelligence, Large Language Models (LLMs) have\ndemonstrated significant advances in user intent understanding and response in\na number of specialized domains, including medicine, law, and finance. However,\nin the unique domain of traditional Chinese medicine (TCM), the performance\nenhancement of LLMs is challenged by the essential differences between its\ntheories and modern medicine, as well as the lack of specialized corpus\nresources. In this paper, we aim to construct and organize a professional\ncorpus in the field of TCM, to endow the large model with professional\nknowledge that is characteristic of TCM theory, and to successfully develop the\nQibo model based on LLaMA, which is the first LLM in the field of TCM to\nundergo a complete training process from pre-training to Supervised Fine-Tuning\n(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for\nevaluating the performance of LLMs, which is a specialized tool for evaluating\nthe performance of LLMs in the TCM domain. This tool will provide an important\nbasis for quantifying and comparing the understanding and application\ncapabilities of different models in the field of traditional Chinese medicine,\nand provide guidance for future research directions and practical applications\nof intelligent assistants for traditional Chinese medicine. Finally, we\nconducted sufficient experiments to prove that Qibo has good performance in the\nfield of traditional Chinese medicine.\n","authors":["Heyi Zhang","Xin Wang","Zhaopeng Meng","Yongzhe Jia","Dawei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09282v4","updated":"2024-03-24T07:06:19Z","published":"2024-02-14T16:10:45Z","title":"Leveraging Large Language Models for Enhanced NLP Task Performance\n  through Knowledge Distillation and Optimized Training Strategies","summary":"  Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural\nLanguage Processing (NLP), showing potential in traditional tasks such as Named\nEntity Recognition (NER). Our study explores a three-phase training strategy\nthat harnesses GPT-4's capabilities to enhance the BERT model's performance on\nNER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC\ndataset without fine-tuning. We then train BERT using a mix of original and\nLLM-annotated data, analyzing the efficacy of LLM annotations against\ntraditional methods. The second phase involves comparative experiments with\ndifferent training regimens, assessing the synergy between distilled and\noriginal data. We observe that sequential strategies, particularly a simple mix\nof training first with distilled data followed by original data, significantly\nboost performance. In the third phase, we investigate various data blending\ntechniques, including sigmoid and power decay functions, to optimize the\ntraining process further. Our results indicate that a strategic mix of\ndistilled and original data markedly elevates the NER capabilities of BERT. Our\napproach presents a scalable methodology that reduces manual annotation costs\nand increases efficiency, making it especially pertinent in resource-limited\nand closed-network environments. The study concludes that while the 'Simple\nMix' strategy yields the best results, understanding its underlying mechanisms\nrequires further research. Future work will also focus on refining prompt\ndesigns and enhancing annotation selection processes, aiming to extend our\nmethodology to diverse NLP tasks.\n","authors":["Yining Huang","Keke Tang","Meilian Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09282v4.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16038v1","updated":"2024-03-24T06:49:07Z","published":"2024-03-24T06:49:07Z","title":"Monotonic Paraphrasing Improves Generalization of Language Model\n  Prompting","summary":"  Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.\n","authors":["Qin Liu","Fei Wang","Nan Xu","Tianyi Yan","Tao Meng","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10647v3","updated":"2024-03-24T06:46:19Z","published":"2024-01-19T11:48:09Z","title":"Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language\n  Models","summary":"  In the rapidly advancing field of artificial intelligence, the concept of\nRed-Teaming or Jailbreaking large language models (LLMs) has emerged as a\ncrucial area of study. This approach is especially significant in terms of\nassessing and enhancing the safety and robustness of these models. This paper\ninvestigates the intricate consequences of such modifications through model\nediting, uncovering a complex relationship between enhancing model accuracy and\npreserving its ethical integrity. Our in-depth analysis reveals a striking\nparadox: while injecting accurate information is crucial for model reliability,\nit can paradoxically destabilize the model's foundational framework, resulting\nin unpredictable and potentially unsafe behaviors. Additionally, we propose a\nbenchmark dataset NicheHazardQA to investigate this unsafe behavior both within\nthe same and cross topical domain. This aspect of our research sheds light on\nhow the edits, impact the model's safety metrics and guardrails. Our findings\nshow that model editing serves as a cost-effective tool for topical red-teaming\nby methodically applying targeted edits and evaluating the resultant model\nbehavior.\n","authors":["Rima Hazra","Sayan Layek","Somnath Banerjee","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.10647v3.pdf","comment":"Under review.\n  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}"},{"id":"http://arxiv.org/abs/2307.07697v6","updated":"2024-03-24T06:42:47Z","published":"2023-07-15T03:31:38Z","title":"Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph","summary":"  Although large language models (LLMs) have achieved significant success in\nvarious tasks, they often struggle with hallucination problems, especially in\nscenarios requiring deep and responsible reasoning. These issues could be\npartially addressed by introducing external knowledge graphs (KG) in LLM\nreasoning. In this paper, we propose a new LLM-KG integrating paradigm\n``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to\ninteractively explore related entities and relations on KGs and perform\nreasoning based on the retrieved knowledge. We further implement this paradigm\nby introducing a new approach called Think-on-Graph (ToG), in which the LLM\nagent iteratively executes beam search on KG, discovers the most promising\nreasoning paths, and returns the most likely reasoning results. We use a number\nof well-designed experiments to examine and illustrate the following advantages\nof ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has\nthe ability of knowledge traceability and knowledge correctability by\nleveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible\nplug-and-play framework for different LLMs, KGs and prompting strategies\nwithout any additional training cost; 4) the performance of ToG with small LLM\nmodels could exceed large LLM such as GPT-4 in certain scenarios and this\nreduces the cost of LLM deployment and application. As a training-free method\nwith lower computational cost and better generality, ToG achieves overall SOTA\nin 6 out of 9 datasets where most previous SOTAs rely on additional training.\n","authors":["Jiashuo Sun","Chengjin Xu","Lumingyuan Tang","Saizhuo Wang","Chen Lin","Yeyun Gong","Lionel M. Ni","Heung-Yeung Shum","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2307.07697v6.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16033v1","updated":"2024-03-24T06:28:54Z","published":"2024-03-24T06:28:54Z","title":"Node Classification via Semantic-Structural Attention-Enhanced Graph\n  Convolutional Networks","summary":"  Graph data, also known as complex network data, is omnipresent across various\ndomains and applications. Prior graph neural network models primarily focused\non extracting task-specific structural features through supervised learning\nobjectives, but they fell short in capturing the inherent semantic and\nstructural features of the entire graph. In this paper, we introduce the\nsemantic-structural attention-enhanced graph convolutional network (SSA-GCN),\nwhich not only models the graph structure but also extracts generalized\nunsupervised features to enhance vertex classification performance. The\nSSA-GCN's key contributions lie in three aspects: firstly, it derives semantic\ninformation through unsupervised feature extraction from a knowledge graph\nperspective; secondly, it obtains structural information through unsupervised\nfeature extraction from a complex network perspective; and finally, it\nintegrates these features through a cross-attention mechanism. By leveraging\nthese features, we augment the graph convolutional network, thereby enhancing\nthe model's generalization capabilities. Our experiments on the Cora and\nCiteSeer datasets demonstrate the performance improvements achieved by our\nproposed method. Furthermore, our approach also exhibits excellent accuracy\nunder privacy settings, making it a robust and effective solution for graph\ndata analysis.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11311v2","updated":"2024-03-24T06:21:27Z","published":"2024-03-17T19:12:26Z","title":"Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding","summary":"  Deep multimodal semantic understanding that goes beyond the mere superficial\ncontent relation mining has received increasing attention in the realm of\nartificial intelligence. The challenges of collecting and annotating\nhigh-quality multi-modal data have underscored the significance of few-shot\nlearning. In this paper, we focus on two critical tasks under this context:\nfew-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis\n(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware\nPrompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on\nthe unified vision-language model (VLM). Specifically, we design three experts\nof soft prompts: a text prompt and an image prompt that extract\nmodality-specific features to enrich the single-modal representation, and a\nunified prompt to assist multi-modal interaction. Additionally, we reorganize\nTransformer layers into several blocks and introduce cross-modal prompt\nattention between adjacent blocks, which smoothens the transition from\nsingle-modal representation to multi-modal fusion. On both MSD and MSA datasets\nin few-shot setting, our proposed model not only surpasses the 8.2B model\nInstructBLIP with merely 2% parameters (150M), but also significantly\noutperforms other widely-used prompt methods on VLMs or task-specific methods.\n","authors":["Zichen Wu","Hsiu-Yuan Huang","Fanyi Qu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11311v2.pdf","comment":"LREC-COLING 2024, Long Paper"},{"id":"http://arxiv.org/abs/2403.16008v1","updated":"2024-03-24T04:34:34Z","published":"2024-03-24T04:34:34Z","title":"CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral\n  Therapy-based Mental Health Question Answering","summary":"  The recent advancements in artificial intelligence highlight the potential of\nlanguage models in psychological health support. While models trained on data\nfrom mental health service platform have achieved preliminary success,\nchallenges persist in areas such as data scarcity, quality, and ensuring a\nsolid foundation in psychological techniques. To address these challenges, this\nstudy introduces a novel approach to enhance the precision and efficacy of\npsychological support through large language models. Specifically, we design a\nspecific prompt derived from principles of Cognitive Behavioral Therapy (CBT)\nand have generated the CBT QA dataset, specifically for Chinese psychological\nhealth Q&A based on CBT structured intervention strategies. Unlike previous\nmethods, our dataset emphasizes professional and structured response. Utilizing\nthis dataset, we fine-tuned the large language model, giving birth to CBT-LLM,\nthe large-scale language model specifically designed for Cognitive Behavioral\nTherapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in\ngenerating structured, professional, and highly relevant responses in\npsychological health support tasks, showcasing its practicality and quality.\nThe model is available on Hugging Face:\nhttps://huggingface.co/Hongbin37/CBT-LLM.\n","authors":["Hongbin Na"],"pdf_url":"https://arxiv.org/pdf/2403.16008v1.pdf","comment":"Accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2309.13339v3","updated":"2024-03-24T04:17:28Z","published":"2023-09-23T11:21:12Z","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic","summary":"  Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: \\url{https://github.com/xf-zhao/LoT}.\n","authors":["Xufeng Zhao","Mengdi Li","Wenhao Lu","Cornelius Weber","Jae Hee Lee","Kun Chu","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2309.13339v3.pdf","comment":"Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT"},{"id":"http://arxiv.org/abs/2403.15992v1","updated":"2024-03-24T03:10:07Z","published":"2024-03-24T03:10:07Z","title":"BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval","summary":"  The burgeoning integration of 3D medical imaging into healthcare has led to a\nsubstantial increase in the workload of medical professionals. To assist\nclinicians in their diagnostic processes and alleviate their workload, the\ndevelopment of a robust system for retrieving similar case studies presents a\nviable solution. While the concept holds great promise, the field of 3D medical\ntext-image retrieval is currently limited by the absence of robust evaluation\nbenchmarks and curated datasets. To remedy this, our study presents a\ngroundbreaking dataset, BIMCV-R (This dataset will be released upon\nacceptance.), which includes an extensive collection of 8,069 3D CT volumes,\nencompassing over 2 million slices, paired with their respective radiological\nreports. Expanding upon the foundational work of our dataset, we craft a\nretrieval strategy, MedFinder. This approach employs a dual-stream network\narchitecture, harnessing the potential of large language models to advance the\nfield of medical image retrieval beyond existing text-image retrieval\nsolutions. It marks our preliminary step towards developing a system capable of\nfacilitating text-to-image, image-to-text, and keyword-based retrieval tasks.\n","authors":["Yinda Chen","Che Liu","Xiaoyu Liu","Rossella Arcucci","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.15992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06461v3","updated":"2024-03-24T01:20:49Z","published":"2024-01-12T09:15:20Z","title":"Between Lines of Code: Unraveling the Distinct Patterns of Machine and\n  Human Programmers","summary":"  Large language models have catalyzed an unprecedented wave in code\ngeneration. While achieving significant advances, they blur the distinctions\nbetween machine- and human-authored source code, causing integrity and\nauthenticity issues of software artifacts. Previous methods such as DetectGPT\nhave proven effective in discerning machine-generated texts, but they do not\nidentify and harness the unique patterns of machine-generated code. Thus, its\napplicability falters when applied to code. In this paper, we carefully study\nthe specific patterns that characterize machine- and human-authored code.\nThrough a rigorous analysis of code attributes such as lexical diversity,\nconciseness, and naturalness, we expose unique patterns inherent to each\nsource. We particularly notice that the syntactic segmentation of code is a\ncritical factor in identifying its provenance. Based on our findings, we\npropose DetectCodeGPT, a novel method for detecting machine-generated code,\nwhich improves DetectGPT by capturing the distinct stylized patterns of code.\nDiverging from conventional techniques that depend on external LLMs for\nperturbations, DetectCodeGPT perturbs the code corpus by strategically\ninserting spaces and newlines, ensuring both efficacy and efficiency.\nExperiment results show that our approach significantly outperforms\nstate-of-the-art techniques in detecting machine-generated code.\n","authors":["Yuling Shi","Hongyu Zhang","Chengcheng Wan","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06461v3.pdf","comment":"code available at https://github.com/YerbaPage/DetectCodeGPT"},{"id":"http://arxiv.org/abs/2309.11576v2","updated":"2024-03-24T00:06:49Z","published":"2023-09-20T18:27:19Z","title":"Examining the Limitations of Computational Rumor Detection Models\n  Trained on Static Datasets","summary":"  A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.\n","authors":["Yida Mu","Xingyi Song","Kalina Bontcheva","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2309.11576v2.pdf","comment":"Accepted at LREC-COLING 2024"}]},"2024-03-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.15952v1","updated":"2024-03-23T23:06:32Z","published":"2024-03-23T23:06:32Z","title":"IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language\n  Models","summary":"  The advent of Vision Language Models (VLM) has allowed researchers to\ninvestigate the visual understanding of a neural network using natural\nlanguage. Beyond object classification and detection, VLMs are capable of\nvisual comprehension and common-sense reasoning. This naturally led to the\nquestion: How do VLMs respond when the image itself is inherently unreasonable?\nTo this end, we present IllusionVQA: a diverse dataset of challenging optical\nillusions and hard-to-interpret scenes to test the capability of VLMs in two\ndistinct multiple-choice VQA tasks - comprehension and soft localization.\nGPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the\ncomprehension task and 49.7% on the localization task (4-shot and\nChain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%\naccuracy in comprehension and localization. We discover that In-Context\nLearning (ICL) and Chain-of-Thought reasoning substantially degrade the\nperformance of GeminiPro on the localization task. Tangentially, we discover a\npotential weakness in the ICL capabilities of VLMs: they fail to locate optical\nillusions even when the correct answer is in the context window as a few-shot\nexample.\n","authors":["Haz Sameen Shahgir","Khondker Salman Sayeed","Abhik Bhattacharjee","Wasi Uddin Ahmad","Yue Dong","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2403.15952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15940v1","updated":"2024-03-23T22:02:56Z","published":"2024-03-23T22:02:56Z","title":"Geotokens and Geotransformers","summary":"  In transformer architectures, position encoding primarily provides a sense of\nsequence for input tokens. While the original transformer paper's method has\nshown satisfactory results in general language processing tasks, there have\nbeen new proposals, such as Rotary Position Embedding (RoPE), for further\nimprovement. This paper presents geotokens, input components for transformers,\neach linked to a specific geological location. Unlike typical language\nsequences, for these tokens, the order is not as vital as the geographical\ncoordinates themselves. To represent the relative position in this context and\nto keep a balance between the real world distance and the distance in the\nembedding space, we design a position encoding approach drawing from the RoPE\nstructure but tailored for spherical coordinates.\n","authors":["Eren Unlu"],"pdf_url":"https://arxiv.org/pdf/2403.15940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15938v1","updated":"2024-03-23T21:54:34Z","published":"2024-03-23T21:54:34Z","title":"LlamBERT: Large-scale low-cost data annotation in NLP","summary":"  Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable\nproficiency in a wide range of natural language processing (NLP) tasks. Despite\ntheir effectiveness, the high costs associated with their use pose a challenge.\nWe present LlamBERT, a hybrid approach that leverages LLMs to annotate a small\nsubset of large, unlabeled databases and uses the results for fine-tuning\ntransformer encoders like BERT and RoBERTa. This strategy is evaluated on two\ndiverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our\nresults indicate that the LlamBERT approach slightly compromises on accuracy\nwhile offering much greater cost-effectiveness.\n","authors":["Bálint Csanády","Lajos Muzsai","Péter Vedres","Zoltán Nádasdy","András Lukács"],"pdf_url":"https://arxiv.org/pdf/2403.15938v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.07527v2","updated":"2024-03-23T21:43:04Z","published":"2023-12-12T18:55:43Z","title":"BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy\n  and Reasoning Ability","summary":"  While there are numerous benchmarks comparing the performance of modern\nlanguage models (LMs), end-task evaluations often conflate notions of *factual\naccuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the\nsense of correctly reporting implications of beliefs). Our goal is a dataset\nthat clearly distinguishes these two notions. Our approach is to leverage and\nextend a collection of human-annotated *entailment trees*, engineered to\nexpress both good and bad chains of reasoning, and using a mixture of true and\nfalse facts, in particular including counterfactual examples, to avoid belief\nbias (also known as the \"content effect\"). The resulting dataset, called BaRDa,\ncontains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319\nfalse statements. Testing on four GPT-series models,\nGPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of\n74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This\nshows the clear progression of models towards improved factual accuracy and\nentailment reasoning, and the dataset provides a new benchmark that more\ncleanly separates and quantifies these two notions.\n","authors":["Peter Clark","Bhavana Dalvi Mishra","Oyvind Tafjord"],"pdf_url":"https://arxiv.org/pdf/2312.07527v2.pdf","comment":"Added note about how dataset sampling was performed"},{"id":"http://arxiv.org/abs/2310.16049v2","updated":"2024-03-23T21:21:44Z","published":"2023-10-24T17:59:20Z","title":"MuSR: Testing the Limits of Chain-of-thought with Multistep Soft\n  Reasoning","summary":"  While large language models (LLMs) equipped with techniques like\nchain-of-thought prompting have demonstrated impressive capabilities, they\nstill fall short in their ability to reason robustly in complex settings.\nHowever, evaluating LLM reasoning is challenging because system capabilities\ncontinue to grow while benchmark datasets for tasks like logical deduction have\nremained static. We introduce MuSR, a dataset for evaluating language models on\nmultistep soft reasoning tasks specified in a natural language narrative. This\ndataset has two crucial features. First, it is created through a novel\nneurosymbolic synthetic-to-natural generation algorithm, enabling the\nconstruction of complex reasoning instances that challenge GPT-4 (e.g., murder\nmysteries roughly 1000 words in length) and which can be scaled further as more\ncapable LLMs are released. Second, our dataset instances are free text\nnarratives corresponding to real-world domains of reasoning; this makes it\nsimultaneously much more challenging than other synthetically-crafted\nbenchmarks while remaining realistic and tractable for human annotators to\nsolve with high accuracy. We evaluate a range of LLMs and prompting techniques\non this dataset and characterize the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.\n","authors":["Zayne Sprague","Xi Ye","Kaj Bostrom","Swarat Chaudhuri","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2310.16049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15770v2","updated":"2024-03-23T21:13:35Z","published":"2024-01-28T21:18:05Z","title":"PILOT: Legal Case Outcome Prediction with Case Law","summary":"  Machine learning shows promise in predicting the outcome of legal cases, but\nmost research has concentrated on civil law cases rather than case law systems.\nWe identified two unique challenges in making legal case outcome predictions\nwith case law. First, it is crucial to identify relevant precedent cases that\nserve as fundamental evidence for judges during decision-making. Second, it is\nnecessary to consider the evolution of legal principles over time, as early\ncases may adhere to different legal contexts. In this paper, we proposed a new\nframework named PILOT (PredictIng Legal case OuTcome) for case outcome\nprediction. It comprises two modules for relevant case retrieval and temporal\npattern handling, respectively. To benchmark the performance of existing legal\ncase outcome prediction models, we curated a dataset from a large-scale case\nlaw database. We demonstrate the importance of accurately identifying precedent\ncases and mitigating the temporal shift when making predictions for case law,\nas our method shows a significant improvement over the prior methods that focus\non civil law case outcome predictions.\n","authors":["Lang Cao","Zifeng Wang","Cao Xiao","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2401.15770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17914v2","updated":"2024-03-23T20:21:04Z","published":"2024-02-27T22:06:55Z","title":"Extracting Lexical Features from Dialects via Interpretable Dialect\n  Classifiers","summary":"  Identifying linguistic differences between dialects of a language often\nrequires expert knowledge and meticulous human analysis. This is largely due to\nthe complexity and nuance involved in studying various dialects. We present a\nnovel approach to extract distinguishing lexical features of dialects by\nutilizing interpretable dialect classifiers, even in the absence of human\nexperts. We explore both post-hoc and intrinsic approaches to interpretability,\nconduct experiments on Mandarin, Italian, and Low Saxon, and experimentally\ndemonstrate that our method successfully identifies key language-specific\nlexical features that contribute to dialectal variations.\n","authors":["Roy Xie","Orevaoghene Ahia","Yulia Tsvetkov","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2402.17914v2.pdf","comment":"Code is available at\n  https://github.com/ruoyuxie/interpretable_dialect_classifier"},{"id":"http://arxiv.org/abs/2301.06627v3","updated":"2024-03-23T19:52:33Z","published":"2023-01-16T22:41:19Z","title":"Dissociating language and thought in large language models","summary":"  Large Language Models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence - knowledge of linguistic rules and patterns - and\nfunctional linguistic competence - understanding and using language in the\nworld. We ground this distinction in human neuroscience, which has shown that\nformal and functional competence rely on different neural mechanisms. Although\nLLMs are surprisingly good at formal competence, their performance on\nfunctional competence tasks remains spotty and often requires specialized\nfine-tuning and/or coupling with external modules. We posit that models that\nuse language in human-like ways would need to master both of these competence\ntypes, which, in turn, could require the emergence of mechanisms specialized\nfor formal linguistic competence, distinct from functional competence.\n","authors":["Kyle Mahowald","Anna A. Ivanova","Idan A. Blank","Nancy Kanwisher","Joshua B. Tenenbaum","Evelina Fedorenko"],"pdf_url":"https://arxiv.org/pdf/2301.06627v3.pdf","comment":"The two lead authors contributed equally to this work; published in\n  \"Trends in Cognnitive Sciences\", March 2024"},{"id":"http://arxiv.org/abs/2403.07311v5","updated":"2024-03-23T19:09:27Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of predicting multiple links within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, a challenge increasingly\nresolvable due to advancements in natural language processing (NLP) and KG\nembedding techniques. This paper introduces a novel methodology, the Knowledge\nGraph Large Language Model Framework (KG-LLM), which leverages pivotal NLP\nparadigms, including chain-of-thought (CoT) prompting and in-context learning\n(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a\nCoT prompt, our framework is designed to discern and learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)\nwithin this framework, employing both non-ICL and ICL tasks for a comprehensive\nevaluation. Further, we explore the framework's potential to provide LLMs with\nzero-shot capabilities for handling previously unseen prompts. Our experimental\nfindings discover that integrating ICL and CoT not only augments the\nperformance of our approach but also significantly boosts the models'\ngeneralization capacity, thereby ensuring more precise predictions in\nunfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Yiting Zhang","Chong Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v5.pdf","comment":"23 pages, 2 figures"},{"id":"http://arxiv.org/abs/2309.01157v2","updated":"2024-03-23T17:05:42Z","published":"2023-09-03T12:33:47Z","title":"Large Language Models for Generative Recommendation: A Survey and\n  Visionary Discussions","summary":"  Large language models (LLM) not only have revolutionized the field of natural\nlanguage processing (NLP) but also have the potential to reshape many other\nfields, e.g., recommender systems (RS). However, most of the related work\ntreats an LLM as a component of the conventional recommendation pipeline (e.g.,\nas a feature extractor), which may not be able to fully leverage the generative\npower of LLM. Instead of separating the recommendation process into multiple\nstages, such as score computation and re-ranking, this process can be\nsimplified to one stage with LLM: directly generating recommendations from the\ncomplete pool of items. This survey reviews the progress, methods, and future\ndirections of LLM-based generative recommendation by examining three questions:\n1) What generative recommendation is, 2) Why RS should advance to generative\nrecommendation, and 3) How to implement LLM-based generative recommendation for\nvarious RS tasks. We hope that this survey can provide the context and guidance\nneeded to explore this interesting and emerging topic.\n","authors":["Lei Li","Yongfeng Zhang","Dugang Liu","Li Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01157v2.pdf","comment":"Published as a conference paper at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15886v1","updated":"2024-03-23T16:51:52Z","published":"2024-03-23T16:51:52Z","title":"Leveraging Zero-Shot Prompting for Efficient Language Model Distillation","summary":"  This paper introduces a novel approach for efficiently distilling LLMs into\nsmaller, application-specific models, significantly reducing operational costs\nand manual labor. Addressing the challenge of deploying computationally\nintensive LLMs in specific applications or edge devices, this technique\nutilizes LLMs' reasoning capabilities to generate labels and natural language\nrationales for unlabeled data. Our approach enhances both finetuning and\ndistillation by employing a multi-task training framework where student models\nmimic these rationales alongside teacher predictions. Key contributions include\nthe employment of zero-shot prompting to elicit teacher model rationales,\nreducing the necessity for handcrafted few-shot examples and lowering the\noverall token count required, which directly translates to cost savings given\nthe pay-per-token billing model of major tech companies' LLM APIs.\nAdditionally, the paper investigates the impact of explanation properties on\ndistillation efficiency, demonstrating that minimal performance loss occurs\neven when rationale augmentation is not applied across the entire dataset,\nfacilitating further reductions of tokens. This research marks a step toward\nthe efficient training of task-specific models with minimal human intervention,\noffering substantial cost-savings while maintaining, or even enhancing,\nperformance.\n","authors":["Lukas Vöge","Vincent Gurgul","Stefan Lessmann"],"pdf_url":"https://arxiv.org/pdf/2403.15886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00046v2","updated":"2024-03-23T16:51:11Z","published":"2024-02-29T16:09:02Z","title":"SEED: Customize Large Language Models with Sample-Efficient Adaptation\n  for Code Generation","summary":"  Although Large Language Models (LLMs) have made significant progress in code\ngeneration, they still struggle with code generation tasks in specific\nscenarios. These scenarios usually necessitate the adaptation of LLMs to\nfulfill specific needs, but the limited training samples available in practice\nlead to poor code generation performance. Therefore, how to effectively adapt\nLLMs to new scenarios with few training samples is a major challenge for\ncurrent code generation. In this paper, we propose a novel adaptation approach\nnamed SEED, which stands for Sample-Efficient adaptation with Error-Driven\nlearning for code generation. SEED leverages the errors made by LLMs as\nlearning opportunities, using error revision to overcome its own shortcomings,\nthus achieving efficient learning. Specifically, SEED involves identifying\nerror code generated by LLMs, employing Self-revise for code revision,\noptimizing the model with revised code, and iteratively adapting the process\nfor continuous improvement. Experimental results show that, compared to other\nmainstream fine-tuning approaches, SEED achieves superior performance with few\ntraining samples, showing an average relative improvement of 54.7% in Pass@1 on\nmultiple code generation benchmarks. We also validate the effectiveness of\nSelf-revise, which generates revised code that optimizes the model more\nefficiently compared to the code samples from datasets. Moreover, SEED\nconsistently demonstrates strong performance across various LLMs, underscoring\nits generalizability.\n","authors":["Xue Jiang","Yihong Dong","Zhi Jin","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2403.00046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15885v1","updated":"2024-03-23T16:45:22Z","published":"2024-03-23T16:45:22Z","title":"STEntConv: Predicting Disagreement with Stance Detection and a Signed\n  Graph Convolutional Network","summary":"  The rise of social media platforms has led to an increase in polarised online\ndiscussions, especially on political and socio-cultural topics such as\nelections and climate change. We propose a simple and novel unsupervised method\nto predict whether the authors of two posts agree or disagree, leveraging user\nstances about named entities obtained from their posts. We present STEntConv, a\nmodel which builds a graph of users and named entities weighted by stance and\ntrains a Signed Graph Convolutional Network (SGCN) to detect disagreement\nbetween comment and reply posts. We run experiments and ablation studies and\nshow that including this information improves disagreement detection\nperformance on a dataset of Reddit posts for a range of controversial subreddit\ntopics, without the need for platform-specific features or user history.\n","authors":["Isabelle Lorge","Li Zhang","Xiaowen Dong","Janet B. Pierrehumbert"],"pdf_url":"https://arxiv.org/pdf/2403.15885v1.pdf","comment":"Accepted for the 2024 Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2401.11624v5","updated":"2024-03-23T16:35:45Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15882v1","updated":"2024-03-23T16:26:49Z","published":"2024-03-23T16:26:49Z","title":"VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for\n  Vietnamese Natural Language Understanding","summary":"  The success of Natural Language Understanding (NLU) benchmarks in various\nlanguages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and\nIndoNLU for Indonesian, has facilitated the evaluation of new NLU models across\na wide range of tasks. To establish a standardized set of benchmarks for\nVietnamese NLU, we introduce the first Vietnamese Language Understanding\nEvaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets\ncovering different NLU tasks, including text classification, span extraction,\nand natural language understanding. To provide an insightful overview of the\ncurrent state of Vietnamese NLU, we then evaluate seven state-of-the-art\npre-trained models, including both multilingual and Vietnamese monolingual\nmodels, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new\nstate-of-the-art pre-trained model that achieves superior results across all\ntasks in the VLUE benchmark. Our model combines the proficiency of a\nmultilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT\nis developed based on the XLM-RoBERTa model, with an additional pretraining\nstep utilizing a significant amount of Vietnamese textual data to enhance its\nadaptation to the Vietnamese language. For the purpose of future research,\nCafeBERT is made publicly available for research purposes.\n","authors":["Phong Nguyen-Thuan Do","Son Quoc Tran","Phu Gia Hoang","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15882v1.pdf","comment":"Accepted at NAACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2403.14253v2","updated":"2024-03-23T15:53:50Z","published":"2024-03-21T09:26:04Z","title":"K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional\n  Expression","summary":"  In many literary texts, emotions are indirectly conveyed through descriptions\nof actions, facial expressions, and appearances, necessitating emotion\ninference for narrative understanding. In this paper, we introduce K-Act2Emo, a\nKorean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional\nexpressions and the emotions inferable from them. We categorize reasoning types\ninto inferences in positive situations, inferences in negative situations, and\ninferences when expressions do not serve as emotional cues. Unlike existing\nCSKGs, K-Act2Emo specializes in emotional contexts, and experimental results\nvalidate its effectiveness for training emotion inference models.\nSignificantly, the BART-based knowledge model fine-tuned with K-Act2Emo\noutperforms various existing Korean large language models, achieving\nperformance levels comparable to GPT-4 Turbo.\n","authors":["Kyuhee Kim","Surin Lee","Sangah Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14253v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.15875v1","updated":"2024-03-23T15:52:37Z","published":"2024-03-23T15:52:37Z","title":"LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series\n  classification","summary":"  This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)\nframework, designed to systematically evaluate the adaptability of pre-trained\nlanguage models (PLMs) in accommodating diverse prompts and their integration\nin zero-shot time series (TS) classification. We deploy LAMPER in experimental\nassessments using 128 univariate TS datasets sourced from the UCR archive. Our\nfindings indicate that the feature representation capacity of LAMPER is\ninfluenced by the maximum input token threshold imposed by PLMs.\n","authors":["Zhicheng Du","Zhaotian Xie","Yan Tong","Peiwu Qin"],"pdf_url":"https://arxiv.org/pdf/2403.15875v1.pdf","comment":"Accepted as tiny paper in ICLR 2024"},{"id":"http://arxiv.org/abs/2402.00157v2","updated":"2024-03-23T15:45:57Z","published":"2024-01-31T20:26:32Z","title":"Large Language Models for Mathematical Reasoning: Progresses and\n  Challenges","summary":"  Mathematical reasoning serves as a cornerstone for assessing the fundamental\ncognitive capabilities of human intelligence. In recent times, there has been a\nnotable surge in the development of Large Language Models (LLMs) geared towards\nthe automated resolution of mathematical problems. However, the landscape of\nmathematical problem types is vast and varied, with LLM-oriented techniques\nundergoing evaluation across diverse datasets and settings. This diversity\nmakes it challenging to discern the true advancements and obstacles within this\nburgeoning field. This survey endeavors to address four pivotal dimensions: i)\na comprehensive exploration of the various mathematical problems and their\ncorresponding datasets that have been investigated; ii) an examination of the\nspectrum of LLM-oriented techniques that have been proposed for mathematical\nproblem-solving; iii) an overview of factors and concerns affecting LLMs in\nsolving math; and iv) an elucidation of the persisting challenges within this\ndomain. To the best of our knowledge, this survey stands as one of the first\nextensive examinations of the landscape of LLMs in the realm of mathematics,\nproviding a holistic perspective on the current state, accomplishments, and\nfuture challenges in this rapidly evolving field.\n","authors":["Janice Ahn","Rishu Verma","Renze Lou","Di Liu","Rui Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2402.00157v2.pdf","comment":"EACL 2024 Student Research Workshop, 8 pages"},{"id":"http://arxiv.org/abs/2403.15872v1","updated":"2024-03-23T15:43:30Z","published":"2024-03-23T15:43:30Z","title":"RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts","summary":"  Move structures have been studied in English for Specific Purposes (ESP) and\nEnglish for Academic Purposes (EAP) for decades. However, there are few move\nannotation corpora for Research Article (RA) abstracts. In this paper, we\nintroduce RAAMove, a comprehensive multi-domain corpus dedicated to the\nannotation of move structures in RA abstracts. The primary objective of RAAMove\nis to facilitate move analysis and automatic move identification. This paper\nprovides a thorough discussion of the corpus construction process, including\nthe scheme, data collection, annotation guidelines, and annotation procedures.\nThe corpus is constructed through two stages: initially, expert annotators\nmanually annotate high-quality data; subsequently, based on the human-annotated\ndata, a BERT-based model is employed for automatic annotation with the help of\nexperts' modification. The result is a large-scale and high-quality corpus\ncomprising 33,988 annotated instances. We also conduct preliminary move\nidentification experiments using the BERT-based model to verify the\neffectiveness of the proposed corpus and model. The annotated corpus is\navailable for academic research purposes and can serve as essential resources\nfor move analysis, English language teaching and writing, as well as\nmove/discourse-related tasks in Natural Language Processing (NLP).\n","authors":["Hongzheng Li","Ruojin Wang","Ge Shi","Xing Lv","Lei Lei","Chong Feng","Fang Liu","Jinkun Lin","Yangguang Mei","Lingnan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15872v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14050v2","updated":"2024-03-23T14:46:59Z","published":"2024-03-21T00:20:16Z","title":"Extracting Emotion Phrases from Tweets using BART","summary":"  Sentiment analysis is a natural language processing task that aims to\nidentify and extract the emotional aspects of a text. However, many existing\nsentiment analysis methods primarily classify the overall polarity of a text,\noverlooking the specific phrases that convey sentiment. In this paper, we\napplied an approach to sentiment analysis based on a question-answering\nframework. Our approach leverages the power of Bidirectional Autoregressive\nTransformer (BART), a pre-trained sequence-to-sequence model, to extract a\nphrase from a given text that amplifies a given sentiment polarity. We create a\nnatural language question that identifies the specific emotion to extract and\nthen guide BART to pay attention to the relevant emotional cues in the text. We\nuse a classifier within BART to predict the start and end positions of the\nanswer span within the text, which helps to identify the precise boundaries of\nthe extracted emotion phrase. Our approach offers several advantages over most\nsentiment analysis studies, including capturing the complete context and\nmeaning of the text and extracting precise token spans that highlight the\nintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.\n","authors":["Mahdi Rezapour"],"pdf_url":"https://arxiv.org/pdf/2403.14050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11585v2","updated":"2024-03-23T14:07:54Z","published":"2023-08-19T13:14:15Z","title":"Causal Intersectionality and Dual Form of Gradient Descent for\n  Multimodal Analysis: a Case Study on Hateful Memes","summary":"  Amidst the rapid expansion of Machine Learning (ML) and Large Language Models\n(LLMs), understanding the semantics within their mechanisms is vital. Causal\nanalyses define semantics, while gradient-based methods are essential to\neXplainable AI (XAI), interpreting the model's 'black box'. Integrating these,\nwe investigate how a model's mechanisms reveal its causal effect on\nevidence-based decision-making. Research indicates intersectionality - the\ncombined impact of an individual's demographics - can be framed as an Average\nTreatment Effect (ATE). This paper demonstrates that hateful meme detection can\nbe viewed as an ATE estimation using intersectionality principles, and\nsummarized gradient-based attention scores highlight distinct behaviors of\nthree Transformer models. We further reveal that LLM Llama-2 can discern the\nintersectional aspects of the detection through in-context learning and that\nthe learning process could be explained via meta-gradient, a secondary form of\ngradient. In conclusion, this work furthers the dialogue on Causality and XAI.\nOur code is available online (see External Resources section).\n","authors":["Yosuke Miyanishi","Minh Le Nguyen"],"pdf_url":"https://arxiv.org/pdf/2308.11585v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.07954v2","updated":"2024-03-23T13:54:44Z","published":"2023-11-14T07:13:10Z","title":"A Closer Look at the Self-Verification Abilities of Large Language\n  Models in Logical Reasoning","summary":"  Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.\n","authors":["Ruixin Hong","Hongming Zhang","Xinyu Pang","Dong Yu","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.07954v2.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2309.11093v3","updated":"2024-03-23T13:33:06Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v3.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v1","updated":"2024-03-23T13:24:31Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15822v1","updated":"2024-03-23T12:19:49Z","published":"2024-03-23T12:19:49Z","title":"Computational Sentence-level Metrics Predicting Human Sentence\n  Comprehension","summary":"  The majority of research in computational psycholinguistics has concentrated\non the processing of words. This study introduces innovative methods for\ncomputing sentence-level metrics using multilingual large language models. The\nmetrics developed sentence surprisal and sentence relevance and then are tested\nand compared to validate whether they can predict how humans comprehend\nsentences as a whole across languages. These metrics offer significant\ninterpretability and achieve high accuracy in predicting human sentence reading\nspeeds. Our results indicate that these computational sentence-level metrics\nare exceptionally effective at predicting and elucidating the processing\ndifficulties encountered by readers in comprehending sentences as a whole\nacross a variety of languages. Their impressive performance and generalization\ncapabilities provide a promising avenue for future research in integrating LLMs\nand cognitive science.\n","authors":["Kun Sun","Rong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09725v3","updated":"2024-03-23T11:45:55Z","published":"2023-10-15T04:00:36Z","title":"KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large\n  Language Models","summary":"  Large language models (LLMs) demonstrate remarkable performance on\nknowledge-intensive tasks, suggesting that real-world knowledge is encoded in\ntheir model parameters. However, besides explorations on a few probing tasks in\nlimited knowledge domains, it is not well understood how to evaluate LLMs'\nknowledge systematically and how well their knowledge abilities generalize,\nacross a spectrum of knowledge domains and progressively complex task formats.\nTo this end, we propose KGQuiz, a knowledge-intensive benchmark to\ncomprehensively investigate the knowledge generalization abilities of LLMs.\nKGQuiz is a scalable framework constructed from triplet-based knowledge, which\ncovers three knowledge domains and consists of five tasks with increasing\ncomplexity: true-or-false, multiple-choice QA, blank filling, factual editing,\nand open-ended knowledge generation. To gain a better understanding of LLMs'\nknowledge abilities and their generalization, we evaluate 10 open-source and\nblack-box LLMs on the KGQuiz benchmark across the five knowledge-intensive\ntasks and knowledge domains. Extensive experiments demonstrate that LLMs\nachieve impressive performance in straightforward knowledge QA tasks, while\nsettings and contexts requiring more complex reasoning or employing\ndomain-specific facts still present significant challenges. We envision KGQuiz\nas a testbed to analyze such nuanced variations in performance across domains\nand task formats, and ultimately to understand, evaluate, and improve LLMs'\nknowledge abilities across a wide spectrum of knowledge domains and tasks.\n","authors":["Yuyang Bai","Shangbin Feng","Vidhisha Balachandran","Zhaoxuan Tan","Shiqi Lou","Tianxing He","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2310.09725v3.pdf","comment":"TheWebConf 2024"},{"id":"http://arxiv.org/abs/2305.12519v2","updated":"2024-03-23T11:34:49Z","published":"2023-05-21T17:26:16Z","title":"LLM Paternity Test: Generated Text Detection with LLM Genetic\n  Inheritance","summary":"  Large language models (LLMs) can generate texts that carry the risk of\nvarious misuses, including plagiarism, planting fake reviews on e-commerce\nplatforms, or creating inflammatory false tweets. Detecting whether a text is\nmachine-generated has thus become increasingly important. While existing\ndetection methods exhibit superior performance, they often lack\ngeneralizability due to their heavy dependence on training data. To alleviate\nthis problem, we propose a model-related generated text detection method, the\nLLM Paternity Test (LLM-Pat). Specifically, given any candidate text\n(\\textit{child}), LLM-Pat employs an intermediary LLM (\\textit{parent}) to\nreconstruct a \\textit{sibling} text corresponding to the given text and then\nmeasures the similarity between candidate texts and their sibling texts. High\nsimilarity indicates that the candidate text is machine-generated, akin to\ngenetic traits. We have constructed datasets encompassing four scenarios:\nstudent responses in educational settings, news creation, academic paper\nwriting, and social media bots to assess the performance of LLM-Pat. The\nexperiments show that LLM-Pat outperforms the existing detection methods and is\nmore robust against paraphrasing attacks and re-translating attacks. Besides,\nLLM-Pat can also be used to trace which large language model the text was\ngenerated by. The constructed dataset and code will be released to benefit the\ncommunity.\n","authors":["Xiao Yu","Yuang Qi","Kejiang Chen","Guoqiang Chen","Xi Yang","Pengyuan Zhu","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2305.12519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08644v2","updated":"2024-03-23T11:29:17Z","published":"2024-02-13T18:24:08Z","title":"Tandem Transformers for Inference Efficient LLMs","summary":"  The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.\n","authors":["Aishwarya P S","Pranav Ajit Nair","Yashas Samaga","Toby Boyd","Sanjiv Kumar","Prateek Jain","Praneeth Netrapalli"],"pdf_url":"https://arxiv.org/pdf/2402.08644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15800v1","updated":"2024-03-23T11:14:02Z","published":"2024-03-23T11:14:02Z","title":"MRC-based Nested Medical NER with Co-prediction and Adaptive\n  Pre-training","summary":"  In medical information extraction, medical Named Entity Recognition (NER) is\nindispensable, playing a crucial role in developing medical knowledge graphs,\nenhancing medical question-answering systems, and analyzing electronic medical\nrecords. The challenge in medical NER arises from the complex nested structures\nand sophisticated medical terminologies, distinguishing it from its\ncounterparts in traditional domains. In response to these complexities, we\npropose a medical NER model based on Machine Reading Comprehension (MRC), which\nuses a task-adaptive pre-training strategy to improve the model's capability in\nthe medical field. Meanwhile, our model introduces multiple word-pair\nembeddings and multi-granularity dilated convolution to enhance the model's\nrepresentation ability and uses a combined predictor of Biaffine and MLP to\nimprove the model's recognition performance. Experimental evaluations conducted\non the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our\nproposed model outperforms the compared state-of-the-art (SOTA) models.\n","authors":["Xiaojing Du","Hanjie Zhao","Danyan Xing","Yuxiang Jia","Hongying Zan"],"pdf_url":"https://arxiv.org/pdf/2403.15800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15796v1","updated":"2024-03-23T11:03:31Z","published":"2024-03-23T11:03:31Z","title":"Understanding Emergent Abilities of Language Models from the Loss\n  Perspective","summary":"  Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the models with the same pre-training loss, but different\nmodel and data sizes, generate the same performance on various downstream\ntasks. We also discover that a model exhibits emergent abilities on certain\ntasks -- regardless of the continuity of metrics -- when its pre-training loss\nfalls below a specific threshold. Before reaching this threshold, its\nperformance remains at the level of random guessing. This inspires us to\nredefine emergent abilities as those that manifest in models with lower\npre-training losses, highlighting that these abilities cannot be predicted by\nmerely extrapolating the performance trends of models with higher pre-training\nlosses.\n","authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.15796v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15776v1","updated":"2024-03-23T09:18:53Z","published":"2024-03-23T09:18:53Z","title":"Modeling Unified Semantic Discourse Structure for High-quality Headline\n  Generation","summary":"  Headline generation aims to summarize a long document with a short, catchy\ntitle that reflects the main idea. This requires accurately capturing the core\ndocument semantics, which is challenging due to the lengthy and background\ninformation-rich na ture of the texts. In this work, We propose using a unified\nsemantic discourse structure (S3) to represent document semantics, achieved by\ncombining document-level rhetorical structure theory (RST) trees with\nsentence-level abstract meaning representation (AMR) graphs to construct S3\ngraphs. The hierarchical composition of sentence, clause, and word\nintrinsically characterizes the semantic meaning of the overall document. We\nthen develop a headline generation framework, in which the S3 graphs are\nencoded as contextual features. To consolidate the efficacy of S3 graphs, we\nfurther devise a hierarchical structure pruning mechanism to dynamically screen\nthe redundant and nonessential nodes within the graph. Experimental results on\ntwo headline generation datasets demonstrate that our method outperforms\nexisting state-of-art methods consistently. Our work can be instructive for a\nbroad range of document modeling tasks, more than headline or summarization\ngeneration.\n","authors":["Minghui Xu","Hao Fei","Fei Li","Shengqiong Wu","Rui Sun","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.15776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13571v2","updated":"2024-03-23T08:22:58Z","published":"2024-02-21T07:05:51Z","title":"Multilingual Coreference Resolution in Low-resource South Asian\n  Languages","summary":"  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n","authors":["Ritwik Mishra","Pooja Desur","Rajiv Ratn Shah","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2402.13571v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.06725v2","updated":"2024-03-23T08:14:28Z","published":"2024-03-11T13:44:43Z","title":"Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning","summary":"  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://anonymous.4open.science/r/LoReKT-C619.\n","authors":["Hengyuan Zhang","Zitao Liu","Shuyan Huang","Chenming Shang","Bojun Zhan","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.06725v2.pdf","comment":"29 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15757v1","updated":"2024-03-23T08:03:50Z","published":"2024-03-23T08:03:50Z","title":"User-Side Realization","summary":"  Users are dissatisfied with services. Since the service is not tailor-made\nfor a user, it is natural for dissatisfaction to arise. The problem is, that\neven if users are dissatisfied, they often do not have the means to resolve\ntheir dissatisfaction. The user cannot alter the source code of the service,\nnor can they force the service provider to change. The user has no choice but\nto remain dissatisfied or quit the service. User-side realization offers\nproactive solutions to this problem by providing general algorithms to deal\nwith common problems on the user's side. These algorithms run on the user's\nside and solve the problems without having the service provider change the\nservice itself.\n","authors":["Ryoma Sato"],"pdf_url":"https://arxiv.org/pdf/2403.15757v1.pdf","comment":"Doctoral Thesis"},{"id":"http://arxiv.org/abs/2403.15756v1","updated":"2024-03-23T07:59:30Z","published":"2024-03-23T07:59:30Z","title":"Leveraging Large Language Models for Preliminary Security Risk Analysis:\n  A Mission-Critical Case Study","summary":"  Preliminary security risk analysis (PSRA) provides a quick approach to\nidentify, evaluate and propose remeditation to potential risks in specific\nscenarios. The extensive expertise required for an effective PSRA and the\nsubstantial ammount of textual-related tasks hinder quick assessments in\nmission-critical contexts, where timely and prompt actions are essential. The\nspeed and accuracy of human experts in PSRA significantly impact response time.\nA large language model can quickly summarise information in less time than a\nhuman. To our knowledge, no prior study has explored the capabilities of\nfine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of\nFTM to assist practitioners in PSRA. We manually curated 141 representative\nsamples from over 50 mission-critical analyses archived by the industrial\ncontext team in the last five years.We compared the proficiency of the FTM\nversus seven human experts. Within the industrial context, our approach has\nproven successful in reducing errors in PSRA, hastening security risk\ndetection, and minimizing false positives and negatives. This translates to\ncost savings for the company by averting unnecessary expenses associated with\nimplementing unwarranted countermeasures. Therefore, experts can focus on more\ncomprehensive risk analysis, leveraging LLMs for an effective preliminary\nassessment within a condensed timeframe.\n","authors":["Matteo Esposito","Francesco Palagiano"],"pdf_url":"https://arxiv.org/pdf/2403.15756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15744v1","updated":"2024-03-23T07:16:23Z","published":"2024-03-23T07:16:23Z","title":"On the Fragility of Active Learners","summary":"  Active learning (AL) techniques aim to maximally utilize a labeling budget by\niteratively selecting instances that are most likely to improve prediction\naccuracy. However, their benefit compared to random sampling has not been\nconsistent across various setups, e.g., different datasets, classifiers. In\nthis empirical study, we examine how a combination of different factors might\nobscure any gains from an AL technique.\n  Focusing on text classification, we rigorously evaluate AL techniques over\naround 1000 experiments that vary wrt the dataset, batch size, text\nrepresentation and the classifier. We show that AL is only effective in a\nnarrow set of circumstances. We also address the problem of using metrics that\nare better aligned with real world expectations.\n  The impact of this study is in its insights for a practitioner: (a) the\nchoice of text representation and classifier is as important as that of an AL\ntechnique, (b) choice of the right metric is critical in assessment of the\nlatter, and, finally, (c) reported AL results must be holistically interpreted,\naccounting for variables other than just the query strategy.\n","authors":["Abhishek Ghose","Emma Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15740v1","updated":"2024-03-23T06:36:32Z","published":"2024-03-23T06:36:32Z","title":"Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large\n  Language Models","summary":"  Web user data plays a central role in the ecosystem of pre-trained large\nlanguage models (LLMs) and their fine-tuned variants. Billions of data are\ncrawled from the web and fed to LLMs. How can \\textit{\\textbf{everyday web\nusers}} confirm if LLMs misuse their data without permission? In this work, we\nsuggest that users repeatedly insert personal passphrases into their documents,\nenabling LLMs to memorize them. These concealed passphrases in user documents,\nreferred to as \\textit{ghost sentences}, once they are identified in the\ngenerated content of LLMs, users can be sure that their data is used for\ntraining. To explore the effectiveness and usage of this copyrighting tool, we\ndefine the \\textit{user training data identification} task with ghost\nsentences. Multiple datasets from various sources at different scales are\ncreated and tested with LLMs of different sizes. For evaluation, we introduce a\nlast $k$ words verification manner along with two metrics: document and user\nidentification accuracy. In the specific case of instruction tuning of a 3B\nLLaMA model, 11 out of 16 users with ghost sentences identify their data within\nthe generation content. These 16 users contribute 383 examples to $\\sim$1.8M\ntraining documents. For continuing pre-training of a 1.1B TinyLlama model, 61\nout of 64 users with ghost sentences identify their data within the LLM output.\nThese 64 users contribute 1156 examples to $\\sim$10M training documents.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2403.11838v2","updated":"2024-03-23T06:26:41Z","published":"2024-03-18T14:48:29Z","title":"Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for\n  Language Models","summary":"  Large Language Models (LLMs) exhibit impressive capabilities but also present\nrisks such as biased content generation and privacy issues. One of the current\nalignment techniques includes principle-driven integration, but it faces\nchallenges arising from the imprecision of manually crafted rules and\ninadequate risk perception in models without safety training. To address these,\nwe introduce Guide-Align, a two-stage approach. Initially, a safety-trained\nmodel identifies potential risks and formulates specific guidelines for various\ninputs, establishing a comprehensive library of guidelines and a model for\ninput-guidelines retrieval. Subsequently, the retrieval model correlates new\ninputs with relevant guidelines, which guide LLMs in response generation to\nensure safe and high-quality outputs, thereby aligning with human values. An\nadditional optional stage involves fine-tuning a model with well-aligned\ndatasets generated through the process implemented in the second stage. Our\nmethod customizes guidelines to accommodate diverse inputs, thereby enhancing\nthe fine-grainedness and comprehensiveness of the guideline library.\nFurthermore, it incorporates safety expertise from a safety-trained LLM through\na lightweight retrieval model. We evaluate our approach on three benchmarks,\ndemonstrating significant improvements in LLM security and quality. Notably,\nour fine-tuned model, Labrador, even at 13 billion parameters, outperforms\nGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.\n","authors":["Yi Luo","Zhenghao Lin","Yuhao Zhang","Jiashuo Sun","Chen Lin","Chengjin Xu","Xiangdong Su","Yelong Shen","Jian Guo","Yeyun Gong"],"pdf_url":"https://arxiv.org/pdf/2403.11838v2.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2403.15737v1","updated":"2024-03-23T06:03:37Z","published":"2024-03-23T06:03:37Z","title":"Few-shot Dialogue Strategy Learning for Motivational Interviewing via\n  Inductive Reasoning","summary":"  We consider the task of building a dialogue system that can motivate users to\nadopt positive lifestyle changes: Motivational Interviewing. Addressing such a\ntask requires a system that can infer \\textit{how} to motivate a user\neffectively. We propose DIIT, a framework that is capable of learning and\napplying conversation strategies in the form of natural language inductive\nrules from expert demonstrations. Automatic and human evaluation on\ninstruction-following large language models show natural language strategy\ndescriptions discovered by DIIR can improve active listening skills, reduce\nunsolicited advice, and promote more collaborative and less authoritative\nresponses, outperforming various demonstration utilization methods.\n","authors":["Zhouhang Xie","Bodhisattwa Prasad Majumder","Mengjie Zhao","Yoshinori Maeda","Keiichi Yamada","Hiromi Wakaki","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.15737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15736v1","updated":"2024-03-23T06:03:36Z","published":"2024-03-23T06:03:36Z","title":"LLMs Instruct LLMs:An Extraction and Editing Method","summary":"  The interest in updating Large Language Models (LLMs) without retraining from\nscratch is substantial, yet it comes with some challenges.This is especially\ntrue for situations demanding complex reasoning with limited samples, a\nscenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation\nfor LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and\nRetrieval-Augmented Generation (RAG) are inadequate for this critical issue,\nparticularly evident in our exploration of a specific medical context that\nepitomize the PCRA-LLM's distinct needs.To address the issue, we propose a\nSequential Fusion method to incorporate knowledge from complex context into\nLLMs. This method employs a two-stage framework: initially, it leverages\ngeneral LLMs to construct knowledge graphs (KGs) for extracting knowledge from\ncomplex texts; subsequently, it updates the domain LLMs through knowledge edit.\nAccording to our method, the domain LLM achieved a 71.69\\% accuracy in question\nanswering tasks. Subsequently, we broadened our assessment to a novel dataset\nwe developed in the economics and management field, where our method realized a\n75\\% accuracy. These outcomes underline the efficacy and adaptability of our\napproach for PCRA-LLM across various domains.\n","authors":["Xin Zhang","Tianjie Ju","Huijia Liang","Ying Fu","Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15736v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2403.15729v1","updated":"2024-03-23T05:32:46Z","published":"2024-03-23T05:32:46Z","title":"Towards a \\textbf{RAG}-based Summarization Agent for the Electron-Ion\n  Collider","summary":"  The complexity and sheer volume of information encompassing documents,\npapers, data, and other resources from large-scale experiments demand\nsignificant time and effort to navigate, making the task of accessing and\nutilizing these varied forms of information daunting, particularly for new\ncollaborators and early-career scientists. To tackle this issue, a Retrieval\nAugmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under\ndevelopment. This AI-Agent not only condenses information but also effectively\nreferences relevant responses, offering substantial advantages for\ncollaborators. Our project involves a two-step approach: first, querying a\ncomprehensive vector database containing all pertinent experiment information;\nsecond, utilizing a Large Language Model (LLM) to generate concise summaries\nenriched with citations based on user queries and retrieved data. We describe\nthe evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to\nassess the effectiveness of responses. Furthermore, we describe the concept of\nprompt template-based instruction-tuning which provides flexibility and\naccuracy in summarization. Importantly, the implementation relies on LangChain,\nwhich serves as the foundation of our entire workflow. This integration ensures\nefficiency and scalability, facilitating smooth deployment and accessibility\nfor various user groups within the Electron Ion Collider (EIC) community. This\ninnovative AI-driven framework not only simplifies the understanding of vast\ndatasets but also encourages collaborative participation, thereby empowering\nresearchers. As a demonstration, a web application has been developed to\nexplain each stage of the RAG Agent development in detail.\n","authors":["Karthik Suresh","Neeltje Kackar","Luke Schleck","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2403.15729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04786v2","updated":"2024-03-23T05:21:49Z","published":"2024-03-03T04:46:21Z","title":"Breaking Down the Defenses: A Comparative Survey of Attacks on Large\n  Language Models","summary":"  Large Language Models (LLMs) have become a cornerstone in the field of\nNatural Language Processing (NLP), offering transformative capabilities in\nunderstanding and generating human-like text. However, with their rising\nprominence, the security and vulnerability aspects of these models have\ngarnered significant attention. This paper presents a comprehensive survey of\nthe various forms of attacks targeting LLMs, discussing the nature and\nmechanisms of these attacks, their potential impacts, and current defense\nstrategies. We delve into topics such as adversarial attacks that aim to\nmanipulate model outputs, data poisoning that affects model training, and\nprivacy concerns related to training data exploitation. The paper also explores\nthe effectiveness of different attack methodologies, the resilience of LLMs\nagainst these attacks, and the implications for model integrity and user trust.\nBy examining the latest research, we provide insights into the current\nlandscape of LLM vulnerabilities and defense mechanisms. Our objective is to\noffer a nuanced understanding of LLM attacks, foster awareness within the AI\ncommunity, and inspire robust solutions to mitigate these risks in future\ndevelopments.\n","authors":["Arijit Ghosh Chowdhury","Md Mofijul Islam","Vaibhav Kumar","Faysal Hossain Shezan","Vaibhav Kumar","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2403.04786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15724v1","updated":"2024-03-23T05:20:36Z","published":"2024-03-23T05:20:36Z","title":"PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on\n  Scientific Documents","summary":"  Optical Character Recognition (OCR) is an established task with the objective\nof identifying the text present in an image. While many off-the-shelf OCR\nmodels exist, they are often trained for either scientific (e.g., formulae) or\ngeneric printed English text. Extracting text from chemistry publications\nrequires an OCR model that is capable in both realms. Nougat, a recent tool,\nexhibits strong ability to parse academic documents, but is unable to parse\ntables in PubMed articles, which comprises a significant part of the academic\ncommunity and is the focus of this work. To mitigate this gap, we present the\nPrinted English and Chemical Equations (PEaCE) dataset, containing both\nsynthetic and real-world records, and evaluate the efficacy of\ntransformer-based OCR models when trained on this resource. Given that\nreal-world records contain artifacts not present in synthetic records, we\npropose transformations that mimic such qualities. We perform a suite of\nexperiments to explore the impact of patch size, multi-domain training, and our\nproposed transformations, ultimately finding that models with a small patch\nsize trained on multiple domains using the proposed transformations yield the\nbest performance. Our dataset and code is available at\nhttps://github.com/ZN1010/PEaCE.\n","authors":["Nan Zhang","Connor Heaton","Sean Timothy Okonsky","Prasenjit Mitra","Hilal Ezgi Toraman"],"pdf_url":"https://arxiv.org/pdf/2403.15724v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15715v1","updated":"2024-03-23T04:29:29Z","published":"2024-03-23T04:29:29Z","title":"EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance\n  Detection","summary":"  Stance detection aims to determine the attitude expressed in text towards a\ngiven target. Zero-shot stance detection (ZSSD) has emerged to classify stances\ntowards unseen targets during inference. Recent data augmentation techniques\nfor ZSSD increase transferable knowledge between targets through text or target\naugmentation. However, these methods exhibit limitations. Target augmentation\nlacks logical connections between generated targets and source text, while text\naugmentation relies solely on training data, resulting in insufficient\ngeneralization. To address these issues, we propose an encoder-decoder data\naugmentation (EDDA) framework. The encoder leverages large language models and\nchain-of-thought prompting to summarize texts into target-specific if-then\nrationales, establishing logical relationships. The decoder generates new\nsamples based on these expressions using a semantic correlation word\nreplacement strategy to increase syntactic diversity. We also analyze the\ngenerated expressions to develop a rationale-enhanced network that fully\nutilizes the augmented data. Experiments on benchmark datasets demonstrate our\napproach substantially improves over state-of-the-art ZSSD techniques. The\nproposed EDDA framework increases semantic relevance and syntactic variety in\naugmented texts while enabling interpretable rationale-based learning.\n","authors":["Daijun Ding","Li Dong","Zhichao Huang","Guangning Xu","Xu Huang","Bo Liu","Liwen Jing","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13786v2","updated":"2024-03-23T04:02:01Z","published":"2024-03-20T17:47:49Z","title":"Chain-of-Interaction: Enhancing Large Language Models for Psychiatric\n  Behavior Understanding by Dyadic Contexts","summary":"  Automatic coding patient behaviors is essential to support decision making\nfor psychotherapists during the motivational interviewing (MI), a collaborative\ncommunication intervention approach to address psychiatric issues, such as\nalcohol and drug addiction. While the behavior coding task has rapidly adapted\nmachine learning to predict patient states during the MI sessions, lacking of\ndomain-specific knowledge and overlooking patient-therapist interactions are\nmajor challenges in developing and deploying those models in real practice. To\nencounter those challenges, we introduce the Chain-of-Interaction (CoI)\nprompting method aiming to contextualize large language models (LLMs) for\npsychiatric decision support by the dyadic interactions. The CoI prompting\napproach systematically breaks down the coding task into three key reasoning\nsteps, extract patient engagement, learn therapist question strategies, and\nintegrates dyadic interactions between patients and therapists. This approach\nenables large language models to leverage the coding scheme, patient state, and\ndomain knowledge for patient behavioral coding. Experiments on real-world\ndatasets can prove the effectiveness and flexibility of our prompting method\nwith multiple state-of-the-art LLMs over existing prompting baselines. We have\nconducted extensive ablation analysis and demonstrate the critical role of\ndyadic interactions in applying LLMs for psychotherapy behavior understanding.\n","authors":["Guangzeng Han","Weisi Liu","Xiaolei Huang","Brian Borsari"],"pdf_url":"https://arxiv.org/pdf/2403.13786v2.pdf","comment":"Accepted to IEEE ICHI 2024"},{"id":"http://arxiv.org/abs/2403.15699v1","updated":"2024-03-23T03:32:26Z","published":"2024-03-23T03:32:26Z","title":"FEEL: A Framework for Evaluating Emotional Support Capability with Large\n  Language Models","summary":"  Emotional Support Conversation (ESC) is a typical dialogue that can\neffec-tively assist the user in mitigating emotional pressures. However, owing\nto the inherent subjectivity involved in analyzing emotions, current\nnon-artificial methodologies face challenges in effectively appraising the\nemo-tional support capability. These metrics exhibit a low correlation with\nhuman judgments. Concurrently, manual evaluation methods extremely will cause\nhigh costs. To solve these problems, we propose a novel model FEEL (Framework\nfor Evaluating Emotional Support Capability with Large Lan-guage Models),\nemploying Large Language Models (LLMs) as evaluators to assess emotional\nsupport capabilities. The model meticulously considers var-ious evaluative\naspects of ESC to apply a more comprehensive and accurate evaluation method for\nESC. Additionally, it employs a probability distribu-tion approach for a more\nstable result and integrates an ensemble learning strategy, leveraging multiple\nLLMs with assigned weights to enhance evalua-tion accuracy. To appraise the\nperformance of FEEL, we conduct extensive experiments on existing ESC model\ndialogues. Experimental results demon-strate our model exhibits a substantial\nenhancement in alignment with human evaluations compared to the baselines. Our\nsource code is available at https://github.com/Ansisy/FEEL.\n","authors":["Huaiwen Zhang","Yu Chen","Ming Wang","Shi Feng"],"pdf_url":"https://arxiv.org/pdf/2403.15699v1.pdf","comment":"14 pages,3 figures and 4 tables"},{"id":"http://arxiv.org/abs/2403.15696v1","updated":"2024-03-23T03:18:14Z","published":"2024-03-23T03:18:14Z","title":"MixRED: A Mix-lingual Relation Extraction Dataset","summary":"  Relation extraction is a critical task in the field of natural language\nprocessing with numerous real-world applications. Existing research primarily\nfocuses on monolingual relation extraction or cross-lingual enhancement for\nrelation extraction. Yet, there remains a significant gap in understanding\nrelation extraction in the mix-lingual (or code-switching) scenario, where\nindividuals intermix contents from different languages within sentences,\ngenerating mix-lingual content. Due to the lack of a dedicated dataset, the\neffectiveness of existing relation extraction models in such a scenario is\nlargely unexplored. To address this issue, we introduce a novel task of\nconsidering relation extraction in the mix-lingual scenario called MixRE and\nconstructing the human-annotated dataset MixRED to support this task. In\naddition to constructing the MixRED dataset, we evaluate both state-of-the-art\nsupervised models and large language models (LLMs) on MixRED, revealing their\nrespective advantages and limitations in the mix-lingual scenario. Furthermore,\nwe delve into factors influencing model performance within the MixRE task and\nuncover promising directions for enhancing the performance of both supervised\nmodels and LLMs in this novel task.\n","authors":["Lingxing Kong","Yougang Chu","Zheng Ma","Jianbing Zhang","Liang He","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.15696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03103v2","updated":"2024-03-23T03:15:42Z","published":"2023-09-06T15:41:38Z","title":"ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation\n  Following the Metaphor Identification Procedure","summary":"  This paper presents ContrastWSD, a RoBERTa-based metaphor detection model\nthat integrates the Metaphor Identification Procedure (MIP) and Word Sense\nDisambiguation (WSD) to extract and contrast the contextual meaning with the\nbasic meaning of a word to determine whether it is used metaphorically in a\nsentence. By utilizing the word senses derived from a WSD model, our model\nenhances the metaphor detection process and outperforms other methods that rely\nsolely on contextual embeddings or integrate only the basic definitions and\nother external knowledge. We evaluate our approach on various benchmark\ndatasets and compare it with strong baselines, indicating the effectiveness in\nadvancing metaphor detection.\n","authors":["Mohamad Elzohbi","Richard Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.03103v2.pdf","comment":"9 pages, 2 figures, accepted for the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2305.16582v2","updated":"2024-03-23T03:06:54Z","published":"2023-05-26T02:15:09Z","title":"Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\n  Language Models","summary":"  With the widespread use of language models (LMs) in NLP tasks, researchers\nhave discovered the potential of Chain-of-thought (CoT) to assist LMs in\naccomplishing complex reasoning tasks by generating intermediate steps.\nHowever, human thought processes are often non-linear, rather than simply\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\nreasoning, which models human thought processes not only as a chain but also as\na graph. By representing thought units as nodes and connections between them as\nedges, our approach captures the non-sequential nature of human thinking and\nallows for a more realistic modeling of thought processes. GoT adopts a\ntwo-stage framework with an additional GoT encoder for thought graph\nrepresentation and fuses the graph representation with the original input\nrepresentation through a gated fusion mechanism. We evaluate GoT's performance\non a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task\n(ScienceQA). Our model achieves significant improvement over the strong CoT\nbaseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%\nusing the T5-base model over the state-of-the-art Multimodal-CoT on the\nScienceQA test set.\n","authors":["Yao Yao","Zuchao Li","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.16582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15690v1","updated":"2024-03-23T02:44:20Z","published":"2024-03-23T02:44:20Z","title":"EAGLE: A Domain Generalization Framework for AI-generated Text Detection","summary":"  With the advancement in capabilities of Large Language Models (LLMs), one\nmajor step in the responsible and safe use of such LLMs is to be able to detect\ntext generated by these models. While supervised AI-generated text detectors\nperform well on text generated by older LLMs, with the frequent release of new\nLLMs, building supervised detectors for identifying text from such new models\nwould require new labeled training data, which is infeasible in practice. In\nthis work, we tackle this problem and propose a domain generalization framework\nfor the detection of AI-generated text from unseen target generators. Our\nproposed framework, EAGLE, leverages the labeled data that is available so far\nfrom older language models and learns features invariant across these\ngenerators, in order to detect text generated by an unknown target generator.\nEAGLE learns such domain-invariant features by combining the representational\npower of self-supervised contrastive learning with domain adversarial training.\nThrough our experiments we demonstrate how EAGLE effectively achieves\nimpressive performance in detecting text generated by unseen target generators,\nincluding recent state-of-the-art ones such as GPT-4 and Claude, reaching\ndetection scores of within 4.7% of a fully supervised detector.\n","authors":["Amrita Bhattacharjee","Raha Moraffah","Joshua Garland","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14328v2","updated":"2024-03-23T02:20:02Z","published":"2023-05-23T17:56:33Z","title":"Benchmarking LLM-based Machine Translation on Cultural Awareness","summary":"  Translating cultural-specific content is crucial for effective cross-cultural\ncommunication. However, many MT systems still struggle to translate sentences\ncontaining cultural-specific entities accurately and understandably. Recent\nadvancements in in-context learning utilize lightweight prompts to guide large\nlanguage models (LLMs) in machine translation tasks. Nevertheless, the\neffectiveness of this approach in enhancing machine translation with cultural\nawareness remains uncertain. To address this gap, we introduce a new data\ncuration pipeline to construct a culturally relevant parallel corpus, enriched\nwith annotations of cultural-specific items. Furthermore, we devise a novel\nevaluation metric to assess the understandability of translations in a\nreference-free manner by GPT-4. We evaluate a variety of neural machine\ntranslation (NMT) and LLM-based MT systems using our dataset. Additionally, we\npropose several prompting strategies for LLMs to incorporate external and\ninternal cultural knowledge into the translation process. Our results\ndemonstrate that eliciting explanations can significantly enhance the\nunderstandability of cultural-specific entities, especially those without\nwell-known translations.\n","authors":["Binwei Yao","Ming Jiang","Diyi Yang","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2305.14328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10827v2","updated":"2024-03-23T02:19:18Z","published":"2024-03-16T06:33:44Z","title":"Multi-party Response Generation with Relation Disentanglement","summary":"  Existing neural response generation models have achieved impressive\nimprovements for two-party conversations, which assume that utterances are\nsequentially organized. However, many real-world dialogues involve multiple\ninterlocutors and the structure of conversational context is much more complex,\ne.g. utterances from different interlocutors can occur \"in parallel\". Facing\nthis challenge, there are works trying to model the relations among utterances\nor interlocutors to facilitate response generation with clearer context.\nNonetheless, these methods rely heavily on such relations and all assume that\nthese are given beforehand, which is impractical and hinders the generality of\nsuch methods. In this work, we propose to automatically infer the relations via\nrelational thinking on subtle clues inside the conversation context without any\nhuman label, and leverage these relations to guide the neural response\ngeneration. Specifically, we first apply a deep graph random process to fully\nconsider all possible relations among utterances in the conversational context.\nThen the inferred relation graphs are integrated with a variational\nauto-encoder framework to train a GAN for structure-aware response generation.\nExperimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark\nand the most recent Movie Dialogues show that our method outperforms various\nbaseline models for multi-party response generation.\n","authors":["Tianhao Dai","Chengyu Huang","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.10827v2.pdf","comment":"The paper needs systematic polishment to consider recent development\n  in dialogue"},{"id":"http://arxiv.org/abs/2403.15676v1","updated":"2024-03-23T01:44:57Z","published":"2024-03-23T01:44:57Z","title":"AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs","summary":"  ZKP systems have surged attention and held a fundamental role in contemporary\ncryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented\nthrough arithmetic circuit programming paradigm. However, underconstrained or\noverconstrained circuits may lead to bugs. Underconstrained circuits refer to\ncircuits that lack the necessary constraints, resulting in unexpected solutions\nin the circuit and causing the verifier to accept a bogus witness.\nOverconstrained circuits refer to circuits that are constrained excessively,\nresulting in the circuit lacking necessary solutions and causing the verifier\nto accept no witness, rendering the circuit meaningless. This paper introduces\na novel approach for pinpointing two distinct types of bugs in ZKP circuits.\nThe method involves encoding the arithmetic circuit constraints to polynomial\nequation systems and solving polynomial equation systems over a finite field by\nalgebraic computation. The classification of verification results is refined,\ngreatly enhancing the expressive power of the system. We proposed a tool, AC4,\nto represent the implementation of this method. Experiments demonstrate that\nAC4 represents a substantial 29% increase in the checked ratio compared to\nprior work. Within a solvable range, the checking time of AC4 has also\nexhibited noticeable improvement, demonstrating a magnitude increase compared\nto previous efforts.\n","authors":["Hao Chen","Minyu Chen","Ruibang Liu","Guoqiang Li"],"pdf_url":"https://arxiv.org/pdf/2403.15676v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15673v1","updated":"2024-03-23T01:40:22Z","published":"2024-03-23T01:40:22Z","title":"AI for Biomedicine in the Era of Large Language Models","summary":"  The capabilities of AI for biomedicine span a wide spectrum, from the atomic\nlevel, where it solves partial differential equations for quantum systems, to\nthe molecular level, predicting chemical or protein structures, and further\nextending to societal predictions like infectious disease outbreaks. Recent\nadvancements in large language models, exemplified by models like ChatGPT, have\nshowcased significant prowess in natural language tasks, such as translating\nlanguages, constructing chatbots, and answering questions. When we consider\nbiomedical data, we observe a resemblance to natural language in terms of\nsequences: biomedical literature and health records presented as text,\nbiological sequences or sequencing data arranged in sequences, or sensor data\nlike brain signals as time series. The question arises: Can we harness the\npotential of recent large language models to drive biomedical knowledge\ndiscoveries? In this survey, we will explore the application of large language\nmodels to three crucial categories of biomedical data: 1) textual data, 2)\nbiological sequences, and 3) brain signals. Furthermore, we will delve into\nlarge language model challenges in biomedical research, including ensuring\ntrustworthiness, achieving personalization, and adapting to multi-modal data\nrepresentation\n","authors":["Zhenyu Bi","Sajib Acharjee Dip","Daniel Hajialigol","Sindhura Kommu","Hanwen Liu","Meng Lu","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15673v1.pdf","comment":"8 pages, 3 figures"}]},"2024-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16248v2","updated":"2024-03-26T17:46:26Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17860v1","updated":"2024-03-26T16:49:25Z","published":"2024-03-26T16:49:25Z","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to\n  Minimize High Confidence Misclassifications","summary":"  Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17859v1","updated":"2024-03-26T16:48:13Z","published":"2024-03-26T16:48:13Z","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on\n  Historical American Newspaper Pages","summary":"  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.\n","authors":["Bhawna Piryani","Jamshid Mozafari","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17859v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17856v1","updated":"2024-03-26T16:45:27Z","published":"2024-03-26T16:45:27Z","title":"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation\n  in Five LLMs","summary":"  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n","authors":["David R. Mortensen","Valentina Izrailevitch","Yunze Xiao","Hinrich Schütze","Leonie Weissweiler"],"pdf_url":"https://arxiv.org/pdf/2403.17856v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11278v3","updated":"2024-03-26T16:40:50Z","published":"2023-07-21T00:34:38Z","title":"Generator-Retriever-Generator Approach for Open-Domain Question\n  Answering","summary":"  Open-domain question answering (QA) tasks usually require the retrieval of\nrelevant information from a large corpus to generate accurate answers. We\npropose a novel approach called Generator-Retriever-Generator (GRG) that\ncombines document retrieval techniques with a large language model (LLM), by\nfirst prompting the model to generate contextual documents based on a given\nquestion. In parallel, a dual-encoder network retrieves documents that are\nrelevant to the question from an external corpus. The generated and retrieved\ndocuments are then passed to the second LLM, which generates the final answer.\nBy combining document retrieval and LLM generation, our approach addresses the\nchallenges of open-domain QA, such as generating informative and contextually\nrelevant answers. GRG outperforms the state-of-the-art generate-then-read and\nretrieve-then-read pipelines (GENREAD and RFiD) improving their performance by\nat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,\nrespectively. We provide code, datasets, and checkpoints at\nhttps://github.com/abdoelsayed2016/GRG.\n","authors":["Abdelrahman Abdallah","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2307.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2309.09800v3","updated":"2024-03-26T16:05:51Z","published":"2023-09-18T14:18:19Z","title":"AMuRD: Annotated Arabic-English Receipt Dataset for Key Information\n  Extraction and Classification","summary":"  The extraction of key information from receipts is a complex task that\ninvolves the recognition and extraction of text from scanned receipts. This\nprocess is crucial as it enables the retrieval of essential content and\norganizing it into structured documents for easy access and analysis. In this\npaper, we present AMuRD, a novel multilingual human-annotated dataset\nspecifically designed for information extraction from receipts. This dataset\ncomprises $47,720$ samples and addresses the key challenges in information\nextraction and item classification - the two critical aspects of data analysis\nin the retail industry. Each sample includes annotations for item names and\nattributes such as price, brand, and more. This detailed annotation facilitates\na comprehensive understanding of each item on the receipt. Furthermore, the\ndataset provides classification into $44$ distinct product categories. This\nclassification feature allows for a more organized and efficient analysis of\nthe items, enhancing the usability of the dataset for various applications. In\nour study, we evaluated various language model architectures, e.g., by\nfine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional\nresults, with an F1 score of 97.43\\% and accuracy of 94.99\\% in information\nextraction and classification, and an even higher F1 score of 98.51\\% and\naccuracy of 97.06\\% observed in specific tasks. The dataset and code are\npublicly accessible for further\nresearchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.\n","authors":["Abdelrahman Abdallah","Mahmoud Abdalla","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2309.09800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03742v2","updated":"2024-03-26T16:03:57Z","published":"2023-08-07T17:46:49Z","title":"Training BERT Models to Carry Over a Coding System Developed on One\n  Corpus to Another","summary":"  This paper describes how we train BERT models to carry over a coding system\ndeveloped on the paragraphs of a Hungarian literary journal to another. The aim\nof the coding system is to track trends in the perception of literary\ntranslation around the political transformation in 1989 in Hungary. To evaluate\nnot only task performance but also the consistence of the annotation, moreover,\nto get better predictions from an ensemble, we use 10-fold crossvalidation.\nExtensive hyperparameter tuning is used to obtain the best possible results and\nfair comparisons. To handle label imbalance, we use loss functions and metrics\nrobust to it. Evaluation of the effect of domain shift is carried out by\nsampling a test set from the target domain. We establish the sample size by\nestimating the bootstrapped confidence interval via simulations. This way, we\nshow that our models can carry over one annotation system to the target domain.\nComparisons are drawn to provide insights such as learning multilabel\ncorrelations and confidence penalty improve resistance to domain shift, and\ndomain adaptation on OCR-ed text on another domain improves performance almost\nto the same extent as that on the corpus under study. See our code at\nhttps://codeberg.org/zsamboki/bert-annotator-ensemble.\n","authors":["Dalma Galambos","Pál Zsámboki"],"pdf_url":"https://arxiv.org/pdf/2308.03742v2.pdf","comment":"Camera-ready version, to be presented at the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17816v1","updated":"2024-03-26T15:53:02Z","published":"2024-03-26T15:53:02Z","title":"Graph Language Model (GLM): A new graph-based approach to detect social\n  instabilities","summary":"  This scientific report presents a novel methodology for the early prediction\nof important political events using News datasets. The methodology leverages\nnatural language processing, graph theory, clique analysis, and semantic\nrelationships to uncover hidden predictive signals within the data. Initially,\nwe designed a preliminary version of the method and tested it on a few events.\nThis analysis revealed limitations in the initial research phase. We then\nenhanced the model in two key ways: first, we added a filtration step to only\nconsider politically relevant news before further processing; second, we\nadjusted the input features to make the alert system more sensitive to\nsignificant spikes in the data. After finalizing the improved methodology, we\ntested it on eleven events including US protests, the Ukraine war, and French\nprotests. Results demonstrate the superiority of our approach compared to\nbaseline methods. Through targeted refinements, our model can now provide\nearlier and more accurate predictions of major political events based on subtle\npatterns in news data.\n","authors":["Wallyson Lemes de Oliveira","Vahid Shamsaddini","Ali Ghofrani","Rahul Singh Inda","Jithendra Sai Veeramaneni","Étienne Voutaz"],"pdf_url":"https://arxiv.org/pdf/2403.17816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2401.11911v4","updated":"2024-03-26T15:47:14Z","published":"2024-01-22T12:54:04Z","title":"Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?","summary":"  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n","authors":["Hexiang Tan","Fei Sun","Wanli Yang","Yuanzhuo Wang","Qi Cao","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01748v2","updated":"2024-03-26T15:26:21Z","published":"2024-03-04T05:55:01Z","title":"Decode Neural signal as Speech","summary":"  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n","authors":["Yiqian Yang","Yiqun Duan","Qiang Zhang","Renjing Xu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.01748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17760v1","updated":"2024-03-26T14:51:12Z","published":"2024-03-26T14:51:12Z","title":"Constructions Are So Difficult That Even Large Language Models Get Them\n  Right for the Wrong Reasons","summary":"  In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.\n","authors":["Shijia Zhou","Leonie Weissweiler","Taiqi He","Hinrich Schütze","David R. Mortensen","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.17760v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17752v1","updated":"2024-03-26T14:43:48Z","published":"2024-03-26T14:43:48Z","title":"Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?","summary":"  Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.\n","authors":["Wangyue Li","Liangzhi Li","Tong Xiang","Xiao Liu","Wei Deng","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.17752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17748v1","updated":"2024-03-26T14:40:10Z","published":"2024-03-26T14:40:10Z","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal\n  Dependencies","summary":"  The Universal Dependencies (UD) project has created an invaluable collection\nof treebanks with contributions in over 140 languages. However, the UD\nannotations do not tell the full story. Grammatical constructions that convey\nmeaning through a particular combination of several morphosyntactic elements --\nfor example, interrogative sentences with special markers and/or word orders --\nare not labeled holistically. We argue for (i) augmenting UD annotations with a\n'UCxn' annotation layer for such meaning-bearing grammatical constructions, and\n(ii) approaching this in a typologically informed way so that morphosyntactic\nstrategies can be compared across languages. As a case study, we consider five\nconstruction families in ten languages, identifying instances of each\nconstruction in UD treebanks through the use of morphosyntactic patterns. In\naddition to findings regarding these particular constructions, our study yields\nimportant insights on methodology for describing and identifying constructions\nin language-general and language-particular ways, and lays the foundation for\nfuture constructional enrichment of UD treebanks.\n","authors":["Leonie Weissweiler","Nina Böbel","Kirian Guiller","Santiago Herrera","Wesley Scivetti","Arthur Lorenzi","Nurit Melnik","Archna Bhatia","Hinrich Schütze","Lori Levin","Amir Zeldes","Joakim Nivre","William Croft","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17748v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12721v2","updated":"2024-03-26T14:32:34Z","published":"2024-03-19T13:30:47Z","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched\n  with Linguistic and Genre Annotation","summary":"  This paper presents a collection of highly comparable web corpora of\nSlovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,\ncovering thereby the whole spectrum of official languages in the South Slavic\nlanguage space. The collection of these corpora comprises a total of 13 billion\ntokens of texts from 26 million documents. The comparability of the corpora is\nensured by a comparable crawling setup and the usage of identical crawling and\npost-processing technology. All the corpora were linguistically annotated with\nthe state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and\nenriched with document-level genre information via the Transformer-based\nmultilingual X-GENRE classifier, which further enhances comparability at the\nlevel of linguistic annotation and metadata enrichment. The genre-focused\nanalysis of the resulting corpora shows a rather consistent distribution of\ngenres throughout the seven corpora, with variations in the most prominent\ngenre categories being well-explained by the economic strength of each language\ncommunity. A comparison of the distribution of genre categories across the\ncorpora indicates that web corpora from less developed countries primarily\nconsist of news articles. Conversely, web corpora from economically more\ndeveloped countries exhibit a smaller proportion of news content, with a\ngreater presence of promotional and opinionated texts.\n","authors":["Nikola Ljubešić","Taja Kuzman"],"pdf_url":"https://arxiv.org/pdf/2403.12721v2.pdf","comment":"Accepted to the LREC-COLING 2024 conference"},{"id":"http://arxiv.org/abs/2307.05300v4","updated":"2024-03-26T14:32:33Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v4.pdf","comment":"Accepted as a main conference paper at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17733v1","updated":"2024-03-26T14:20:42Z","published":"2024-03-26T14:20:42Z","title":"Continual Few-shot Event Detection via Hierarchical Augmentation\n  Networks","summary":"  Traditional continual event detection relies on abundant labeled data for\ntraining, which is often impractical to obtain in real-world applications. In\nthis paper, we introduce continual few-shot event detection (CFED), a more\ncommonly encountered scenario when a substantial number of labeled samples are\nnot accessible. The CFED task is challenging as it involves memorizing previous\nevent types and learning new event types with few-shot samples. To mitigate\nthese challenges, we propose a memory-based framework: Hierarchical\nAugmentation Networks (HANet). To memorize previous event types with limited\nmemory, we incorporate prototypical augmentation into the memory set. For the\nissue of learning new event types in few-shot scenarios, we propose a\ncontrastive augmentation module for token representations. Despite comparing\nwith previous state-of-the-art methods, we also conduct comparisons with\nChatGPT. Experiment results demonstrate that our method significantly\noutperforms all of these methods in multiple continual few-shot event detection\ntasks.\n","authors":["Chenlong Zhang","Pengfei Cao","Yubo Chen","Kang Liu","Zhiqiang Zhang","Mengshu Sun","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.17733v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17706v1","updated":"2024-03-26T13:50:34Z","published":"2024-03-26T13:50:34Z","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic\n  Refinement","summary":"  Crafting effective topic models for brief texts, like tweets and news\nheadlines, is essential for capturing the swift shifts in social dynamics.\nTraditional topic models, however, often fall short in accurately representing\nthe semantic intricacies of short texts due to their brevity and lack of\ncontextual data. In our study, we harness the advanced capabilities of Large\nLanguage Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".\nThis approach does not directly involve itself in the initial modeling of\ntopics but focuses on improving topics after they have been mined. By employing\nprompt engineering, we direct LLMs to eliminate off-topic words within a given\ntopic, ensuring that only contextually relevant words are preserved or\nsubstituted with ones that fit better semantically. This method emulates\nhuman-like scrutiny and improvement of topics, thereby elevating the semantic\nquality of the topics generated by various models. Our comprehensive evaluation\nacross three unique datasets has shown that our topic refinement approach\nsignificantly enhances the semantic coherence of topics.\n","authors":["Shuyu Chang","Rui Wang","Peng Ren","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17706v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2305.14230v2","updated":"2024-03-26T13:16:37Z","published":"2023-05-23T16:46:18Z","title":"Exploring Representational Disparities Between Multilingual and\n  Bilingual Translation Models","summary":"  Multilingual machine translation has proven immensely useful for both\nparameter efficiency and overall performance across many language pairs via\ncomplete multilingual parameter sharing. However, some language pairs in\nmultilingual models can see worse performance than in bilingual models,\nespecially in the one-to-many translation setting. Motivated by their empirical\ndifferences, we examine the geometric differences in representations from\nbilingual models versus those from one-to-many multilingual models.\nSpecifically, we compute the isotropy of these representations using intrinsic\ndimensionality and IsoScore, in order to measure how the representations\nutilize the dimensions in their underlying vector space. Using the same\nevaluation data in both models, we find that for a given language pair, its\nmultilingual model decoder representations are consistently less isotropic and\noccupy fewer dimensions than comparable bilingual model decoder\nrepresentations. Additionally, we show that much of the anisotropy in\nmultilingual decoder representations can be attributed to modeling\nlanguage-specific information, therefore limiting remaining representational\ncapacity.\n","authors":["Neha Verma","Kenton Murray","Kevin Duh"],"pdf_url":"https://arxiv.org/pdf/2305.14230v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02270v2","updated":"2024-03-26T13:14:52Z","published":"2024-03-04T17:57:18Z","title":"FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction","summary":"  Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization.\n","authors":["Alessandro Scirè","Karim Ghonim","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2403.02270v2.pdf","comment":"9 pages, long paper"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13737v3","updated":"2024-03-26T13:01:38Z","published":"2024-03-20T16:43:42Z","title":"EthioLLM: Multilingual Large Language Models for Ethiopian Languages\n  with Task Evaluation","summary":"  Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.\n","authors":["Atnafu Lambebo Tonja","Israel Abebe Azime","Tadesse Destaw Belay","Mesay Gemeda Yigezu","Moges Ahmed Mehamed","Abinew Ali Ayele","Ebrahim Chekol Jibril","Michael Melese Woldeyohannis","Olga Kolesnikova","Philipp Slusallek","Dietrich Klakow","Shengwu Xiong","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2403.13737v3.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2403.14117v2","updated":"2024-03-26T12:53:14Z","published":"2024-03-21T04:03:16Z","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"  In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.\n","authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Simon Knight","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"pdf_url":"https://arxiv.org/pdf/2403.14117v2.pdf","comment":"Published as a conference paper at CHI 2024"},{"id":"http://arxiv.org/abs/2403.17661v1","updated":"2024-03-26T12:47:39Z","published":"2024-03-26T12:47:39Z","title":"Language Models for Text Classification: Is In-Context Learning Enough?","summary":"  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n","authors":["Aleksandra Edwards","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2403.17661v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.10592v3","updated":"2024-03-26T12:31:35Z","published":"2023-08-21T09:47:31Z","title":"BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content\n  from Wykop.pl web service","summary":"  Since the Internet is flooded with hate, it is one of the main tasks for NLP\nexperts to master automated online content moderation. However, advancements in\nthis field require improved access to publicly available accurate and\nnon-synthetic datasets of social media content. For the Polish language, such\nresources are very limited. In this paper, we address this gap by presenting a\nnew open dataset of offensive social media content for the Polish language. The\ndataset comprises content from Wykop.pl, a popular online service often\nreferred to as the \"Polish Reddit\", reported by users and banned in the\ninternal moderation process. It contains a total of 691,662 posts and comments,\nevenly divided into two categories: \"harmful\" and \"neutral\" (\"non-harmful\").\nThe anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000\nfor each class), along with preprocessing scripts have been made publicly\navailable. Furthermore the paper offers valuable insights into real-life\ncontent moderation processes and delves into an analysis of linguistic features\nand content characteristics of the dataset. Moreover, a comprehensive\nanonymization procedure has been meticulously described and applied. The\nprevalent biases encountered in similar datasets, including post-moderation and\npre-selection biases, are also discussed.\n","authors":["Anna Kołos","Inez Okulska","Kinga Głąbińska","Agnieszka Karlińska","Emilia Wiśnios","Paweł Ellerik","Andrzej Prałat"],"pdf_url":"https://arxiv.org/pdf/2308.10592v3.pdf","comment":"Accepted for LREC-COLING 2024 Conference"},{"id":"http://arxiv.org/abs/2403.17647v1","updated":"2024-03-26T12:29:18Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17645v1","updated":"2024-03-26T12:27:32Z","published":"2024-03-26T12:27:32Z","title":"DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition","summary":"  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n","authors":["Yi-Cheng Wang","Hsin-Wei Wang","Bi-Cheng Yan","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13334v2","updated":"2024-03-26T12:24:46Z","published":"2024-03-20T06:37:59Z","title":"Hyacinth6B: A large language model for Traditional Chinese","summary":"  This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.\n","authors":["Chih-Wei Song","Yin-Te Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.13334v2.pdf","comment":"14pages"},{"id":"http://arxiv.org/abs/2403.17640v1","updated":"2024-03-26T12:21:51Z","published":"2024-03-26T12:21:51Z","title":"REFeREE: A REference-FREE Model-Based Metric for Text Simplification","summary":"  Text simplification lacks a universal standard of quality, and annotated\nreference simplifications are scarce and costly. We propose to alleviate such\nlimitations by introducing REFeREE, a reference-free model-based metric with a\n3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage\nand can be applied to any quality standard as long as a small number of human\nannotations are available. Our experiments show that our metric outperforms\nexisting reference-based metrics in predicting overall ratings and reaches\ncompetitive and consistent performance in predicting specific ratings while\nrequiring no reference simplifications at inference time.\n","authors":["Yichen Huang","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2403.17640v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17636v1","updated":"2024-03-26T12:11:29Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v1.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2310.12541v3","updated":"2024-03-26T12:04:44Z","published":"2023-10-19T07:46:54Z","title":"Large Language Model for Multi-objective Evolutionary Optimization","summary":"  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving\nmultiobjective optimization problems (MOPs). Many MOEAs have been proposed in\nthe past decades, of which the search operators need a carefully handcrafted\ndesign with domain knowledge. Recently, some attempts have been made to replace\nthe manually designed operators in MOEAs with learning-based operators (e.g.,\nneural network models). However, much effort is still required for designing\nand training such models, and the learned operators might not generalize well\non new problems. To tackle the above challenges, this work investigates a novel\napproach that leverages the powerful large language model (LLM) to design MOEA\noperators. With proper prompt engineering, we successfully let a general LLM\nserve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a\nzero-shot manner. In addition, by learning from the LLM behavior, we further\ndesign an explicit white-box operator with randomness and propose a new version\nof decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on\ndifferent test benchmarks show that our proposed method can achieve competitive\nperformance with widely used MOEAs. It is also promising to see the operator\nonly learned from a few instances can have robust generalization performance on\nunseen problems with quite different patterns and settings. The results reveal\nthe potential benefits of using pre-trained LLMs in the design of MOEAs.To\nfoster reproducibility and accessibility, the source code is\nhttps://github.com/FeiLiu36/LLM4MOEA.\n","authors":["Fei Liu","Xi Lin","Zhenkun Wang","Shunyu Yao","Xialiang Tong","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17612v1","updated":"2024-03-26T11:45:22Z","published":"2024-03-26T11:45:22Z","title":"\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations\n  for Emotion Intensity Modeling","summary":"  Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.\n","authors":["Christopher Bagdon","Prathamesh Karmalker","Harsha Gurulingappa","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2403.17612v1.pdf","comment":"accepted for publication in NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17611v1","updated":"2024-03-26T11:44:49Z","published":"2024-03-26T11:44:49Z","title":"Denoising Table-Text Retrieval for Open-Domain Question Answering","summary":"  In table-text open-domain question answering, a retriever system retrieves\nrelevant evidence from tables and text to answer questions. Previous studies in\ntable-text open-domain question answering have two common challenges: firstly,\ntheir retrievers can be affected by false-positive labels in training datasets;\nsecondly, they may struggle to provide appropriate evidence for questions that\nrequire reasoning across the table. To address these issues, we propose\nDenoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a\ndenoised training dataset with fewer false positive labels by discarding\ninstances with lower question-relevance scores measured through a false\npositive detection model. Subsequently, we integrate table-level ranking\ninformation into the retriever to assist in finding evidence for questions that\ndemand reasoning across the table. To encode this ranking information, we\nfine-tune a rank-aware column encoder to identify minimum and maximum values\nwithin a column. Experimental results demonstrate that DoTTeR significantly\noutperforms strong baselines on both retrieval recall and downstream QA tasks.\nOur code is available at https://github.com/deokhk/DoTTeR.\n","authors":["Deokhyung Kang","Baikjin Jung","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17611v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.14790v2","updated":"2024-03-26T11:29:21Z","published":"2023-05-24T06:43:23Z","title":"Advancing Topic Segmentation and Outline Generation in Chinese Texts:\n  The Paragraph-level Topic Representation, Corpus, and Benchmark","summary":"  Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings, unveiling the\ndiscourse topic structure of a document. Compared with sentence-level topic\nstructure, the paragraph-level topic structure can quickly grasp and understand\nthe overall context of the document from a higher level, benefitting many\ndownstream tasks such as summarization, discourse parsing, and information\nretrieval. However, the lack of large-scale, high-quality Chinese\nparagraph-level topic structure corpora restrained relative research and\napplications. To fill this gap, we build the Chinese paragraph-level topic\nrepresentation, corpus, and benchmark in this paper. Firstly, we propose a\nhierarchical paragraph-level topic structure representation with three layers\nto guide the corpus construction. Then, we employ a two-stage man-machine\ncollaborative annotation method to construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), achieving high quality. We also\nbuild several strong baselines, including ChatGPT, to validate the\ncomputability of CPTS on two fundamental tasks (topic segmentation and outline\ngeneration) and preliminarily verified its usefulness for the downstream task\n(discourse parsing).\n","authors":["Feng Jiang","Weihao Liu","Xiaomin Chu","Peifeng Li","Qiaoming Zhu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2305.14790v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13318v2","updated":"2024-03-26T11:26:04Z","published":"2023-09-23T09:24:05Z","title":"Spanish Resource Grammar version 2023","summary":"  We present the latest version of the Spanish Resource Grammar (SRG), a\ngrammar of Spanish implemented in the HPSG formalism. Such grammars encode a\ncomplex set of hypotheses about syntax making them a resource for empirical\ntesting of linguistic theory. They also encode a strict notion of\ngrammaticality which makes them a resource for natural language processing\napplications in computer-assisted language learning. This version of the SRG\nuses the recent version of the Freeling morphological analyzer and is released\nalong with an automatically created, manually verified treebank of 2,291\nsentences. We explain the treebanking process, emphasizing how it is different\nfrom treebanking with manual annotation and how it contributes to\nempirically-driven development of syntactic theory. The treebanks' high level\nof consistency and detail makes them a resource for training high-quality\nsemantic parsers and generally systems that benefit from precise and detailed\nsemantics. Finally, we present the grammar's coverage and overgeneration on 100\nsentences from a learner corpus, a new research line related to developing\nmethodologies for robust empirical evaluation of hypotheses in second language\nacquisition.\n","authors":["Olga Zamaraeva","Lorena S. Allegue","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2309.13318v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08644v3","updated":"2024-03-26T11:13:56Z","published":"2024-02-13T18:24:08Z","title":"Tandem Transformers for Inference Efficient LLMs","summary":"  The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.\n","authors":["Aishwarya P S","Pranav Ajit Nair","Yashas Samaga","Toby Boyd","Sanjiv Kumar","Prateek Jain","Praneeth Netrapalli"],"pdf_url":"https://arxiv.org/pdf/2402.08644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17599v1","updated":"2024-03-26T11:09:58Z","published":"2024-03-26T11:09:58Z","title":"Coimagining the Future of Voice Assistants with Cultural Sensitivity","summary":"  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the\nuser experience (UX) is often limited, leading to underuse, disengagement, and\nabandonment. Co-designing interactions for VAs with potential end-users can be\nuseful. Crowdsourcing this process online and anonymously may add value.\nHowever, most work has been done in the English-speaking West on dialogue data\nsets. We must be sensitive to cultural differences in language, social\ninteractions, and attitudes towards technology. Our aims were to explore the\nvalue of co-designing VAs in the non-Western context of Japan and demonstrate\nthe necessity of cultural sensitivity. We conducted an online elicitation study\n(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined\ndialogues (N = 282) and activities (N = 73) with future VAs. We discuss the\nimplications for coimagining interactions with future VAs, offer design\nguidelines for the Japanese and English-speaking US contexts, and suggest\nopportunities for cultural plurality in VA design and scholarship.\n","authors":["Katie Seaborn","Yuto Sawa","Mizuki Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17599v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2402.11537v2","updated":"2024-03-26T10:45:40Z","published":"2024-02-18T10:36:05Z","title":"Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning","summary":"  Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Zhouhao Sun","Jun Shi","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2402.11537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08274v4","updated":"2024-03-26T10:36:31Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models'(LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as binary classifications\nfor large language models. Specifically, LLMs make the decision based on the\nexternal corpus and its world knowledge, giving the reason for the judgment for\nfactual verification. This method is tailored for semi-structured web articles,\nwherein we designate the main title as the tail entity and explicitly\nincorporate it into the context, and the potential head entities are matched\nbased on a biomedical thesaurus. Moreover, lengthy contents are sliced into\ntext chunks, embedded, and retrieved with additional embedding models. Results:\nUsing an open-source LLM, we extracted 248659 relation triplets of three\ndistinct relation types from three reputable biomedical websites. To assess the\nefficacy of the basic pipeline employed for biomedical relation extraction, we\ncurated a benchmark dataset annotated by a medical expert. Evaluation results\nindicate that the pipeline exhibits performance comparable to that of GPT-4.\nCase studies further illuminate challenges faced by contemporary LLMs in the\ncontext of biomedical relation extraction for semi-structured web articles.\nConclusion: The proposed method has demonstrated its effectiveness in\nleveraging the strengths of LLMs for high-throughput biomedical relation\nextraction. Its adaptability is evident, as it can be seamlessly extended to\ndiverse semi-structured biomedical websites, facilitating the extraction of\nvarious types of biomedical relations with ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15885v2","updated":"2024-03-26T10:26:04Z","published":"2024-03-23T16:45:22Z","title":"STEntConv: Predicting Disagreement with Stance Detection and a Signed\n  Graph Convolutional Network","summary":"  The rise of social media platforms has led to an increase in polarised online\ndiscussions, especially on political and socio-cultural topics such as\nelections and climate change. We propose a simple and novel unsupervised method\nto predict whether the authors of two posts agree or disagree, leveraging user\nstances about named entities obtained from their posts. We present STEntConv, a\nmodel which builds a graph of users and named entities weighted by stance and\ntrains a Signed Graph Convolutional Network (SGCN) to detect disagreement\nbetween comment and reply posts. We run experiments and ablation studies and\nshow that including this information improves disagreement detection\nperformance on a dataset of Reddit posts for a range of controversial subreddit\ntopics, without the need for platform-specific features or user history.\n","authors":["Isabelle Lorge","Li Zhang","Xiaowen Dong","Janet B. Pierrehumbert"],"pdf_url":"https://arxiv.org/pdf/2403.15885v2.pdf","comment":"Accepted for the 2024 Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.17564v1","updated":"2024-03-26T10:14:12Z","published":"2024-03-26T10:14:12Z","title":"Task-Oriented Paraphrase Analytics","summary":"  Since paraphrasing is an ill-defined task, the term \"paraphrasing\" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n","authors":["Marcel Gohsen","Matthias Hagen","Martin Potthast","Benno Stein"],"pdf_url":"https://arxiv.org/pdf/2403.17564v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02541v4","updated":"2024-03-26T10:13:06Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v4.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17556v1","updated":"2024-03-26T10:04:24Z","published":"2024-03-26T10:04:24Z","title":"m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt","summary":"  Multilingual translation supports multiple translation directions by\nprojecting all languages in a shared space, but the translation quality is\nundermined by the difference between languages in the text-only modality,\nespecially when the number of languages is large. To bridge this gap, we\nintroduce visual context as the universal language-independent representation\nto facilitate multilingual translation. In this paper, we propose a framework\nto leverage the multimodal prompt to guide the Multimodal Multilingual neural\nMachine Translation (m3P), which aligns the representations of different\nlanguages with the same meaning and generates the conditional vision-language\nmemory for translation. We construct a multilingual multimodal instruction\ndataset (InstrMulti102) to support 102 languages. Our method aims to minimize\nthe representation distance of different languages by regarding the image as a\ncentral language. Experimental results show that m3P outperforms previous\ntext-only baselines and multilingual multimodal methods by a large margin.\nFurthermore, the probing experiments validate the effectiveness of our method\nin enhancing translation under the low-resource and massively multilingual\nscenario.\n","authors":["Jian Yang","Hongcheng Guo","Yuwei Yin","Jiaqi Bai","Bing Wang","Jiaheng Liu","Xinnian Liang","Linzheng Cahi","Liqun Yang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2403.17556v1.pdf","comment":"COLING 2024"},{"id":"http://arxiv.org/abs/2403.17553v1","updated":"2024-03-26T10:01:01Z","published":"2024-03-26T10:01:01Z","title":"RuBia: A Russian Language Bias Detection Dataset","summary":"  Warning: this work contains upsetting or disturbing content.\n  Large language models (LLMs) tend to learn the social and cultural biases\npresent in the raw pre-training data. To test if an LLM's behavior is fair,\nfunctional datasets are employed, and due to their purpose, these datasets are\nhighly language and culture-specific. In this paper, we address a gap in the\nscope of multilingual bias evaluation by presenting a bias detection dataset\nspecifically designed for the Russian language, dubbed as RuBia. The RuBia\ndataset is divided into 4 domains: gender, nationality, socio-economic status,\nand diverse, each of the domains is further divided into multiple fine-grained\nsubdomains. Every example in the dataset consists of two sentences with the\nfirst reinforcing a potentially harmful stereotype or trope and the second\ncontradicting it. These sentence pairs were first written by volunteers and\nthen validated by native-speaking crowdsourcing workers. Overall, there are\nnearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To\nillustrate the dataset's purpose, we conduct a diagnostic evaluation of\nstate-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'\npredisposition to social biases.\n","authors":["Veronika Grigoreva","Anastasiia Ivanova","Ilseyar Alimova","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2403.17553v1.pdf","comment":"accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17552v1","updated":"2024-03-26T09:59:45Z","published":"2024-03-26T09:59:45Z","title":"Naive Bayes-based Context Extension for Large Language Models","summary":"  Large Language Models (LLMs) have shown promising in-context learning\nabilities. However, conventional In-Context Learning (ICL) approaches are often\nimpeded by length limitations of transformer architecture, which pose\nchallenges when attempting to effectively integrate supervision from a\nsubstantial number of demonstration examples. In this paper, we introduce a\nnovel framework, called Naive Bayes-based Context Extension (NBCE), to enable\nexisting LLMs to perform ICL with an increased number of demonstrations by\nsignificantly expanding their context size. Importantly, this expansion does\nnot require fine-tuning or dependence on particular model architectures, all\nthe while preserving linear efficiency. NBCE initially splits the context into\nequal-sized windows fitting the target LLM's maximum length. Then, it\nintroduces a voting mechanism to select the most relevant window, regarded as\nthe posterior context. Finally, it employs Bayes' theorem to generate the test\ntask. Our experimental results demonstrate that NBCE substantially enhances\nperformance, particularly as the number of demonstration examples increases,\nconsistently outperforming alternative methods. The NBCE code will be made\npublicly accessible. The code NBCE is available at:\nhttps://github.com/amurtadha/NBCE-master\n","authors":["Jianlin Su","Murtadha Ahmed"," Wenbo","Luo Ao","Mingren Zhu","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17552v1.pdf","comment":"Accepted to main NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17546v1","updated":"2024-03-26T09:51:43Z","published":"2024-03-26T09:51:43Z","title":"Decoding excellence: Mapping the demand for psychological traits of\n  operations and supply chain professionals through text mining","summary":"  The current study proposes an innovative methodology for the profiling of\npsychological traits of Operations Management (OM) and Supply Chain Management\n(SCM) professionals. We use innovative methods and tools of text mining and\nsocial network analysis to map the demand for relevant skills from a set of job\ndescriptions, with a focus on psychological characteristics. The proposed\napproach aims to evaluate the market demand for specific traits by combining\nrelevant psychological constructs, text mining techniques, and an innovative\nmeasure, namely, the Semantic Brand Score. We apply the proposed methodology to\na dataset of job descriptions for OM and SCM professionals, with the objective\nof providing a mapping of their relevant required skills, including\npsychological characteristics. In addition, the analysis is then detailed by\nconsidering the region of the organization that issues the job description, its\norganizational size, and the seniority level of the open position in order to\nunderstand their nuances. Finally, topic modeling is used to examine key\ncomponents and their relative significance in job descriptions. By employing a\nnovel methodology and considering contextual factors, we provide an innovative\nunderstanding of the attitudinal traits that differentiate professionals. This\nresearch contributes to talent management, recruitment practices, and\nprofessional development initiatives, since it provides new figures and\nperspectives to improve the effectiveness and success of Operations Management\nand Supply Chain Management professionals.\n","authors":["S. Di Luozzo","A. Fronzetti Colladon","M. M. Schiraldi"],"pdf_url":"https://arxiv.org/pdf/2403.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17545v1","updated":"2024-03-26T09:49:35Z","published":"2024-03-26T09:49:35Z","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions","summary":"  Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.\n","authors":["Shun Inadumi","Seiya Kawano","Akishige Yuguchi","Yasutomo Kawanishi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.17545v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17540v1","updated":"2024-03-26T09:43:15Z","published":"2024-03-26T09:43:15Z","title":"Large Language Models Are State-of-the-Art Evaluator for Grammatical\n  Error Correction","summary":"  Large Language Models (LLMs) have been reported to outperform existing\nautomatic evaluation metrics in some tasks, such as text summarization and\nmachine translation. However, there has been a lack of research on LLMs as\nevaluators in grammatical error correction (GEC). In this study, we investigate\nthe performance of LLMs in GEC evaluation by employing prompts designed to\nincorporate various evaluation criteria inspired by previous research. Our\nextensive experimental results demonstrate that GPT-4 achieved Kendall's rank\ncorrelation of 0.662 with human judgments, surpassing all existing methods.\nFurthermore, in recent GEC evaluations, we have underscored the significance of\nthe LLMs scale and particularly emphasized the importance of fluency among\nevaluation criteria.\n","authors":["Masamune Kobayashi","Masato Mita","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2403.17540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17536v1","updated":"2024-03-26T09:41:21Z","published":"2024-03-26T09:41:21Z","title":"ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent\n  Classifier and Slot Filler","summary":"  State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.\n","authors":["Paramita Mirza","Viju Sudhi","Soumya Ranjan Sahoo","Sinchana Ramakanth Bhat"],"pdf_url":"https://arxiv.org/pdf/2403.17536v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17534v1","updated":"2024-03-26T09:39:53Z","published":"2024-03-26T09:39:53Z","title":"Sparse Logistic Regression with High-order Features for Automatic\n  Grammar Rule Extraction from Treebanks","summary":"  Descriptive grammars are highly valuable, but writing them is time-consuming\nand difficult. Furthermore, while linguists typically use corpora to create\nthem, grammar descriptions often lack quantitative data. As for formal\ngrammars, they can be challenging to interpret. In this paper, we propose a new\nmethod to extract and explore significant fine-grained grammar patterns and\npotential syntactic grammar rules from treebanks, in order to create an\neasy-to-understand corpus-based grammar. More specifically, we extract\ndescriptions and rules across different languages for two linguistic phenomena,\nagreement and word order, using a large search space and paying special\nattention to the ranking order of the extracted rules. For that, we use a\nlinear classifier to extract the most salient features that predict the\nlinguistic phenomena under study. We associate statistical information to each\nrule, and we compare the ranking of the model's results to those of other\nquantitative and statistical measures. Our method captures both well-known and\nless well-known significant grammar rules in Spanish, French, and Wolof.\n","authors":["Santiago Herrera","Caio Corro","Sylvain Kahane"],"pdf_url":"https://arxiv.org/pdf/2403.17534v1.pdf","comment":"Published in LREC-Coling 2024 proceedings"},{"id":"http://arxiv.org/abs/2403.17528v1","updated":"2024-03-26T09:31:55Z","published":"2024-03-26T09:31:55Z","title":"Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual\n  Applications","summary":"  Prior work on multilingual sentence embedding has demonstrated that the\nefficient use of natural language inference (NLI) data to build\nhigh-performance models can outperform conventional methods. However, the\npotential benefits from the recent ``exponential'' growth of language models\nwith billions of parameters have not yet been fully explored. In this paper, we\nintroduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based\nmultilingual sentence embedding, by extending Sentence T5, an existing\nmonolingual model. By employing the low-rank adaptation (LoRA) technique, we\nhave achieved a successful scaling of the model's size to 5.7 billion\nparameters. We conducted experiments to evaluate the performance of sentence\nembedding and verified that the method outperforms the NLI-based prior\napproach. Furthermore, we also have confirmed a positive correlation between\nthe size of the model and its performance. It was particularly noteworthy that\nlanguages with fewer resources or those with less linguistic similarity to\nEnglish benefited more from the parameter increase. Our model is available at\nhttps://huggingface.co/pkshatech/m-ST5.\n","authors":["Chihiro Yano","Akihiko Fukuchi","Shoko Fukasawa","Hideyuki Tachibana","Yotaro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17528v1.pdf","comment":"Accepted in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08281v4","updated":"2024-03-26T09:29:51Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Weilin Zhao","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17524v1","updated":"2024-03-26T09:25:57Z","published":"2024-03-26T09:25:57Z","title":"Provably Secure Disambiguating Neural Linguistic Steganography","summary":"  Recent research in provably secure neural linguistic steganography has\noverlooked a crucial aspect: the sender must detokenize stegotexts to avoid\nraising suspicion from the eavesdropper. The segmentation ambiguity problem,\nwhich arises when using language models based on subwords, leads to occasional\ndecoding failures in all neural language steganography implementations based on\nthese models. Current solutions to this issue involve altering the probability\ndistribution of candidate words, rendering them incompatible with provably\nsecure steganography. We propose a novel secure disambiguation method named\nSyncPool, which effectively addresses the segmentation ambiguity problem. We\ngroup all tokens with prefix relationships in the candidate pool before the\nsteganographic embedding algorithm runs to eliminate uncertainty among\nambiguous tokens. To enable the receiver to synchronize the sampling process of\nthe sender, a shared cryptographically-secure pseudorandom number generator\n(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does\nnot change the size of the candidate pool or the distribution of tokens and\nthus is applicable to provably secure language steganography methods. We\nprovide theoretical proofs and experimentally demonstrate the applicability of\nour solution to various languages and models, showing its potential to\nsignificantly improve the reliability and security of neural linguistic\nsteganography systems.\n","authors":["Yuang Qi","Kejiang Chen","Kai Zeng","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17516v1","updated":"2024-03-26T09:18:59Z","published":"2024-03-26T09:18:59Z","title":"MapGuide: A Simple yet Effective Method to Reconstruct Continuous\n  Language from Brain Activities","summary":"  Decoding continuous language from brain activity is a formidable yet\npromising field of research. It is particularly significant for aiding people\nwith speech disabilities to communicate through brain signals. This field\naddresses the complex task of mapping brain signals to text. The previous best\nattempt reverse-engineered this process in an indirect way: it began by\nlearning to encode brain activity from text and then guided text generation by\naligning with predicted brain responses. In contrast, we propose a simple yet\neffective method that guides text reconstruction by directly comparing them\nwith the predicted text embeddings mapped from brain activities. Comprehensive\nexperiments reveal that our method significantly outperforms the current\nstate-of-the-art model, showing average improvements of 77% and 54% on BLEU and\nMETEOR scores. We further validate the proposed modules through detailed\nablation studies and case analyses and highlight a critical correlation: the\nmore precisely we map brain activities to text embeddings, the better the text\nreconstruction results. Such insight can simplify the task of reconstructing\nlanguage from brain activities for future work, emphasizing the importance of\nimproving brain-to-text-embedding mapping techniques.\n","authors":["Xinpei Zhao","Jingyuan Sun","Shaonan Wang","Jing Ye","Xiaohan Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2403.17516v1.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2305.16031v2","updated":"2024-03-26T09:16:36Z","published":"2023-05-25T13:08:10Z","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence\n  Learning","summary":"  Learning quality document embeddings is a fundamental problem in natural\nlanguage processing (NLP), information retrieval (IR), recommendation systems,\nand search engines. Despite recent advances in the development of\ntransformer-based models that produce sentence embeddings with self-contrastive\nlearning, the encoding of long documents (Ks of words) is still challenging\nwith respect to both efficiency and quality considerations. Therefore, we train\nLongfomer-based document encoders using a state-of-the-art unsupervised\ncontrastive learning method (SimCSE). Further on, we complement the baseline\nmethod -- siamese neural network -- with additional convex neural networks\nbased on functional Bregman divergence aiming to enhance the quality of the\noutput document representations. We show that overall the combination of a\nself-contrastive siamese network and our proposed neural Bregman network\noutperforms the baselines in two linear classification settings on three long\ndocument topic classification tasks from the legal and biomedical domains.\n","authors":["Daniel Saggau","Mina Rezaei","Bernd Bischl","Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2305.16031v2.pdf","comment":"5 pages, short paper at Findings of ACL 2023"},{"id":"http://arxiv.org/abs/2403.17497v1","updated":"2024-03-26T08:58:28Z","published":"2024-03-26T08:58:28Z","title":"Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies","summary":"  In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.\n","authors":["Philipp Sadler","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2403.17497v1.pdf","comment":"9 pages, Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17491v1","updated":"2024-03-26T08:47:23Z","published":"2024-03-26T08:47:23Z","title":"DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation","summary":"  The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.\n","authors":["Xinyu Ning","Yutong Zhao","Yitong Liu","Hongwen Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17491v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.14974v3","updated":"2024-03-26T08:46:07Z","published":"2023-09-25T09:21:25Z","title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts","summary":"  In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n","authors":["Thibault Clérice"],"pdf_url":"https://arxiv.org/pdf/2309.14974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17486v1","updated":"2024-03-26T08:32:39Z","published":"2024-03-26T08:32:39Z","title":"KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with\n  Adaptive Angular margin Contrastive Learning","summary":"  Previous work on multimodal sentence embedding has proposed multimodal\ncontrastive learning and achieved promising results. However, by taking the\nrest of the batch as negative samples without reviewing when forming\ncontrastive pairs, those studies encountered many suspicious and noisy negative\nexamples, significantly affecting the methods' overall performance. In this\nwork, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning\nof Sentence Embeddings), a novel approach that enhances the discrimination and\ngeneralizability of multimodal representation and inherits the knowledge from\nthe teacher model to learn the difference between positive and negative\ninstances and via that, can detect noisy and wrong negative samples effectively\nbefore they are calculated in the contrastive objective. Furthermore, to\novercome the limitation of modeling the variation within negative pairs, we\nintroduce a new contrastive objective, AdapACSE (Adaptive Angular Margin\nSupervised Contrastive Learning for Multimodal sentence embeddings), that\nenhances the discriminative representation by strengthening the margin within\nthe angular space while capturing varying semantics within the negative.\nExperimental results on widely used Semantic Textual Similarity (STS)\nbenchmarks demonstrate the effectiveness of our approach.\n","authors":["Cong-Duy Nguyen","Thong Nguyen","Xiaobao Wu","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2403.17486v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2309.11888v2","updated":"2024-03-26T08:04:36Z","published":"2023-09-21T08:45:41Z","title":"High-order Joint Constituency and Dependency Parsing","summary":"  This work revisits the topic of jointly parsing constituency and dependency\ntrees, i.e., to produce compatible constituency and dependency trees\nsimultaneously for input sentences, which is attractive considering that the\ntwo types of trees are complementary in representing syntax. The original work\nof Zhou and Zhao (2019) performs joint parsing only at the inference phase.\nThey train two separate parsers under the multi-task learning framework (i.e.,\none shared encoder and two independent decoders). They design an ad-hoc dynamic\nprogramming-based decoding algorithm of $O(n^5)$ time complexity for finding\noptimal compatible tree pairs. Compared to their work, we make progress in\nthree aspects: (1) adopting a much more efficient decoding algorithm of\n$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,\ninstead of only at the inference phase, (3) proposing high-order scoring\ncomponents to promote constituent-dependency interaction. We conduct\nexperiments and analysis on seven languages, covering both rich-resource and\nlow-resource scenarios. Results and analysis show that joint modeling leads to\na modest overall performance boost over separate modeling, but substantially\nimproves the complete matching ratio of whole trees, thanks to the explicit\nmodeling of tree compatibility.\n","authors":["Yanggan Gu","Yang Hou","Zhefeng Wang","Xinyu Duan","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2309.11888v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17445v1","updated":"2024-03-26T07:23:46Z","published":"2024-03-26T07:23:46Z","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model","summary":"  Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.\n","authors":["Jiqun Chu","Zuoquan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17445v1.pdf","comment":"12 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.16662v2","updated":"2024-03-26T07:13:15Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v2.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2403.14633v2","updated":"2024-03-26T07:12:40Z","published":"2024-02-16T23:18:19Z","title":"Born With a Silver Spoon? Investigating Socioeconomic Bias in Large\n  Language Models","summary":"  Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.\n","authors":["Smriti Singh","Shuvam Keshari","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2403.14633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17431v1","updated":"2024-03-26T06:57:23Z","published":"2024-03-26T06:57:23Z","title":"Robust and Scalable Model Editing for Large Language Models","summary":"  Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.\n","authors":["Yingfa Chen","Zhengyan Zhang","Xu Han","Chaojun Xiao","Zhiyuan Liu","Chen Chen","Kuai Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.17431v1.pdf","comment":"LREC-COLING 2024 paper, 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17428v1","updated":"2024-03-26T06:50:04Z","published":"2024-03-26T06:50:04Z","title":"Aligning Large Language Models for Enhancing Psychiatric Interviews\n  through Symptom Delineation and Summarization","summary":"  Recent advancements in Large Language Models (LLMs) have accelerated their\nusage in various domains. Given the fact that psychiatric interviews are\ngoal-oriented and structured dialogues between the professional interviewer and\nthe interviewee, it is one of the most underexplored areas where LLMs can\ncontribute substantial value. Here, we explore the use of LLMs for enhancing\npsychiatric interviews, by analyzing counseling data from North Korean\ndefectors with traumatic events and mental health issues. Specifically, we\ninvestigate whether LLMs can (1) delineate the part of the conversation that\nsuggests psychiatric symptoms and name the symptoms, and (2) summarize\nstressors and symptoms, based on the interview dialogue transcript. Here, the\ntranscript data was labeled by mental health experts for training and\nevaluation of LLMs. Our experimental results show that appropriately prompted\nLLMs can achieve high performance on both the symptom delineation task and the\nsummarization task. This research contributes to the nascent field of applying\nLLMs to psychiatric interview and demonstrates their potential effectiveness in\naiding mental health practitioners.\n","authors":["Jae-hee So","Joonhwan Chang","Eunji Kim","Junho Na","JiYeon Choi","Jy-yong Sohn","Byung-Hoon Kim","Sang Hui Chu"],"pdf_url":"https://arxiv.org/pdf/2403.17428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17413v1","updated":"2024-03-26T06:12:21Z","published":"2024-03-26T06:12:21Z","title":"LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error\n  Correction","summary":"  Over-correction is a critical problem in Chinese grammatical error correction\n(CGEC) task. Recent work using model ensemble methods based on voting can\neffectively mitigate over-correction and improve the precision of the GEC\nsystem. However, these methods still require the output of several GEC systems\nand inevitably lead to reduced error recall. In this light, we propose the\nLM-Combiner, a rewriting model that can directly modify the over-correction of\nGEC system outputs without a model ensemble. Specifically, we train the model\non an over-correction dataset constructed through the proposed K-fold cross\ninference method, which allows it to directly generate filtered sentences by\ncombining the original and the over-corrected text. In the inference stage, we\ndirectly take the original sentences and the output results of other systems as\ninput and then obtain the filtered sentences through LM-Combiner. Experiments\non the FCGEC dataset show that our proposed method effectively alleviates the\nover-correction of the original system (+18.2 Precision) while ensuring the\nerror recall remains unchanged. Besides, we find that LM-Combiner still has a\ngood rewriting performance even with small parameters and few training data,\nand thus can cost-effectively mitigate the over-correction of black-box GEC\nsystems (e.g., ChatGPT).\n","authors":["Yixuan Wang","Baoxin Wang","Yijun Liu","Dayong Wu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.17413v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2403.17411v1","updated":"2024-03-26T06:11:07Z","published":"2024-03-26T06:11:07Z","title":"PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large\n  Language Models","summary":"  Prompt compression is an innovative method for efficiently condensing input\nprompts while preserving essential information. To facilitate quick-start\nservices, user-friendly interfaces, and compatibility with common datasets and\nmetrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is\na unified plug-and-play solution for compressing prompts in Large Language\nModels (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and\nmetrics for comprehensive performance evaluation. PCToolkit boasts a modular\ndesign, allowing for easy integration of new datasets and metrics through\nportable and user-friendly interfaces. In this paper, we outline the key\ncomponents and functionalities of PCToolkit. We conducted evaluations of the\ncompressors within PCToolkit across various natural language tasks, including\nreconstruction, summarization, mathematical problem-solving, question\nanswering, few-shot learning, synthetic tasks, code completion, boolean\nexpressions, multiple choice questions, and lies recognition.\n","authors":["Jinyi Li","Yihuai Lan","Lei Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17411v1.pdf","comment":"For open-source repository, see\n  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression"},{"id":"http://arxiv.org/abs/2403.17407v1","updated":"2024-03-26T05:55:21Z","published":"2024-03-26T05:55:21Z","title":"Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens","summary":"  Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.\n","authors":["S M Jishanul Islam","Sadia Ahmmed","Sahid Hossain Mustakim"],"pdf_url":"https://arxiv.org/pdf/2403.17407v1.pdf","comment":"This work became the champion of the Bhashamul challenge"},{"id":"http://arxiv.org/abs/2403.17385v1","updated":"2024-03-26T05:11:51Z","published":"2024-03-26T05:11:51Z","title":"ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity\n  Recognition","summary":"  In this work, we revisit the problem of semi-supervised named entity\nrecognition (NER) focusing on extremely light supervision, consisting of a\nlexicon containing only 10 examples per class. We introduce ELLEN, a simple,\nfully modular, neuro-symbolic method that blends fine-tuned language models\nwith linguistic rules. These rules include insights such as ''One Sense Per\nDiscourse'', using a Masked Language Model as an unsupervised NER, leveraging\npart-of-speech tags to identify and eliminate unlabeled entities as false\nnegatives, and other intuitions about classifier confidence scores in local and\nglobal context. ELLEN achieves very strong performance on the CoNLL-2003\ndataset when using the minimal supervision from the lexicon above. It also\noutperforms most existing (and considerably more complex) semi-supervised NER\nmethods under the same supervision settings commonly used in the literature\n(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a\nzero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and\nachieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also\nachieves over 75% of the performance of a strong, fully supervised model\ntrained on gold data. Our code is available at:\nhttps://github.com/hriaz17/ELLEN.\n","authors":["Haris Riaz","Razvan-Gabriel Dumitru","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2403.17385v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2112.06166v2","updated":"2024-03-26T04:26:18Z","published":"2021-12-12T06:25:15Z","title":"Topic Detection and Tracking with Time-Aware Document Embeddings","summary":"  The time at which a message is communicated is a vital piece of metadata in\nmany real-world natural language processing tasks such as Topic Detection and\nTracking (TDT). TDT systems aim to cluster a corpus of news articles by event,\nand in that context, stories that describe the same event are likely to have\nbeen written at around the same time. Prior work on time modeling for TDT takes\nthis into account, but does not well capture how time interacts with the\nsemantic nature of the event. For example, stories about a tropical storm are\nlikely to be written within a short time interval, while stories about a movie\nrelease may appear over weeks or months. In our work, we design a neural method\nthat fuses temporal and textual information into a single representation of\nnews documents for event detection. We fine-tune these time-aware document\nembeddings with a triplet loss architecture, integrate the model into\ndownstream TDT systems, and evaluate the systems on two benchmark TDT data sets\nin English. In the retrospective setting, we apply clustering algorithms to the\ntime-aware embeddings and show substantial improvements over baselines on the\nNews2013 data set. In the online streaming setting, we add our document encoder\nto an existing state-of-the-art TDT pipeline and demonstrate that it can\nbenefit the overall performance. We conduct ablation studies on the time\nrepresentation and fusion algorithm strategies, showing that our proposed model\noutperforms alternative strategies. Finally, we probe the model to examine how\nit handles recurring events more effectively than previous TDT systems.\n","authors":["Hang Jiang","Doug Beeferman","Weiquan Mao","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2112.06166v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.06463v2","updated":"2024-03-26T04:23:12Z","published":"2023-08-12T04:05:57Z","title":"GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher","summary":"  Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.\n","authors":["Youliang Yuan","Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Pinjia He","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2308.06463v2.pdf","comment":"Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.17368v1","updated":"2024-03-26T04:07:08Z","published":"2024-03-26T04:07:08Z","title":"ChatGPT Rates Natural Language Explanation Quality Like Humans: But on\n  Which Scales?","summary":"  As AI becomes more integral in our lives, the need for transparency and\nresponsibility grows. While natural language explanations (NLEs) are vital for\nclarifying the reasoning behind AI decisions, evaluating them through human\njudgments is complex and resource-intensive due to subjectivity and the need\nfor fine-grained ratings. This study explores the alignment between ChatGPT and\nhuman assessments across multiple scales (i.e., binary, ternary, and 7-Likert\nscale). We sample 300 data instances from three NLE datasets and collect 900\nhuman annotations for both informativeness and clarity scores as the text\nquality measurement. We further conduct paired comparison experiments under\ndifferent ranges of subjectivity scores, where the baseline comes from 8,346\nhuman annotations. Our results show that ChatGPT aligns better with humans in\nmore coarse-grained scales. Also, paired comparisons and dynamic prompting\n(i.e., providing semantically similar examples in the prompt) improve the\nalignment. This research advances our understanding of large language models'\ncapabilities to assess the text explanation quality in different configurations\nfor responsible AI development.\n","authors":["Fan Huang","Haewoon Kwak","Kunwoo Park","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2403.17368v1.pdf","comment":"Accpeted by LREC-COLING 2024 main conference, long paper"},{"id":"http://arxiv.org/abs/2403.17363v1","updated":"2024-03-26T03:58:52Z","published":"2024-03-26T03:58:52Z","title":"Extracting Biomedical Entities from Noisy Audio Transcripts","summary":"  Automatic Speech Recognition (ASR) technology is fundamental in transcribing\nspoken language into text, with considerable applications in the clinical\nrealm, including streamlining medical transcription and integrating with\nElectronic Health Record (EHR) systems. Nevertheless, challenges persist,\nespecially when transcriptions contain noise, leading to significant drops in\nperformance when Natural Language Processing (NLP) models are applied. Named\nEntity Recognition (NER), an essential clinical task, is particularly affected\nby such noise, often termed the ASR-NLP gap. Prior works have primarily studied\nASR's efficiency in clean recordings, leaving a research gap concerning the\nperformance in noisy environments. This paper introduces a novel dataset,\nBioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,\nfocusing on extracting adverse drug reactions and mentions of entities from the\nBrief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a\ncomprehensive collection of almost 2,000 clean and noisy recordings. In\naddressing the noise challenge, we present an innovative transcript-cleaning\nmethod using GPT4, investigating both zero-shot and few-shot methodologies. Our\nstudy further delves into an error analysis, shedding light on the types of\nerrors in transcription software, corrections by GPT4, and the challenges GPT4\nfaces. This paper aims to foster improved understanding and potential solutions\nfor the ASR-NLP gap, ultimately supporting enhanced healthcare documentation\npractices.\n","authors":["Nima Ebadi","Kellen Morgan","Adrian Tan","Billy Linares","Sheri Osborn","Emma Majors","Jeremy Davis","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2403.17363v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17361v1","updated":"2024-03-26T03:54:25Z","published":"2024-03-26T03:54:25Z","title":"Bridging Textual and Tabular Worlds for Fact Verification: A\n  Lightweight, Attention-Based Model","summary":"  FEVEROUS is a benchmark and research initiative focused on fact extraction\nand verification tasks involving unstructured text and structured tabular data.\nIn FEVEROUS, existing works often rely on extensive preprocessing and utilize\nrule-based transformations of data, leading to potential context loss or\nmisleading encodings. This paper introduces a simple yet powerful model that\nnullifies the need for modality conversion, thereby preserving the original\nevidence's context. By leveraging pre-trained models on diverse text and\ntabular datasets and by incorporating a lightweight attention-based mechanism,\nour approach efficiently exploits latent connections between different data\ntypes, thereby yielding comprehensive and reliable verdict predictions. The\nmodel's modular structure adeptly manages multi-modal information, ensuring the\nintegrity and authenticity of the original evidence are uncompromised.\nComparative analyses reveal that our approach exhibits competitive performance,\naligning itself closely with top-tier models on the FEVEROUS benchmark.\n","authors":["Shirin Dabbaghi Varnosfaderani","Canasai Kruengkrai","Ramin Yahyapour","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2403.17361v1.pdf","comment":"Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation"},{"id":"http://arxiv.org/abs/2403.17359v1","updated":"2024-03-26T03:51:01Z","published":"2024-03-26T03:51:01Z","title":"Chain-of-Action: Faithful and Multimodal Question Answering through\n  Large Language Models","summary":"  We present a Chain-of-Action (CoA) framework for multimodal and\nretrieval-augmented Question-Answering (QA). Compared to the literature, CoA\novercomes two major challenges of current QA applications: (i) unfaithful\nhallucination that is inconsistent with real-time or domain facts and (ii) weak\nreasoning performance over compositional information. Our key contribution is a\nnovel reasoning-retrieval mechanism that decomposes a complex question into a\nreasoning chain via systematic prompting and pre-designed actions.\nMethodologically, we propose three types of domain-adaptable `Plug-and-Play'\nactions for retrieving real-time information from heterogeneous sources. We\nalso propose a multi-reference faith score (MRFS) to verify and resolve\nconflicts in the answers. Empirically, we exploit both public benchmarks and a\nWeb3 case study to demonstrate the capability of CoA over other methods.\n","authors":["Zhenyu Pan","Haozheng Luo","Manling Li","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18152v1","updated":"2024-03-26T23:32:52Z","published":"2024-03-26T23:32:52Z","title":"Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency","summary":"  Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.\n","authors":["Toyin Aguda","Suchetha Siddagangappa","Elena Kochkina","Simerjot Kaur","Dongsheng Wang","Charese Smiley","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.18152v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2310.19055v2","updated":"2024-03-26T22:59:36Z","published":"2023-10-29T16:02:46Z","title":"A Few-Shot Learning Focused Survey on Recent Named Entity Recognition\n  and Relation Classification Methods","summary":"  Named Entity Recognition (NER) and Relation Classification (RC) are important\nsteps in extracting information from unstructured text and formatting it into a\nmachine-readable format. We present a survey of recent deep learning models\nthat address named entity recognition and relation classification, with focus\non few-shot learning performance. Our survey is helpful for researchers in\nknowing the recent techniques in text mining and extracting structured\ninformation from raw text.\n","authors":["Sakher Khalil Alqaaidi","Elika Bozorgi","Afsaneh Shams","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2310.19055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05882v2","updated":"2024-03-26T22:54:48Z","published":"2023-06-09T13:24:27Z","title":"Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems","summary":"  Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n","authors":["Silvia Alma Piazzolla","Beatrice Savoldi","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2306.05882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05677v2","updated":"2024-03-26T22:53:56Z","published":"2023-12-09T20:51:48Z","title":"Batched Low-Rank Adaptation of Foundation Models","summary":"  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.\n","authors":["Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2312.05677v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18125v1","updated":"2024-03-26T22:08:33Z","published":"2024-03-26T22:08:33Z","title":"For those who don't know (how) to ask: Building a dataset of technology\n  questions for digital newcomers","summary":"  While the rise of large language models (LLMs) has created rich new\nopportunities to learn about digital technology, many on the margins of this\ntechnology struggle to gain and maintain competency due to lexical or\nconceptual barriers that prevent them from asking appropriate questions.\nAlthough there have been many efforts to understand factuality of LLM-created\ncontent and ability of LLMs to answer questions, it is not well understood how\nunclear or nonstandard language queries affect the model outputs. We propose\nthe creation of a dataset that captures questions of digital newcomers and\noutsiders, utilizing data we have compiled from a decade's worth of one-on-one\ntutoring. In this paper we lay out our planned efforts and some potential uses\nof this dataset.\n","authors":["Evan Lucas","Kelly S. Steelman","Leo C. Ureel","Charles Wallace"],"pdf_url":"https://arxiv.org/pdf/2403.18125v1.pdf","comment":"Presented at the AI4ED workshop at AAAI 2024"},{"id":"http://arxiv.org/abs/2403.18120v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"  Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.\n","authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18120v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18121v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"ChatGPT Role-play Dataset: Analysis of User Motives and Model\n  Naturalness","summary":"  Recent advances in interactive large language models like ChatGPT have\nrevolutionized various domains; however, their behavior in natural and\nrole-play conversation settings remains underexplored. In our study, we address\nthis gap by deeply investigating how ChatGPT behaves during conversations in\ndifferent settings by analyzing its interactions in both a normal way and a\nrole-play setting. We introduce a novel dataset of broad range of human-AI\nconversations annotated with user motives and model naturalness to examine (i)\nhow humans engage with the conversational AI model, and (ii) how natural are AI\nmodel responses. Our study highlights the diversity of user motives when\ninteracting with ChatGPT and variable AI naturalness, showing not only the\nnuanced dynamics of natural conversations between humans and AI, but also\nproviding new avenues for improving the effectiveness of human-AI\ncommunication.\n","authors":["Yufei Tao","Ameeta Agrawal","Judit Dombi","Tetyana Sydorenko","Jung In Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18121v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.03997v3","updated":"2024-03-26T21:32:51Z","published":"2023-06-06T20:19:33Z","title":"Sentiment Analysis in Finance: From Transformers Back to eXplainable\n  Lexicons (XLex)","summary":"  Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.\n","authors":["Maryan Rizinski","Hristijan Peshov","Kostadin Mishev","Milos Jovanovik","Dimitar Trajanov"],"pdf_url":"https://arxiv.org/pdf/2306.03997v3.pdf","comment":"Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:\n  https://ieeexplore.ieee.org/document/10380556"},{"id":"http://arxiv.org/abs/2403.18105v1","updated":"2024-03-26T21:04:29Z","published":"2024-03-26T21:04:29Z","title":"Large Language Models for Education: A Survey and Outlook","summary":"  The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.\n","authors":["Shen Wang","Tianlong Xu","Hang Li","Chaoli Zhang","Joleen Liang","Jiliang Tang","Philip S. Yu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.18105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18098v1","updated":"2024-03-26T20:47:32Z","published":"2024-03-26T20:47:32Z","title":"GPTs and Language Barrier: A Cross-Lingual Legal QA Examination","summary":"  In this paper, we explore the application of Generative Pre-trained\nTransformers (GPTs) in cross-lingual legal Question-Answering (QA) systems\nusing the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a\nset of related legal articles that serve as context, the objective is to\ndetermine whether the statement is legally valid, i.e., if it can be inferred\nfrom the provided contextual articles or not, which is also known as an\nentailment task. By benchmarking four different combinations of English and\nJapanese prompts and data, we provide valuable insights into GPTs' performance\nin multilingual legal QA scenarios, contributing to the development of more\nefficient and accurate cross-lingual QA solutions in the legal domain.\n","authors":["Ha-Thanh Nguyen","Hiroaki Yamada","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18098v1.pdf","comment":"NLP 2024, Kobe, Japan"},{"id":"http://arxiv.org/abs/2403.18093v1","updated":"2024-03-26T20:25:53Z","published":"2024-03-26T20:25:53Z","title":"Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large\n  Language Models","summary":"  Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.\n","authors":["Hai-Long Nguyen","Duc-Minh Nguyen","Tan-Minh Nguyen","Ha-Thanh Nguyen","Thi-Hai-Yen Vuong","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2403.18093v1.pdf","comment":"JURISIN 2024"},{"id":"http://arxiv.org/abs/2402.09654v2","updated":"2024-03-26T20:12:18Z","published":"2024-02-15T01:38:50Z","title":"GPT-4's assessment of its performance in a USMLE-based case study","summary":"  This study investigates GPT-4's assessment of its performance in healthcare\napplications. A simple prompting technique was used to prompt the LLM with\nquestions taken from the United States Medical Licensing Examination (USMLE)\nquestionnaire and it was tasked to evaluate its confidence score before posing\nthe question and after asking the question. The questionnaire was categorized\ninto two groups-questions with feedback (WF) and questions with no feedback(NF)\npost-question. The model was asked to provide absolute and relative confidence\nscores before and after each question. The experimental findings were analyzed\nusing statistical tools to study the variability of confidence in WF and NF\ngroups. Additionally, a sequential analysis was conducted to observe the\nperformance variation for the WF and NF groups. Results indicate that feedback\ninfluences relative confidence but doesn't consistently increase or decrease\nit. Understanding the performance of LLM is paramount in exploring its utility\nin sensitive areas like healthcare. This study contributes to the ongoing\ndiscourse on the reliability of AI, particularly of LLMs like GPT-4, within\nhealthcare, offering insights into how feedback mechanisms might be optimized\nto enhance AI-assisted medical education and decision support.\n","authors":["Uttam Dhakal","Aniket Kumar Singh","Suman Devkota","Yogesh Sapkota","Bishal Lamichhane","Suprinsa Paudyal","Chandra Dhakal"],"pdf_url":"https://arxiv.org/pdf/2402.09654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12492v2","updated":"2024-03-26T19:28:15Z","published":"2024-01-23T05:20:35Z","title":"Comparing Pre-trained Human Language Models: Is it Better with Human\n  Context as Groups, Individual Traits, or Both?","summary":"  Incorporating human context into language models is the next frontier for\nhuman-centered natural language processing. Currently, two pre-training methods\nexist: group-wise attributes (e.g., over-45-year-olds) or individual traits.\nGroup attributes are coarse -- not all 45-year-olds write the same way -- while\nmodeling individual traits allows for a more personalized representation, but\nrequires more complex modeling and data. So far, it is unclear which\npre-training approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on 5 user- and document-level tasks. We find that pre-training with\nboth group and individual features significantly improves the two user-level\nregression tasks like age estimation and personality assessment. Pre-training\non individual users significantly improves the three document-level\nclassification tasks like stance and topic detection. It even does well for\ndownstream tasks without historical user data. Our results suggest both\napproaches have specific use cases, opening new avenues for human-centered\nlanguage modeling.\n","authors":["Nikita Soni","Niranjan Balasubramanian","H. Andrew Schwartz","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2401.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18058v1","updated":"2024-03-26T19:24:18Z","published":"2024-03-26T19:24:18Z","title":"COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning","summary":"  Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA\n","authors":["Yuelin Bai","Xinrun Du","Yiming Liang","Yonggang Jin","Ziqiang Liu","Junting Zhou","Tianyu Zheng","Xincheng Zhang","Nuo Ma","Zekun Wang","Ruibin Yuan","Haihong Wu","Hongquan Lin","Wenhao Huang","Jiajun Zhang","Wenhu Chen","Chenghua Lin","Jie Fu","Min Yang","Shiwen Ni","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18051v1","updated":"2024-03-26T19:08:20Z","published":"2024-03-26T19:08:20Z","title":"Supervisory Prompt Training","summary":"  The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.\n","authors":["Jean Ghislain Billa","Min Oh","Liang Du"],"pdf_url":"https://arxiv.org/pdf/2403.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18031v1","updated":"2024-03-26T18:38:14Z","published":"2024-03-26T18:38:14Z","title":"The Impact of Syntactic and Semantic Proximity on Machine Translation\n  with Back-Translation","summary":"  Unsupervised on-the-fly back-translation, in conjunction with multilingual\npretraining, is the dominant method for unsupervised neural machine\ntranslation. Theoretically, however, the method should not work in general. We\ntherefore conduct controlled experiments with artificial languages to determine\nwhat properties of languages make back-translation an effective training\nmethod, covering lexical, syntactic, and semantic properties. We find, contrary\nto popular belief, that (i) parallel word frequency distributions, (ii)\npartially shared vocabulary, and (iii) similar syntactic structure across\nlanguages are not sufficient to explain the success of back-translation. We\nshow however that even crude semantic signal (similar lexical fields across\nlanguages) does improve alignment of two languages through back-translation. We\nconjecture that rich semantic dependencies, parallel across languages, are at\nthe root of the success of unsupervised methods based on back-translation.\nOverall, the success of unsupervised machine translation was far from being\nanalytically guaranteed. Instead, it is another proof that languages of the\nworld share deep similarities, and we hope to show how to identify which of\nthese similarities can serve the development of unsupervised, cross-linguistic\ntools.\n","authors":["Nicolas Guerin","Shane Steinert-Threlkeld","Emmanuel Chemla"],"pdf_url":"https://arxiv.org/pdf/2403.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10438v6","updated":"2024-03-26T18:36:31Z","published":"2022-11-18T18:59:33Z","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large\n  Language Models","summary":"  Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, existing methods cannot maintain accuracy and hardware efficiency at\nthe same time. We propose SmoothQuant, a training-free, accuracy-preserving,\nand general-purpose post-training quantization (PTQ) solution to enable 8-bit\nweight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are not, SmoothQuant smooths the\nactivation outliers by offline migrating the quantization difficulty from\nactivations to weights with a mathematically equivalent transformation.\nSmoothQuant enables an INT8 quantization of both weights and activations for\nall the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,\nLlama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x\nspeedup and 2x memory reduction for LLMs with negligible loss in accuracy.\nSmoothQuant enables serving 530B LLM within a single node. Our work offers a\nturn-key solution that reduces hardware costs and democratizes LLMs. Code is\navailable at https://github.com/mit-han-lab/smoothquant.\n","authors":["Guangxuan Xiao","Ji Lin","Mickael Seznec","Hao Wu","Julien Demouth","Song Han"],"pdf_url":"https://arxiv.org/pdf/2211.10438v6.pdf","comment":"ICML 2023. First two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.18024v1","updated":"2024-03-26T18:22:05Z","published":"2024-03-26T18:22:05Z","title":"Enriching Word Usage Graphs with Cluster Definitions","summary":"  We present a dataset of word usage graphs (WUGs), where the existing WUGs for\nmultiple languages are enriched with cluster labels functioning as sense\ndefinitions. They are generated from scratch by fine-tuned encoder-decoder\nlanguage models. The conducted human evaluation has shown that these\ndefinitions match the existing clusters in WUGs better than the definitions\nchosen from WordNet by two baseline systems. At the same time, the method is\nstraightforward to use and easy to extend to new languages. The resulting\nenriched datasets can be extremely helpful for moving on to explainable\nsemantic change modeling.\n","authors":["Mariia Fedorova","Andrey Kutuzov","Nikolay Arefyev","Dominik Schlechtweg"],"pdf_url":"https://arxiv.org/pdf/2403.18024v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14814v2","updated":"2024-03-26T18:10:10Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v2.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.18018v1","updated":"2024-03-26T18:07:10Z","published":"2024-03-26T18:07:10Z","title":"DORE: A Dataset For Portuguese Definition Generation","summary":"  Definition modelling (DM) is the task of automatically generating a\ndictionary definition for a specific word. Computational systems that are\ncapable of DM can have numerous applications benefiting a wide range of\naudiences. As DM is considered a supervised natural language generation\nproblem, these systems require large annotated datasets to train the machine\nlearning (ML) models. Several DM datasets have been released for English and\nother high-resource languages. While Portuguese is considered a\nmid/high-resource language in most natural language processing tasks and is\nspoken by more than 200 million native speakers, there is no DM dataset\navailable for Portuguese. In this research, we fill this gap by introducing\nDORE; the first dataset for Definition MOdelling for PoRtuguEse containing more\nthan 100,000 definitions. We also evaluate several deep learning based DM\nmodels on DORE and report the results. The dataset and the findings of this\npaper will facilitate research and study of Portuguese in wider contexts.\n","authors":["Anna Beatriz Dimas Furtado","Tharindu Ranasinghe","Frédéric Blain","Ruslan Mitkov"],"pdf_url":"https://arxiv.org/pdf/2403.18018v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2305.14718v4","updated":"2024-03-26T18:07:01Z","published":"2023-05-24T04:42:17Z","title":"Leftover-Lunch: Advantage-based Offline Reinforcement Learning for\n  Language Models","summary":"  Reinforcement Learning with Human Feedback (RLHF) is the most prominent\nmethod for Language Model (LM) alignment. However, RLHF is an unstable and\ndata-hungry process that continually requires new high-quality LM-generated\ndata for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new\nclass of offline policy gradient algorithms that enable RL training on any\npre-existing data. By assuming the entire LM output sequence as a single\naction, A-LoL allows incorporating sequence-level classifiers or human-designed\nscoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL\nonly trains on positive advantage (leftover) data points, making it resilient\nto noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable\nLM training recipe.\n  We demonstrate the effectiveness of A-LoL and its variants with a set of four\ndifferent language generation tasks. We compare against both online RL (PPO)\nand recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\nbaselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\nbeing rated more safe and helpful than the baselines according to humans.\nAdditionally, in the remaining three tasks, A-LoL could optimize multiple\ndistinct reward functions even when using noisy or suboptimal training data.\n  We also release our experimental code. https://github.com/abaheti95/LoL-RL\n","authors":["Ashutosh Baheti","Ximing Lu","Faeze Brahman","Ronan Le Bras","Maarten Sap","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2305.14718v4.pdf","comment":"published at ICLR 2024"}]},"2024-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18804v1","updated":"2024-03-27T17:50:00Z","published":"2024-03-27T17:50:00Z","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation","summary":"  The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2403.18804v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18803v1","updated":"2024-03-27T17:49:31Z","published":"2024-03-27T17:49:31Z","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models","summary":"  Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Daniel Gillis","Judi McCuaig"],"pdf_url":"https://arxiv.org/pdf/2403.18803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2403.18783v1","updated":"2024-03-27T17:31:39Z","published":"2024-03-27T17:31:39Z","title":"Towards a World-English Language Model for On-Device Virtual Assistants","summary":"  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.\n","authors":["Rricha Jalota","Lyan Verwimp","Markus Nussbaum-Thom","Amr Mousa","Arturo Argueta","Youssef Oualil"],"pdf_url":"https://arxiv.org/pdf/2403.18783v1.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v1","updated":"2024-03-27T17:20:39Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: Robust Evaluation Framework using Large Language Model via\n  Checklist","summary":"  We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2403.18771v1.pdf","comment":"HEAL at CHI 2024"},{"id":"http://arxiv.org/abs/2403.18769v1","updated":"2024-03-27T17:13:38Z","published":"2024-03-27T17:13:38Z","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","summary":"  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n","authors":["Liang Lu","Jingzhi Wang","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2403.18769v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18746v1","updated":"2024-03-27T16:45:02Z","published":"2024-03-27T16:45:02Z","title":"CYCLE: Learning to Self-Refine the Code Generation","summary":"  Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.\n","authors":["Yangruibo Ding","Marcus J. Min","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2403.18746v1.pdf","comment":"Camera-ready for OOPSLA'24"},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18697v1","updated":"2024-03-27T15:46:25Z","published":"2024-03-27T15:46:25Z","title":"The Invalsi Benchmark: measuring Language Models Mathematical and\n  Language understanding in Italian","summary":"  While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.\n","authors":["Andrea Esuli","Giovanni Puccetti"],"pdf_url":"https://arxiv.org/pdf/2403.18697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.11128v2","updated":"2024-03-27T15:22:53Z","published":"2024-03-17T07:34:12Z","title":"Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities","summary":"  With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.\n","authors":["Honglin Mu","Yang Xu","Yunlong Feng","Xiaofeng Han","Yitong Li","Yutai Hou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.11128v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13320v2","updated":"2024-03-27T14:57:29Z","published":"2023-09-23T09:35:55Z","title":"GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification","summary":"  We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2309.13320v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18647v1","updated":"2024-03-27T14:54:27Z","published":"2024-03-27T14:54:27Z","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens","summary":"  We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.\n","authors":["Chengbo Liu","Yong Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18647v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.04507v2","updated":"2024-03-27T14:50:56Z","published":"2024-03-07T14:07:00Z","title":"NLPre: a revised approach towards language-centric benchmarking of\n  Natural Language Preprocessing systems","summary":"  With the advancements of transformer-based architectures, we observe the rise\nof natural language preprocessing (NLPre) tools capable of solving preliminary\nNLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or\nmorphological analysis) without any external linguistic guidance. It is arduous\nto compare novel solutions to well-entrenched preprocessing toolkits, relying\non rule-based morphological analysers or dictionaries. Aware of the\nshortcomings of existing NLPre evaluation approaches, we investigate a novel\nmethod of reliable and fair evaluation and performance reporting. Inspired by\nthe GLUE benchmark, the proposed language-centric benchmarking system enables\ncomprehensive ongoing evaluation of multiple NLPre tools, while credibly\ntracking their performance. The prototype application is configured for Polish\nand integrated with the thoroughly assembled NLPre-PL benchmark. Based on this\nbenchmark, we conduct an extensive evaluation of a variety of Polish NLPre\nsystems. To facilitate the construction of benchmarking environments for other\nlanguages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full\ncustomization of the publicly released source code of the benchmarking system.\nThe links to all the resources (deployed platforms, source code, trained\nmodels, datasets etc.) can be found on the project website:\nhttps://sites.google.com/view/nlpre-benchmark.\n","authors":["Martyna Wiącek","Piotr Rybak","Łukasz Pszenny","Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2403.04507v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18624v1","updated":"2024-03-27T14:34:29Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18609v1","updated":"2024-03-27T14:26:41Z","published":"2024-03-27T14:26:41Z","title":"A survey on learning models of spiking neural membrane systems and\n  spiking neural networks","summary":"  Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.\n","authors":["Prithwineel Paul","Petr Sosik","Lucie Ciencialova"],"pdf_url":"https://arxiv.org/pdf/2403.18609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2308.12531v2","updated":"2024-03-27T13:46:37Z","published":"2023-08-24T03:40:54Z","title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction","summary":"  Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.\n","authors":["Wenjun Kong","Yamei Xia"],"pdf_url":"https://arxiv.org/pdf/2308.12531v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18555v1","updated":"2024-03-27T13:34:59Z","published":"2024-03-27T13:34:59Z","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","summary":"  Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.\n","authors":["Philip Kenneweg","Sarah Schröder","Alexander Schulz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v3","updated":"2024-03-27T13:29:31Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v3.pdf","comment":"Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18504v1","updated":"2024-03-27T12:33:42Z","published":"2024-03-27T12:33:42Z","title":"AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA","summary":"  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n","authors":["Felix Virgo","Fei Cheng","Lis Kanashiro Pereira","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2403.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03544v2","updated":"2024-03-27T10:53:42Z","published":"2023-04-07T08:49:43Z","title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual\n  Topic Modeling","summary":"  Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n","authors":["Xiaobao Wu","Xinshuai Dong","Thong Nguyen","Chaoqun Liu","Liangming Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2304.03544v2.pdf","comment":"Accepted to AAAI2023 conference. Code is available at\n  https://github.com/BobXWu/InfoCTM"},{"id":"http://arxiv.org/abs/2309.13322v2","updated":"2024-03-27T10:50:24Z","published":"2023-09-23T09:51:37Z","title":"From Text to Source: Results in Detecting Large Language Model-Generated\n  Content","summary":"  The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2309.13322v2.pdf","comment":"Accepted to COLING-LREC 2024"},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18430v1","updated":"2024-03-27T10:36:17Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.18426v1","updated":"2024-03-27T10:27:28Z","published":"2024-03-27T10:27:28Z","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","summary":"  Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.\n","authors":["Jamshid Mozafari","Anubhav Jangra","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.18426v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17647v2","updated":"2024-03-27T10:07:59Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2403.18365v1","updated":"2024-03-27T08:57:21Z","published":"2024-03-27T08:57:21Z","title":"BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models","summary":"  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.\n","authors":["Haitao Li","Qingyao Ai","Jia Chen","Qian Dong","Zhijing Wu","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18365v1.pdf","comment":"11pages"},{"id":"http://arxiv.org/abs/2307.16071v2","updated":"2024-03-27T08:56:01Z","published":"2023-07-29T20:42:50Z","title":"ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus","summary":"  We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.\n","authors":["Tolulope Ogunremi","Kola Tubosun","Anuoluwapo Aremu","Iroro Orife","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2307.16071v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18350v1","updated":"2024-03-27T08:42:31Z","published":"2024-03-27T08:42:31Z","title":"Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language","summary":"  The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).\n","authors":["Ali Mahboub","Muhy Eddin Za'ter","Bashar Alfrou","Yazan Estaitia","Adnan Jaljuli","Asma Hakouz"],"pdf_url":"https://arxiv.org/pdf/2403.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v1","updated":"2024-03-27T08:39:56Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Da Ma","Situo Zhang","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18341v1","updated":"2024-03-27T08:32:19Z","published":"2024-03-27T08:32:19Z","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","summary":"  With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.\n","authors":["Xiusi Chen","Hongzhi Wen","Sreyashi Nag","Chen Luo","Qingyu Yin","Ruirui Li","Zheng Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18341v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11399v2","updated":"2024-03-27T07:05:22Z","published":"2024-03-18T01:14:47Z","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","summary":"  The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.\n","authors":["Dongjae Shin","Hyunseok Lim","Inho Won","Changsu Choi","Minjun Kim","Seungwoo Song","Hangyeol Yoo","Sangmin Kim","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.11399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12468v3","updated":"2024-03-27T06:46:56Z","published":"2023-02-24T05:48:53Z","title":"Adapting Knowledge for Few-shot Table-to-Text Generation","summary":"  Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.\n","authors":["Zhixin Guo","Minyxuan Yan","Jiexing Qi","Jianping Zhou","Ziwei He","Guanjie Zheng","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2302.12468v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04415"},{"id":"http://arxiv.org/abs/2403.18295v1","updated":"2024-03-27T06:43:58Z","published":"2024-03-27T06:43:58Z","title":"Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning","summary":"  Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.\n","authors":["Yongwei Zhou","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06201v3","updated":"2024-03-27T06:31:42Z","published":"2024-01-11T15:45:11Z","title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","summary":"  To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.\n","authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2401.06201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18277v1","updated":"2024-03-27T06:13:04Z","published":"2024-03-27T06:13:04Z","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","summary":"  Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.\n","authors":["Yejin Yoon","Jungyeon Lee","Kangsan Kim","Chanhee Park","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18277v1.pdf","comment":"Accepted to LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17636v2","updated":"2024-03-27T05:55:35Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.08590v2","updated":"2024-03-27T05:53:58Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18253v1","updated":"2024-03-27T04:51:42Z","published":"2024-03-27T04:51:42Z","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","summary":"  Metaphors are ubiquitous in daily life, yet detecting them poses a\nsignificant challenge. Previous approaches often struggled with improper\napplication of language rules and overlooked the issue of data sparsity. To\naddress these challenges, we introduce knowledge distillation and prompt\nlearning into metaphor detection. Specifically, we devise a prompt learning\ntemplate tailored for the metaphor detection task. By masking target words and\nproviding relevant prompt information, we guide the model to accurately infer\nthe contextual meaning of these words. This approach not only mitigates the\ninterference from the literal meaning of target words but also ensures the\nproper utilization of MIP language rules for metaphor detection. Moreover, we\nemploy a teacher model equipped with prior knowledge to generate meaningful\nsoft labels, guiding the optimization process of the student model. The\ninclusion of soft labels, akin to label smoothing, helps alleviate the model's\ntendency towards over-confidence and effectively addresses the challenge of\ndata sparsity. Experimental results demonstrate that our proposed model\nachieves state-of-the-art performance across multiple datasets.\n","authors":["Kaidi Jia","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18251v1","updated":"2024-03-27T04:47:10Z","published":"2024-03-27T04:47:10Z","title":"Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too","summary":"  English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.\n","authors":["Abteen Ebrahimi","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2403.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18249v1","updated":"2024-03-27T04:39:18Z","published":"2024-03-27T04:39:18Z","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges","summary":"  Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.\n","authors":["Yanshen Sun","Jianfeng He","Limeng Cui","Shuo Lei","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14965v4","updated":"2024-03-27T04:38:44Z","published":"2023-05-24T09:57:37Z","title":"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks","summary":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.\n","authors":["Abhinav Rao","Sachin Vashistha","Atharva Naik","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2305.14965v4.pdf","comment":"Accepted at LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18182v1","updated":"2024-03-27T01:19:23Z","published":"2024-03-27T01:19:23Z","title":"ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus","summary":"  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.\n","authors":["Injy Hamed","Fadhl Eryani","David Palfreyman","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2403.18182v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2308.11138v3","updated":"2024-03-27T00:29:33Z","published":"2023-08-22T02:39:42Z","title":"NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints","summary":"  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n","authors":["Peiheng Gao","Ning Sun","Xuefeng Wang","Chen Yang","Ričardas Zitikis"],"pdf_url":"https://arxiv.org/pdf/2308.11138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null}]}}