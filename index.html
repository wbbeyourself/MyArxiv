<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Modularity Transferable? A Case Study through the Lens of Knowledge
  Distillation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Modular Deep Learning showcases its potential in various Natural
Language Processing applications. Parameter-efficient fine-tuning (PEFT)
modularity has been shown to work for various use cases, from domain adaptation
to multilingual setups. However, all this work covers the case where the
modular components are trained and deployed within one single Pre-trained
Language Model (PLM). This model-specific setup is a substantial limitation on
the very modularity that modular architectures are trying to achieve. We ask
whether current modular approaches are transferable between models and whether
we can transfer the modules from more robust and larger PLMs to smaller ones.
In this work, we aim to fill this gap via a lens of Knowledge Distillation,
commonly used for model compression, and present an extremely straightforward
approach to transferring pre-trained, task-specific PEFT modules between
same-family PLMs. Moreover, we propose a method that allows the transfer of
modules between incompatible PLMs without any change in the inference
complexity. The experiments on Named Entity Recognition, Natural Language
Inference, and Paraphrase Identification tasks over multiple languages and PEFT
methods showcase the initial potential of transferable modularity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projective Methods for Mitigating Gender Bias in <span class="highlight-title">Pre-train</span>ed Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of gender bias in NLP has a long history tied to debiasing static
word embeddings. More recently, attention has shifted to debiasing pre-trained
language models. We study to what extent the simplest projective debiasing
methods, developed for word embeddings, can help when applied to BERT's
internal representations. Projective methods are fast to implement, use a small
number of saved parameters, and make no updates to the existing model
parameters. We evaluate the efficacy of the methods in reducing both intrinsic
bias, as measured by BERT's next sentence prediction task, and in mitigating
observed bias in a downstream setting when fine-tuned. To this end, we also
provide a critical analysis of a popular gender-bias assessment test for
quantifying intrinsic bias, resulting in an enhanced test set and new bias
measures. We find that projective methods can be effective at both intrinsic
bias and downstream bias mitigation, but that the two outcomes are not
necessarily correlated. This finding serves as a warning that intrinsic bias
test sets, based either on language modeling tasks or next sentence prediction,
should not be the only benchmark in developing a debiased language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large <span class="highlight-title">language models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a World-English Language Model for On-Device Virtual Assistants <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo Argueta, Youssef Oualil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are
generally language-, region-, and in some cases, device-dependent, which
increases the effort to scale and maintain them. Combining NNLMs for one or
more of the categories is one way to improve scalability. In this work, we
combine regional variants of English to build a ``World English'' NNLM for
on-device VAs. In particular, we investigate the application of adapter
bottlenecks to model dialect-specific characteristics in our existing
production NNLMs {and enhance the multi-dialect baselines}. We find that
adapter modules are more effective in modeling dialects than specializing
entire sub-networks. Based on this insight and leveraging the design of our
production models, we introduce a new architecture for World English NNLM that
meets the accuracy, latency, and memory constraints of our single-dialect
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckEval: Robust Evaluation Framework using Large Language Model via
  Checklist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheckEval, a novel evaluation framework using Large Language
Models, addressing the challenges of ambiguity and inconsistency in current
evaluation methods. CheckEval addresses these challenges by dividing evaluation
criteria into detailed sub-aspects and constructing a checklist of Boolean
questions for each, simplifying the evaluation. This approach not only renders
the process more interpretable but also significantly enhances the robustness
and reliability of results by focusing on specific evaluation dimensions.
Validated through a focused case study using the SummEval benchmark, CheckEval
indicates a strong correlation with human judgments. Furthermore, it
demonstrates a highly consistent Inter-Annotator Agreement. These findings
highlight the effectiveness of CheckEval for objective, flexible, and precise
evaluations. By offering a customizable and interactive framework, CheckEval
sets a new standard for the use of LLMs in evaluation, responding to the
evolving needs of the field and establishing a clear method for future
LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HEAL at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Neural Protoform Reconstruction via Reflex Prediction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Lu, Jingzhi Wang, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protolanguage reconstruction is central to historical linguistics. The
comparative method, one of the most influential theoretical and methodological
frameworks in the history of the language sciences, allows linguists to infer
protoforms (reconstructed ancestral words) from their reflexes (related modern
words) based on the assumption of regular sound change. Not surprisingly,
numerous computational linguists have attempted to operationalize comparative
reconstruction through various computational models, the most successful of
which have been supervised encoder-decoder models, which treat the problem of
predicting protoforms given sets of reflexes as a sequence-to-sequence problem.
We argue that this framework ignores one of the most important aspects of the
comparative method: not only should protoforms be inferable from cognate sets
(sets of related reflexes) but the reflexes should also be inferable from the
protoforms. Leveraging another line of research -- reflex prediction -- we
propose a system in which candidate protoforms from a reconstruction model are
reranked by a reflex prediction model. We show that this more complete
implementation of the comparative method allows us to surpass state-of-the-art
protoform reconstruction methods on three of four Chinese and Romance datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYCLE: Learning to Self-Refine the Code <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained code language models have achieved promising performance in code
generation and improved the programming efficiency of human developers.
However, their self-refinement capability is typically overlooked by the
existing evaluations of code LMs, which focus only on the accuracy of the
one-time prediction. For the cases when code LMs fail to implement the correct
program, developers actually find it hard to debug and fix the faulty
prediction since it is not written by the developers themselves. Unfortunately,
our study reveals that code LMs cannot efficiently self-refine their faulty
generations as well.
  In this paper, we propose CYCLE framework, learning to self-refine the faulty
generation according to the available feedback, such as the execution results
reported by the test suites. We evaluate CYCLE on three popular code generation
benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE
successfully maintains, sometimes improves, the quality of one-time code
generation, while significantly improving the self-refinement capability of
code LMs. We implement four variants of CYCLE with varied numbers of parameters
across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently
boosts the code generation performance, by up to 63.5%, across benchmarks and
varied model sizes. We also notice that CYCLE outperforms code LMs that have
3$\times$ more parameters in self-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for OOPSLA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-<span class="highlight-title">Language Models</span> with
  <span class="highlight-title">Instruct</span>ion Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Invalsi Benchmark: measuring <span class="highlight-title">Language Models</span> Mathematical and
  Language understanding in Italian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Esuli, Giovanni Puccetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Italian is by all metrics a high resource language, currently, there
are isn't a Language Model pre-trained exclusively in this language. This
results in a lower number of available benchmarks to evaluate the performance
of language models in Italian.
  This work presents two new benchmarks to evaluate the models performance on
mathematical understanding and language understanding in Italian. These
benchmarks are based on real tests that are undertaken by students of age
between 11 and 18 within the Italian school system and have therefore been
validated by several experts in didactics and pedagogy.
  To validate this dataset we evaluate the performance of 9 language models
that are the best performing when writing in Italian, including our own
fine-tuned models. We show that this is a challenging benchmark where current
language models are bound by 60\% accuracy.
  We believe that the release of this dataset paves the way for improving
future models mathematical and language understanding in Italian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDSAT: Accelerating LLM Inference through Speculative Decoding with
  Semantic Adaptive Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbo Liu, Yong Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an acceleration scheme for large language models (LLMs) through
Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary
objective of this design is to enhance the LLM model's ability to generate
draft tokens more accurately without compromising the model's accuracy. The
core strategies involve: 1) Fine-tune the model by incorporating semantic
adaptive tokens that possess flexible decoding capabilities without changing
its structure, allowing them to generate high-quality draft tokens. 2) By
employing a training method that does not affect the standard tokens, the model
can acquire parallel decoding abilities atop its original framework with
minimal training overhead. 3) We have designed the "two-step-draft-then-verify"
generation strategies using both greedy search and nucleus sampling.
Experiments conducted on the CodeLlama-13B and 7B models have yielded speed
increases of over 3.5X and 3.0X, respectively. Please refer to
https://github.com/hasuoshenyun/SDSAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Vulnerability Detection with Code <span class="highlight-title">Language Models</span>: How Far Are We? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, <span class="highlight-author">Xinyun Chen</span>, Basel Alomair, David Wagner, Baishakhi Ray, Yizheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the rising interest in code language models (code LMs) and
vulnerability detection, we study the effectiveness of code LMs for detecting
vulnerabilities. Our analysis reveals significant shortcomings in existing
vulnerability datasets, including poor data quality, low label accuracy, and
high duplication rates, leading to unreliable model performance in realistic
vulnerability detection scenarios. Additionally, the evaluation methods used
with these datasets are not representative of real-world vulnerability
detection.
  To address these challenges, we introduce PrimeVul, a new dataset for
training and evaluating code LMs for vulnerability detection. PrimeVul
incorporates a novel set of data labeling techniques that achieve comparable
label accuracy to human-verified benchmarks while significantly expanding the
dataset. It also implements a rigorous data de-duplication and chronological
data splitting strategy to mitigate data leakage issues, alongside introducing
more realistic evaluation metrics and settings. This comprehensive approach
aims to provide a more accurate assessment of code LMs' performance in
real-world conditions.
  Evaluating code LMs on PrimeVul reveals that existing benchmarks
significantly overestimate the performance of these models. For instance, a
state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on
PrimeVul. Attempts to improve performance through advanced training techniques
and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin
to random guessing in the most stringent settings. These findings underscore
the considerable gap between current capabilities and the practical
requirements for deploying code LMs in security roles, highlighting the need
for more innovative research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on learning models of spiking neural membrane systems and
  spiking neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithwineel Paul, Petr Sosik, Lucie Ciencialova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNN) are a biologically inspired model of neural
networks with certain brain-like properties. In the past few decades, this
model has received increasing attention in computer science community, owing
also to the successful phenomenon of deep learning. In SNN, communication
between neurons takes place through the spikes and spike trains. This
differentiates these models from the ``standard'' artificial neural networks
(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking
neural P systems (SNPS) can be considered a branch of SNN based more on the
principles of formal automata, with many variants developed within the
framework of the membrane computing theory. In this paper, we first briefly
compare structure and function, advantages and drawbacks of SNN and SNPS. A key
part of the article is a survey of recent results and applications of machine
learning and deep learning models of both SNN and SNPS formalisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Sentence Embedders through Contrastive Word Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Alexander Schulz, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last years, various sentence embedders have been an integral part in
the success of current machine learning approaches to Natural Language
Processing (NLP). Unfortunately, multiple sources have shown that the bias,
inherent in the datasets upon which these embedding methods are trained, is
learned by them. A variety of different approaches to remove biases in
embeddings exists in the literature. Most of these approaches are applicable to
word embeddings and in fewer cases to sentence embeddings. It is problematic
that most debiasing approaches are directly transferred from word embeddings,
therefore these approaches fail to take into account the nonlinear nature of
sentence embedders and the embeddings they produce. It has been shown in
literature that bias information is still present if sentence embeddings are
debiased using such methods. In this contribution, we explore an approach to
remove linear and nonlinear bias information for NLP solutions, without
impacting downstream performance. We compare our approach to common debiasing
methods on classical bias metrics and on bias metrics which take nonlinear
information into account.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large <span class="highlight-title">language models</span>, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AcTED: Automatic Acquisition of Typical Event Duration for
  Semi-supervised Temporal Commonsense QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voting-driven semi-supervised approach to automatically acquire
the typical duration of an event and use it as pseudo-labeled data. The human
evaluation demonstrates that our pseudo labels exhibit surprisingly high
accuracy and balanced coverage. In the temporal commonsense QA task,
experimental results show that using only pseudo examples of 400 events, we
achieve performance comparable to the existing BERT-based weakly supervised
approaches that require a significant amount of training examples. When
compared to the RoBERTa baselines, our best approach establishes
state-of-the-art performance with a 7% improvement in Exact Match.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: <span class="highlight-title">Pre-train</span> a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring language relations through syntactic distances and geographic
  proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan De Gregorio, Raúl Toral, David Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Languages are grouped into families that share common linguistic traits.
While this approach has been successful in understanding genetic relations
between diverse languages, more analyses are needed to accurately quantify
their relatedness, especially in less studied linguistic levels such as syntax.
Here, we explore linguistic distances using series of parts of speech (POS)
extracted from the Universal Dependencies dataset. Within an
information-theoretic framework, we show that employing POS trigrams maximizes
the possibility of capturing syntactic variations while being at the same time
compatible with the amount of available data. Linguistic connections are then
established by assessing pairwise distances based on the POS distributions.
Intriguingly, our analysis reveals definite clusters that correspond to well
known language families and groups, with exceptions explained by distinct
morphological typologies. Furthermore, we obtain a significant correlation
between language similarity and geographic distance, which underscores the
influence of spatial proximity on language kinships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriviaHG: A <span class="highlight-title">Dataset</span> for Automatic Hint <span class="highlight-title">Generation</span> from Factoid Questions <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Mozafari, Anubhav Jangra, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, individuals tend to engage in dialogues with Large Language Models,
seeking answers to their questions. In times when such answers are readily
accessible to anyone, the stimulation and preservation of human's cognitive
abilities, as well as the assurance of maintaining good reasoning skills by
humans becomes crucial. This study addresses such needs by proposing hints
(instead of final answers or before giving answers) as a viable solution. We
introduce a framework for the automatic hint generation for factoid questions,
employing it to construct TriviaHG, a novel large-scale dataset featuring
160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.
Additionally, we present an automatic evaluation method that measures the
Convergence and Familiarity quality attributes of hints. To evaluate the
TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals
to annotate 2,791 hints and tasked 6 humans with answering questions using the
provided hints. The effectiveness of hints varied, with success rates of 96%,
78%, and 36% for questions with easy, medium, and hard answers, respectively.
Moreover, the proposed automatic evaluation methods showed a robust correlation
with annotators' results. Conclusively, the findings highlight three key
insights: the facilitative role of hints in resolving unknown questions, the
dependence of hint quality on answer difficulty, and the feasibility of
employing automatic evaluation methods for hint assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, <span class="highlight-author">Percy Liang</span>, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Attributed Text <span class="highlight-title">Generation</span> of Large <span class="highlight-title">Language Models</span> via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLADE: Enhancing Black-box Large <span class="highlight-title">Language Models</span> with Small
  Domain-Specific Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable
of addressing a diverse range of tasks. However, general LLMs, which are
developed on open-domain data, may lack the domain-specific knowledge essential
for tasks in vertical domains, such as legal, medical, etc. To address this
issue, previous approaches either conduct continuous pre-training with
domain-specific data or employ retrieval augmentation to support general LLMs.
Unfortunately, these strategies are either cost-intensive or unreliable in
practical applications. To this end, we present a novel framework named BLADE,
which enhances Black-box LArge language models with small Domain-spEcific
models. BLADE consists of a black-box LLM and a small domain-specific LM. The
small LM preserves domain-specific knowledge and offers specialized insights,
while the general LLM contributes robust language comprehension and reasoning
capabilities. Specifically, our method involves three steps: 1) pre-training
the small LM with domain-specific data, 2) fine-tuning this model using
knowledge instruction data, and 3) joint Bayesian optimization of the general
LLM and the small LM. Extensive experiments conducted on public legal and
medical benchmarks reveal that BLADE significantly outperforms existing
approaches. This shows the potential of BLADE as an effective and
cost-efficient solution in adapting general LLMs for vertical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-<span class="highlight-title">Generation</span> (RAG) for Arabic Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mahboub, Muhy Eddin Za'ter, Bashar Alfrou, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often generate erroneous outputs, known as
hallucinations, due to their limitations in discerning questions beyond their
knowledge scope. While addressing hallucination has been a focal point in
research, previous efforts primarily concentrate on enhancing correctness
without giving due consideration to the significance of rejection mechanisms.
In this paper, we conduct a comprehensive examination of the role of rejection,
introducing the notion of model reliability along with corresponding metrics.
These metrics measure the model's ability to provide accurate responses while
adeptly rejecting questions exceeding its knowledge boundaries, thereby
minimizing hallucinations. To improve the inherent reliability of LLMs, we
present a novel alignment framework called Reinforcement Learning from
Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically
determine the model's knowledge boundary and trains a reliable reward model to
encourage the refusal of out-of-knowledge questions. Experimental results on
mathematical questions affirm the substantial efficacy of RLKF in significantly
enhancing LLM reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterAlign: Iterative Constitutional Alignment of Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), aligning LLMs
with human values and societal norms to ensure their reliability and safety has
become crucial. Reinforcement learning with human feedback (RLHF) and
Constitutional AI (CAI) have been proposed for LLM alignment. However, these
methods require either heavy human annotations or explicitly pre-defined
constitutions, which are labor-intensive and resource-consuming. To overcome
these drawbacks, we study constitution-based LLM alignment and propose a
data-driven constitution discovery and self-alignment framework called
IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM
and automatically discovers new constitutions using a stronger LLM. These
constitutions are then used to guide self-correction of the base LLM. Such a
constitution discovery pipeline can be run iteratively and automatically to
discover new constitutions that specifically target the alignment gaps in the
current LLM. Empirical results on several safety benchmark datasets and
multiple base LLMs show that IterAlign successfully improves truthfulness,
helpfulness, harmlessness and honesty, improving the LLM alignment by up to
$13.5\%$ in harmlessness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual <span class="highlight-title">Instruct</span>ion Tuning with Large <span class="highlight-title">Language Models</span> for Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Zhou, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements highlight the success of instruction tuning with large
language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical
reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as
incorrect, missing, and redundant steps in CoT generation leading to
inaccuracies in answer predictions. To alleviate this problem, we propose a
dual instruction tuning strategy to meticulously model mathematical reasoning
from both forward and reverse directions. This involves introducing the
Intermediate Reasoning State Prediction task (forward reasoning) and the
Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'
understanding and execution of instructions. Training instances for these tasks
are constructed based on existing mathematical instruction tuning datasets.
Subsequently, LLMs undergo multi-task fine-tuning using both existing
mathematical instructions and the newly created data. Comprehensive experiments
validate the effectiveness and domain generalization of the dual instruction
tuning strategy across various mathematical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BlendX: Complex Multi-Intent Detection with Blended Patterns <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) systems are commonly designed with the
presumption that each utterance represents a single intent. However, this
assumption may not accurately reflect real-world situations, where users
frequently express multiple intents within a single utterance. While there is
an emerging interest in multi-intent detection (MID), existing in-domain
datasets such as MixATIS and MixSNIPS have limitations in their formulation. To
address these issues, we present BlendX, a suite of refined datasets featuring
more diverse patterns than their predecessors, elevating both its complexity
and diversity. For dataset construction, we utilize both rule-based heuristics
as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a
similarity-driven strategy for utterance selection. To ensure the quality of
the proposed datasets, we also introduce three novel metrics that assess the
statistical properties of an utterance related to word count, conjunction use,
and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art
MID models struggle with the challenges posed by the new datasets, highlighting
the need to reexamine the current state of the MID field. The dataset is
available at https://github.com/HYU-NLP/BlendX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era
  of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Interactive Regional Understanding in Vision-Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-PK: Metaphor Detection via <span class="highlight-title">Prompt</span> Learning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Jia, Rongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are ubiquitous in daily life, yet detecting them poses a
significant challenge. Previous approaches often struggled with improper
application of language rules and overlooked the issue of data sparsity. To
address these challenges, we introduce knowledge distillation and prompt
learning into metaphor detection. Specifically, we devise a prompt learning
template tailored for the metaphor detection task. By masking target words and
providing relevant prompt information, we guide the model to accurately infer
the contextual meaning of these words. This approach not only mitigates the
interference from the literal meaning of target words but also ensures the
proper utilization of MIP language rules for metaphor detection. Moreover, we
employ a teacher model equipped with prior knowledge to generate meaningful
soft labels, guiding the optimization process of the student model. The
inclusion of soft labels, akin to label smoothing, helps alleviate the model's
tendency towards over-confidence and effectively addresses the challenge of
data sparsity. Experimental results demonstrate that our proposed model
achieves state-of-the-art performance across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Since the Scientific Literature Is Multilingual, Our Models Should Be
  Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abteen Ebrahimi, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English has long been assumed the $\textit{lingua franca}$ of scientific
research, and this notion is reflected in the natural language processing (NLP)
research involving scientific document representation. In this position piece,
we quantitatively show that the literature is largely multilingual and argue
that current models and benchmarks should reflect this linguistic diversity. We
provide evidence that text-based models fail to create meaningful
representations for non-English papers and highlight the negative user-facing
impacts of using English-only models non-discriminately across a multilingual
domain. We end with suggestions for the NLP community on how to improve
performance on non-English documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Deceptive Power of LLM-Generated Fake News: A Study of
  Real-World Detection Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have enabled the creation
of fake news, particularly in complex fields like healthcare. Studies highlight
the gap in the deceptive power of LLM-generated fake news with and without
human assistance, yet the potential of prompting techniques has not been fully
explored. Thus, this work aims to determine whether prompting strategies can
effectively narrow this gap. Current LLM-based fake news attacks require human
intervention for information gathering and often miss details and fail to
maintain context consistency. Therefore, to better understand threat tactics,
we propose a strong fake news attack method called conditional
Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,
VLPrompt eliminates the need for additional data collection while maintaining
contextual coherence and preserving the intricacies of the original text. To
propel future research on detecting VLPrompt attacks, we created a new dataset
named VLPrompt fake news (VLPFN) containing real and fake texts. Our
experiments, including various detection methods and novel human study metrics,
were conducted to assess their performance on our dataset, yielding numerous
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech
  Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Injy Hamed, Fadhl Eryani, David Palfreyman, Nizar Habash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech
corpus. The corpus comprises twelve hours of Zoom meetings involving multiple
speakers role-playing a work situation where Students brainstorm ideas for a
certain topic and then discuss it with an Interlocutor. The meetings cover
different topics and are divided into phases with different language setups.
The corpus presents a challenging set for automatic speech recognition (ASR),
including two languages (Arabic and English) with Arabic spoken in multiple
variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English
used with various accents. Adding to the complexity of the corpus, there is
also code-switching between these languages and dialects. As part of our work,
we take inspiration from established sets of transcription guidelines to
present a set of guidelines handling issues of conversational speech,
code-switching and orthography of both languages. We further enrich the corpus
with two layers of annotations; (1) dialectness level annotation for the
portion of the corpus where mixing occurs between different variants of Arabic,
and (2) automatic morphological annotations, including tokenization,
lemmatization, and part-of-speech tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in <span class="highlight-title">language models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of Large Language Models (LLMs), AI assistants' ability to
utilize tools, especially through API calls, has advanced notably. This
progress has necessitated more accurate evaluation methods. Many existing
studies adopt static evaluation, where they assess AI assistants' API call
based on pre-defined dialogue histories. However, such evaluation method can be
misleading, as an AI assistant might fail in generating API calls from
preceding human interaction in real cases. Instead of the resource-intensive
method of direct human-machine interactions, we propose Automated Dynamic
Evaluation (AutoDE) to assess an assistant's API call capability without human
involvement. In our framework, we endeavor to closely mirror genuine human
conversation patterns in human-machine interactions, using a LLM-based user
agent, equipped with a user script to ensure human alignment. Experimental
results highlight that AutoDE uncovers errors overlooked by static evaluations,
aligning more closely with human assessment. Testing four AI assistants using
our crafted benchmark, our method further mirrored human evaluation compared to
conventional static evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlotScript: A Resource and Tool for Low Resource Writing System
  Identification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Kargaran, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GlotScript, an open resource and tool for low resource writing
system identification. GlotScript-R is a resource that provides the attested
writing systems for more than 7,000 languages. It is compiled by aggregating
information from existing writing system resources. GlotScript-T is a writing
system identification tool that covers all 161 Unicode 15.0 scripts. For an
input text, it returns its script distribution where scripts are identified by
ISO 15924 codes. We also present two use cases for GlotScript. First, we
demonstrate that GlotScript can help cleaning multilingual corpora such as mC4
and OSCAR. Second, we analyze the tokenization of a number of language models
such as GPT-4 using GlotScript and provide insights on the coverage of low
resource scripts and languages by each language model. We hope that GlotScript
will become a useful resource for work on low resource languages in the NLP
community. GlotScript-R and GlotScript-T are available at
https://github.com/cisnlp/GlotScript.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLPre: a revised approach towards language-centric benchmarking of
  Natural Language Preprocessing systems <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martyna Wiącek, Piotr Rybak, Łukasz Pszenny, Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of transformer-based architectures, we observe the rise
of natural language preprocessing (NLPre) tools capable of solving preliminary
NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or
morphological analysis) without any external linguistic guidance. It is arduous
to compare novel solutions to well-entrenched preprocessing toolkits, relying
on rule-based morphological analysers or dictionaries. Aware of the
shortcomings of existing NLPre evaluation approaches, we investigate a novel
method of reliable and fair evaluation and performance reporting. Inspired by
the GLUE benchmark, the proposed language-centric benchmarking system enables
comprehensive ongoing evaluation of multiple NLPre tools, while credibly
tracking their performance. The prototype application is configured for Polish
and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this
benchmark, we conduct an extensive evaluation of a variety of Polish NLPre
systems. To facilitate the construction of benchmarking environments for other
languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full
customization of the publicly released source code of the benchmarking system.
The links to all the resources (deployed platforms, source code, trained
models, datasets etc.) can be found on the project website:
https://sites.google.com/view/nlpre-benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARE: Co-Attention Network for Joint Entity and Relation Extraction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Kong, Yamei Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint entity and relation extraction is the fundamental task of information
extraction, consisting of two subtasks: named entity recognition and relation
extraction. However, most existing joint extraction methods suffer from issues
of feature confusion or inadequate interaction between the two subtasks.
Addressing these challenges, in this work, we propose a Co-Attention network
for joint entity and Relation Extraction (CARE). Our approach includes adopting
a parallel encoding strategy to learn separate representations for each
subtask, aiming to avoid feature overlap or confusion. At the core of our
approach is the co-attention module that captures two-way interaction between
the two subtasks, allowing the model to leverage entity information for
relation prediction and vice versa, thus promoting mutual enhancement. Through
extensive experiments on three benchmark datasets for joint entity and relation
extraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model
outperforms existing baseline models. Our code will be available at
https://github.com/kwj0x7f/CARE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak <span class="highlight-title">Prompt</span>s can
  Fool Large <span class="highlight-title">Language Models</span> Easily <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to
provide useful and safe responses. However, adversarial prompts known as
'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially
harmful content. Exploring jailbreak prompts can help to better reveal the
weaknesses of LLMs and further steer us to secure them. Unfortunately, existing
jailbreak methods either suffer from intricate manual design or require
optimization on other white-box models, which compromises either generalization
or efficiency. In this paper, we generalize jailbreak prompt attacks into two
aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we
propose ReNeLLM, an automatic framework that leverages LLMs themselves to
generate effective jailbreak prompts. Extensive experiments demonstrate that
ReNeLLM significantly improves the attack success rate while greatly reducing
the time cost compared to existing baselines. Our study also reveals the
inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze
the failure of LLMs defense from the perspective of prompt execution priority,
and propose corresponding defense strategies. We hope that our research can
catalyze both the academic community and LLMs developers towards the provision
of safer and more regulated LLMs. The code is available at
https://github.com/NJUNLP/ReNeLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based <span class="highlight-title">Language Models</span> <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable <span class="highlight-title">Generation</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual
  Topic Modeling <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liangming Pan, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2023 conference. Code is available at
  https://github.com/BobXWu/InfoCTM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Source: Results in Detecting Large Language Model-Generated
  Content <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Antoun, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Large Language Models (LLMs), celebrated for their
ability to generate human-like text, has raised concerns about misinformation
and ethical implications. Addressing these concerns necessitates the
development of robust methods to detect and attribute text generated by LLMs.
This paper investigates "Cross-Model Detection," by evaluating whether a
classifier trained to distinguish between source LLM-generated and
human-written text can also detect text from a target LLM without further
training. The study comprehensively explores various LLM sizes and families,
and assesses the impact of conversational fine-tuning techniques, quantization,
and watermarking on classifier generalization. The research also explores Model
Attribution, encompassing source model identification, model family, and model
size classification, in addition to quantization and watermarking detection.
Our results reveal several key findings: a clear inverse relationship between
classifier effectiveness and model size, with larger LLMs being more
challenging to detect, especially when the classifier is trained on data from
smaller models. Training on data from similarly sized LLMs can improve
detection performance from larger models but may lead to decreased performance
when dealing with smaller models. Additionally, model attribution experiments
show promising results in identifying source models and model families,
highlighting detectable signatures in LLM-generated text, with particularly
remarkable outcomes in watermarking detection, while no detectable signatures
of quantization were observed. Overall, our study contributes valuable insights
into the interplay of model size, family, and training data in LLM detection
and attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING-LREC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Subgraph <span class="highlight-title">Generation</span> for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented <span class="highlight-title">Generation</span> for Large <span class="highlight-title">Language Models</span>: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive development of large language models (LLMs) is expanding into
the realm of large multimodal models (LMMs), which incorporate multiple types
of data beyond text. However, the nature of multimodal models leads to
significant expenses in the creation of training data. Furthermore,
constructing multilingual data for LMMs presents its own set of challenges due
to language diversity and complexity. Therefore, in this study, we propose two
cost-effective methods to solve this problem: (1) vocabulary expansion and
pretraining of multilingual LLM for specific languages, and (2) automatic and
elaborate construction of multimodal datasets using GPT4-V. Based on015 these
methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal
training dataset. Additionally, we developed a bilingual multimodal model that
exhibits excellent performance in both Korean and English, surpassing existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Knowledge for Few-shot Table-to-Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Guanjie Zheng, Xinbing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the lack of domain-specific knowledge
makes it challenging to bridge the topological gap between tabular data and
text, especially in real-world applications with limited resources. To mitigate
the limitation of insufficient labeled data, we propose a novel framework:
Adapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt
unlabeled domain-specific knowledge into the model, which brings at least three
benefits: (1) it injects representation of normal table-related descriptions to
bridge the topological gap between tabular data and texts; (2) it enables us to
use large amounts of unlabeled domain-specific knowledge fully, which can
alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it
allows us to design various tasks to employ the domain-specific knowledge.
Extensive experiments and analyses are conducted on three open-domain, few-shot
natural language generation (NLG) data sets: Humans, Songs, and Books. Compared
to previous state-of-the-art approaches, our model achieves superior
performance in terms of both fluency and accuracy as judged by human and
automatic evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.04415</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EASYTOOL: Enhancing LLM-based Agents with Concise Tool <span class="highlight-title">Instruct</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address intricate real-world tasks, there has been a rising interest in
tool utilization in applications of large language models (LLMs). To develop
LLM-based agents, it usually requires LLMs to understand many tool functions
from different tool documentation. But these documentations could be diverse,
redundant or incomplete, which immensely affects the capability of LLMs in
using tools. To solve this, we introduce EASYTOOL, a framework transforming
diverse and lengthy tool documentation into a unified and concise tool
instruction for easier tool usage. EasyTool purifies essential information from
extensive tool documentation of different sources, and elaborates a unified
interface (i.e., tool instruction) to offer standardized tool descriptions and
functionalities for LLM-based agents. Extensive experiments on multiple
different tasks demonstrate that EasyTool can significantly reduce token
consumption and improve the performance of tool utilization in real-world
scenarios. Our code will be available at
\url{https://github.com/microsoft/JARVIS/} in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mix-Initiative Response <span class="highlight-title">Generation</span> with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyunJin Kim, Young Jin Kim, JinYeong Bak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) show impressive performance in various
downstream NLP tasks. However, pre-training large language models demands
substantial memory and training compute. Furthermore, due to the substantial
resources required, many PLM weights are confidential. Consequently, users are
compelled to share their data with model owners for fine-tuning specific tasks.
To overcome the limitations, we introduce Plug-in External Memory Adaptation
(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM
fine-tuning without requiring access to all the weights. PEMA integrates with
context representations from test data during inference to perform downstream
tasks. It uses external memory to store PLM-generated context representations
mapped with target tokens. Our method utilizes weight matrices of LoRA-like
bottlenecked adapter in the PLM's final layer to enhance efficiency. Our
approach also includes Gradual Unrolling, a novel interpolation strategy to
improve generation quality. We validate PEMA's effectiveness through
experiments on syntactic and real datasets for machine translation and style
transfer. Our findings show that PEMA outperforms other PEFT approaches in
memory and latency efficiency for training, and also excels in maintaining
sentence meaning and generating appropriate language and styles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14965v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14965v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent explorations with commercial Large Language Models (LLMs) have shown
that non-expert users can jailbreak LLMs by simply manipulating their prompts;
resulting in degenerate output behavior, privacy and security breaches,
offensive outputs, and violations of content regulator policies. Limited
studies have been conducted to formalize and analyze these attacks and their
mitigations. We bridge this gap by proposing a formalism and a taxonomy of
known (and possible) jailbreaks. We survey existing jailbreak methods and their
effectiveness on open-source and commercial LLMs (such as GPT-based models,
OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak
detection in terms of their effectiveness against known attacks. For further
analysis, we release a dataset of model outputs across 3700 jailbreak prompts
over 4 tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large <span class="highlight-title">Language Models</span> for Global and Local Semantic
  Representations <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Language Models</span> are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration <span class="highlight-title">Prompt</span>ing Improves
  Mathematical Reasoning in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans W. A. Hanley, Zakir Durumeric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to disinformation and propaganda from Russian online media
following the invasion of Ukraine, Russian media outlets such as Russia Today
and Sputnik News were banned throughout Europe. To maintain viewership, many of
these Russian outlets began to heavily promote their content on messaging
services like Telegram. In this work, we study how 16 Russian media outlets
interacted with and utilized 732 Telegram channels throughout 2022. Leveraging
the foundational model MPNet, DP-means clustering, and Hawkes processes, we
trace how narratives spread between news sites and Telegram channels. We show
that news outlets not only propagate existing narratives through Telegram but
that they source material from the messaging platform. For example, across the
websites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of
articles discussed content that originated/resulted from activity on Telegram.
Finally, tracking the spread of individual topics, we measure the rate at which
news outlets and Telegram channels disseminate content within the Russian media
ecosystem, finding that websites like ura.news and Telegram channels such as
@genshab are the most effective at disseminating their content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICWSM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLP-based detection of systematic anomalies among the narratives of
  consumer complaints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ričardas Zitikis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChroniclingAmericaQA: A Large-scale Question Answering <span class="highlight-title">Dataset</span> based on
  Historical American Newspaper Pages <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed <span class="highlight-title">Language Models</span> Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report <span class="highlight-title">Generation</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructions Are So Difficult That Even Large <span class="highlight-title">Language Models</span> Get Them
  Right for the Wrong Reasons <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Schütze, David R. Mortensen, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we make a contribution that can be understood from two
perspectives: from an NLP perspective, we introduce a small challenge dataset
for NLI with large lexical overlap, which minimises the possibility of models
discerning entailment solely based on token distinctions, and show that GPT-4
and Llama 2 fail it with strong bias. We then create further challenging
sub-tasks in an effort to explain this failure. From a Computational
Linguistics perspective, we identify a group of constructions with three
classes of adjectives which cannot be distinguished by surface features. This
enables us to probe for LLM's understanding of these constructions in various
ways, and we find that they fail in a variety of ways to distinguish between
them, suggesting that they don't adequately represent their meaning or capture
the lexical properties of phrasal heads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can multiple-choice questions really be useful in detecting the
  abilities of LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice questions (MCQs) are widely used in the evaluation of large
language models (LLMs) due to their simplicity and efficiency. However, there
are concerns about whether MCQs can truly measure LLM's capabilities,
particularly in knowledge-intensive scenarios where long-form generation (LFG)
answers are required. The misalignment between the task and the evaluation
method demands a thoughtful analysis of MCQ's efficacy, which we undertake in
this paper by evaluating nine LLMs on four question-answering (QA) datasets in
two languages: Chinese and English. We identify a significant issue: LLMs
exhibit an order sensitivity in bilingual MCQs, favoring answers located at
specific positions, i.e., the first position. We further quantify the gap
between MCQs and long-form generation questions (LFGQs) by comparing their
direct outputs, token logits, and embeddings. Our results reveal a relatively
low correlation between answers from MCQs and LFGQs for identical questions.
Additionally, we propose two methods to quantify the consistency and confidence
of LLMs' output, which can be generalized to other QA evaluation benchmarks.
Notably, our analysis challenges the idea that the higher the consistency, the
greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms
of expected calibration error. Finally, the misalignment between MCQs and LFGQs
is not only reflected in the evaluation performance but also in the embedding
space. Our code and models can be accessed at
https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UCxn: Typologically Informed Annotation of Constructions Atop Universal
  Dependencies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Weissweiler, Nina Böbel, Kirian Guiller, Santiago Herrera, Wesley Scivetti, Arthur Lorenzi, Nurit Melnik, Archna Bhatia, Hinrich Schütze, Lori Levin, Amir Zeldes, Joakim Nivre, William Croft, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has created an invaluable collection
of treebanks with contributions in over 140 languages. However, the UD
annotations do not tell the full story. Grammatical constructions that convey
meaning through a particular combination of several morphosyntactic elements --
for example, interrogative sentences with special markers and/or word orders --
are not labeled holistically. We argue for (i) augmenting UD annotations with a
'UCxn' annotation layer for such meaning-bearing grammatical constructions, and
(ii) approaching this in a typologically informed way so that morphosyntactic
strategies can be compared across languages. As a case study, we consider five
construction families in ten languages, identifying instances of each
construction in UD treebanks through the use of morphosyntactic patterns. In
addition to findings regarding these particular constructions, our study yields
important insights on methodology for describing and identifying constructions
in language-general and language-particular ways, and lays the foundation for
future constructional enrichment of UD treebanks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Few-shot Event Detection via Hierarchical Augmentation
  Networks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Zhang, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional continual event detection relies on abundant labeled data for
training, which is often impractical to obtain in real-world applications. In
this paper, we introduce continual few-shot event detection (CFED), a more
commonly encountered scenario when a substantial number of labeled samples are
not accessible. The CFED task is challenging as it involves memorizing previous
event types and learning new event types with few-shot samples. To mitigate
these challenges, we propose a memory-based framework: Hierarchical
Augmentation Networks (HANet). To memorize previous event types with limited
memory, we incorporate prototypical augmentation into the memory set. For the
issue of learning new event types in few-shot scenarios, we propose a
contrastive augmentation module for token representations. Despite comparing
with previous state-of-the-art methods, we also conduct comparisons with
ChatGPT. Experiment results demonstrate that our method significantly
outperforms all of these methods in multiple continual few-shot event detection
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Short Text Modeling: Leveraging Large <span class="highlight-title">Language Models</span> for Topic
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Chang, Rui Wang, Peng Ren, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting effective topic models for brief texts, like tweets and news
headlines, is essential for capturing the swift shifts in social dynamics.
Traditional topic models, however, often fall short in accurately representing
the semantic intricacies of short texts due to their brevity and lack of
contextual data. In our study, we harness the advanced capabilities of Large
Language Models (LLMs) to introduce a novel approach termed "Topic Refinement".
This approach does not directly involve itself in the initial modeling of
topics but focuses on improving topics after they have been mined. By employing
prompt engineering, we direct LLMs to eliminate off-topic words within a given
topic, ensuring that only contextually relevant words are preserved or
substituted with ones that fit better semantically. This method emulates
human-like scrutiny and improvement of topics, thereby elevating the semantic
quality of the topics generated by various models. Our comprehensive evaluation
across three unique datasets has shown that our topic refinement approach
significantly enhances the semantic coherence of topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language Models</span> for Text Classification: Is In-Context Learning Enough? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandra Edwards, Jose Camacho-Collados
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent foundational language models have shown state-of-the-art performance
in many NLP tasks in zero- and few-shot settings. An advantage of these models
over more standard approaches based on fine-tuning is the ability to understand
instructions written in natural language (prompts), which helps them generalise
better to different tasks and domains without the need for specific training
data. This makes them suitable for addressing text classification problems for
domains with limited amounts of annotated instances. However, existing research
is limited in scale and lacks understanding of how text generation models
combined with prompting techniques compare to more established methods for text
classification such as fine-tuning masked language models. In this paper, we
address this research gap by performing a large-scale evaluation study for 16
text classification datasets covering binary, multiclass, and multilabel
problems. In particular, we compare zero- and few-shot approaches of large
language models to fine-tuning smaller language models. We also analyse the
results by prompt, classification type, domain, and number of labels. In
general, the results show how fine-tuning smaller and more efficient language
models can still outperform few-shot approaches of larger language models,
which have room for improvement when it comes to text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Subgraph <span class="highlight-title">Generation</span> for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DANCER: Entity Description Augmented Named Entity Corrector for
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition (E2E ASR) systems often suffer from
mistranscription of domain-specific phrases, such as named entities, sometimes
leading to catastrophic failures in downstream tasks. A family of fast and
lightweight named entity correction (NEC) models for ASR have recently been
proposed, which normally build on phonetic-level edit distance algorithms and
have shown impressive NEC performance. However, as the named entity (NE) list
grows, the problems of phonetic confusion in the NE list are exacerbated; for
example, homophone ambiguities increase substantially. In view of this, we
proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),
which leverages entity descriptions to provide additional information to
facilitate mitigation of phonetic confusion for NEC on ASR transcription. To
this end, an efficient entity description augmented masked language model
(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to
adapt swiftly to domain-specific entities for the NEC task. A series of
experiments conducted on the AISHELL-1 and Homophone datasets confirm the
effectiveness of our modeling approach. DANCER outperforms a strong baseline,
the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate
(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More
notably, when tested on Homophone that contain named entities of high phonetic
confusion, DANCER offers a more pronounced CER reduction of 46% relatively over
PED-NEC for named entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFeREE: A REference-FREE Model-Based Metric for Text Simplification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Huang, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text simplification lacks a universal standard of quality, and annotated
reference simplifications are scarce and costly. We propose to alleviate such
limitations by introducing REFeREE, a reference-free model-based metric with a
3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage
and can be applied to any quality standard as long as a small number of human
annotations are available. Our experiments show that our metric outperforms
existing reference-based metrics in predicting overall ratings and reaches
competitive and consistent performance in predicting specific ratings while
requiring no reference simplifications at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mix-Initiative Response <span class="highlight-title">Generation</span> with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "You are an expert annotator": Automatic Best-Worst-Scaling Annotations
  for Emotion Intensity Modeling <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Bagdon, Prathamesh Karmalker, Harsha Gurulingappa, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling corpora constitutes a bottleneck to create models for new tasks or
domains. Large language models mitigate the issue with automatic corpus
labeling methods, particularly for categorical annotations. Some NLP tasks such
as emotion intensity prediction, however, require text regression, but there is
no work on automating annotations for continuous label assignments. Regression
is considered more challenging than classification: The fact that humans
perform worse when tasked to choose values from a rating scale lead to
comparative annotation methods, including best-worst scaling. This raises the
question if large language model-based annotation methods show similar
patterns, namely that they perform worse on rating scale annotation tasks than
on comparative annotation tasks. To study this, we automate emotion intensity
predictions and compare direct rating scale predictions, pairwise comparisons
and best-worst scaling. We find that the latter shows the highest reliability.
A transformer regressor fine-tuned on these data performs nearly on par with a
model trained on the original manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication in NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Table-Text Retrieval for Open-Domain Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokhyung Kang, Baikjin Jung, Yunsu Kim, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In table-text open-domain question answering, a retriever system retrieves
relevant evidence from tables and text to answer questions. Previous studies in
table-text open-domain question answering have two common challenges: firstly,
their retrievers can be affected by false-positive labels in training datasets;
secondly, they may struggle to provide appropriate evidence for questions that
require reasoning across the table. To address these issues, we propose
Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a
denoised training dataset with fewer false positive labels by discarding
instances with lower question-relevance scores measured through a false
positive detection model. Subsequently, we integrate table-level ranking
information into the retriever to assist in finding evidence for questions that
demand reasoning across the table. To encode this ranking information, we
fine-tune a rank-aware column encoder to identify minimum and maximum values
within a column. Experimental results demonstrate that DoTTeR significantly
outperforms strong baselines on both retrieval recall and downstream QA tasks.
Our code is available at https://github.com/deokhk/DoTTeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coimagining the Future of Voice Assistants with Cultural Sensitivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Seaborn, Yuto Sawa, Mizuki Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the
user experience (UX) is often limited, leading to underuse, disengagement, and
abandonment. Co-designing interactions for VAs with potential end-users can be
useful. Crowdsourcing this process online and anonymously may add value.
However, most work has been done in the English-speaking West on dialogue data
sets. We must be sensitive to cultural differences in language, social
interactions, and attitudes towards technology. Our aims were to explore the
value of co-designing VAs in the non-Western context of Japan and demonstrate
the necessity of cultural sensitivity. We conducted an online elicitation study
(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined
dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the
implications for coimagining interactions with future VAs, offer design
guidelines for the Japanese and English-speaking US contexts, and suggest
opportunities for cultural plurality in VA design and scholarship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Paraphrase Analytics <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Gohsen, Matthias Hagen, Martin Potthast, Benno Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since paraphrasing is an ill-defined task, the term "paraphrasing" covers
text transformation tasks with different characteristics. Consequently,
existing paraphrasing studies have applied quite different (explicit and
implicit) criteria as to when a pair of texts is to be considered a paraphrase,
all of which amount to postulating a certain level of semantic or lexical
similarity. In this paper, we conduct a literature review and propose a
taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using
classifiers trained to identify the tasks that a given paraphrasing instance
fits, we find that the distributions of task-specific instances in the known
paraphrase corpora vary substantially. This means that the use of these
corpora, without the respective paraphrase conditions being clearly defined
(which is the normal case), must lead to incomparable and misleading results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ m3P: Towards Multimodal Multilingual Translation with Multimodal <span class="highlight-title">Prompt</span> <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, Jiaheng Liu, Xinnian Liang, Linzheng Cahi, Liqun Yang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual translation supports multiple translation directions by
projecting all languages in a shared space, but the translation quality is
undermined by the difference between languages in the text-only modality,
especially when the number of languages is large. To bridge this gap, we
introduce visual context as the universal language-independent representation
to facilitate multilingual translation. In this paper, we propose a framework
to leverage the multimodal prompt to guide the Multimodal Multilingual neural
Machine Translation (m3P), which aligns the representations of different
languages with the same meaning and generates the conditional vision-language
memory for translation. We construct a multilingual multimodal instruction
dataset (InstrMulti102) to support 102 languages. Our method aims to minimize
the representation distance of different languages by regarding the image as a
central language. Experimental results show that m3P outperforms previous
text-only baselines and multilingual multimodal methods by a large margin.
Furthermore, the probing experiments validate the effectiveness of our method
in enhancing translation under the low-resource and massively multilingual
scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuBia: A Russian Language Bias Detection <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, Ekaterina Artemova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this work contains upsetting or disturbing content.
  Large language models (LLMs) tend to learn the social and cultural biases
present in the raw pre-training data. To test if an LLM's behavior is fair,
functional datasets are employed, and due to their purpose, these datasets are
highly language and culture-specific. In this paper, we address a gap in the
scope of multilingual bias evaluation by presenting a bias detection dataset
specifically designed for the Russian language, dubbed as RuBia. The RuBia
dataset is divided into 4 domains: gender, nationality, socio-economic status,
and diverse, each of the domains is further divided into multiple fine-grained
subdomains. Every example in the dataset consists of two sentences with the
first reinforcing a potentially harmful stereotype or trope and the second
contradicting it. These sentence pairs were first written by volunteers and
then validated by native-speaking crowdsourcing workers. Overall, there are
nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To
illustrate the dataset's purpose, we conduct a diagnostic evaluation of
state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'
predisposition to social biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Naive Bayes-based Context Extension for Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlin Su, Murtadha Ahmed,  Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising in-context learning
abilities. However, conventional In-Context Learning (ICL) approaches are often
impeded by length limitations of transformer architecture, which pose
challenges when attempting to effectively integrate supervision from a
substantial number of demonstration examples. In this paper, we introduce a
novel framework, called Naive Bayes-based Context Extension (NBCE), to enable
existing LLMs to perform ICL with an increased number of demonstrations by
significantly expanding their context size. Importantly, this expansion does
not require fine-tuning or dependence on particular model architectures, all
the while preserving linear efficiency. NBCE initially splits the context into
equal-sized windows fitting the target LLM's maximum length. Then, it
introduces a voting mechanism to select the most relevant window, regarded as
the posterior context. Finally, it employs Bayes' theorem to generate the test
task. Our experimental results demonstrate that NBCE substantially enhances
performance, particularly as the number of demonstration examples increases,
consistently outperforming alternative methods. The NBCE code will be made
publicly accessible. The code NBCE is available at:
https://github.com/amurtadha/NBCE-master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to main NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding excellence: Mapping the demand for psychological traits of
  operations and supply chain professionals through text mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current study proposes an innovative methodology for the profiling of
psychological traits of Operations Management (OM) and Supply Chain Management
(SCM) professionals. We use innovative methods and tools of text mining and
social network analysis to map the demand for relevant skills from a set of job
descriptions, with a focus on psychological characteristics. The proposed
approach aims to evaluate the market demand for specific traits by combining
relevant psychological constructs, text mining techniques, and an innovative
measure, namely, the Semantic Brand Score. We apply the proposed methodology to
a dataset of job descriptions for OM and SCM professionals, with the objective
of providing a mapping of their relevant required skills, including
psychological characteristics. In addition, the analysis is then detailed by
considering the region of the organization that issues the job description, its
organizational size, and the seniority level of the open position in order to
understand their nuances. Finally, topic modeling is used to examine key
components and their relative significance in job descriptions. By employing a
novel methodology and considering contextual factors, we provide an innovative
understanding of the attitudinal traits that differentiate professionals. This
research contributes to talent management, recruitment practices, and
professional development initiatives, since it provides new figures and
perspectives to improve the effectiveness and success of Operations Management
and Supply Chain Management professionals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> Are State-of-the-Art Evaluator for Grammatical
  Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masamune Kobayashi, Masato Mita, Mamoru Komachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been reported to outperform existing
automatic evaluation metrics in some tasks, such as text summarization and
machine translation. However, there has been a lack of research on LLMs as
evaluators in grammatical error correction (GEC). In this study, we investigate
the performance of LLMs in GEC evaluation by employing prompts designed to
incorporate various evaluation criteria inspired by previous research. Our
extensive experimental results demonstrate that GPT-4 achieved Kendall's rank
correlation of 0.662 with human judgments, surpassing all existing methods.
Furthermore, in recent GEC evaluations, we have underscored the significance of
the LLMs scale and particularly emphasized the importance of fluency among
evaluation criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILLUMINER: <span class="highlight-title">Instruct</span>ion-tuned Large <span class="highlight-title">Language Models</span> as Few-shot Intent
  Classifier and Slot Filler <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth Bhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art intent classification (IC) and slot filling (SF) methods
often rely on data-intensive deep learning models, limiting their practicality
for industry applications. Large language models on the other hand,
particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable
zero-shot performance across various natural language tasks. This study
evaluates Instruct-LLMs on popular benchmark datasets for IC and SF,
emphasizing their capacity to learn from fewer examples. We introduce
ILLUMINER, an approach framing IC and SF as language generation tasks for
Instruct-LLMs, with a more efficient SF-prompting method compared to prior
work. A comprehensive comparison with multiple baselines shows that our
approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint
IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot
filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation
study demonstrates that parameter-efficient fine-tuning requires less than 6%
of training data to yield comparable performance with traditional full-weight
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Logistic Regression with High-order Features for Automatic
  Grammar Rule Extraction from Treebanks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Herrera, Caio Corro, Sylvain Kahane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Descriptive grammars are highly valuable, but writing them is time-consuming
and difficult. Furthermore, while linguists typically use corpora to create
them, grammar descriptions often lack quantitative data. As for formal
grammars, they can be challenging to interpret. In this paper, we propose a new
method to extract and explore significant fine-grained grammar patterns and
potential syntactic grammar rules from treebanks, in order to create an
easy-to-understand corpus-based grammar. More specifically, we extract
descriptions and rules across different languages for two linguistic phenomena,
agreement and word order, using a large search space and paying special
attention to the ranking order of the extracted rules. For that, we use a
linear classifier to extract the most salient features that predict the
linguistic phenomena under study. We associate statistical information to each
rule, and we compare the ranking of the model's results to those of other
quantitative and statistical measures. Our method captures both well-known and
less well-known significant grammar rules in Spanish, French, and Wolof.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in LREC-Coling 2024 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual
  Applications <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana, Yotaro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work on multilingual sentence embedding has demonstrated that the
efficient use of natural language inference (NLI) data to build
high-performance models can outperform conventional methods. However, the
potential benefits from the recent ``exponential'' growth of language models
with billions of parameters have not yet been fully explored. In this paper, we
introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based
multilingual sentence embedding, by extending Sentence T5, an existing
monolingual model. By employing the low-rank adaptation (LoRA) technique, we
have achieved a successful scaling of the model's size to 5.7 billion
parameters. We conducted experiments to evaluate the performance of sentence
embedding and verified that the method outperforms the NLI-based prior
approach. Furthermore, we also have confirmed a positive correlation between
the size of the model and its performance. It was particularly noteworthy that
languages with fewer resources or those with less linguistic similarity to
English benefited more from the parameter increase. Our model is available at
https://huggingface.co/pkshatech/m-ST5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Secure Disambiguating Neural Linguistic Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in provably secure neural linguistic steganography has
overlooked a crucial aspect: the sender must detokenize stegotexts to avoid
raising suspicion from the eavesdropper. The segmentation ambiguity problem,
which arises when using language models based on subwords, leads to occasional
decoding failures in all neural language steganography implementations based on
these models. Current solutions to this issue involve altering the probability
distribution of candidate words, rendering them incompatible with provably
secure steganography. We propose a novel secure disambiguation method named
SyncPool, which effectively addresses the segmentation ambiguity problem. We
group all tokens with prefix relationships in the candidate pool before the
steganographic embedding algorithm runs to eliminate uncertainty among
ambiguous tokens. To enable the receiver to synchronize the sampling process of
the sender, a shared cryptographically-secure pseudorandom number generator
(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does
not change the size of the candidate pool or the distribution of tokens and
thus is applicable to provably secure language steganography methods. We
provide theoretical proofs and experimentally demonstrate the applicability of
our solution to various languages and models, showing its potential to
significantly improve the reliability and security of neural linguistic
steganography systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MapGuide: A Simple yet Effective Method to Reconstruct Continuous
  Language from Brain Activities <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding continuous language from brain activity is a formidable yet
promising field of research. It is particularly significant for aiding people
with speech disabilities to communicate through brain signals. This field
addresses the complex task of mapping brain signals to text. The previous best
attempt reverse-engineered this process in an indirect way: it began by
learning to encode brain activity from text and then guided text generation by
aligning with predicted brain responses. In contrast, we propose a simple yet
effective method that guides text reconstruction by directly comparing them
with the predicted text embeddings mapped from brain activities. Comprehensive
experiments reveal that our method significantly outperforms the current
state-of-the-art model, showing average improvements of 77% and 54% on BLEU and
METEOR scores. We further validate the proposed modules through detailed
ablation studies and case analyses and highlight a critical correlation: the
more precisely we map brain activities to text embeddings, the better the text
reconstruction results. Such insight can simplify the task of reconstructing
language from brain activities for future work, emphasizing the importance of
improving brain-to-text-embedding mapping techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent <span class="highlight-title">Instruct</span>ion Giving and Following Policies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DGoT: Dynamic Graph of Thoughts for Scientific Abstract <span class="highlight-title">Generation</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Ning, Yutong Zhao, Yitong Liu, Hongwen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The method of training language models based on domain datasets has obtained
significant achievements in the task of generating scientific paper abstracts.
However, such models face problems of generalization and expensive training
costs. The use of large language models (LLMs) to solve the task of generating
paper abstracts saves the cost of model training. However, due to the
hallucination problem of LLM, it is often necessary to improve the reliability
of the results through multi-round query prompt approach such as Graph of
Thoughts (GoT), which also brings additional reasoning costs. In this paper, we
propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages
of the existing GoT prompt approach, but also dynamically adjust the graph
structure according to data characteristics while reducing model reasoning
cost. Experimental results show that our method's cost-effectiveness in
abstract generation tasks is only 43.7% to 56.4% of other multi-round query
prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with
  Adaptive Angular margin Contrastive Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on multimodal sentence embedding has proposed multimodal
contrastive learning and achieved promising results. However, by taking the
rest of the batch as negative samples without reviewing when forming
contrastive pairs, those studies encountered many suspicious and noisy negative
examples, significantly affecting the methods' overall performance. In this
work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning
of Sentence Embeddings), a novel approach that enhances the discrimination and
generalizability of multimodal representation and inherits the knowledge from
the teacher model to learn the difference between positive and negative
instances and via that, can detect noisy and wrong negative samples effectively
before they are calculated in the contrastive objective. Furthermore, to
overcome the limitation of modeling the variation within negative pairs, we
introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin
Supervised Contrastive Learning for Multimodal sentence embeddings), that
enhances the discriminative representation by strengthening the margin within
the angular space while capturing varying semantics within the negative.
Experimental results on widely used Semantic Textual Similarity (STS)
benchmarks demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Large <span class="highlight-title">Language Models</span> for Enhancing Psychiatric Interviews
  through Symptom Delineation and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-hee So, Joonhwan Chang, Eunji Kim, Junho Na, JiYeon Choi, Jy-yong Sohn, Byung-Hoon Kim, Sang Hui Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have accelerated their
usage in various domains. Given the fact that psychiatric interviews are
goal-oriented and structured dialogues between the professional interviewer and
the interviewee, it is one of the most underexplored areas where LLMs can
contribute substantial value. Here, we explore the use of LLMs for enhancing
psychiatric interviews, by analyzing counseling data from North Korean
defectors with traumatic events and mental health issues. Specifically, we
investigate whether LLMs can (1) delineate the part of the conversation that
suggests psychiatric symptoms and name the symptoms, and (2) summarize
stressors and symptoms, based on the interview dialogue transcript. Here, the
transcript data was labeled by mental health experts for training and
evaluation of LLMs. Our experimental results show that appropriately prompted
LLMs can achieve high performance on both the symptom delineation task and the
summarization task. This research contributes to the nascent field of applying
LLMs to psychiatric interview and demonstrates their potential effectiveness in
aiding mental health practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error
  Correction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Baoxin Wang, Yijun Liu, Dayong Wu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-correction is a critical problem in Chinese grammatical error correction
(CGEC) task. Recent work using model ensemble methods based on voting can
effectively mitigate over-correction and improve the precision of the GEC
system. However, these methods still require the output of several GEC systems
and inevitably lead to reduced error recall. In this light, we propose the
LM-Combiner, a rewriting model that can directly modify the over-correction of
GEC system outputs without a model ensemble. Specifically, we train the model
on an over-correction dataset constructed through the proposed K-fold cross
inference method, which allows it to directly generate filtered sentences by
combining the original and the over-corrected text. In the inference stage, we
directly take the original sentences and the output results of other systems as
input and then obtain the filtered sentences through LM-Combiner. Experiments
on the FCGEC dataset show that our proposed method effectively alleviates the
over-correction of the original system (+18.2 Precision) while ensuring the
error recall remains unchanged. Besides, we find that LM-Combiner still has a
good rewriting performance even with small parameters and few training data,
and thus can cost-effectively mitigate the over-correction of black-box GEC
systems (e.g., ChatGPT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCToolkit: A Unified Plug-and-Play <span class="highlight-title">Prompt</span> Compression Toolkit of Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt compression is an innovative method for efficiently condensing input
prompts while preserving essential information. To facilitate quick-start
services, user-friendly interfaces, and compatibility with common datasets and
metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is
a unified plug-and-play solution for compressing prompts in Large Language
Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and
metrics for comprehensive performance evaluation. PCToolkit boasts a modular
design, allowing for easy integration of new datasets and metrics through
portable and user-friendly interfaces. In this paper, we outline the key
components and functionalities of PCToolkit. We conducted evaluations of the
compressors within PCToolkit across various natural language tasks, including
reconstruction, summarization, mathematical problem-solving, question
answering, few-shot learning, synthetic tasks, code completion, boolean
expressions, multiple choice questions, and lies recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For open-source repository, see
  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity
  Recognition <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haris Riaz, Razvan-Gabriel Dumitru, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we revisit the problem of semi-supervised named entity
recognition (NER) focusing on extremely light supervision, consisting of a
lexicon containing only 10 examples per class. We introduce ELLEN, a simple,
fully modular, neuro-symbolic method that blends fine-tuned language models
with linguistic rules. These rules include insights such as ''One Sense Per
Discourse'', using a Masked Language Model as an unsupervised NER, leveraging
part-of-speech tags to identify and eliminate unlabeled entities as false
negatives, and other intuitions about classifier confidence scores in local and
global context. ELLEN achieves very strong performance on the CoNLL-2003
dataset when using the minimal supervision from the lexicon above. It also
outperforms most existing (and considerably more complex) semi-supervised NER
methods under the same supervision settings commonly used in the literature
(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a
zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and
achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also
achieves over 75% of the performance of a strong, fully supervised model
trained on gold data. Our code is available at:
https://github.com/hriaz17/ELLEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span> Rates Natural Language Explanation Quality Like Humans: But on
  Which Scales? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI becomes more integral in our lives, the need for transparency and
responsibility grows. While natural language explanations (NLEs) are vital for
clarifying the reasoning behind AI decisions, evaluating them through human
judgments is complex and resource-intensive due to subjectivity and the need
for fine-grained ratings. This study explores the alignment between ChatGPT and
human assessments across multiple scales (i.e., binary, ternary, and 7-Likert
scale). We sample 300 data instances from three NLE datasets and collect 900
human annotations for both informativeness and clarity scores as the text
quality measurement. We further conduct paired comparison experiments under
different ranges of subjectivity scores, where the baseline comes from 8,346
human annotations. Our results show that ChatGPT aligns better with humans in
more coarse-grained scales. Also, paired comparisons and dynamic prompting
(i.e., providing semantically similar examples in the prompt) improve the
alignment. This research advances our understanding of large language models'
capabilities to assess the text explanation quality in different configurations
for responsible AI development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by LREC-COLING 2024 main conference, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Biomedical Entities from Noisy Audio Transcripts <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Ebadi, Kellen Morgan, Adrian Tan, Billy Linares, Sheri Osborn, Emma Majors, Jeremy Davis, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) technology is fundamental in transcribing
spoken language into text, with considerable applications in the clinical
realm, including streamlining medical transcription and integrating with
Electronic Health Record (EHR) systems. Nevertheless, challenges persist,
especially when transcriptions contain noise, leading to significant drops in
performance when Natural Language Processing (NLP) models are applied. Named
Entity Recognition (NER), an essential clinical task, is particularly affected
by such noise, often termed the ASR-NLP gap. Prior works have primarily studied
ASR's efficiency in clean recordings, leaving a research gap concerning the
performance in noisy environments. This paper introduces a novel dataset,
BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,
focusing on extracting adverse drug reactions and mentions of entities from the
Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a
comprehensive collection of almost 2,000 clean and noisy recordings. In
addressing the noise challenge, we present an innovative transcript-cleaning
method using GPT4, investigating both zero-shot and few-shot methodologies. Our
study further delves into an error analysis, shedding light on the types of
errors in transcription software, corrections by GPT4, and the challenges GPT4
faces. This paper aims to foster improved understanding and potential solutions
for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Textual and Tabular Worlds for Fact Verification: A
  Lightweight, Attention-Based Model <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Dabbaghi Varnosfaderani, Canasai Kruengkrai, Ramin Yahyapour, Junichi Yamagishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FEVEROUS is a benchmark and research initiative focused on fact extraction
and verification tasks involving unstructured text and structured tabular data.
In FEVEROUS, existing works often rely on extensive preprocessing and utilize
rule-based transformations of data, leading to potential context loss or
misleading encodings. This paper introduces a simple yet powerful model that
nullifies the need for modality conversion, thereby preserving the original
evidence's context. By leveraging pre-trained models on diverse text and
tabular datasets and by incorporating a lightweight attention-based mechanism,
our approach efficiently exploits latent connections between different data
types, thereby yielding comprehensive and reliable verdict predictions. The
model's modular structure adeptly manages multi-modal information, ensuring the
integrity and authenticity of the original evidence are uncompromised.
Comparative analyses reveal that our approach exhibits competitive performance,
aligning itself closely with top-tier models on the FEVEROUS benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Action: Faithful and Multimodal Question Answering through
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Chain-of-Action (CoA) framework for multimodal and
retrieval-augmented Question-Answering (QA). Compared to the literature, CoA
overcomes two major challenges of current QA applications: (i) unfaithful
hallucination that is inconsistent with real-time or domain facts and (ii) weak
reasoning performance over compositional information. Our key contribution is a
novel reasoning-retrieval mechanism that decomposes a complex question into a
reasoning chain via systematic prompting and pre-designed actions.
Methodologically, we propose three types of domain-adaptable `Plug-and-Play'
actions for retrieving real-time information from heterogeneous sources. We
also propose a multi-reference faith score (MRFS) to verify and resolve
conflicts in the answers. Empirically, we exploit both public benchmarks and a
Web3 case study to demonstrate the capability of CoA over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> as Financial Data Annotators: A Study on
  Effectiveness and Efficiency <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting labeled datasets in finance is challenging due to scarcity of
domain experts and higher cost of employing them. While Large Language Models
(LLMs) have demonstrated remarkable performance in data annotation tasks on
general domain datasets, their effectiveness on domain specific datasets
remains underexplored. To address this gap, we investigate the potential of
LLMs as efficient data annotators for extracting relations in financial
documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,
and MPT Instruct) against expert annotators and crowdworkers. We demonstrate
that the current state-of-the-art LLMs can be sufficient alternatives to
non-expert crowdworkers. We analyze models using various prompts and parameter
settings and find that customizing the prompts for each relation group by
providing specific examples belonging to those groups is paramount.
Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify
outputs that may require expert attention. Finally, we perform an extensive
time, cost and error analysis and provide recommendations for the collection
and usage of automated annotations in domain-specific settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ For those who don't know (how) to ask: Building a <span class="highlight-title">dataset</span> of technology
  questions for digital newcomers <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the rise of large language models (LLMs) has created rich new
opportunities to learn about digital technology, many on the margins of this
technology struggle to gain and maintain competency due to lexical or
conceptual barriers that prevent them from asking appropriate questions.
Although there have been many efforts to understand factuality of LLM-created
content and ability of LLMs to answer questions, it is not well understood how
unclear or nonstandard language queries affect the model outputs. We propose
the creation of a dataset that captures questions of digital newcomers and
outsiders, utilizing data we have compiled from a decade's worth of one-on-one
tutoring. In this paper we lay out our planned efforts and some potential uses
of this dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AI4ED workshop at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span> Role-play <span class="highlight-title">Dataset</span>: Analysis of User Motives and Model
  Naturalness <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in interactive large language models like ChatGPT have
revolutionized various domains; however, their behavior in natural and
role-play conversation settings remains underexplored. In our study, we address
this gap by deeply investigating how ChatGPT behaves during conversations in
different settings by analyzing its interactions in both a normal way and a
role-play setting. We introduce a novel dataset of broad range of human-AI
conversations annotated with user motives and model naturalness to examine (i)
how humans engage with the conversational AI model, and (ii) how natural are AI
model responses. Our study highlights the diversity of user motives when
interacting with ChatGPT and variable AI naturalness, showing not only the
nuanced dynamics of natural conversations between humans and AI, but also
providing new avenues for improving the effectiveness of human-AI
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> for Education: A <span class="highlight-title">Survey</span> and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has brought in a new era of
possibilities in the realm of education. This survey paper summarizes the
various technologies of LLMs in educational settings from multifaceted
perspectives, encompassing student and teacher assistance, adaptive learning,
and commercial tools. We systematically review the technological advancements
in each perspective, organize related datasets and benchmarks, and identify the
risks and challenges associated with deploying LLMs in education. Furthermore,
we outline future research opportunities, highlighting the potential promising
directions. Our survey aims to provide a comprehensive technological picture
for educators, researchers, and policymakers to harness the power of LLMs to
revolutionize educational practices and foster a more effective personalized
learning environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>s and Language Barrier: A Cross-Lingual Legal QA Examination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the application of Generative Pre-trained
Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems
using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a
set of related legal articles that serve as context, the objective is to
determine whether the statement is legally valid, i.e., if it can be inferred
from the provided contextual articles or not, which is also known as an
entailment task. By benchmarking four different combinations of English and
Japanese prompts and data, we provide valuable insights into GPTs' performance
in multilingual legal QA scenarios, contributing to the development of more
efficient and accurate cross-lingual QA solutions in the legal domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP 2024, Kobe, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models with billions of parameters, such as GPT-3.5, GPT-4,
and LLaMA, are increasingly prevalent. Numerous studies have explored effective
prompting techniques to harness the power of these LLMs for various research
problems. Retrieval, specifically in the legal data domain, poses a challenging
task for the direct application of Prompting techniques due to the large number
and substantial length of legal articles. This research focuses on maximizing
the potential of prompting by placing it as the final phase of the retrieval
system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based
Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating
prompting techniques on LLMs into the retrieval system significantly improves
retrieval accuracy. However, error analysis reveals several existing issues in
the retrieval system that still need resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JURISIN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Convolutional <span class="highlight-title">Transformer</span>: Harmonizing Real vs. Complex
  Multi-View Spectral Operators for Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers used in vision have been investigated through diverse
architectures - ViT, PVT, and Swin. These have worked to improve the attention
mechanism and make it more efficient. Differently, the need for including local
information was felt, leading to incorporating convolutions in transformers
such as CPVT and CvT. Global information is captured using a complex Fourier
basis to achieve global token mixing through various methods, such as AFNO,
GFNet, and Spectformer. We advocate combining three diverse views of data -
local, global, and long-range dependence. We also investigate the simplest
global representation using only the real domain spectral representation -
obtained through the Hartley transform. We use a convolutional operator in the
initial layers to capture local information. Through these two contributions,
we are able to optimize and obtain a spectral convolution transformer (SCT)
that provides improved performance over the state-of-the-art methods while
reducing the number of parameters. Through extensive experiments, we show that
SCT-C-small gives state-of-the-art performance on the ImageNet dataset and
reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge
reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as
CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on
downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project
page is available on this webpage.\url{https://github.com/badripatro/sct}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COIG-CQIA: Quality is All You Need for Chinese <span class="highlight-title">Instruct</span>ion Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there have been significant advancements in large language models
(LLMs), particularly focused on the English language. These advancements have
enabled these LLMs to understand and execute complex instructions with
unprecedented accuracy and fluency. However, despite these advancements, there
remains a noticeable gap in the development of Chinese instruction tuning. The
unique linguistic features and cultural depth of the Chinese language pose
challenges for instruction tuning tasks. Existing datasets are either derived
from English-centric LLMs or are ill-suited for aligning with the interaction
patterns of real-world Chinese users. To bridge this gap, we introduce
COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to
build a diverse, wide-ranging instruction-tuning dataset to better align model
behavior with human interactions. To this end, we collect a high-quality
human-written corpus from various sources on the Chinese Internet, including
Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus
was rigorously filtered and carefully processed to form the COIG-CQIA dataset.
Furthermore, we train models of various scales on different subsets of CQIA,
following in-depth evaluation and analyses. The findings from our experiments
offer valuable insights for selecting and developing Chinese instruction-tuning
datasets. We also find that models trained on CQIA-Subset achieve competitive
results in human assessment as well as knowledge and security benchmarks. Data
are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervisory <span class="highlight-title">Prompt</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ghislain Billa, Min Oh, Liang Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Large Language Models (LLMs) relies heavily on the quality
of prompts, which are often manually engineered and task-specific, making them
costly and non-scalable. We propose a novel approach, Supervisory Prompt
Training (SPT). SPT automates the generation of highly effective prompts using
a dual LLM system. In this system, one LLM, the generator, performs a task
while the other, the corrector, provides feedback and generates improved
prompts. In contrast to earlier techniques, both the generator and corrector
collaboratively and continuously improve their prompts over time. We also
introduce the concept of \textit{impact scores} to measure the sentence-level
effectiveness of the prompts. Our method was tested on four benchmarks, testing
the level of hallucinations in LLMs. Notably, we were able to increase the
accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT
advances LLMs by refining prompts to enhance performance and reduce
hallucinations, offering an efficient and scalable alternative to traditional
model fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Syntactic and Semantic Proximity on Machine Translation
  with Back-Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Guerin, Shane Steinert-Threlkeld, Emmanuel Chemla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised on-the-fly back-translation, in conjunction with multilingual
pretraining, is the dominant method for unsupervised neural machine
translation. Theoretically, however, the method should not work in general. We
therefore conduct controlled experiments with artificial languages to determine
what properties of languages make back-translation an effective training
method, covering lexical, syntactic, and semantic properties. We find, contrary
to popular belief, that (i) parallel word frequency distributions, (ii)
partially shared vocabulary, and (iii) similar syntactic structure across
languages are not sufficient to explain the success of back-translation. We
show however that even crude semantic signal (similar lexical fields across
languages) does improve alignment of two languages through back-translation. We
conjecture that rich semantic dependencies, parallel across languages, are at
the root of the success of unsupervised methods based on back-translation.
Overall, the success of unsupervised machine translation was far from being
analytically guaranteed. Instead, it is another proof that languages of the
world share deep similarities, and we hope to show how to identify which of
these similarities can serve the development of unsupervised, cross-linguistic
tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Pre-train</span>ed Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enriching Word Usage Graphs with Cluster Definitions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariia Fedorova, Andrey Kutuzov, Nikolay Arefyev, Dominik Schlechtweg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dataset of word usage graphs (WUGs), where the existing WUGs for
multiple languages are enriched with cluster labels functioning as sense
definitions. They are generated from scratch by fine-tuned encoder-decoder
language models. The conducted human evaluation has shown that these
definitions match the existing clusters in WUGs better than the definitions
chosen from WordNet by two baseline systems. At the same time, the method is
straightforward to use and easy to extend to new languages. The resulting
enriched datasets can be extremely helpful for moving on to explainable
semantic change modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DORE: A <span class="highlight-title">Dataset</span> For Portuguese Definition <span class="highlight-title">Generation</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Frédéric Blain, Ruslan Mitkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Definition modelling (DM) is the task of automatically generating a
dictionary definition for a specific word. Computational systems that are
capable of DM can have numerous applications benefiting a wide range of
audiences. As DM is considered a supervised natural language generation
problem, these systems require large annotated datasets to train the machine
learning (ML) models. Several DM datasets have been released for English and
other high-resource languages. While Portuguese is considered a
mid/high-resource language in most natural language processing tasks and is
spoken by more than 200 million native speakers, there is no DM dataset
available for Portuguese. In this research, we fill this gap by introducing
DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more
than 100,000 definitions. We also evaluate several deep learning based DM
models on DORE and report the results. The dataset and the findings of this
paper will facilitate research and study of Portuguese in wider contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large <span class="highlight-title">Language Models</span> Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generator-Retriever-Generator Approach for Open-Domain Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMuRD: Annotated Arabic-English Receipt <span class="highlight-title">Dataset</span> for Key Information
  Extraction and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of key information from receipts is a complex task that
involves the recognition and extraction of text from scanned receipts. This
process is crucial as it enables the retrieval of essential content and
organizing it into structured documents for easy access and analysis. In this
paper, we present AMuRD, a novel multilingual human-annotated dataset
specifically designed for information extraction from receipts. This dataset
comprises $47,720$ samples and addresses the key challenges in information
extraction and item classification - the two critical aspects of data analysis
in the retail industry. Each sample includes annotations for item names and
attributes such as price, brand, and more. This detailed annotation facilitates
a comprehensive understanding of each item on the receipt. Furthermore, the
dataset provides classification into $44$ distinct product categories. This
classification feature allows for a more organized and efficient analysis of
the items, enhancing the usability of the dataset for various applications. In
our study, we evaluated various language model architectures, e.g., by
fine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional
results, with an F1 score of 97.43\% and accuracy of 94.99\% in information
extraction and classification, and an even higher F1 score of 98.51\% and
accuracy of 97.06\% observed in specific tasks. The dataset and code are
publicly accessible for further
researchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">BERT</span> Models to Carry Over a Coding System Developed on One
  Corpus to Another <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalma Galambos, Pál Zsámboki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes how we train BERT models to carry over a coding system
developed on the paragraphs of a Hungarian literary journal to another. The aim
of the coding system is to track trends in the perception of literary
translation around the political transformation in 1989 in Hungary. To evaluate
not only task performance but also the consistence of the annotation, moreover,
to get better predictions from an ensemble, we use 10-fold crossvalidation.
Extensive hyperparameter tuning is used to obtain the best possible results and
fair comparisons. To handle label imbalance, we use loss functions and metrics
robust to it. Evaluation of the effect of domain shift is carried out by
sampling a test set from the target domain. We establish the sample size by
estimating the bootstrapped confidence interval via simulations. This way, we
show that our models can carry over one annotation system to the target domain.
Comparisons are drawn to provide insights such as learning multilabel
correlations and confidence penalty improve resistance to domain shift, and
domain adaptation on OCR-ed text on another domain improves performance almost
to the same extent as that on the corpus under study. See our code at
https://codeberg.org/zsamboki/bert-annotator-ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized <span class="highlight-title">Instruct</span>ion <span class="highlight-title">Generation</span> of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How <span class="highlight-title">Language Models</span> Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decode Neural signal as Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-<span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Ljubešić, Taja Kuzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a collection of highly comparable web corpora of
Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,
covering thereby the whole spectrum of official languages in the South Slavic
language space. The collection of these corpora comprises a total of 13 billion
tokens of texts from 26 million documents. The comparability of the corpora is
ensured by a comparable crawling setup and the usage of identical crawling and
post-processing technology. All the corpora were linguistically annotated with
the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and
enriched with document-level genre information via the Transformer-based
multilingual X-GENRE classifier, which further enhances comparability at the
level of linguistic annotation and metadata enrichment. The genre-focused
analysis of the resulting corpora shows a rather consistent distribution of
genres throughout the seven corpora, with variations in the most prominent
genre categories being well-explained by the economic strength of each language
community. A comparison of the distribution of genre categories across the
corpora indicates that web corpora from less developed countries primarily
consist of news articles. Conversely, web corpora from economically more
developed countries exhibit a smaller proportion of news content, with a
greater presence of promotional and opinionated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the LREC-COLING 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large <span class="highlight-title">Language Models</span>: A
  Task-Solving Agent through Multi-Persona Self-Collaboration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a main conference paper at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Representational Disparities Between Multilingual and
  Bilingual Translation Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Verma, Kenton Murray, Kevin Duh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation has proven immensely useful for both
parameter efficiency and overall performance across many language pairs via
complete multilingual parameter sharing. However, some language pairs in
multilingual models can see worse performance than in bilingual models,
especially in the one-to-many translation setting. Motivated by their empirical
differences, we examine the geometric differences in representations from
bilingual models versus those from one-to-many multilingual models.
Specifically, we compute the isotropy of these representations using intrinsic
dimensionality and IsoScore, in order to measure how the representations
utilize the dimensions in their underlying vector space. Using the same
evaluation data in both models, we find that for a given language pair, its
multilingual model decoder representations are consistently less isotropic and
occupy fewer dimensions than comparable bilingual model decoder
representations. Additionally, we show that much of the anisotropy in
multilingual decoder representations can be attributed to modeling
language-specific information, therefore limiting remaining representational
capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENICE: Factuality Evaluation of summarization based on Natural language
  Inference and Claim Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scirè, Karim Ghonim, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text summarization, particularly with the advent of
Large Language Models (LLMs), have shown remarkable performance. However, a
notable challenge persists as a substantial number of automatically-generated
summaries exhibit factual inconsistencies, such as hallucinations. In response
to this issue, various approaches for the evaluation of consistency for
summarization have emerged. Yet, these newly-introduced metrics face several
limitations, including lack of interpretability, focus on short document
summaries (e.g., news articles), and computational impracticality, especially
for LLM-based metrics. To address these shortcomings, we propose Factuality
Evaluation of summarization based on Natural language Inference and Claim
Extraction (FENICE), a more interpretable and efficient factuality-oriented
metric. FENICE leverages an NLI-based alignment between information in the
source document and a set of atomic facts, referred to as claims, extracted
from the summary. Our metric sets a new state of the art on AGGREFACT, the
de-facto benchmark for factuality evaluation. Moreover, we extend our
evaluation to a more challenging setting by conducting a human annotation
process of long-form summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EthioLLM: Multilingual Large <span class="highlight-title">Language Models</span> for Ethiopian Languages
  with Task Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Design Space for Intelligent and Interactive Writing Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our era of rapid technological advancement, the research landscape for
writing assistants has become increasingly fragmented across various research
communities. We seek to address this challenge by proposing a design space as a
structured way to examine and explore the multidimensional space of intelligent
and interactive writing assistants. Through a large community collaboration, we
explore five aspects of writing assistants: task, user, technology,
interaction, and ecosystem. Within each aspect, we define dimensions (i.e.,
fundamental components of an aspect) and codes (i.e., potential options for
each dimension) by systematically reviewing 115 papers. Our design space aims
to offer researchers and designers a practical tool to navigate, comprehend,
and compare the various possibilities of writing assistants, and aid in the
envisioning and design of new writing assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAN-PL: a Novel Polish <span class="highlight-title">Dataset</span> of Banned Harmful and Offensive Content
  from Wykop.pl web service <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kołos, Inez Okulska, Kinga Głąbińska, Agnieszka Karlińska, Emilia Wiśnios, Paweł Ellerik, Andrzej Prałat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the Internet is flooded with hate, it is one of the main tasks for NLP
experts to master automated online content moderation. However, advancements in
this field require improved access to publicly available accurate and
non-synthetic datasets of social media content. For the Polish language, such
resources are very limited. In this paper, we address this gap by presenting a
new open dataset of offensive social media content for the Polish language. The
dataset comprises content from Wykop.pl, a popular online service often
referred to as the "Polish Reddit", reported by users and banned in the
internal moderation process. It contains a total of 691,662 posts and comments,
evenly divided into two categories: "harmful" and "neutral" ("non-harmful").
The anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000
for each class), along with preprocessing scripts have been made publicly
available. Furthermore the paper offers valuable insights into real-life
content moderation processes and delves into an analysis of linguistic features
and content characteristics of the dataset. Moreover, a comprehensive
anonymization procedure has been meticulously described and applied. The
prevalent biases encountered in similar datasets, including post-moderation and
pre-selection biases, are also discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for LREC-COLING 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyacinth6B: A large language model for Traditional Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Wei Song, Yin-Te Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model for Multi-objective Evolutionary Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the search operators need a carefully handcrafted
design with domain knowledge. Recently, some attempts have been made to replace
the manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
on new problems. To tackle the above challenges, this work investigates a novel
approach that leverages the powerful large language model (LLM) to design MOEA
operators. With proper prompt engineering, we successfully let a general LLM
serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a
zero-shot manner. In addition, by learning from the LLM behavior, we further
design an explicit white-box operator with randomness and propose a new version
of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.To
foster reproducibility and accessibility, the source code is
https://github.com/FeiLiu36/LLM4MOEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Topic Segmentation and Outline <span class="highlight-title">Generation</span> in Chinese Texts:
  The Paragraph-level Topic Representation, Corpus, and Benchmark <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Jiang, Weihao Liu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic segmentation and outline generation strive to divide a document into
coherent topic sections and generate corresponding subheadings, unveiling the
discourse topic structure of a document. Compared with sentence-level topic
structure, the paragraph-level topic structure can quickly grasp and understand
the overall context of the document from a higher level, benefitting many
downstream tasks such as summarization, discourse parsing, and information
retrieval. However, the lack of large-scale, high-quality Chinese
paragraph-level topic structure corpora restrained relative research and
applications. To fill this gap, we build the Chinese paragraph-level topic
representation, corpus, and benchmark in this paper. Firstly, we propose a
hierarchical paragraph-level topic structure representation with three layers
to guide the corpus construction. Then, we employ a two-stage man-machine
collaborative annotation method to construct the largest Chinese
Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also
build several strong baselines, including ChatGPT, to validate the
computability of CPTS on two fundamental tasks (topic segmentation and outline
generation) and preliminarily verified its usefulness for the downstream task
(discourse parsing).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spanish Resource Grammar version 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zamaraeva, Lorena S. Allegue, Carlos Gómez-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the latest version of the Spanish Resource Grammar (SRG), a
grammar of Spanish implemented in the HPSG formalism. Such grammars encode a
complex set of hypotheses about syntax making them a resource for empirical
testing of linguistic theory. They also encode a strict notion of
grammaticality which makes them a resource for natural language processing
applications in computer-assisted language learning. This version of the SRG
uses the recent version of the Freeling morphological analyzer and is released
along with an automatically created, manually verified treebank of 2,291
sentences. We explain the treebanking process, emphasizing how it is different
from treebanking with manual annotation and how it contributes to
empirically-driven development of syntactic theory. The treebanks' high level
of consistency and detail makes them a resource for training high-quality
semantic parsers and generally systems that benefit from precise and detailed
semantics. Finally, we present the grammar's coverage and overgeneration on 100
sentences from a learner corpus, a new research line related to developing
methodologies for robust empirical evaluation of hypotheses in second language
acquisition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion <span class="highlight-title">Generation</span> from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tandem <span class="highlight-title">Transformer</span>s for Inference Efficient LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autoregressive nature of conventional large language models (LLMs)
inherently limits inference speed, as tokens are generated sequentially. While
speculative and parallel decoding techniques attempt to mitigate this, they
face limitations: either relying on less accurate smaller models for generation
or failing to fully leverage the base LLM's representations.
  We introduce a novel architecture, Tandem transformers, to address these
issues. This architecture uniquely combines (1) a small autoregressive model
and (2) a large model operating in block mode (processing multiple tokens
simultaneously). The small model's predictive accuracy is substantially
enhanced by granting it attention to the large model's richer representations.
On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko
demonstrates a 3.3% improvement in next-token prediction accuracy over a
standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter
model with comparable downstream performance. We further incorporate the tandem
model within the speculative decoding (SPEED) framework where the large model
validates tokens from the small model. This ensures that the Tandem of
PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster
than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream
task accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering the Impact of <span class="highlight-title">Pretrain</span>ing Data on Large <span class="highlight-title">Language Models</span>
  through Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through pretraining on a corpus with various sources, Large Language Models
(LLMs) have gained impressive performance. However, the impact of each
component of the pretraining corpus remains opaque. As a result, the
organization of the pretraining corpus is still empirical and may deviate from
the optimal. To address this issue, we systematically analyze the impact of 48
datasets from 5 major categories of pretraining data of LLMs and measure their
impacts on LLMs using benchmarks about nine major categories of model
capabilities. Our analyses provide empirical results about the contribution of
multiple corpora on the performances of LLMs, along with their joint impact
patterns, including complementary, orthogonal, and correlational relationships.
We also identify a set of ``high-impact data'' such as Books that is
significantly related to a set of model capabilities. These findings provide
insights into the organization of data to support more efficient pretraining of
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-throughput Biomedical Relation Extraction for Semi-Structured Web
  Articles Empowered by Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songchi Zhou, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models'(LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as binary classifications
for large language models. Specifically, LLMs make the decision based on the
external corpus and its world knowledge, giving the reason for the judgment for
factual verification. This method is tailored for semi-structured web articles,
wherein we designate the main title as the tail entity and explicitly
incorporate it into the context, and the potential head entities are matched
based on a biomedical thesaurus. Moreover, lengthy contents are sliced into
text chunks, embedded, and retrieved with additional embedding models. Results:
Using an open-source LLM, we extracted 248659 relation triplets of three
distinct relation types from three reputable biomedical websites. To assess the
efficacy of the basic pipeline employed for biomedical relation extraction, we
curated a benchmark dataset annotated by a medical expert. Evaluation results
indicate that the pipeline exhibits performance comparable to that of GPT-4.
Case studies further illuminate challenges faced by contemporary LLMs in the
context of biomedical relation extraction for semi-structured web articles.
Conclusion: The proposed method has demonstrated its effectiveness in
leveraging the strengths of LLMs for high-throughput biomedical relation
extraction. Its adaptability is evident, as it can be seamlessly extended to
diverse semi-structured biomedical websites, facilitating the extraction of
various types of biomedical relations with ease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWESuite: Phonetic Word Embeddings and Tasks They Facilitate <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02541v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02541v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping words into a fixed-dimensional vector space is the backbone of modern
NLP. While most word embedding methods successfully encode semantic
information, they overlook phonetic information that is crucial for many tasks.
We develop three methods that use articulatory features to build phonetically
informed word embeddings. To address the inconsistent evaluation of existing
phonetic word embedding methods, we also contribute a task suite to fairly
evaluate past, current, and future methods. We evaluate both (1) intrinsic
aspects of phonetic word embeddings, such as word retrieval and correlation
with sound similarity, and (2) extrinsic performance on tasks such as rhyme and
cognate detection and sound analogies. We hope our task suite will promote
reproducibility and inspire future phonetic embedding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Document Embeddings via Self-Contrastive Bregman Divergence
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Saggau, Mina Rezaei, Bernd Bischl, Ilias Chalkidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning quality document embeddings is a fundamental problem in natural
language processing (NLP), information retrieval (IR), recommendation systems,
and search engines. Despite recent advances in the development of
transformer-based models that produce sentence embeddings with self-contrastive
learning, the encoding of long documents (Ks of words) is still challenging
with respect to both efficiency and quality considerations. Therefore, we train
Longfomer-based document encoders using a state-of-the-art unsupervised
contrastive learning method (SimCSE). Further on, we complement the baseline
method -- siamese neural network -- with additional convex neural networks
based on functional Bregman divergence aiming to enhance the quality of the
output document representations. We show that overall the combination of a
self-contrastive siamese network and our proposed neural Bregman network
outperforms the baselines in two linear classification settings on three long
document topic classification tasks from the legal and biomedical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, short paper at Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Clérice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose to evaluate the use of deep learning methods for
semantic classification at the sentence level to accelerate the process of
corpus building in the field of humanities and linguistics, a traditional and
time-consuming task. We introduce a novel corpus comprising around 2500
sentences spanning from 300 BCE to 900 CE including sexual semantics (medical,
erotica, etc.). We evaluate various sentence classification approaches and
different input embedding layers, and show that all consistently outperform
simple token-based searches. We explore the integration of idiolectal and
sociolectal metadata embeddings (centuries, author, type of writing), but find
that it leads to overfitting. Our results demonstrate the effectiveness of this
approach, achieving high precision and true positive rates (TPR) of
respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset
size on the model performances (420 instead of 2013), and show that, while our
models perform worse, they still offer a high enough precision and TPR, even
without MLM, respectively 69% and 51%. Given the result, we provide an analysis
of the attention mechanism as a supporting added value for humanists in order
to produce more data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-order Joint Constituency and Dependency Parsing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanggan Gu, Yang Hou, Zhefeng Wang, Xinyu Duan, Zhenghua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work revisits the topic of jointly parsing constituency and dependency
trees, i.e., to produce compatible constituency and dependency trees
simultaneously for input sentences, which is attractive considering that the
two types of trees are complementary in representing syntax. The original work
of Zhou and Zhao (2019) performs joint parsing only at the inference phase.
They train two separate parsers under the multi-task learning framework (i.e.,
one shared encoder and two independent decoders). They design an ad-hoc dynamic
programming-based decoding algorithm of $O(n^5)$ time complexity for finding
optimal compatible tree pairs. Compared to their work, we make progress in
three aspects: (1) adopting a much more efficient decoding algorithm of
$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,
instead of only at the inference phase, (3) proposing high-order scoring
components to promote constituent-dependency interaction. We conduct
experiments and analysis on seven languages, covering both rich-resource and
low-resource scenarios. Results and analysis show that joint modeling leads to
a modest overall performance boost over separate modeling, but substantially
improves the complete matching ratio of whole trees, thanks to the explicit
modeling of tree compatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Born With a Silver Spoon? Investigating Socioeconomic Bias in Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socioeconomic bias in society exacerbates disparities, influencing access to
opportunities and resources based on individuals' economic and social
backgrounds. This pervasive issue perpetuates systemic inequalities, hindering
the pursuit of inclusive progress as a society. In this paper, we investigate
the presence of socioeconomic bias, if any, in large language models. To this
end, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that
illustrate hypothetical scenarios that involve underprivileged people
performing ethically ambiguous actions due to their circumstances, and ask
whether the action is ethically justified. Further, this dataset has a
dual-labeling scheme and has been annotated by people belonging to both ends of
the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of
socioeconomic bias expressed in large language models and the variation of this
degree as a function of model size. We also perform qualitative analysis to
analyze the nature of this bias. Our analysis reveals that while humans
disagree on which situations require empathy toward the underprivileged, most
large language models are unable to empathize with the socioeconomically
underprivileged regardless of the situation. To foster further research in this
domain, we make SilverSpoon and our evaluation harness publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Detection and Tracking with Time-Aware Document Embeddings <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jiang, Doug Beeferman, Weiquan Mao, Deb Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time at which a message is communicated is a vital piece of metadata in
many real-world natural language processing tasks such as Topic Detection and
Tracking (TDT). TDT systems aim to cluster a corpus of news articles by event,
and in that context, stories that describe the same event are likely to have
been written at around the same time. Prior work on time modeling for TDT takes
this into account, but does not well capture how time interacts with the
semantic nature of the event. For example, stories about a tropical storm are
likely to be written within a short time interval, while stories about a movie
release may appear over weeks or months. In our work, we design a neural method
that fuses temporal and textual information into a single representation of
news documents for event detection. We fine-tune these time-aware document
embeddings with a triplet loss architecture, integrate the model into
downstream TDT systems, and evaluate the systems on two benchmark TDT data sets
in English. In the retrospective setting, we apply clustering algorithms to the
time-aware embeddings and show substantial improvements over baselines on the
News2013 data set. In the online streaming setting, we add our document encoder
to an existing state-of-the-art TDT pipeline and demonstrate that it can
benefit the overall performance. We conduct ablation studies on the time
representation and fusion algorithm strategies, showing that our proposed model
outperforms alternative strategies. Finally, we probe the model to examine how
it handles recurring events more effectively than previous TDT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New <span class="highlight-title">Generation</span> of Portuguese Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing <span class="highlight-title">Human Feedback</span> for <span class="highlight-title">Instruct</span>ional Visual Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Few-Shot Learning Focused <span class="highlight-title">Survey</span> on Recent Named Entity Recognition
  and Relation Classification Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakher Khalil Alqaaidi, Elika Bozorgi, Afsaneh Shams, Krzysztof Kochut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) and Relation Classification (RC) are important
steps in extracting information from unstructured text and formatting it into a
machine-readable format. We present a survey of recent deep learning models
that address named entity recognition and relation classification, with focus
on few-shot learning performance. Our survey is helpful for researchers in
knowing the recent techniques in text mining and extracting structured
information from raw text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good, but not always Fair: An Evaluation of Gender Bias for three
  commercial Machine Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sentiment Analysis in Finance: From <span class="highlight-title">Transformer</span>s Back to eXplainable
  Lexicons (XLex) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryan Rizinski, Hristijan Peshov, Kostadin Mishev, Milos Jovanovik, Dimitar Trajanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexicon-based sentiment analysis (SA) in finance leverages specialized,
manually annotated lexicons created by human experts to extract sentiment from
financial texts. Although lexicon-based methods are simple to implement and
fast to operate on textual data, they require considerable manual annotation
efforts to create, maintain, and update the lexicons. These methods are also
considered inferior to the deep learning-based approaches, such as transformer
models, which have become dominant in various NLP tasks due to their remarkable
performance. However, transformers require extensive data and computational
resources for both training and testing. Additionally, they involve significant
prediction times, making them unsuitable for real-time production environments
or systems with limited processing capabilities. In this paper, we introduce a
novel methodology named eXplainable Lexicons (XLex) that combines the
advantages of both lexicon-based methods and transformer models. We propose an
approach that utilizes transformers and SHapley Additive exPlanations (SHAP)
for explainability to learn financial lexicons. Our study presents four main
contributions. Firstly, we demonstrate that transformer-aided explainable
lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald
(LM) lexicon, reducing the human involvement in annotating, maintaining, and
updating the lexicons. Secondly, we show that the resulting lexicon outperforms
the standard LM lexicon in SA of financial datasets. Thirdly, we illustrate
that the lexicon-based approach is significantly more efficient in terms of
model speed and size compared to transformers. Lastly, the XLex approach is
inherently more interpretable than transformer models as lexicon models rely on
predefined rules, allowing for better insights into the results of SA and
making the XLex approach a viable tool for financial decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:
  https://ieeexplore.ieee.org/document/10380556</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4's assessment of its performance in a USMLE-based case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota, Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates GPT-4's assessment of its performance in healthcare
applications. A simple prompting technique was used to prompt the LLM with
questions taken from the United States Medical Licensing Examination (USMLE)
questionnaire and it was tasked to evaluate its confidence score before posing
the question and after asking the question. The questionnaire was categorized
into two groups-questions with feedback (WF) and questions with no feedback(NF)
post-question. The model was asked to provide absolute and relative confidence
scores before and after each question. The experimental findings were analyzed
using statistical tools to study the variability of confidence in WF and NF
groups. Additionally, a sequential analysis was conducted to observe the
performance variation for the WF and NF groups. Results indicate that feedback
influences relative confidence but doesn't consistently increase or decrease
it. Understanding the performance of LLM is paramount in exploring its utility
in sensitive areas like healthcare. This study contributes to the ongoing
discourse on the reliability of AI, particularly of LLMs like GPT-4, within
healthcare, offering insights into how feedback mechanisms might be optimized
to enhance AI-assisted medical education and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing <span class="highlight-title">Pre-train</span>ed Human <span class="highlight-title">Language Models</span>: Is it Better with Human
  Context as Groups, Individual Traits, or Both? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human context into language models is the next frontier for
human-centered natural language processing. Currently, two pre-training methods
exist: group-wise attributes (e.g., over-45-year-olds) or individual traits.
Group attributes are coarse -- not all 45-year-olds write the same way -- while
modeling individual traits allows for a more personalized representation, but
requires more complex modeling and data. So far, it is unclear which
pre-training approach benefits what tasks. We compare pre-training models with
human context via 1) group attributes, 2) individual users, and 3) a combined
approach on 5 user- and document-level tasks. We find that pre-training with
both group and individual features significantly improves the two user-level
regression tasks like age estimation and personality assessment. Pre-training
on individual users significantly improves the three document-level
classification tasks like stance and topic detection. It even does well for
downstream tasks without historical user data. Our results suggest both
approaches have specific use cases, opening new avenues for human-centered
language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothQuant: Accurate and Efficient Post-Training Quantization for Large
  <span class="highlight-title">Language Models</span> <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10438v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10438v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, existing methods cannot maintain accuracy and hardware efficiency at
the same time. We propose SmoothQuant, a training-free, accuracy-preserving,
and general-purpose post-training quantization (PTQ) solution to enable 8-bit
weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that
weights are easy to quantize while activations are not, SmoothQuant smooths the
activation outliers by offline migrating the quantization difficulty from
activations to weights with a mathematically equivalent transformation.
SmoothQuant enables an INT8 quantization of both weights and activations for
all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG,
Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x
speedup and 2x memory reduction for LLMs with negligible loss in accuracy.
SmoothQuant enables serving 530B LLM within a single node. Our work offers a
turn-key solution that reduces hardware costs and democratizes LLMs. Code is
available at https://github.com/mit-han-lab/smoothquant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023. First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The opportunities and risks of large <span class="highlight-title">language models</span> in mental health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global rates of mental health concerns are rising and there is increasing
realization that existing models of mental healthcare will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health-related tasks. In this review, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs application
to mental health and encourage adoption of strategies to mitigate these risks.
The urgent need for mental health support must be balanced with responsible
development, testing, and deployment of mental health LLMs. Especially critical
is ensuring that mental health LLMs are fine-tuned for mental health, enhance
mental health equity, adhere to ethical standards, and that people, including
those with lived experience with mental health concerns, are involved in all
stages from development through deployment. Prioritizing these efforts will
minimize potential harms to mental health and maximize the likelihood that LLMs
will positively impact mental health globally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leftover-Lunch: Advantage-based Offline Reinforcement Learning for
  <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14718v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14718v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) is the most prominent
method for Language Model (LM) alignment. However, RLHF is an unstable and
data-hungry process that continually requires new high-quality LM-generated
data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new
class of offline policy gradient algorithms that enable RL training on any
pre-existing data. By assuming the entire LM output sequence as a single
action, A-LoL allows incorporating sequence-level classifiers or human-designed
scoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL
only trains on positive advantage (leftover) data points, making it resilient
to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable
LM training recipe.
  We demonstrate the effectiveness of A-LoL and its variants with a set of four
different language generation tasks. We compare against both online RL (PPO)
and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL
baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant
(HHA), LMs trained with A-LoL methods achieve the highest diversity while also
being rated more safe and helpful than the baselines according to humans.
Additionally, in the remaining three tasks, A-LoL could optimize multiple
distinct reward functions even when using noisy or suboptimal training data.
  We also release our experimental code. https://github.com/abaheti95/LoL-RL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToXCL: A Unified Framework for Toxic Speech Detection and Explanation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of online toxic speech is a pertinent problem posing
threats to demographic groups. While explicit toxic speech contains offensive
lexical signals, implicit one consists of coded or indirect language.
Therefore, it is crucial for models not only to detect implicit toxic speech
but also to explain its toxicity. This draws a unique need for unified
frameworks that can effectively detect and explain implicit toxic speech. Prior
works mainly formulated the task of toxic speech detection and explanation as a
text generation problem. Nonetheless, models trained using this strategy can be
prone to suffer from the consequent error propagation problem. Moreover, our
experiments reveal that the detection results of such models are much lower
than those that focus only on the detection task. To bridge these gaps, we
introduce ToXCL, a unified framework for the detection and explanation of
implicit toxic speech. Our model consists of three modules: a (i) Target Group
Generator to generate the targeted demographic group(s) of a given post; an
(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit
toxic speech and is boosted by a (iii) Teacher Classifier via knowledge
distillation, and the decoder generates the necessary explanation. ToXCL
achieves new state-of-the-art effectiveness, and outperforms baselines
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who is bragging more online? A large scale analysis of bragging in
  social media <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mali Jin, Daniel Preoţiuc-Pietro, A. Seza Doğruöz, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bragging is the act of uttering statements that are likely to be positively
viewed by others and it is extensively employed in human communication with the
aim to build a positive self-image of oneself. Social media is a natural
platform for users to employ bragging in order to gain admiration, respect,
attention and followers from their audiences. Yet, little is known about the
scale of bragging online and its characteristics. This paper employs
computational sociolinguistics methods to conduct the first large scale study
of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,
temporal dynamics and impact of demographic factors. Our study shows that the
prevalence of bragging decreases over time within the same population of users.
In addition, younger, more educated and popular users in the U.S. are more
likely to brag. Finally, we conduct an extensive linguistics analysis to unveil
specific bragging themes associated with different user traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grammatical vs Spelling Error Correction: An Investigation into the
  Responsiveness of <span class="highlight-title">Transformer</span>-based <span class="highlight-title">Language Models</span> using BART and MarianMT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text continues to remain a relevant form of representation for information.
Text documents are created either in digital native platforms or through the
conversion of other media files such as images and speech. While the digital
native text is invariably obtained through physical or virtual keyboards,
technologies such as OCR and speech recognition are utilized to transform the
images and speech signals into text content. All these variety of mechanisms of
text generation also introduce errors into the captured text.
  This project aims at analyzing different kinds of error that occurs in text
documents. The work employs two of the advanced deep neural network-based
language models, namely, BART and MarianMT, to rectify the anomalies present in
the text. Transfer learning of these models with available dataset is performed
to finetune their capacity for error correction. A comparative study is
conducted to investigate the effectiveness of these models in handling each of
the defined error categories. It is observed that while both models can bring
down the erroneous sentences by 20+%, BART can handle spelling errors far
better (24.6%) than grammatical errors (8.8%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically Enriched Cross-Lingual Sentence Embeddings for
  Crisis-related Social Media Texts <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks such as semantic search and clustering on crisis-related social media
texts enhance our comprehension of crisis discourse, aiding decision-making and
targeted interventions. Pre-trained language models have advanced performance
in crisis informatics, but their contextual embeddings lack semantic
meaningfulness. Although the CrisisTransformers family includes a sentence
encoder to address the semanticity issue, it remains monolingual, processing
only English texts. Furthermore, employing separate models for different
languages leads to embeddings in distinct vector spaces, introducing challenges
when comparing semantic similarities between multi-lingual texts. Therefore, we
propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed
crisis-related social media texts for over 50 languages, such that texts with
similar meanings are in close proximity within the same vector space,
irrespective of language diversity. Results in sentence encoding and sentence
matching tasks are promising, suggesting these models could serve as robust
baselines when embedding multi-lingual crisis-related social media texts. The
models are publicly available at: https://huggingface.co/crisistransformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISCRAM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Grounding: Annotation and Analysis of Grounding Acts and
  Grounding Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful conversations often rest on common understanding, where all
parties are on the same page about the information being shared. This process,
known as conversational grounding, is crucial for building trustworthy dialog
systems that can accurately keep track of and recall the shared information.
The proficiencies of an agent in grounding the conveyed information
significantly contribute to building a reliable dialog system. Despite recent
advancements in dialog systems, there exists a noticeable deficit in their
grounding capabilities. Traum provided a framework for conversational grounding
introducing Grounding Acts and Grounding Units, but substantial progress,
especially in the realm of Large Language Models, remains lacking. To bridge
this gap, we present the annotation of two dialog corpora employing Grounding
Acts, Grounding Units, and a measure of their degree of grounding. We discuss
our key findings during the annotation and also provide a baseline model to
test the performance of current Language Models in categorizing the grounding
acts of the dialogs. Our work aims to provide a useful resource for further
research in making conversations with machines better understood and more
reliable in natural day-to-day collaborative dialogs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain
  Machine Generated Text Detection Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Language Models (LLMs) exhibit remarkable ability to generate
fluent content across a wide spectrum of user queries. However, this capability
has raised concerns regarding misinformation and personal information leakage.
In this paper, we present our methods for the SemEval2024 Task8, aiming to
detect machine-generated text across various domains in both mono-lingual and
multi-lingual contexts. Our study comprehensively analyzes various methods to
detect machine-generated text, including statistical, neural, and pre-trained
model approaches. We also detail our experimental setup and perform a in-depth
error analysis to evaluate the effectiveness of these methods. Our methods
obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for
subtask-B. Furthermore, we also highlight the challenges and essential factors
for consideration in future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large <span class="highlight-title">Language Models</span> (or Humans) Distill Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the potential of large language models (LLMs) to distill text:
to remove the textual traces of an undesired forbidden variable. We employ a
range of LLMs with varying architectures and training approaches to distill
text by identifying and removing information about the target variable while
preserving other relevant signals. Our findings shed light on the strengths and
limitations of LLMs in addressing the distillation and provide insights into
the strategies for leveraging these models in computational social science
investigations involving text data. In particular, we show that in the strong
test of removing sentiment, the statistical association between the processed
text and sentiment is still clearly detectable to machine learning classifiers
post-LLM-distillation. Furthermore, we find that human annotators also struggle
to distill sentiment while preserving other semantic content. This suggests
there may be limited separability between concept variables in some text
contexts, highlighting limitations of methods relying on text-level
transformations and also raising questions about the robustness of distillation
methods that achieve statistical independence in representation space if this
is difficult for human coders operating on raw text to attain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE: A Poincare Explanation Method for Fast Text Hierarchy <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models in NLP hinders their widespread
application. The research focus has shifted to Hierarchical Attribution (HA)
for its ability to model feature interactions. Recent works model
non-contiguous combinations with a time-costly greedy search in Eculidean
spaces, neglecting underlying linguistic information in feature
representations. In this work, we introduce a novel method, namely Poincar\'e
Explanation (PE), for modeling feature interactions using hyperbolic spaces in
an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a
framework to project the embeddings into hyperbolic spaces, which exhibit
better inductive biases for syntax and semantic hierarchical structures.
Eventually, we prove that the hierarchical clustering process in the projected
space could be viewed as building a minimum spanning tree and propose a time
efficient algorithm. Experimental results demonstrate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Information Extraction in Few-Shot Relation Classification
  through Contrastive Representation Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating relationships between entity pairs with limited labeled
instances poses a significant challenge in few-shot relation classification.
Representations of textual data extract rich information spanning the domain,
entities, and relations. In this paper, we introduce a novel approach to
enhance information extraction combining multiple sentence representations and
contrastive learning. While representations in relation classification are
commonly extracted using entity marker tokens, we argue that substantial
information within the internal model representations remains untapped. To
address this, we propose aligning multiple sentence representations, such as
the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.
Our method employs contrastive learning to extract complementary discriminative
information from these individual representations. This is particularly
relevant in low-resource settings where information is scarce. Leveraging
multiple sentence representations is especially effective in distilling
discriminative information for relation classification when additional
information, like relation descriptions, are not available. We validate the
adaptability of our approach, maintaining robust performance in scenarios that
include relation descriptions, and showcasing its flexibility to adapt to
different resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and <span class="highlight-title">Review</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Junhua, Tan Yong Keat, Fu Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Construction of a Large-Scale Corpus for Geoparsing Using
  Wikipedia Hyperlinks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyaki Ohno, Hirotaka Kameko, Keisuke Shirai, Taichi Nishimura, Shinsuke Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geoparsing is the task of estimating the latitude and longitude (coordinates)
of location expressions in texts. Geoparsing must deal with the ambiguity of
the expressions that indicate multiple locations with the same notation. For
evaluating geoparsing systems, several corpora have been proposed in previous
work. However, these corpora are small-scale and suffer from the coverage of
location expressions on general domains. In this paper, we propose Wikipedia
Hyperlink-based Location Linking (WHLL), a novel method to construct a
large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages
hyperlinks in Wikipedia to annotate multiple location expressions with
coordinates. With this method, we constructed the WHLL corpus, a new
large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,
each containing about 7.8 unique location expressions. 45.6% of location
expressions are ambiguous and refer to more than one location with the same
notation. In each article, location expressions of the article title and those
hyperlinks to other articles are assigned with coordinates. By utilizing
hyperlinks, we can accurately assign location expressions with coordinates even
with ambiguous location expressions in the texts. Experimental results show
that there remains room for improvement by disambiguating location expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Named Entity Recognition via Superposition Concept
  Discrimination <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Hongyu Lin, Xianpei Han, Yaojie Lu, Shanshan Jiang, Bin Dong, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot NER aims to identify entities of target types with only limited
number of illustrative instances. Unfortunately, few-shot NER is severely
challenged by the intrinsic precise generalization problem, i.e., it is hard to
accurately determine the desired target type due to the ambiguity stemming from
information deficiency. In this paper, we propose Superposition Concept
Discriminator (SuperCD), which resolves the above challenge via an active
learning paradigm. Specifically, a concept extractor is first introduced to
identify superposition concepts from illustrative instances, with each concept
corresponding to a possible generalization boundary. Then a superposition
instance retriever is applied to retrieve corresponding instances of these
superposition concepts from large-scale text corpus. Finally, annotators are
asked to annotate the retrieved instances and these annotated instances
together with original illustrative instances are used to learn FS-NER models.
To this end, we learn a universal concept extractor and superposition instance
retriever using a large-scale openly available knowledge bases. Experiments
show that SuperCD can effectively identify superposition concepts from
illustrative instances, retrieve superposition instances from large-scale
corpus, and significantly improve the few-shot NER performance with minimal
additional efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on How Attention Scores in the <span class="highlight-title">BERT</span> Model are Aware of Lexical
  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjun Jang, Sungjoo Byun, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines whether the attention scores between tokens in the BERT
model significantly vary based on lexical categories during the fine-tuning
process for downstream tasks. Drawing inspiration from the notion that in human
language processing, syntactic and semantic information is parsed differently,
we categorize tokens in sentences according to their lexical categories and
focus on changes in attention scores among these categories. Our hypothesis
posits that in downstream tasks that prioritize semantic information, attention
scores centered on content words are enhanced, while in cases emphasizing
syntactic information, attention scores centered on function words are
intensified. Through experimentation conducted on six tasks from the GLUE
benchmark dataset, we substantiate our hypothesis regarding the fine-tuning
process. Furthermore, our additional investigations reveal the presence of BERT
layers that consistently assign more bias to specific lexical categories,
irrespective of the task, highlighting the existence of task-agnostic lexical
category preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,
  Data, and Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu Ming Zhang, Xiaowei Ma, Xiangguo Lyu, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing interests to improve
clinical efficiency for medical diagnosis, owing to their unprecedented
performance in modelling natural language. Ensuring the safe and reliable
clinical applications, the evaluation of LLMs indeed becomes critical for
better mitigating the potential risks, e.g., hallucinations. However, current
evaluation methods heavily rely on labor-intensive human participation to
achieve human-preferred judgements. To overcome this challenge, we propose an
automatic evaluation paradigm tailored to assess the LLMs' capabilities in
delivering clinical services, e.g., disease diagnosis and treatment. The
evaluation paradigm contains three basic elements: metric, data, and algorithm.
Specifically, inspired by professional clinical practice pathways, we formulate
a LLM-specific clinical pathway (LCP) to define the clinical capabilities that
a doctor agent should possess. Then, Standardized Patients (SPs) from the
medical education are introduced as the guideline for collecting medical data
for evaluation, which can well ensure the completeness of the evaluation
procedure. Leveraging these steps, we develop a multi-agent framework to
simulate the interactive environment between SPs and a doctor agent, which is
equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the
behaviors of a doctor agent are in accordance with LCP. The above paradigm can
be extended to any similar clinical scenarios to automatically evaluate the
LLMs' medical capabilities. Applying such paradigm, we construct an evaluation
benchmark in the field of urology, including a LCP, a SPs dataset, and an
automated RAE. Extensive experiments are conducted to demonstrate the
effectiveness of the proposed approach, providing more insights for LLMs' safe
and reliable deployments in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KIT-19: A Comprehensive Korean <span class="highlight-title">Instruct</span>ion Toolkit on 19 Tasks for
  Fine-Tuning Korean Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjun Jang, Sungjoo Byun, Hyemi Jo, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Tuning on Large Language Models is an essential process for model
to function well and achieve high performance in specific tasks. Accordingly,
in mainstream languages such as English, instruction-based datasets are being
constructed and made publicly available. In the case of Korean, publicly
available models and datasets all rely on using the output of ChatGPT or
translating datasets built in English. In this paper, We introduce
\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.
\textit{KIT-19} is a dataset created in an instruction format, comprising 19
existing open-source datasets for Korean NLP tasks. In this paper, we train a
Korean Pretrained LLM using \textit{KIT-19} to demonstrate its effectiveness.
The experimental results show that the model trained on \textit{KIT-19}
significantly outperforms existing Korean LLMs. Based on the its quality and
empirical results, this paper proposes that \textit{KIT-19} has the potential
to make a substantial contribution to the future improvement of Korean LLMs'
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeS: Natural Language to Code Repository via Multi-Layer Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of large language models (LLMs) on code-related
tasks has shown the potential of fully automated software development. In light
of this, we introduce a new software engineering task, namely Natural Language
to code Repository (NL2Repo). This task aims to generate an entire code
repository from its natural language requirements. To address this task, we
propose a simple yet effective framework CodeS, which decomposes NL2Repo into
multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three
modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first
generates a repository's directory structure for given requirements;
FileSketcher then generates a file sketch for each file in the generated
structure; SketchFiller finally fills in the details for each function in the
generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry
out evaluations through both automated benchmarking and manual feedback
analysis. For benchmark-based evaluation, we craft a repository-oriented
benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For
feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30
participants in conducting empirical studies. Extensive experiments prove the
effectiveness and practicality of CodeS on the NL2Repo task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/NL2Code/CodeS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/BatsResearch/ex2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large <span class="highlight-title">Language Models</span> with Runtime Behavior of Program
  Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models for code (i.e., code LLMs) have shown strong code
understanding and generation capabilities. To evaluate the capabilities of code
LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval
and ClassEval). Code reasoning is one of the most essential abilities of code
LLMs, but existing benchmarks for code reasoning are not sufficient. Typically,
they focus on predicting the input and output of a program, ignoring the
evaluation of the intermediate behavior during program execution, as well as
the logical consistency (e.g., the model should not give the correct output if
the prediction of execution path is wrong) when performing the reasoning. To
address these problems, in this paper, we propose a framework, namely REval,
for evaluating code reasoning abilities and consistency of code LLMs with
program execution. We utilize existing code benchmarks and adapt them to new
benchmarks within our framework. A large-scale empirical study is conducted and
most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning
(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation
(i.e., an average IC score of 10.3). Evaluation results of current code LLMs
reflect the urgent need for the community to strengthen the code reasoning
capability of code LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstUPR : <span class="highlight-title">Instruct</span>ion-based Unsupervised Passage Reranking with Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This manuscript was originally written and submitted in
  June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based <span class="highlight-title">Language Models</span> <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is There a One-Model-Fits-All Approach to Information Extraction?
  Revisiting Task Definition Biases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Definition bias is a negative phenomenon that can mislead models. Definition
bias in information extraction appears not only across datasets from different
domains but also within datasets sharing the same domain. We identify two types
of definition bias in IE: bias among information extraction datasets and bias
between information extraction datasets and instruction tuning datasets. To
systematically investigate definition bias, we conduct three probing
experiments to quantitatively analyze it and discover the limitations of
unified information extraction and large language models in solving definition
bias. To mitigate definition bias in information extraction, we propose a
multi-stage framework consisting of definition bias measurement, bias-aware
fine-tuning, and task-specific bias mitigation. Experimental results
demonstrate the effectiveness of our framework in addressing definition bias.
Resources of this paper can be found at
https://github.com/EZ-hwh/definition-bias
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skews in the Phenomenon Space Hinder Generalization in Text-to-Image
  <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature on text-to-image generation is plagued by issues of faithfully
composing entities with relations. But there lacks a formal understanding of
how entity-relation compositions can be effectively learned. Moreover, the
underlying phenomenon space that meaningfully reflects the problem structure is
not well-defined, leading to an arms race for larger quantities of data in the
hope that generalization emerges out of large-scale pretraining. We hypothesize
that the underlying phenomenological coverage has not been proportionally
scaled up, leading to a skew of the presented phenomenon which harms
generalization. We introduce statistical metrics that quantify both the
linguistic and visual skew of a dataset for relational learning, and show that
generalization failures of text-to-image generation are a direct result of
incomplete or unbalanced phenomenological coverage. We first perform
experiments in a synthetic domain and demonstrate that systematically
controlled metrics are strongly predictive of generalization performance. Then
we move to natural images and show that simple distribution perturbations in
light of our theories boost generalization without enlarging the absolute data
size. This work informs an important direction towards quality-enhancing the
data diversity or balance orthogonal to scaling up the absolute size. Our
discussions point out important open questions on 1) Evaluation of generated
entity-relation compositions, and 2) Better models for reasoning with abstract
relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Facet <span class="highlight-title">Generation</span> with LLM Editing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joosung Lee, Jinhong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOCOST: State-Space Models for Long Document Abstractive Summarization <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models are a low-complexity alternative to transformers for
encoding long sequences and capturing long-term dependencies. We propose
LOCOST: an encoder-decoder architecture based on state-space models for
conditional text generation with long context inputs. With a computational
complexity of $O(L \log L)$, this architecture can handle significantly longer
sequences than state-of-the-art models that are based on sparse attention
patterns. We evaluate our model on a series of long document abstractive
summarization tasks. The model reaches a performance level that is 93-96%
comparable to the top-performing sparse transformers of the same size while
saving up to 50% memory during training and up to 87% during inference.
Additionally, LOCOST effectively handles input texts exceeding 600K tokens at
inference time, setting new state-of-the-art results on full-book summarization
and opening new perspectives for long input processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 7 tables, EACL 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointer-Generator Networks for Low-Resource Machine Translation: Don't
  Copy That! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niyati Bafna, Philipp Koehn, David Yarowsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformer-based neural machine translation (NMT) is very effective in
high-resource settings, many languages lack the necessary large parallel
corpora to benefit from it. In the context of low-resource (LR) MT between two
closely-related languages, a natural intuition is to seek benefits from
structural "shortcuts", such as copying subwords from the source to the target,
given that such language pairs often share a considerable number of identical
words, cognates, and borrowings. We test Pointer-Generator Networks for this
purpose for six language pairs over a variety of resource ranges, and find weak
improvements for most settings. However, analysis shows that the model does not
show greater improvements for closely-related vs. more distant language pairs,
or for lower resource ranges, and that the models do not exhibit the expected
usage of the mechanism for shared subwords. Our discussion of the reasons for
this behaviour highlights several general challenges for LR NMT, such as modern
tokenization strategies, noisy real-world conditions, and linguistic
complexities. We call for better scrutiny of linguistically motivated
improvements to NMT given the blackbox nature of Transformer models, as well as
for a focus on the above problems in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongHeads: Multi-Head Attention is Secretly a Long Context Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. LongHeads achieves 100% accuracy at the 128k
length on passkey retrieval task, verifying LongHeads's efficacy in extending
the usable context window for existing models. We release our code at
https://github.com/LuLuLuyi/LongHeads .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Relationship between Skill Neurons and Robustness in <span class="highlight-title">Prompt</span>
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ackermann, Xenia Ohmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Based on experiments with RoBERTa, it
has been suggested that Prompt Tuning activates specific neurons in the
transformer's feed-forward networks, that are highly predictive and selective
for the given task. In this paper, we study the robustness of Prompt Tuning in
relation to these "skill neurons", using RoBERTa and T5. We show that prompts
tuned for a specific task are transferable to tasks of the same type but are
not very robust to adversarial data. While prompts tuned for RoBERTa yield
below-chance performance on adversarial data, prompts tuned for T5 are slightly
more robust and retain above-chance performance in two out of three cases. At
the same time, we replicate the finding that skill neurons exist in RoBERTa and
further show that skill neurons also exist in T5. Interestingly, the skill
neurons of T5 determined on non-adversarial data are also among the most
predictive neurons on the adversarial data, which is not the case for RoBERTa.
We conclude that higher adversarial robustness may be related to a model's
ability to consistently activate the relevant skill neurons on adversarial
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis demonstrates that our enhanced
dataset poses a challenge for these systems. Moreover, we demonstrate that our
dataset can be effectively used for training purposes, enabling a system to
consistently acknowledge the user's backstory while also successfully moving
the task forward in the same turn, as confirmed by human evaluation. These
findings highlight the benefits of generating novel chitchat-TOD scenarios to
test TOD systems more thoroughly and improve their resilience to natural user
interferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties
  in Generative <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19531v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19531v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models are usually pretrained on large text corpus via
predicting the next token (i.e., sub-word/word/phrase) given the previous ones.
Recent works have demonstrated the impressive performance of large generative
language models on downstream tasks. However, existing generative language
models generally neglect an inherent challenge in text corpus during training,
i.e., the imbalance between frequent tokens and infrequent ones. It can lead a
language model to be dominated by common and easy-to-learn tokens, thereby
overlooking the infrequent and difficult-to-learn ones. To alleviate that, we
propose a MiLe Loss function for mitigating the bias of learning difficulties
with tokens. During training, it can dynamically assess the learning difficulty
of a to-be-learned token, according to the information entropy of the
corresponding predicted probability distribution over the vocabulary. Then it
scales the training loss adaptively, trying to lead the model to focus more on
the difficult-to-learn tokens. On the Pile dataset, we train generative
language models at different scales of 468M, 1.2B, and 6.7B parameters.
Experiments reveal that models incorporating the proposed MiLe Loss can gain
consistent performance improvement on downstream benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01479v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01479v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To share or not to share: What risks would laypeople accept to give
  sensitive data to differentially-private NLP systems? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Weiss, Frauke Kreuter, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the NLP community has adopted central differential privacy as a
go-to framework for privacy-preserving model training or data sharing, the
choice and interpretation of the key parameter, privacy budget $\varepsilon$
that governs the strength of privacy protection, remains largely arbitrary. We
argue that determining the $\varepsilon$ value should not be solely in the
hands of researchers or system developers, but must also take into account the
actual people who share their potentially sensitive data. In other words: Would
you share your instant messages for $\varepsilon$ of 10? We address this
research gap by designing, implementing, and conducting a behavioral experiment
(311 lay participants) to study the behavior of people in uncertain
decision-making situations with respect to privacy-threatening situations.
Framing the risk perception in terms of two realistic NLP scenarios and using a
vignette behavioral study help us determine what $\varepsilon$ thresholds would
lead lay people to be willing to share sensitive textual data - to our
knowledge, the first study of its kind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024; final camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthFC: Verifying Health Claims with Evidence-Based Medical
  Fact-Checking <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Vladika, Phillip Schneider, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital age, seeking health advice on the Internet has become a common
practice. At the same time, determining the trustworthiness of online medical
content is increasingly challenging. Fact-checking has emerged as an approach
to assess the veracity of factual claims using evidence from credible knowledge
sources. To help advance automated Natural Language Processing (NLP) solutions
for this task, in this paper we introduce a novel dataset HealthFC. It consists
of 750 health-related claims in German and English, labeled for veracity by
medical experts and backed with evidence from systematic reviews and clinical
trials. We provide an analysis of the dataset, highlighting its characteristics
and challenges. The dataset can be used for NLP tasks related to automated
fact-checking, such as evidence retrieval, claim verification, or explanation
generation. For testing purposes, we provide baseline systems based on
different approaches, examine their performance, and discuss the findings. We
show that the dataset is a challenging test bed with a high potential for
future use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Wang, D. Ma, D. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long text generation, such as novel writing and discourse-level translation
with extremely long contexts, presents significant challenges to current
language models. Existing methods mainly focus on extending the model's context
window through strategies like length extrapolation. However, these approaches
demand substantial hardware resources during the training and/or inference
phases. Our proposed method, Temp-Lora, introduces an alternative concept.
Instead of relying on the KV cache to store all context information, we embeds
this information directly into a temporary Lora module. In the process of long
text generation, this module is progressively trained with text generated
previously. This approach not only efficiently preserves contextual knowledge
but also prevents any permanent alteration to the model's parameters given that
the module is discarded post-generation. Extensive experiments on the PG19
language modeling benchmark and the GuoFeng discourse-level translation
benchmark validate the effectiveness of Temp-Lora. Our results show that: 1)
Temp-Lora substantially enhances generation quality for long text, as indicated
by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%
decrease in PPL along with a 113.2% increase in BLEU score on a subset of
GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text
generation methods, and 3) Temp-Lora can greatly reduce computational costs by
shortening the context window. For example, we can ensure a moderate
improvement in generation quality (a decrease of 3.8% in PPL) while enabling a
51.5% memory usage reduction and a 60.0% decrease in latency for inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Distillation of Table-based Reasoning Ability from LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their enormous
parameter size and extremely high requirements for compute power pose
challenges for their practical deployment. Recent research has revealed that
specific capabilities of LLMs, such as numerical reasoning, can be transferred
to smaller models through distillation. Some studies explore the potential of
leveraging LLMs to perform table-based reasoning. However, there has been no
prior work focusing on table reasoning skills in smaller models specifically
tailored for scientific table-to-text generation tasks. In this paper, we
propose a novel table-based reasoning distillation approach, with the aim of
distilling LLMs into tailored smaller models. Our experimental results have
shown that a 220 million parameter model (Flan-T5-base) fine-tuned using
distilled data, not only achieves a significant improvement compared to
traditionally fine-tuned baselines, but also surpasses specific LLMs on a
scientific table-to-text generation dataset. Our code is available at
https://github.com/Bernard-Yang/DistillTableCoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-<span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14566v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14566v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing
nuanced understanding and interpretation of visual data. The benchmark
comprises 346 images paired with 1129 questions, all meticulously crafted by
human experts. We introduce a novel structure for these visual questions
designed to establish control groups. This structure enables us to conduct a
quantitative analysis of the models' response tendencies, logical consistency,
and various failure modes. In our evaluation on HallusionBench, we benchmarked
15 different models, highlighting a 31.42% question-pair accuracy achieved by
the state-of-the-art GPT-4V. Notably, all other evaluated models achieve
accuracy below 16%. Moreover, our analysis not only highlights the observed
failure modes, including language hallucination and visual illusion, but also
deepens an understanding of these pitfalls. Our comprehensive case studies
within HallusionBench shed light on the challenges of hallucination and
illusion in LVLMs. Based on these insights, we suggest potential pathways for
their future improvement. The benchmark and codebase can be accessed at
https://github.com/tianyi-lab/HallusionBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Confidence Estimation and Calibration in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks in various domains. Despite their impressive performance,
they can be unreliable due to factual errors in their generations. Assessing
their confidence and calibrating them across different tasks can help mitigate
risks and enable LLMs to produce better generations. There has been a lot of
recent research aiming to address this, but there has been no comprehensive
overview to organize it and outline the main lessons learned. The present
survey aims to bridge this gap. In particular, we outline the challenges and we
summarize recent technical advancements for LLM confidence estimation and
calibration. We further discuss their applications and suggest promising
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 page, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring <span class="highlight-title">ChatGPT</span> and its Impact on Society 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Asraful Haque, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has been around for a while, but suddenly it has
received more attention than ever before. Thanks to innovations from companies
like Google, Microsoft, Meta, and other major brands in technology. OpenAI,
though, has triggered the button with its ground-breaking invention ChatGPT.
ChatGPT is a Large Language Model (LLM) based on Transformer architecture that
has the ability to generate human-like responses in a conversational context.
It uses deep learning algorithms to generate natural language responses to
input text. Its large number of parameters, contextual generation, and
open-domain training make it a versatile and effective tool for a wide range of
applications, from chatbots to customer service to language translation. It has
the potential to revolutionize various industries and transform the way we
interact with technology. However, the use of ChatGPT has also raised several
concerns, including ethical, social, and employment challenges, which must be
carefully considered to ensure the responsible use of this technology. The
article provides an overview of ChatGPT, delving into its architecture and
training process. It highlights the potential impacts of ChatGPT on the
society. In this paper, we suggest some approaches involving technology,
regulation, education, and ethics in an effort to maximize ChatGPT's benefits
while minimizing its negative impacts. This study is expected to contribute to
a greater understanding of ChatGPT and aid in predicting the potential changes
it may bring about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA: Sparse Linear Attention with Estimated Attention Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer architecture has driven breakthroughs in recent years on
tasks which require modeling pairwise relationships between sequential
elements, as is the case in natural language understanding. However, long
seqeuences pose a problem due to the quadratic complexity of the attention
operation. Previous research has aimed to lower the complexity by sparsifying
or linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix and often
require complete retraining from scratch. Furthermore, previous sparse and
linear approaches lose interpretability if they cannot produce full attention
matrices. To address these challenges, we propose SEA: Sparse linear attention
with an Estimated Attention mask. SEA estimates the attention matrix with
linear complexity via kernel-based linear attention, then subsequently creates
a sparse attention matrix with a top-k selection to perform a sparse attention
operation. For language modeling tasks (Wikitext2), previous linear and sparse
attention methods show roughly two-fold worse perplexity scores over the
quadratic OPT-1.3B baseline, while SEA achieves better perplexity than
OPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable
attention matrix. We believe that our work will have a large practical impact,
as it opens the possibility of running large transformers on resource-limited
devices with less memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Situated Natural Language Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Zhu, Haoming Jiang, Jingfeng Yang, Sreyashi Nag, Chao Zhang, Jie Huang, Yifan Gao, Frank Rudzicz, Bing Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language is among the most accessible tools for explaining decisions
to humans, and large pretrained language models (PLMs) have demonstrated
impressive abilities to generate coherent natural language explanations (NLE).
The existing NLE research perspectives do not take the audience into account.
An NLE can have high textual quality, but it might not accommodate audiences'
needs and preference. To address this limitation, we propose an alternative
perspective, \textit{situated} NLE. On the evaluation side, we set up automated
evaluation scores. These scores describe the properties of NLEs in lexical,
semantic, and pragmatic categories. On the generation side, we identify three
prompt engineering techniques and assess their applicability on the situations.
Situated NLE provides a perspective and facilitates further research on the
generation and evaluation of explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transfer Attack to Image Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model based Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11432v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11432v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Hai Lin, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> in Biomedical and Health Informatics: A
  Bibliometric <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have rapidly become important tools in
Biomedical and Health Informatics (BHI), enabling new ways to analyze data,
treat patients, and conduct research. This bibliometric review aims to provide
a panoramic view of how LLMs have been used in BHI by examining research
articles and collaboration networks from 2022 to 2023. It further explores how
LLMs can improve Natural Language Processing (NLP) applications in various BHI
areas like medical diagnosis, patient engagement, electronic health record
management, and personalized medicine. To do this, our bibliometric review
identifies key trends, maps out research networks, and highlights major
developments in this fast-moving field. Lastly, it discusses the ethical
concerns and practical challenges of using LLMs in BHI, such as data privacy
and reliable medical recommendations. Looking ahead, we consider how LLMs could
further transform biomedical research as well as healthcare delivery and
patient outcomes. This comprehensive review serves as a resource for
stakeholders in healthcare, including researchers, clinicians, and
policymakers, to understand the current state and future potential of LLMs in
BHI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented <span class="highlight-title">Generation</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Chouhan, Michael Gertz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved
  Phrase Graphs <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Peng, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the patent phrase similarity inference task, which measures the
semantic similarity between two patent phrases. As patent documents employ
legal and highly technical language, existing semantic textual similarity
methods that use localized contextual information do not perform satisfactorily
in inferring patent phrase similarity. To address this, we introduce a
graph-augmented approach to amplify the global contextual information of the
patent phrases. For each patent phrase, we construct a phrase graph that links
to its focal patents and a list of patents that are either cited by or cite
these focal patents. The augmented phrase embedding is then derived from
combining its localized contextual embedding with its global embedding within
the phrase graph. We further propose a self-supervised learning objective that
capitalizes on the retrieved topology to refine both the contextualized
embedding and the graph parameters in an end-to-end manner. Experimental
results from a unique patent phrase similarity dataset demonstrate that our
approach significantly enhances the representation of patent phrases, resulting
in marked improvements in similarity inference in a self-supervised fashion.
Substantial improvements are also observed in the supervised setting,
underscoring the potential benefits of leveraging retrieved phrase graph
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Sequence-to-Sequence Models for Abstractive Text Summarization
  Using Meta Heuristic Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Saxena, Ashutosh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human society transitions into the information age, reduction in our
attention span is a contingency, and people who spend time reading lengthy news
articles are decreasing rapidly and the need for succinct information is higher
than ever before. Therefore, it is essential to provide a quick overview of
important news by concisely summarizing the top news article and the most
intuitive headline. When humans try to make summaries, they extract the
essential information from the source and add useful phrases and grammatical
annotations from the original extract. Humans have a unique ability to create
abstractions. However, automatic summarization is a complicated problem to
solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive
text summarization has been ascending as far as prevalence. Numerous innovative
strategies have been proposed to develop the current seq2seq models further,
permitting them to handle different issues like saliency, familiarity, and
human lucidness and create excellent synopses. In this article, we aimed toward
enhancing the present architectures and models for abstractive text
summarization. The modifications have been aimed at fine-tuning
hyper-parameters, attempting specific encoder-decoder combinations. We examined
many experiments on an extensively used CNN/DailyMail dataset to check the
effectiveness of various models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQL-Encoder: Improving NL2SQL In-Context Learning Through a
  Context-Aware Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting structural similarity between queries is essential for selecting
examples in in-context learning models. However, assessing structural
similarity based solely on the natural language expressions of queries, without
considering SQL queries, presents a significant challenge. This paper explores
the significance of this similarity metric and proposes a model for accurately
estimating it. To achieve this, we leverage a dataset comprising 170k question
pairs, meticulously curated to train a similarity prediction model. Our
comprehensive evaluation demonstrates that the proposed model adeptly captures
the structural similarity between questions, as evidenced by improvements in
Kendall-Tau distance and precision@k metrics. Notably, our model outperforms
strong competitive embedding models from OpenAI and Cohere. Furthermore,
compared to these competitive models, our proposed encoder enhances the
downstream performance of NL2SQL models in 1-shot in-context learning scenarios
by 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% for
CodeLlama-13B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language
  Models <span class="chip">NAACL-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, Yvette Graham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is widely studied for its
effectiveness and efficiency in the era of large language models. Low-rank
adaptation (LoRA) has demonstrated commendable performance as a popular and
representative method. However, it is implemented with a fixed intrinsic rank
that might not be the ideal setting for the downstream tasks. Recognizing the
need for more flexible downstream task adaptation, we extend the methodology of
LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)
that enables dynamic adjustments to the intrinsic rank during the adaptation
process. First, we propose a novel method, AB-LoRA, that can effectively
estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we
gradually prune abundant and negatively impacting LoRA ranks and allocate the
pruned LoRA budgets to important Transformer modules needing higher ranks. We
have conducted experiments on various tasks, and the experimental results
demonstrate that our ALoRA method can outperform the recent baselines with
comparable tunable parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace Defense: Discarding Adversarial Perturbations by Learning a
  Subspace for Clean Signals <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks
that place carefully crafted perturbations on normal examples to fool DNNs. To
better understand such attacks, a characterization of the features carried by
adversarial examples is needed. In this paper, we tackle this challenge by
inspecting the subspaces of sample features through spectral analysis. We first
empirically show that the features of either clean signals or adversarial
perturbations are redundant and span in low-dimensional linear subspaces
respectively with minimal overlap, and the classical low-dimensional subspace
projection can suppress perturbation features out of the subspace of clean
signals. This makes it possible for DNNs to learn a subspace where only
features of clean signals exist while those of perturbations are discarded,
which can facilitate the distinction of adversarial examples. To prevent the
residual perturbations that is inevitable in subspace learning, we propose an
independence criterion to disentangle clean signals from perturbations.
Experimental results show that the proposed strategy enables the model to
inherently suppress adversaries, which not only boosts model robustness but
also motivates new directions of effective adversarial defense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-<span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjoo Byun, Jiseung Hong, Sumin Park, Dongjun Jang, Jean Seo, Minseok Kim, Chaeyoung Oh, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) plays a pivotal role in medical Natural
Language Processing (NLP). Yet, there has not been an open-source medical NER
dataset specifically for the Korean language. To address this, we utilized
ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which
we are now presenting to the public. With the KBMC dataset, we noticed an
impressive 20% increase in medical NER performance compared to models trained
on general Korean NER datasets. This research underscores the significant
benefits and importance of using specialized tools and datasets, like ChatGPT,
to enhance language processing in specialized fields such as healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happens to a <span class="highlight-title">Dataset</span> Transformed by a Projection-based Concept
  Removal Method? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the behavior of methods that use linear projections to remove
information about a concept from a language representation, and we consider the
question of what happens to a dataset transformed by such a method. A
theoretical analysis and experiments on real-world and synthetic data show that
these methods inject strong statistical dependencies into the transformed
datasets. After applying such a method, the representation space is highly
structured: in the transformed space, an instance tends to be located near
instances of the opposite label. As a consequence, the original labeling can in
some cases be reconstructed by applying an anti-clustering method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Little Leak Will Sink a Great Ship: <span class="highlight-title">Survey</span> of Transparency for Large
  <span class="highlight-title">Language Models</span> from Start to Finish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kaneko, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on massive web-crawled corpora. This
poses risks of leakage, including personal information, copyrighted texts, and
benchmark datasets. Such leakage leads to undermining human trust in AI due to
potential unauthorized generation of content or overestimation of performance.
We establish the following three criteria concerning the leakage issues: (1)
leakage rate: the proportion of leaked data in training data, (2) output rate:
the ease of generating leaked data, and (3) detection rate: the detection
performance of leaked versus non-leaked data. Despite the leakage rate being
the origin of data leakage issues, it is not understood how it affects the
output rate and detection rate. In this paper, we conduct an experimental
survey to elucidate the relationship between the leakage rate and both the
output rate and detection rate for personal information, copyrighted texts, and
benchmark data. Additionally, we propose a self-detection approach that uses
few-shot learning in which LLMs detect whether instances are present or absent
in their training data, in contrast to previous methods that do not employ
explicit learning. To explore the ease of generating leaked information, we
create a dataset of prompts designed to elicit personal information,
copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs
produce leaked information in most cases despite less such data in their
training set. This indicates even small amounts of leaked data can greatly
affect outputs. Our self-detection method showed superior performance compared
to existing detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Lexical Ambiguity Detection and Word Sense Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miuru Abeysiriwardana, Deshan Sumanathilaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores techniques that focus on understanding and resolving
ambiguity in language within the field of natural language processing (NLP),
highlighting the complexity of linguistic phenomena such as polysemy and
homonymy and their implications for computational models. Focusing extensively
on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from
deep learning techniques to leveraging lexical resources and knowledge graphs
like WordNet. The paper introduces cutting-edge methodologies like word sense
extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy
by predicting new word senses. It examines specific applications in biomedical
disambiguation and language specific optimisation and discusses the
significance of cognitive metaphors in discourse analysis. The research
identifies persistent challenges in the field, such as the scarcity of sense
annotated corpora and the complexity of informal clinical texts. It concludes
by suggesting future directions, including using large language models, visual
WSD, and multilingual WSD systems, emphasising the ongoing evolution in
addressing lexical complexities in NLP. This thinking perspective highlights
the advancement in this field to enable computers to understand language more
accurately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 3 tables, Accepted by 20th IEEE International
  Colloquium on Signal Processing & its Applications (CSPA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WangchanLion and WangchanX MRC Eval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn Payoungkhamdee, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report describes the development of WangchanLion, an
instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in
the Thai language. Our model is based on SEA-LION and a collection of
instruction following datasets. To promote open research and reproducibility,
we publically release all training data, code, and the final model weights
under the Apache-2 license. To assess the contextual understanding capability,
we conducted extensive experimental studies using two Thai MRC datasets, XQuAD
and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to
comprehend the context and produce an answer faithful to the reference one in
0-shot and 1-shot settings. In addition, our evaluation goes beyond the
traditional MRC. We propose a new evaluation scheme assessing the answer's
correctness, helpfulness, conciseness, and contextuality. Evaluation results
provide insight into how we can improve our model in the future. Our code is
public at https://github.com/vistec-AI/WangchanLion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Label <span class="highlight-title">Dataset</span> of French Fake News: Human and Machine Insights <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Icard, François Maine, Morgane Casanova, Géraud Faye, Julien Chanson, Guillaume Gadek, Ghislain Atemezing, François Bancilhon, Paul Égré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of
French press considered unreliable by expert agencies, annotated using 11
labels by 8 annotators. By collecting more labels than usual, by more
annotators than is typically done, we can identify features that humans
consider as characteristic of fake news, and compare them to the predictions of
automated classifiers. We present a topic and genre analysis using Gate Cloud,
indicative of the prevalence of satire-like text in the corpus. We then use the
subjectivity analyzer VAGO, and a neural version of it, to clarify the link
between ascriptions of the label Subjective and ascriptions of the label Fake
News. The annotated dataset is available online at the following url:
https://github.com/obs-info/obsinfox
  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,
Exaggeration, French Press
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper to appear in the Proceedings of the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Compiler for Arabic Programming Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argument Quality Assessment in the Age of <span class="highlight-title">Instruct</span>ion-Following Large
  <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computational treatment of arguments on controversial issues has been
subject to extensive NLP research, due to its envisioned impact on opinion
formation, decision making, writing education, and the like. A critical task in
any such application is the assessment of an argument's quality - but it is
also particularly challenging. In this position paper, we start from a brief
survey of argument quality research, where we identify the diversity of quality
notions and the subjectiveness of their perception as the main hurdles towards
substantial progress on argument quality assessment. We argue that the
capabilities of instruction-following large language models (LLMs) to leverage
knowledge across contexts enable a much more reliable assessment. Rather than
just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they
need to be instructed systematically with argumentation theories and scenarios
as well as with ways to solve argument-related problems. We discuss the
real-world opportunities and ethical issues emerging thereby.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qibo: A Large Language Model for Traditional Chinese Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyi Zhang, Xin Wang, Zhaopeng Meng, Yongzhe Jia, Dawei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Artificial Intelligence, Large Language Models (LLMs) have
demonstrated significant advances in user intent understanding and response in
a number of specialized domains, including medicine, law, and finance. However,
in the unique domain of traditional Chinese medicine (TCM), the performance
enhancement of LLMs is challenged by the essential differences between its
theories and modern medicine, as well as the lack of specialized corpus
resources. In this paper, we aim to construct and organize a professional
corpus in the field of TCM, to endow the large model with professional
knowledge that is characteristic of TCM theory, and to successfully develop the
Qibo model based on LLaMA, which is the first LLM in the field of TCM to
undergo a complete training process from pre-training to Supervised Fine-Tuning
(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for
evaluating the performance of LLMs, which is a specialized tool for evaluating
the performance of LLMs in the TCM domain. This tool will provide an important
basis for quantifying and comparing the understanding and application
capabilities of different models in the field of traditional Chinese medicine,
and provide guidance for future research directions and practical applications
of intelligent assistants for traditional Chinese medicine. Finally, we
conducted sufficient experiments to prove that Qibo has good performance in the
field of traditional Chinese medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monotonic Paraphrasing Improves Generalization of Language Model
  <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of large language models (LLMs) may vary with different prompts
or instructions of even the same task. One commonly recognized factor for this
phenomenon is the model's familiarity with the given prompt or instruction,
which is typically estimated by its perplexity. However, finding the prompt
with the lowest perplexity is challenging, given the enormous space of possible
prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),
an end-to-end decoding strategy that paraphrases given prompts or instructions
into their lower perplexity counterparts based on an ensemble of a paraphrase
LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or
instruction executor) that constrains the generation for lower perplexity. The
ensemble decoding process can efficiently paraphrase the original prompt
without altering its semantic meaning, while monotonically decreasing the
perplexity of each generation as calculated by the target LM. We explore in
detail both greedy and search-based decoding as two alternative decoding
schemes of MonoPara. Notably, MonoPara does not require any training and can
monotonically lower the perplexity of the paraphrased prompt or instruction,
leading to improved performance of zero-shot LM prompting as evaluated on a
wide selection of tasks. In addition, MonoPara is also shown to effectively
improve LMs' generalization on perturbed and unseen task instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Node Classification via Semantic-Structural Attention-Enhanced Graph
  Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph data, also known as complex network data, is omnipresent across various
domains and applications. Prior graph neural network models primarily focused
on extracting task-specific structural features through supervised learning
objectives, but they fell short in capturing the inherent semantic and
structural features of the entire graph. In this paper, we introduce the
semantic-structural attention-enhanced graph convolutional network (SSA-GCN),
which not only models the graph structure but also extracts generalized
unsupervised features to enhance vertex classification performance. The
SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic
information through unsupervised feature extraction from a knowledge graph
perspective; secondly, it obtains structural information through unsupervised
feature extraction from a complex network perspective; and finally, it
integrates these features through a cross-attention mechanism. By leveraging
these features, we augment the graph convolutional network, thereby enhancing
the model's generalization capabilities. Our experiments on the Cora and
CiteSeer datasets demonstrate the performance improvements achieved by our
proposed method. Furthermore, our approach also exhibits excellent accuracy
under privacy settings, making it a robust and effective solution for graph
data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral
  Therapy-based Mental Health Question Answering <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in artificial intelligence highlight the potential of
language models in psychological health support. While models trained on data
from mental health service platform have achieved preliminary success,
challenges persist in areas such as data scarcity, quality, and ensuring a
solid foundation in psychological techniques. To address these challenges, this
study introduces a novel approach to enhance the precision and efficacy of
psychological support through large language models. Specifically, we design a
specific prompt derived from principles of Cognitive Behavioral Therapy (CBT)
and have generated the CBT QA dataset, specifically for Chinese psychological
health Q&A based on CBT structured intervention strategies. Unlike previous
methods, our dataset emphasizes professional and structured response. Utilizing
this dataset, we fine-tuned the large language model, giving birth to CBT-LLM,
the large-scale language model specifically designed for Cognitive Behavioral
Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in
generating structured, professional, and highly relevant responses in
psychological health support tasks, showcasing its practicality and quality.
The model is available on Hugging Face:
https://huggingface.co/Hongbin37/CBT-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIMCV-R: A Landmark <span class="highlight-title">Dataset</span> for 3D CT Text-Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning integration of 3D medical imaging into healthcare has led to a
substantial increase in the workload of medical professionals. To assist
clinicians in their diagnostic processes and alleviate their workload, the
development of a robust system for retrieving similar case studies presents a
viable solution. While the concept holds great promise, the field of 3D medical
text-image retrieval is currently limited by the absence of robust evaluation
benchmarks and curated datasets. To remedy this, our study presents a
groundbreaking dataset, BIMCV-R (This dataset will be released upon
acceptance.), which includes an extensive collection of 8,069 3D CT volumes,
encompassing over 2 million slices, paired with their respective radiological
reports. Expanding upon the foundational work of our dataset, we craft a
retrieval strategy, MedFinder. This approach employs a dual-stream network
architecture, harnessing the potential of large language models to advance the
field of medical image retrieval beyond existing text-image retrieval
solutions. It marks our preliminary step towards developing a system capable of
facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking and Improving Data Credibility: A Study with <span class="highlight-title">Dataset</span>s for
  Training Harmless <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown promise in various tasks but can be affected by
undesired data during training, fine-tuning, or alignment. For example, if some
unsafe conversations are wrongly annotated as safe ones, the model fine-tuned
on these samples may be harmful. Therefore, the correctness of annotations,
i.e., the credibility of the dataset, is important. This study focuses on the
credibility of real-world datasets, including the popular benchmarks Jigsaw
Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that
can be used for training a harmless language model. Given the cost and
difficulty of cleaning these datasets by humans, we introduce a systematic
framework for evaluating the credibility of datasets, identifying label errors,
and evaluating the influence of noisy labels in the curated language data,
specifically focusing on unsafe comments and conversation classification. With
the framework, we find and fix an average of 6.16% label errors in 11 datasets
constructed from the above benchmarks. The data credibility and downstream
learning performance can be remarkably improved by directly fixing label
errors, indicating the significance of cleaning existing real-world datasets.
We provide an open-source tool, Docta, for data cleaning at
https://github.com/Docta-ai/docta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Potential of <span class="highlight-title">ChatGPT</span>: A Comprehensive Exploration of its
  Applications, Advantages, Limitations, and Future Directions in Natural
  Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02017v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02017v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Hariri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating <span class="highlight-title">Prompt</span> Complexity for Zero-Shot Classification: A Study of
  Large <span class="highlight-title">Language Models</span> in Computational Social Science <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos Aletras, Carolina Scarton, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned Large Language Models (LLMs) have exhibited impressive
language understanding and the capacity to generate responses that follow
specific prompts. However, due to the computational demands associated with
training these models, their applications often adopt a zero-shot setting. In
this paper, we evaluate the zero-shot performance of two publicly accessible
LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social
Science classification tasks, while also investigating the effects of various
prompting strategies. Our experiments investigate the impact of prompt
complexity, including the effect of incorporating label definitions into the
prompt; use of synonyms for label names; and the influence of integrating past
memories during foundation model training. The findings indicate that in a
zero-shot setting, current LLMs are unable to match the performance of smaller,
fine-tuned baseline transformer models (such as BERT-large). Additionally, we
find that different prompting strategies can significantly affect
classification accuracy, with variations in accuracy and F1 scores exceeding
10\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-TPT: Calibrated Test-Time <span class="highlight-title">Prompt</span> Tuning for Vision-<span class="highlight-title">Language Models</span> via
  Text Feature Dispersion <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration, which is a crucial
aspect for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data. The code is publicly accessible at
https://github.com/hee-suk-yoon/C-TPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLEX: Continuous Length Extrapolation for Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Large Language Models (LLMs) are pioneering advances in
many natural language processing tasks, however, their exceptional capabilities
are restricted within the preset context window of Transformer. Position
Embedding (PE) scaling methods, while effective in extending the context window
to a specific length, demonstrate either notable limitations in their
extrapolation abilities or sacrificing partial performance within the context
window. Length extrapolation methods, although theoretically capable of
extending the context window beyond the training sequence length, often
underperform in practical long-context applications. To address these
challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We
generalise the PE scaling approaches to model the continuous dynamics by
ordinary differential equations over the length scaling factor, thereby
overcoming the constraints of current PE scaling methods designed for specific
lengths. Moreover, by extending the dynamics to desired context lengths beyond
the training sequence length, CLEX facilitates the length extrapolation with
impressive performance in practical tasks. We demonstrate that CLEX can be
seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such
as LLaMA and GPT-NeoX, with negligible impact on training and inference
latency. Experimental results reveal that CLEX can effectively extend the
context window to over 4x or almost 8x training length, with no deterioration
in performance. Furthermore, when evaluated on the practical LongBench
benchmark, our model trained on a 4k length exhibits competitive performance
against state-of-the-art open-source models trained on context lengths up to
32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining Temporalities on Stance Detection towards COVID-19 Vaccination <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Mali Jin, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies have highlighted the importance of vaccination as an
effective strategy to control the transmission of the COVID-19 virus. It is
crucial for policymakers to have a comprehensive understanding of the public's
stance towards vaccination on a large scale. However, attitudes towards
COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved
over time on social media. Thus, it is necessary to account for possible
temporal shifts when analysing these stances. This study aims to examine the
impact of temporal concept drift on stance detection towards COVID-19
vaccination on Twitter. To this end, we evaluate a range of transformer-based
models using chronological (splitting the training, validation, and test sets
in order of time) and random splits (randomly splitting these three sets) of
social media data. Our findings reveal significant discrepancies in model
performance between random and chronological splits in several existing
COVID-19-related datasets; specifically, chronological splits significantly
reduce the accuracy of stance classification. Therefore, real-world stance
detection approaches need to be further refined to incorporate temporal factors
as a key consideration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanEval-XL: A Multilingual Code <span class="highlight-title">Generation</span> Benchmark for Cross-lingual
  Natural Language Generalization <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Peng, Yekun Chai, Xuhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have made significant progress in generating
codes from textual prompts. However, existing benchmarks have mainly
concentrated on translating English prompts to multilingual codes or have been
constrained to very limited natural languages (NLs). These benchmarks have
overlooked the vast landscape of massively multilingual NL to multilingual
code, leaving a critical gap in the evaluation of multilingual LLMs. In
response, we introduce HumanEval-XL, a massively multilingual code generation
benchmark specifically crafted to address this deficiency. HumanEval-XL
establishes connections between 23 NLs and 12 programming languages (PLs), and
comprises of a collection of 22,080 prompts with an average of 8.33 test cases.
By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a
comprehensive evaluation platform for multilingual LLMs, allowing the
assessment of the understanding of different NLs. Our work serves as a
pioneering step towards filling the void in evaluating NL generalization in the
area of multilingual code generation. We make our evaluation code and data
publicly available at \url{https://github.com/FloatAI/humaneval-xl}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Spacerini, a tool that integrates the Pyserini toolkit for
reproducible information retrieval research with Hugging Face to enable the
seamless construction and deployment of interactive search engines. Spacerini
makes state-of-the-art sparse and dense retrieval models more accessible to
non-IR practitioners while minimizing deployment effort. This is useful for NLP
researchers who want to better understand and validate their research by
performing qualitative analyses of training corpora, for IR researchers who
want to demonstrate new retrieval models integrated into the growing Pyserini
ecosystem, and for third parties reproducing the work of other researchers.
Spacerini is open source and includes utilities for loading, preprocessing,
indexing, and deploying search engines locally and remotely. We demonstrate a
portfolio of 13 search engines created with Spacerini for different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWESuite: Phonetic Word Embeddings and Tasks They Facilitate <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping words into a fixed-dimensional vector space is the backbone of modern
NLP. While most word embedding methods successfully encode semantic
information, they overlook phonetic information that is crucial for many tasks.
We develop three methods that use articulatory features to build phonetically
informed word embeddings. To address the inconsistent evaluation of existing
phonetic word embedding methods, we also contribute a task suite to fairly
evaluate past, current, and future methods. We evaluate both (1) intrinsic
aspects of phonetic word embeddings, such as word retrieval and correlation
with sound similarity, and (2) extrinsic performance on tasks such as rhyme and
cognate detection and sound analogies. We hope our task suite will promote
reproducibility and inspire future phonetic embedding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confusing charge prediction is a challenging task in legal AI, which involves
predicting confusing charges based on fact descriptions. While existing charge
prediction methods have shown impressive performance, they face significant
challenges when dealing with confusing charges, such as Snatch and Robbery. In
the legal domain, constituent elements play a pivotal role in distinguishing
confusing charges. Constituent elements are fundamental behaviors underlying
criminal punishment and have subtle distinctions among charges. In this paper,
we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces
domain knowledge regarding constituent elements to guide the model in making
judgments on confusing charges, much like a judge's reasoning process.
Specifically, we first construct a legal knowledge graph containing constituent
elements to help select keywords for each charge, forming a word bag.
Subsequently, to guide the model's attention towards the differentiating
information for each charge within the context, we expand the attention
mechanism and introduce a new loss function with attention supervision through
words in the word bag. We construct the confusing charges dataset from
real-world judicial documents. Experiments demonstrate the effectiveness of our
method, especially in maintaining exceptional performance in imbalanced label
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanCLIP: Learning Text-enhanced Urban Region Profiling with
  Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing from the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban region profiling from web-sourced data is of utmost importance for
urban planning and sustainable development. We are witnessing a rising trend of
LLMs for various fields, especially dealing with multi-modal data research such
as vision-language learning, where the text modality serves as a supplement
information for the image. Since textual modality has never been introduced
into modality combinations in urban region profiling, we aim to answer two
fundamental questions in this paper: i) Can textual modality enhance urban
region profiling? ii) and if so, in what ways and with regard to which aspects?
To answer the questions, we leverage the power of Large Language Models (LLMs)
and introduce the first-ever LLM-enhanced framework that integrates the
knowledge of textual modality into urban imagery profiling, named LLM-enhanced
Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).
Specifically, it first generates a detailed textual description for each
satellite image by an open-source Image-to-Text LLM. Then, the model is trained
on the image-text pairs, seamlessly unifying natural language supervision for
urban visual representation learning, jointly with contrastive loss and
language modeling loss. Results on predicting three urban indicators in four
major Chinese metropolises demonstrate its superior performance, with an
average improvement of 6.1% on R^2 compared to the state-of-the-art methods.
Our code and the image-language dataset will be released upon paper
notification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large <span class="highlight-title">Language Models</span> for Enhanced NLP Task Performance
  through Knowledge Distillation and Optimized Training Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Huang, Keke Tang, Meilian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural
Language Processing (NLP), showing potential in traditional tasks such as Named
Entity Recognition (NER). Our study explores a three-phase training strategy
that harnesses GPT-4's capabilities to enhance the BERT model's performance on
NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC
dataset without fine-tuning. We then train BERT using a mix of original and
LLM-annotated data, analyzing the efficacy of LLM annotations against
traditional methods. The second phase involves comparative experiments with
different training regimens, assessing the synergy between distilled and
original data. We observe that sequential strategies, particularly a simple mix
of training first with distilled data followed by original data, significantly
boost performance. In the third phase, we investigate various data blending
techniques, including sigmoid and power decay functions, to optimize the
training process further. Our results indicate that a strategic mix of
distilled and original data markedly elevates the NER capabilities of BERT. Our
approach presents a scalable methodology that reduces manual annotation costs
and increases efficiency, making it especially pertinent in resource-limited
and closed-network environments. The study concludes that while the 'Simple
Mix' strategy yields the best results, understanding its underlying mechanisms
requires further research. Future work will also focus on refining prompt
designs and enhancing annotation selection processes, aiming to extend our
methodology to diverse NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly advancing field of artificial intelligence, the concept of
Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a
crucial area of study. This approach is especially significant in terms of
assessing and enhancing the safety and robustness of these models. This paper
investigates the intricate consequences of such modifications through model
editing, uncovering a complex relationship between enhancing model accuracy and
preserving its ethical integrity. Our in-depth analysis reveals a striking
paradox: while injecting accurate information is crucial for model reliability,
it can paradoxically destabilize the model's foundational framework, resulting
in unpredictable and potentially unsafe behaviors. Additionally, we propose a
benchmark dataset NicheHazardQA to investigate this unsafe behavior both within
the same and cross topical domain. This aspect of our research sheds light on
how the edits, impact the model's safety metrics and guardrails. Our findings
show that model editing serves as a cost-effective tool for topical red-teaming
by methodically applying targeted edits and evaluating the resultant model
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph: Deep and Responsible Reasoning of Large Language Model
  on Knowledge Graph <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07697v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07697v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have achieved significant success in
various tasks, they often struggle with hallucination problems, especially in
scenarios requiring deep and responsible reasoning. These issues could be
partially addressed by introducing external knowledge graphs (KG) in LLM
reasoning. In this paper, we propose a new LLM-KG integrating paradigm
``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to
interactively explore related entities and relations on KGs and perform
reasoning based on the retrieved knowledge. We further implement this paradigm
by introducing a new approach called Think-on-Graph (ToG), in which the LLM
agent iteratively executes beam search on KG, discovers the most promising
reasoning paths, and returns the most likely reasoning results. We use a number
of well-designed experiments to examine and illustrate the following advantages
of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has
the ability of knowledge traceability and knowledge correctability by
leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible
plug-and-play framework for different LLMs, KGs and prompting strategies
without any additional training cost; 4) the performance of ToG with small LLM
models could exceed large LLM such as GPT-4 in certain scenarios and this
reduces the cost of LLM deployment and application. As a training-free method
with lower computational cost and better generality, ToG achieves overall SOTA
in 6 out of 9 datasets where most previous SOTAs rely on additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture-of-<span class="highlight-title">Prompt</span>-Experts for Multi-modal Semantic Understanding <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multimodal semantic understanding that goes beyond the mere superficial
content relation mining has received increasing attention in the realm of
artificial intelligence. The challenges of collecting and annotating
high-quality multi-modal data have underscored the significance of few-shot
learning. In this paper, we focus on two critical tasks under this context:
few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis
(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware
Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on
the unified vision-language model (VLM). Specifically, we design three experts
of soft prompts: a text prompt and an image prompt that extract
modality-specific features to enrich the single-modal representation, and a
unified prompt to assist multi-modal interaction. Additionally, we reorganize
Transformer layers into several blocks and introduce cross-modal prompt
attention between adjacent blocks, which smoothens the transition from
single-modal representation to multi-modal fusion. On both MSD and MSA datasets
in few-shot setting, our proposed model not only surpasses the 8.2B model
InstructBLIP with merely 2% parameters (150M), but also significantly
outperforms other widely-used prompt methods on VLMs or task-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large <span class="highlight-title">Language Models</span>
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: \url{https://github.com/xf-zhao/LoT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Between Lines of Code: Unraveling the Distinct Patterns of Machine and
  Human Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06461v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06461v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine- and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine- and human-authored code.
Through a rigorous analysis of code attributes such as lexical diversity,
conciseness, and naturalness, we expose unique patterns inherent to each
source. We particularly notice that the syntactic segmentation of code is a
critical factor in identifying its provenance. Based on our findings, we
propose DetectCodeGPT, a novel method for detecting machine-generated code,
which improves DetectGPT by capturing the distinct stylized patterns of code.
Diverging from conventional techniques that depend on external LLMs for
perturbations, DetectCodeGPT perturbs the code corpus by strategically
inserting spaces and newlines, ensuring both efficacy and efficiency.
Experiment results show that our approach significantly outperforms
state-of-the-art techniques in detecting machine-generated code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at https://github.com/YerbaPage/DetectCodeGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Limitations of Computational Rumor Detection Models
  Trained on Static <span class="highlight-title">Dataset</span>s <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial aspect of a rumor detection model is its ability to generalize,
particularly its ability to detect emerging, previously unknown rumors. Past
research has indicated that content-based (i.e., using solely source posts as
input) rumor detection models tend to perform less effectively on unseen
rumors. At the same time, the potential of context-based models remains largely
untapped. The main contribution of this paper is in the in-depth evaluation of
the performance gap between content and context-based models specifically on
detecting new, unseen rumors. Our empirical findings demonstrate that
context-based models are still overly dependent on the information derived from
the rumors' source post and tend to overlook the significant role that
contextual information can play. We also study the effect of data split
strategies on classifier performance. Based on our experimental results, the
paper also offers practical suggestions on how to minimize the effects of
temporal concept drift in static datasets during the training of rumor
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IllusionVQA: A Challenging Optical Illusion <span class="highlight-title">Dataset</span> for Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, Rifat Shahriyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Vision Language Models (VLM) has allowed researchers to
investigate the visual understanding of a neural network using natural
language. Beyond object classification and detection, VLMs are capable of
visual comprehension and common-sense reasoning. This naturally led to the
question: How do VLMs respond when the image itself is inherently unreasonable?
To this end, we present IllusionVQA: a diverse dataset of challenging optical
illusions and hard-to-interpret scenes to test the capability of VLMs in two
distinct multiple-choice VQA tasks - comprehension and soft localization.
GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the
comprehension task and 49.7% on the localization task (4-shot and
Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100%
accuracy in comprehension and localization. We discover that In-Context
Learning (ICL) and Chain-of-Thought reasoning substantially degrade the
performance of GeminiPro on the localization task. Tangentially, we discover a
potential weakness in the ICL capabilities of VLMs: they fail to locate optical
illusions even when the correct answer is in the context window as a few-shot
example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geotokens and Geo<span class="highlight-title">transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eren Unlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In transformer architectures, position encoding primarily provides a sense of
sequence for input tokens. While the original transformer paper's method has
shown satisfactory results in general language processing tasks, there have
been new proposals, such as Rotary Position Embedding (RoPE), for further
improvement. This paper presents geotokens, input components for transformers,
each linked to a specific geological location. Unlike typical language
sequences, for these tokens, the order is not as vital as the geographical
coordinates themselves. To represent the relative position in this context and
to keep a balance between the real world distance and the distance in the
embedding space, we design a position encoding approach drawing from the RoPE
structure but tailored for spherical coordinates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Llam<span class="highlight-title">BERT</span>: Large-scale low-cost data annotation in NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bálint Csanády, Lajos Muzsai, Péter Vedres, Zoltán Nádasdy, András Lukács
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable
proficiency in a wide range of natural language processing (NLP) tasks. Despite
their effectiveness, the high costs associated with their use pose a challenge.
We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small
subset of large, unlabeled databases and uses the results for fine-tuning
transformer encoders like BERT and RoBERTa. This strategy is evaluated on two
diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our
results indicate that the LlamBERT approach slightly compromises on accuracy
while offering much greater cost-effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Zero-Shot <span class="highlight-title">Prompt</span>ing for Efficient Language Model Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Vöge, Vincent Gurgul, Stefan Lessmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach for efficiently distilling LLMs into
smaller, application-specific models, significantly reducing operational costs
and manual labor. Addressing the challenge of deploying computationally
intensive LLMs in specific applications or edge devices, this technique
utilizes LLMs' reasoning capabilities to generate labels and natural language
rationales for unlabeled data. Our approach enhances both finetuning and
distillation by employing a multi-task training framework where student models
mimic these rationales alongside teacher predictions. Key contributions include
the employment of zero-shot prompting to elicit teacher model rationales,
reducing the necessity for handcrafted few-shot examples and lowering the
overall token count required, which directly translates to cost savings given
the pay-per-token billing model of major tech companies' LLM APIs.
Additionally, the paper investigates the impact of explanation properties on
distillation efficiency, demonstrating that minimal performance loss occurs
even when rationale augmentation is not applied across the entire dataset,
facilitating further reductions of tokens. This research marks a step toward
the efficient training of task-specific models with minimal human intervention,
offering substantial cost-savings while maintaining, or even enhancing,
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for
  Vietnamese Natural Language Understanding <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phong Nguyen-Thuan Do, Son Quoc Tran, Phu Gia Hoang, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Natural Language Understanding (NLU) benchmarks in various
languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and
IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across
a wide range of tasks. To establish a standardized set of benchmarks for
Vietnamese NLU, we introduce the first Vietnamese Language Understanding
Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets
covering different NLU tasks, including text classification, span extraction,
and natural language understanding. To provide an insightful overview of the
current state of Vietnamese NLU, we then evaluate seven state-of-the-art
pre-trained models, including both multilingual and Vietnamese monolingual
models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new
state-of-the-art pre-trained model that achieves superior results across all
tasks in the VLUE benchmark. Our model combines the proficiency of a
multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT
is developed based on the XLM-RoBERTa model, with an additional pretraining
step utilizing a significant amount of Vietnamese textual data to enhance its
adaptation to the Vietnamese language. For the purpose of future research,
CafeBERT is made publicly available for research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAMPER: LanguAge Model and <span class="highlight-title">Prompt</span> EngineeRing for zero-shot time series
  classification <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Du, Zhaotian Xie, Yan Tong, Peiwu Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)
framework, designed to systematically evaluate the adaptability of pre-trained
language models (PLMs) in accommodating diverse prompts and their integration
in zero-shot time series (TS) classification. We deploy LAMPER in experimental
assessments using 128 univariate TS datasets sourced from the UCR archive. Our
findings indicate that the feature representation capacity of LAMPER is
influenced by the maximum input token threshold imposed by PLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as tiny paper in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAAMove: A Corpus for Analyzing Moves in Research Article Abstracts <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzheng Li, Ruojin Wang, Ge Shi, Xing Lv, Lei Lei, Chong Feng, Fang Liu, Jinkun Lin, Yangguang Mei, Lingnan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Move structures have been studied in English for Specific Purposes (ESP) and
English for Academic Purposes (EAP) for decades. However, there are few move
annotation corpora for Research Article (RA) abstracts. In this paper, we
introduce RAAMove, a comprehensive multi-domain corpus dedicated to the
annotation of move structures in RA abstracts. The primary objective of RAAMove
is to facilitate move analysis and automatic move identification. This paper
provides a thorough discussion of the corpus construction process, including
the scheme, data collection, annotation guidelines, and annotation procedures.
The corpus is constructed through two stages: initially, expert annotators
manually annotate high-quality data; subsequently, based on the human-annotated
data, a BERT-based model is employed for automatic annotation with the help of
experts' modification. The result is a large-scale and high-quality corpus
comprising 33,988 annotated instances. We also conduct preliminary move
identification experiments using the BERT-based model to verify the
effectiveness of the proposed corpus and model. The annotated corpus is
available for academic research purposes and can serve as essential resources
for move analysis, English language teaching and writing, as well as
move/discourse-related tasks in Natural Language Processing (NLP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centered Masking for Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Sentence-level Metrics Predicting Human Sentence
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of research in computational psycholinguistics has concentrated
on the processing of words. This study introduces innovative methods for
computing sentence-level metrics using multilingual large language models. The
metrics developed sentence surprisal and sentence relevance and then are tested
and compared to validate whether they can predict how humans comprehend
sentences as a whole across languages. These metrics offer significant
interpretability and achieve high accuracy in predicting human sentence reading
speeds. Our results indicate that these computational sentence-level metrics
are exceptionally effective at predicting and elucidating the processing
difficulties encountered by readers in comprehending sentences as a whole
across a variety of languages. Their impressive performance and generalization
capabilities provide a promising avenue for future research in integrating LLMs
and cognitive science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRC-based Nested Medical NER with Co-prediction and Adaptive
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojing Du, Hanjie Zhao, Danyan Xing, Yuxiang Jia, Hongying Zan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical information extraction, medical Named Entity Recognition (NER) is
indispensable, playing a crucial role in developing medical knowledge graphs,
enhancing medical question-answering systems, and analyzing electronic medical
records. The challenge in medical NER arises from the complex nested structures
and sophisticated medical terminologies, distinguishing it from its
counterparts in traditional domains. In response to these complexities, we
propose a medical NER model based on Machine Reading Comprehension (MRC), which
uses a task-adaptive pre-training strategy to improve the model's capability in
the medical field. Meanwhile, our model introduces multiple word-pair
embeddings and multi-granularity dilated convolution to enhance the model's
representation ability and uses a combined predictor of Biaffine and MLP to
improve the model's recognition performance. Experimental evaluations conducted
on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our
proposed model outperforms the compared state-of-the-art (SOTA) models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Emergent Abilities of <span class="highlight-title">Language Models</span> from the Loss
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have put into question the belief that emergent abilities in
language models are exclusive to large models. This skepticism arises from two
observations: 1) smaller models can also exhibit high performance on emergent
abilities and 2) there is doubt on the discontinuous metrics used to measure
these abilities. In this paper, we propose to study emergent abilities in the
lens of pre-training loss, instead of model size or training compute. We
demonstrate that the models with the same pre-training loss, but different
model and data sizes, generate the same performance on various downstream
tasks. We also discover that a model exhibits emergent abilities on certain
tasks -- regardless of the continuity of metrics -- when its pre-training loss
falls below a specific threshold. Before reaching this threshold, its
performance remains at the level of random guessing. This inspires us to
redefine emergent abilities as those that manifest in models with lower
pre-training losses, highlighting that these abilities cannot be predicted by
merely extrapolating the performance trends of models with higher pre-training
losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Unified Semantic Discourse Structure for High-quality Headline
  <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Xu, Hao Fei, Fei Li, Shengqiong Wu, Rui Sun, Chong Teng, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Headline generation aims to summarize a long document with a short, catchy
title that reflects the main idea. This requires accurately capturing the core
document semantics, which is challenging due to the lengthy and background
information-rich na ture of the texts. In this work, We propose using a unified
semantic discourse structure (S3) to represent document semantics, achieved by
combining document-level rhetorical structure theory (RST) trees with
sentence-level abstract meaning representation (AMR) graphs to construct S3
graphs. The hierarchical composition of sentence, clause, and word
intrinsically characterizes the semantic meaning of the overall document. We
then develop a headline generation framework, in which the S3 graphs are
encoded as contextual features. To consolidate the efficacy of S3 graphs, we
further devise a hierarchical structure pruning mechanism to dynamically screen
the redundant and nonessential nodes within the graph. Experimental results on
two headline generation datasets demonstrate that our method outperforms
existing state-of-art methods consistently. Our work can be instructive for a
broad range of document modeling tasks, more than headline or summarization
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-Side Realization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users are dissatisfied with services. Since the service is not tailor-made
for a user, it is natural for dissatisfaction to arise. The problem is, that
even if users are dissatisfied, they often do not have the means to resolve
their dissatisfaction. The user cannot alter the source code of the service,
nor can they force the service provider to change. The user has no choice but
to remain dissatisfied or quit the service. User-side realization offers
proactive solutions to this problem by providing general algorithms to deal
with common problems on the user's side. These algorithms run on the user's
side and solve the problems without having the service provider change the
service itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large <span class="highlight-title">Language Models</span> for Preliminary Security Risk Analysis:
  A Mission-Critical Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Esposito, Francesco Palagiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preliminary security risk analysis (PSRA) provides a quick approach to
identify, evaluate and propose remeditation to potential risks in specific
scenarios. The extensive expertise required for an effective PSRA and the
substantial ammount of textual-related tasks hinder quick assessments in
mission-critical contexts, where timely and prompt actions are essential. The
speed and accuracy of human experts in PSRA significantly impact response time.
A large language model can quickly summarise information in less time than a
human. To our knowledge, no prior study has explored the capabilities of
fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of
FTM to assist practitioners in PSRA. We manually curated 141 representative
samples from over 50 mission-critical analyses archived by the industrial
context team in the last five years.We compared the proficiency of the FTM
versus seven human experts. Within the industrial context, our approach has
proven successful in reducing errors in PSRA, hastening security risk
detection, and minimizing false positives and negatives. This translates to
cost savings for the company by averting unnecessary expenses associated with
implementing unwarranted countermeasures. Therefore, experts can focus on more
comprehensive risk analysis, leveraging LLMs for an effective preliminary
assessment within a condensed timeframe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Fragility of Active Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Ghose, Emma Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning (AL) techniques aim to maximally utilize a labeling budget by
iteratively selecting instances that are most likely to improve prediction
accuracy. However, their benefit compared to random sampling has not been
consistent across various setups, e.g., different datasets, classifiers. In
this empirical study, we examine how a combination of different factors might
obscure any gains from an AL technique.
  Focusing on text classification, we rigorously evaluate AL techniques over
around 1000 experiments that vary wrt the dataset, batch size, text
representation and the classifier. We show that AL is only effective in a
narrow set of circumstances. We also address the problem of using metrics that
are better aligned with real world expectations.
  The impact of this study is in its insights for a practitioner: (a) the
choice of text representation and classifier is as important as that of an AL
technique, (b) choice of the right metric is critical in assessment of the
latter, and, finally, (c) reported AL results must be holistically interpreted,
accounting for variables other than just the query strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web user data plays a central role in the ecosystem of pre-trained large
language models (LLMs) and their fine-tuned variants. Billions of data are
crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web
users}} confirm if LLMs misuse their data without permission? In this work, we
suggest that users repeatedly insert personal passphrases into their documents,
enabling LLMs to memorize them. These concealed passphrases in user documents,
referred to as \textit{ghost sentences}, once they are identified in the
generated content of LLMs, users can be sure that their data is used for
training. To explore the effectiveness and usage of this copyrighting tool, we
define the \textit{user training data identification} task with ghost
sentences. Multiple datasets from various sources at different scales are
created and tested with LLMs of different sizes. For evaluation, we introduce a
last $k$ words verification manner along with two metrics: document and user
identification accuracy. In the specific case of instruction tuning of a 3B
LLaMA model, 11 out of 16 users with ghost sentences identify their data within
the generation content. These 16 users contribute 383 examples to $\sim$1.8M
training documents. For continuing pre-training of a 1.1B TinyLlama model, 61
out of 64 users with ghost sentences identify their data within the LLM output.
These 64 users contribute 1156 examples to $\sim$10M training documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Dialogue Strategy Learning for Motivational Interviewing via
  Inductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhang Xie, Bodhisattwa Prasad Majumder, Mengjie Zhao, Yoshinori Maeda, Keiichi Yamada, Hiromi Wakaki, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of building a dialogue system that can motivate users to
adopt positive lifestyle changes: Motivational Interviewing. Addressing such a
task requires a system that can infer \textit{how} to motivate a user
effectively. We propose DIIT, a framework that is capable of learning and
applying conversation strategies in the form of natural language inductive
rules from expert demonstrations. Automatic and human evaluation on
instruction-following large language models show natural language strategy
descriptions discovered by DIIR can improve active listening skills, reduce
unsolicited advice, and promote more collaborative and less authoritative
responses, outperforming various demonstration utilization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs <span class="highlight-title">Instruct</span> LLMs:An Extraction and Editing Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interest in updating Large Language Models (LLMs) without retraining from
scratch is substantial, yet it comes with some challenges.This is especially
true for situations demanding complex reasoning with limited samples, a
scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation
for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and
Retrieval-Augmented Generation (RAG) are inadequate for this critical issue,
particularly evident in our exploration of a specific medical context that
epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a
Sequential Fusion method to incorporate knowledge from complex context into
LLMs. This method employs a two-stage framework: initially, it leverages
general LLMs to construct knowledge graphs (KGs) for extracting knowledge from
complex texts; subsequently, it updates the domain LLMs through knowledge edit.
According to our method, the domain LLM achieved a 71.69\% accuracy in question
answering tasks. Subsequently, we broadened our assessment to a novel dataset
we developed in the economics and management field, where our method realized a
75\% accuracy. These outcomes underline the efficacy and adaptability of our
approach for PCRA-LLM across various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion
  Collider 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEaCE: A Chemistry-Oriented <span class="highlight-title">Dataset</span> for Optical Character Recognition on
  Scientific Documents <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Zhang, Connor Heaton, Sean Timothy Okonsky, Prasenjit Mitra, Hilal Ezgi Toraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Character Recognition (OCR) is an established task with the objective
of identifying the text present in an image. While many off-the-shelf OCR
models exist, they are often trained for either scientific (e.g., formulae) or
generic printed English text. Extracting text from chemistry publications
requires an OCR model that is capable in both realms. Nougat, a recent tool,
exhibits strong ability to parse academic documents, but is unable to parse
tables in PubMed articles, which comprises a significant part of the academic
community and is the focus of this work. To mitigate this gap, we present the
Printed English and Chemical Equations (PEaCE) dataset, containing both
synthetic and real-world records, and evaluate the efficacy of
transformer-based OCR models when trained on this resource. Given that
real-world records contain artifacts not present in synthetic records, we
propose transformations that mimic such qualities. We perform a suite of
experiments to explore the impact of patch size, multi-domain training, and our
proposed transformations, ultimately finding that models with a small patch
size trained on multiple domains using the proposed transformations yield the
best performance. Our dataset and code is available at
https://github.com/ZN1010/PEaCE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijun Ding, Li Dong, Zhichao Huang, Guangning Xu, Xu Huang, Bo Liu, Liwen Jing, Bowen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection aims to determine the attitude expressed in text towards a
given target. Zero-shot stance detection (ZSSD) has emerged to classify stances
towards unseen targets during inference. Recent data augmentation techniques
for ZSSD increase transferable knowledge between targets through text or target
augmentation. However, these methods exhibit limitations. Target augmentation
lacks logical connections between generated targets and source text, while text
augmentation relies solely on training data, resulting in insufficient
generalization. To address these issues, we propose an encoder-decoder data
augmentation (EDDA) framework. The encoder leverages large language models and
chain-of-thought prompting to summarize texts into target-specific if-then
rationales, establishing logical relationships. The decoder generates new
samples based on these expressions using a semantic correlation word
replacement strategy to increase syntactic diversity. We also analyze the
generated expressions to develop a rationale-enhanced network that fully
utilizes the augmented data. Experiments on benchmark datasets demonstrate our
approach substantially improves over state-of-the-art ZSSD techniques. The
proposed EDDA framework increases semantic relevance and syntactic variety in
augmented texts while enabling interpretable rationale-based learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FEEL: A Framework for Evaluating Emotional Support Capability with Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaiwen Zhang, Yu Chen, Ming Wang, Shi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional Support Conversation (ESC) is a typical dialogue that can
effec-tively assist the user in mitigating emotional pressures. However, owing
to the inherent subjectivity involved in analyzing emotions, current
non-artificial methodologies face challenges in effectively appraising the
emo-tional support capability. These metrics exhibit a low correlation with
human judgments. Concurrently, manual evaluation methods extremely will cause
high costs. To solve these problems, we propose a novel model FEEL (Framework
for Evaluating Emotional Support Capability with Large Lan-guage Models),
employing Large Language Models (LLMs) as evaluators to assess emotional
support capabilities. The model meticulously considers var-ious evaluative
aspects of ESC to apply a more comprehensive and accurate evaluation method for
ESC. Additionally, it employs a probability distribu-tion approach for a more
stable result and integrates an ensemble learning strategy, leveraging multiple
LLMs with assigned weights to enhance evalua-tion accuracy. To appraise the
performance of FEEL, we conduct extensive experiments on existing ESC model
dialogues. Experimental results demon-strate our model exhibits a substantial
enhancement in alignment with human evaluations compared to the baselines. Our
source code is available at https://github.com/Ansisy/FEEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,3 figures and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixRED: A Mix-lingual Relation Extraction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxing Kong, Yougang Chu, Zheng Ma, Jianbing Zhang, Liang He, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is a critical task in the field of natural language
processing with numerous real-world applications. Existing research primarily
focuses on monolingual relation extraction or cross-lingual enhancement for
relation extraction. Yet, there remains a significant gap in understanding
relation extraction in the mix-lingual (or code-switching) scenario, where
individuals intermix contents from different languages within sentences,
generating mix-lingual content. Due to the lack of a dedicated dataset, the
effectiveness of existing relation extraction models in such a scenario is
largely unexplored. To address this issue, we introduce a novel task of
considering relation extraction in the mix-lingual scenario called MixRE and
constructing the human-annotated dataset MixRED to support this task. In
addition to constructing the MixRED dataset, we evaluate both state-of-the-art
supervised models and large language models (LLMs) on MixRED, revealing their
respective advantages and limitations in the mix-lingual scenario. Furthermore,
we delve into factors influencing model performance within the MixRE task and
uncover promising directions for enhancing the performance of both supervised
models and LLMs in this novel task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAGLE: A Domain Generalization Framework for AI-generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement in capabilities of Large Language Models (LLMs), one
major step in the responsible and safe use of such LLMs is to be able to detect
text generated by these models. While supervised AI-generated text detectors
perform well on text generated by older LLMs, with the frequent release of new
LLMs, building supervised detectors for identifying text from such new models
would require new labeled training data, which is infeasible in practice. In
this work, we tackle this problem and propose a domain generalization framework
for the detection of AI-generated text from unseen target generators. Our
proposed framework, EAGLE, leverages the labeled data that is available so far
from older language models and learns features invariant across these
generators, in order to detect text generated by an unknown target generator.
EAGLE learns such domain-invariant features by combining the representational
power of self-supervised contrastive learning with domain adversarial training.
Through our experiments we demonstrate how EAGLE effectively achieves
impressive performance in detecting text generated by unseen target generators,
including recent state-of-the-art ones such as GPT-4 and Claude, reaching
detection scores of within 4.7% of a fully supervised detector.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Minyu Chen, Ruibang Liu, Guoqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ZKP systems have surged attention and held a fundamental role in contemporary
cryptography. Zk-SNARK protocols dominate the ZKP usage, often implemented
through arithmetic circuit programming paradigm. However, underconstrained or
overconstrained circuits may lead to bugs. Underconstrained circuits refer to
circuits that lack the necessary constraints, resulting in unexpected solutions
in the circuit and causing the verifier to accept a bogus witness.
Overconstrained circuits refer to circuits that are constrained excessively,
resulting in the circuit lacking necessary solutions and causing the verifier
to accept no witness, rendering the circuit meaningless. This paper introduces
a novel approach for pinpointing two distinct types of bugs in ZKP circuits.
The method involves encoding the arithmetic circuit constraints to polynomial
equation systems and solving polynomial equation systems over a finite field by
algebraic computation. The classification of verification results is refined,
greatly enhancing the expressive power of the system. We proposed a tool, AC4,
to represent the implementation of this method. Experiments demonstrate that
AC4 represents a substantial 29% increase in the checked ratio compared to
prior work. Within a solvable range, the checking time of AC4 has also
exhibited noticeable improvement, demonstrating a magnitude increase compared
to previous efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI for Biomedicine in the Era of Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Bi, Sajib Acharjee Dip, Daniel Hajialigol, Sindhura Kommu, Hanwen Liu, Meng Lu, Xuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of AI for biomedicine span a wide spectrum, from the atomic
level, where it solves partial differential equations for quantum systems, to
the molecular level, predicting chemical or protein structures, and further
extending to societal predictions like infectious disease outbreaks. Recent
advancements in large language models, exemplified by models like ChatGPT, have
showcased significant prowess in natural language tasks, such as translating
languages, constructing chatbots, and answering questions. When we consider
biomedical data, we observe a resemblance to natural language in terms of
sequences: biomedical literature and health records presented as text,
biological sequences or sequencing data arranged in sequences, or sensor data
like brain signals as time series. The question arises: Can we harness the
potential of recent large language models to drive biomedical knowledge
discoveries? In this survey, we will explore the application of large language
models to three crucial categories of biomedical data: 1) textual data, 2)
biological sequences, and 3) brain signals. Furthermore, we will delve into
large language model challenges in biomedical research, including ensuring
trustworthiness, achieving personalization, and adapting to multi-modal data
representation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BaRDa: A Belief and Reasoning <span class="highlight-title">Dataset</span> that Separates Factual Accuracy
  and Reasoning Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Clark, Bhavana Dalvi Mishra, Oyvind Tafjord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While there are numerous benchmarks comparing the performance of modern
language models (LMs), end-task evaluations often conflate notions of *factual
accuracy* ("truth") and *reasoning ability* ("rationality", or "honesty" in the
sense of correctly reporting implications of beliefs). Our goal is a dataset
that clearly distinguishes these two notions. Our approach is to leverage and
extend a collection of human-annotated *entailment trees*, engineered to
express both good and bad chains of reasoning, and using a mixture of true and
false facts, in particular including counterfactual examples, to avoid belief
bias (also known as the "content effect"). The resulting dataset, called BaRDa,
contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319
false statements. Testing on four GPT-series models,
GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of
74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This
shows the clear progression of models towards improved factual accuracy and
entailment reasoning, and the dataset provides a new benchmark that more
cleanly separates and quantifies these two notions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added note about how dataset sampling was performed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuSR: Testing the Limits of Chain-of-thought with Multistep Soft
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) equipped with techniques like
chain-of-thought prompting have demonstrated impressive capabilities, they
still fall short in their ability to reason robustly in complex settings.
However, evaluating LLM reasoning is challenging because system capabilities
continue to grow while benchmark datasets for tasks like logical deduction have
remained static. We introduce MuSR, a dataset for evaluating language models on
multistep soft reasoning tasks specified in a natural language narrative. This
dataset has two crucial features. First, it is created through a novel
neurosymbolic synthetic-to-natural generation algorithm, enabling the
construction of complex reasoning instances that challenge GPT-4 (e.g., murder
mysteries roughly 1000 words in length) and which can be scaled further as more
capable LLMs are released. Second, our dataset instances are free text
narratives corresponding to real-world domains of reasoning; this makes it
simultaneously much more challenging than other synthetically-crafted
benchmarks while remaining realistic and tractable for human annotators to
solve with high accuracy. We evaluate a range of LLMs and prompting techniques
on this dataset and characterize the gaps that remain for techniques like
chain-of-thought to perform robust reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PILOT: Legal Case Outcome Prediction with Case Law 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Cao, Zifeng Wang, Cao Xiao, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning shows promise in predicting the outcome of legal cases, but
most research has concentrated on civil law cases rather than case law systems.
We identified two unique challenges in making legal case outcome predictions
with case law. First, it is crucial to identify relevant precedent cases that
serve as fundamental evidence for judges during decision-making. Second, it is
necessary to consider the evolution of legal principles over time, as early
cases may adhere to different legal contexts. In this paper, we proposed a new
framework named PILOT (PredictIng Legal case OuTcome) for case outcome
prediction. It comprises two modules for relevant case retrieval and temporal
pattern handling, respectively. To benchmark the performance of existing legal
case outcome prediction models, we curated a dataset from a large-scale case
law database. We demonstrate the importance of accurately identifying precedent
cases and mitigating the temporal shift when making predictions for case law,
as our method shows a significant improvement over the prior methods that focus
on civil law case outcome predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Lexical Features from Dialects via Interpretable Dialect
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Xie, Orevaoghene Ahia, Yulia Tsvetkov, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying linguistic differences between dialects of a language often
requires expert knowledge and meticulous human analysis. This is largely due to
the complexity and nuance involved in studying various dialects. We present a
novel approach to extract distinguishing lexical features of dialects by
utilizing interpretable dialect classifiers, even in the absence of human
experts. We explore both post-hoc and intrinsic approaches to interpretability,
conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally
demonstrate that our method successfully identifies key language-specific
lexical features that contribute to dialectal variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/ruoyuxie/interpretable_dialect_classifier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissociating language and thought in large <span class="highlight-title">language models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Mahowald, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, Evelina Fedorenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have come closest among all models to date to
mastering human language, yet opinions about their linguistic and cognitive
capabilities remain split. Here, we evaluate LLMs using a distinction between
formal linguistic competence - knowledge of linguistic rules and patterns - and
functional linguistic competence - understanding and using language in the
world. We ground this distinction in human neuroscience, which has shown that
formal and functional competence rely on different neural mechanisms. Although
LLMs are surprisingly good at formal competence, their performance on
functional competence tasks remains spotty and often requires specialized
fine-tuning and/or coupling with external modules. We posit that models that
use language in human-like ways would need to master both of these competence
types, which, in turn, could require the emergence of mechanisms specialized
for formal linguistic competence, distinct from functional competence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The two lead authors contributed equally to this work; published in
  "Trends in Cognnitive Sciences", March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Large Language Model (KG-LLM) for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07311v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07311v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large <span class="highlight-title">Language Models</span> for Generative Recommendation: A <span class="highlight-title">Survey</span> and
  Visionary Discussions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) not only have revolutionized the field of natural
language processing (NLP) but also have the potential to reshape many other
fields, e.g., recommender systems (RS). However, most of the related work
treats an LLM as a component of the conventional recommendation pipeline (e.g.,
as a feature extractor), which may not be able to fully leverage the generative
power of LLM. Instead of separating the recommendation process into multiple
stages, such as score computation and re-ranking, this process can be
simplified to one stage with LLM: directly generating recommendations from the
complete pool of items. This survey reviews the progress, methods, and future
directions of LLM-based generative recommendation by examining three questions:
1) What generative recommendation is, 2) Why RS should advance to generative
recommendation, and 3) How to implement LLM-based generative recommendation for
various RS tasks. We hope that this survey can provide the context and guidance
needed to explore this interesting and emerging topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEED: Customize Large <span class="highlight-title">Language Models</span> with Sample-Efficient Adaptation
  for Code <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Jiang, Yihong Dong, Zhi Jin, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have made significant progress in code
generation, they still struggle with code generation tasks in specific
scenarios. These scenarios usually necessitate the adaptation of LLMs to
fulfill specific needs, but the limited training samples available in practice
lead to poor code generation performance. Therefore, how to effectively adapt
LLMs to new scenarios with few training samples is a major challenge for
current code generation. In this paper, we propose a novel adaptation approach
named SEED, which stands for Sample-Efficient adaptation with Error-Driven
learning for code generation. SEED leverages the errors made by LLMs as
learning opportunities, using error revision to overcome its own shortcomings,
thus achieving efficient learning. Specifically, SEED involves identifying
error code generated by LLMs, employing Self-revise for code revision,
optimizing the model with revised code, and iteratively adapting the process
for continuous improvement. Experimental results show that, compared to other
mainstream fine-tuning approaches, SEED achieves superior performance with few
training samples, showing an average relative improvement of 54.7% in Pass@1 on
multiple code generation benchmarks. We also validate the effectiveness of
Self-revise, which generates revised code that optimizes the model more
efficiently compared to the code samples from datasets. Moreover, SEED
consistently demonstrates strong performance across various LLMs, underscoring
its generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Learning with Retrieved Demonstrations for <span class="highlight-title">Language Models</span>: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional
  Expression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuhee Kim, Surin Lee, Sangah Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many literary texts, emotions are indirectly conveyed through descriptions
of actions, facial expressions, and appearances, necessitating emotion
inference for narrative understanding. In this paper, we introduce K-Act2Emo, a
Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional
expressions and the emotions inferable from them. We categorize reasoning types
into inferences in positive situations, inferences in negative situations, and
inferences when expressions do not serve as emotional cues. Unlike existing
CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results
validate its effectiveness for training emotion inference models.
Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo
outperforms various existing Korean large language models, achieving
performance levels comparable to GPT-4 Turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large <span class="highlight-title">Language Models</span> for Mathematical Reasoning: Progresses and
  Challenges <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning serves as a cornerstone for assessing the fundamental
cognitive capabilities of human intelligence. In recent times, there has been a
notable surge in the development of Large Language Models (LLMs) geared towards
the automated resolution of mathematical problems. However, the landscape of
mathematical problem types is vast and varied, with LLM-oriented techniques
undergoing evaluation across diverse datasets and settings. This diversity
makes it challenging to discern the true advancements and obstacles within this
burgeoning field. This survey endeavors to address four pivotal dimensions: i)
a comprehensive exploration of the various mathematical problems and their
corresponding datasets that have been investigated; ii) an examination of the
spectrum of LLM-oriented techniques that have been proposed for mathematical
problem-solving; iii) an overview of factors and concerns affecting LLMs in
solving math; and iv) an elucidation of the persisting challenges within this
domain. To the best of our knowledge, this survey stands as one of the first
extensive examinations of the landscape of LLMs in the realm of mathematics,
providing a holistic perspective on the current state, accomplishments, and
future challenges in this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024 Student Research Workshop, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Emotion Phrases from Tweets using BART 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Rezapour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is a natural language processing task that aims to
identify and extract the emotional aspects of a text. However, many existing
sentiment analysis methods primarily classify the overall polarity of a text,
overlooking the specific phrases that convey sentiment. In this paper, we
applied an approach to sentiment analysis based on a question-answering
framework. Our approach leverages the power of Bidirectional Autoregressive
Transformer (BART), a pre-trained sequence-to-sequence model, to extract a
phrase from a given text that amplifies a given sentiment polarity. We create a
natural language question that identifies the specific emotion to extract and
then guide BART to pay attention to the relevant emotional cues in the text. We
use a classifier within BART to predict the start and end positions of the
answer span within the text, which helps to identify the precise boundaries of
the extracted emotion phrase. Our approach offers several advantages over most
sentiment analysis studies, including capturing the complete context and
meaning of the text and extracting precise token spans that highlight the
intended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Intersectionality and Dual Form of Gradient Descent for
  Multimodal Analysis: a Case Study on Hateful Memes <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yosuke Miyanishi, Minh Le Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid expansion of Machine Learning (ML) and Large Language Models
(LLMs), understanding the semantics within their mechanisms is vital. Causal
analyses define semantics, while gradient-based methods are essential to
eXplainable AI (XAI), interpreting the model's 'black box'. Integrating these,
we investigate how a model's mechanisms reveal its causal effect on
evidence-based decision-making. Research indicates intersectionality - the
combined impact of an individual's demographics - can be framed as an Average
Treatment Effect (ATE). This paper demonstrates that hateful meme detection can
be viewed as an ATE estimation using intersectionality principles, and
summarized gradient-based attention scores highlight distinct behaviors of
three Transformer models. We further reveal that LLM Llama-2 can discern the
intersectional aspects of the detection through in-context learning and that
the learning process could be explained via meta-gradient, a secondary form of
gradient. In conclusion, this work furthers the dialogue on Causality and XAI.
Our code is available online (see External Resources section).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at the Self-Verification Abilities of Large Language
  Models in Logical Reasoning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-pop Lyric Translation: <span class="highlight-title">Dataset</span>, Analysis, and Neural-Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyric translation, a field studied for over a century, is now attracting
computational linguistics researchers. We identified two limitations in
previous studies. Firstly, lyric translation studies have predominantly focused
on Western genres and languages, with no previous study centering on K-pop
despite its popularity. Second, the field of lyric translation suffers from a
lack of publicly available datasets; to the best of our knowledge, no such
dataset exists. To broaden the scope of genres and languages in lyric
translation studies, we introduce a novel singable lyric translation dataset,
approximately 89\% of which consists of K-pop song lyrics. This dataset aligns
Korean and English lyrics line-by-line and section-by-section. We leveraged
this dataset to unveil unique characteristics of K-pop lyric translation,
distinguishing it from other extensively studied genres, and to construct a
neural lyric translation model, thereby underscoring the importance of a
dedicated dataset for singable lyric translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable performance on
knowledge-intensive tasks, suggesting that real-world knowledge is encoded in
their model parameters. However, besides explorations on a few probing tasks in
limited knowledge domains, it is not well understood how to evaluate LLMs'
knowledge systematically and how well their knowledge abilities generalize,
across a spectrum of knowledge domains and progressively complex task formats.
To this end, we propose KGQuiz, a knowledge-intensive benchmark to
comprehensively investigate the knowledge generalization abilities of LLMs.
KGQuiz is a scalable framework constructed from triplet-based knowledge, which
covers three knowledge domains and consists of five tasks with increasing
complexity: true-or-false, multiple-choice QA, blank filling, factual editing,
and open-ended knowledge generation. To gain a better understanding of LLMs'
knowledge abilities and their generalization, we evaluate 10 open-source and
black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive
tasks and knowledge domains. Extensive experiments demonstrate that LLMs
achieve impressive performance in straightforward knowledge QA tasks, while
settings and contexts requiring more complex reasoning or employing
domain-specific facts still present significant challenges. We envision KGQuiz
as a testbed to analyze such nuanced variations in performance across domains
and task formats, and ultimately to understand, evaluate, and improve LLMs'
knowledge abilities across a wide spectrum of knowledge domains and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Paternity Test: Generated Text Detection with LLM Genetic
  Inheritance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can generate texts that carry the risk of
various misuses, including plagiarism, planting fake reviews on e-commerce
platforms, or creating inflammatory false tweets. Detecting whether a text is
machine-generated has thus become increasingly important. While existing
detection methods exhibit superior performance, they often lack
generalizability due to their heavy dependence on training data. To alleviate
this problem, we propose a model-related generated text detection method, the
LLM Paternity Test (LLM-Pat). Specifically, given any candidate text
(\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to
reconstruct a \textit{sibling} text corresponding to the given text and then
measures the similarity between candidate texts and their sibling texts. High
similarity indicates that the candidate text is machine-generated, akin to
genetic traits. We have constructed datasets encompassing four scenarios:
student responses in educational settings, news creation, academic paper
writing, and social media bots to assess the performance of LLM-Pat. The
experiments show that LLM-Pat outperforms the existing detection methods and is
more robust against paraphrasing attacks and re-translating attacks. Besides,
LLM-Pat can also be used to trace which large language model the text was
generated by. The constructed dataset and code will be released to benefit the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tandem <span class="highlight-title">Transformer</span>s for Inference Efficient LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autoregressive nature of conventional large language models (LLMs)
inherently limits inference speed, as tokens are generated sequentially. While
speculative and parallel decoding techniques attempt to mitigate this, they
face limitations: either relying on less accurate smaller models for generation
or failing to fully leverage the base LLM's representations.
  We introduce a novel architecture, Tandem transformers, to address these
issues. This architecture uniquely combines (1) a small autoregressive model
and (2) a large model operating in block mode (processing multiple tokens
simultaneously). The small model's predictive accuracy is substantially
enhanced by granting it attention to the large model's richer representations.
On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko
demonstrates a 3.3% improvement in next-token prediction accuracy over a
standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter
model with comparable downstream performance. We further incorporate the tandem
model within the speculative decoding (SPEED) framework where the large model
validates tokens from the small model. This ensures that the Tandem of
PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster
than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream
task accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Coreference Resolution in Low-resource South Asian
  Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Mishra, Pooja Desur, Rajiv Ratn Shah, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coreference resolution involves the task of identifying text spans within a
discourse that pertain to the same real-world entity. While this task has been
extensively explored in the English language, there has been a notable scarcity
of publicly accessible resources and models for coreference resolution in South
Asian languages. We introduce a Translated dataset for Multilingual Coreference
Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools
for translation and word-alignment. Nearly all of the predicted translations
successfully pass a sanity check, and 75% of English references align with
their predicted translations. Using multilingual encoders, two off-the-shelf
coreference resolution models were trained on a concatenation of TransMuCoRes
and a Hindi coreference resolution dataset with manual annotations. The best
performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,
respectively, on our test-split of Hindi golden set. This study is the first to
evaluate an end-to-end coreference resolution model on a Hindi golden set.
Furthermore, this work underscores the limitations of current coreference
evaluation metrics when applied to datasets with split antecedents, advocating
for the development of more suitable evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Low-Resource Knowledge Tracing Tasks by Supervised
  <span class="highlight-title">Pre-train</span>ing and Importance Mechanism Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Zitao Liu, Shuyan Huang, Chenming Shang, Bojun Zhan, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on
their historical interactions. Recently, the deep learning based KT (DLKT)
approaches have achieved impressive performance in the KT task. These DLKT
models heavily rely on the large number of available student interactions.
However, due to various reasons such as budget constraints and privacy
concerns, observed interactions are very limited in many real-world scenarios,
a.k.a, low-resource KT datasets. Directly training a DLKT model on a
low-resource KT dataset may lead to overfitting and it is difficult to choose
the appropriate deep neural architecture. Therefore, in this paper, we propose
a low-resource KT framework called LoReKT to address above challenges. Inspired
by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn
transferable parameters and representations from rich-resource KT datasets
during the pre-training stage and subsequently facilitate effective adaptation
to low-resource KT datasets. Specifically, we simplify existing sophisticated
DLKT model architectures with purely a stack of transformer decoders. We design
an encoding mechanism to incorporate student interactions from multiple KT data
sources and develop an importance mechanism to prioritize updating parameters
with high importance while constraining less important ones during the
fine-tuning stage. We evaluate LoReKT on six public KT datasets and
experimental results demonstrate the superiority of our approach in terms of
AUC and Accuracy. To encourage reproducible research, we make our data and code
publicly available at https://anonymous.4open.science/r/LoReKT-C619.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for
  <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit impressive capabilities but also present
risks such as biased content generation and privacy issues. One of the current
alignment techniques includes principle-driven integration, but it faces
challenges arising from the imprecision of manually crafted rules and
inadequate risk perception in models without safety training. To address these,
we introduce Guide-Align, a two-stage approach. Initially, a safety-trained
model identifies potential risks and formulates specific guidelines for various
inputs, establishing a comprehensive library of guidelines and a model for
input-guidelines retrieval. Subsequently, the retrieval model correlates new
inputs with relevant guidelines, which guide LLMs in response generation to
ensure safe and high-quality outputs, thereby aligning with human values. An
additional optional stage involves fine-tuning a model with well-aligned
datasets generated through the process implemented in the second stage. Our
method customizes guidelines to accommodate diverse inputs, thereby enhancing
the fine-grainedness and comprehensiveness of the guideline library.
Furthermore, it incorporates safety expertise from a safety-trained LLM through
a lightweight retrieval model. We evaluate our approach on three benchmarks,
demonstrating significant improvements in LLM security and quality. Notably,
our fine-tuned model, Labrador, even at 13 billion parameters, outperforms
GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking Down the Defenses: A Comparative <span class="highlight-title">Survey</span> of Attacks on Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become a cornerstone in the field of
Natural Language Processing (NLP), offering transformative capabilities in
understanding and generating human-like text. However, with their rising
prominence, the security and vulnerability aspects of these models have
garnered significant attention. This paper presents a comprehensive survey of
the various forms of attacks targeting LLMs, discussing the nature and
mechanisms of these attacks, their potential impacts, and current defense
strategies. We delve into topics such as adversarial attacks that aim to
manipulate model outputs, data poisoning that affects model training, and
privacy concerns related to training data exploitation. The paper also explores
the effectiveness of different attack methodologies, the resilience of LLMs
against these attacks, and the implications for model integrity and user trust.
By examining the latest research, we provide insights into the current
landscape of LLM vulnerabilities and defense mechanisms. Our objective is to
offer a nuanced understanding of LLM attacks, foster awareness within the AI
community, and inspire robust solutions to mitigate these risks in future
developments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Interaction: Enhancing Large <span class="highlight-title">Language Models</span> for Psychiatric
  Behavior Understanding by Dyadic Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzeng Han, Weisi Liu, Xiaolei Huang, Brian Borsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic coding patient behaviors is essential to support decision making
for psychotherapists during the motivational interviewing (MI), a collaborative
communication intervention approach to address psychiatric issues, such as
alcohol and drug addiction. While the behavior coding task has rapidly adapted
machine learning to predict patient states during the MI sessions, lacking of
domain-specific knowledge and overlooking patient-therapist interactions are
major challenges in developing and deploying those models in real practice. To
encounter those challenges, we introduce the Chain-of-Interaction (CoI)
prompting method aiming to contextualize large language models (LLMs) for
psychiatric decision support by the dyadic interactions. The CoI prompting
approach systematically breaks down the coding task into three key reasoning
steps, extract patient engagement, learn therapist question strategies, and
integrates dyadic interactions between patients and therapists. This approach
enables large language models to leverage the coding scheme, patient state, and
domain knowledge for patient behavioral coding. Experiments on real-world
datasets can prove the effectiveness and flexibility of our prompting method
with multiple state-of-the-art LLMs over existing prompting baselines. We have
conducted extensive ablation analysis and demonstrate the critical role of
dyadic interactions in applying LLMs for psychotherapy behavior understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContrastWSD: Enhancing Metaphor Detection with Word Sense Disambiguation
  Following the Metaphor Identification Procedure <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Elzohbi, Richard Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ContrastWSD, a RoBERTa-based metaphor detection model
that integrates the Metaphor Identification Procedure (MIP) and Word Sense
Disambiguation (WSD) to extract and contrast the contextual meaning with the
basic meaning of a word to determine whether it is used metaphorically in a
sentence. By utilizing the word senses derived from a WSD model, our model
enhances the metaphor detection process and outperforms other methods that rely
solely on contextual embeddings or integrate only the basic definitions and
other external knowledge. We evaluate our approach on various benchmark
datasets and compare it with strong baselines, indicating the effectiveness in
advancing metaphor detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted for the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Yao, Zuchao Li, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread use of language models (LMs) in NLP tasks, researchers
have discovered the potential of Chain-of-thought (CoT) to assist LMs in
accomplishing complex reasoning tasks by generating intermediate steps.
However, human thought processes are often non-linear, rather than simply
sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)
reasoning, which models human thought processes not only as a chain but also as
a graph. By representing thought units as nodes and connections between them as
edges, our approach captures the non-sequential nature of human thinking and
allows for a more realistic modeling of thought processes. GoT adopts a
two-stage framework with an additional GoT encoder for thought graph
representation and fuses the graph representation with the original input
representation through a gated fusion mechanism. We evaluate GoT's performance
on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task
(ScienceQA). Our model achieves significant improvement over the strong CoT
baseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%
using the T5-base model over the state-of-the-art Multimodal-CoT on the
ScienceQA test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Benchmarking LLM-based Machine Translation on Cultural Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binwei Yao, Ming Jiang, <span class="highlight-author">Diyi Yang</span>, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating cultural-specific content is crucial for effective cross-cultural
communication. However, many MT systems still struggle to translate sentences
containing cultural-specific entities accurately and understandably. Recent
advancements in in-context learning utilize lightweight prompts to guide large
language models (LLMs) in machine translation tasks. Nevertheless, the
effectiveness of this approach in enhancing machine translation with cultural
awareness remains uncertain. To address this gap, we introduce a new data
curation pipeline to construct a culturally relevant parallel corpus, enriched
with annotations of cultural-specific items. Furthermore, we devise a novel
evaluation metric to assess the understandability of translations in a
reference-free manner by GPT-4. We evaluate a variety of neural machine
translation (NMT) and LLM-based MT systems using our dataset. Additionally, we
propose several prompting strategies for LLMs to incorporate external and
internal cultural knowledge into the translation process. Our results
demonstrate that eliciting explanations can significantly enhance the
understandability of cultural-specific entities, especially those without
well-known translations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-party Response <span class="highlight-title">Generation</span> with Relation Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Dai, Chengyu Huang, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing neural response generation models have achieved impressive
improvements for two-party conversations, which assume that utterances are
sequentially organized. However, many real-world dialogues involve multiple
interlocutors and the structure of conversational context is much more complex,
e.g. utterances from different interlocutors can occur "in parallel". Facing
this challenge, there are works trying to model the relations among utterances
or interlocutors to facilitate response generation with clearer context.
Nonetheless, these methods rely heavily on such relations and all assume that
these are given beforehand, which is impractical and hinders the generality of
such methods. In this work, we propose to automatically infer the relations via
relational thinking on subtle clues inside the conversation context without any
human label, and leverage these relations to guide the neural response
generation. Specifically, we first apply a deep graph random process to fully
consider all possible relations among utterances in the conversational context.
Then the inferred relation graphs are integrated with a variational
auto-encoder framework to train a GAN for structure-aware response generation.
Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark
and the most recent Movie Dialogues show that our method outperforms various
baseline models for multi-party response generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper needs systematic polishment to consider recent development
  in dialogue</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have shown significant reasoning capabilities
by connecting a visual encoder and a large language model. LMMs typically use a
fixed amount of visual tokens, such as the penultimate layer features in the
CLIP visual encoder, as the prefix content. Recent LMMs incorporate more
complex visual inputs, such as high-resolution images and videos, which
increase the number of visual tokens significantly. However, due to the design
of the Transformer architecture, computational costs associated with these
models tend to increase quadratically with the number of input tokens. To
tackle this problem, we explore a token reduction mechanism and find, similar
to prior work, that many visual tokens are spatially redundant. Based on this,
we propose PruMerge, a novel adaptive visual token reduction approach, which
largely reduces the number of visual tokens while maintaining comparable model
performance. We first select the unpruned visual tokens based on their
similarity to class tokens and spatial tokens. We then cluster the pruned
tokens based on key similarity and merge the clustered tokens with the unpruned
tokens to supplement their information. Empirically, when applied to LLaVA-1.5,
our approach can compress the visual tokens by 14.4 times on average, and
achieve comparable performance across diverse visual question-answering and
reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llava-prumerge.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can large <span class="highlight-title">language models</span> explore in-context? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transfer Attack to Image Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Knowledge-Grounded Natural Language Understanding and <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Whitehouse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis investigates how natural language understanding and generation
with transformer models can benefit from grounding the models with knowledge
representations and addresses the following key research questions: (i) Can
knowledge of entities extend its benefits beyond entity-centric tasks, such as
entity linking? (ii) How can we faithfully and effectively extract such
structured knowledge from raw text, especially noisy web text? (iii) How do
other types of knowledge, beyond structured knowledge, contribute to improving
NLP tasks?
  Studies in this thesis find that incorporating relevant and up-to-date
knowledge of entities benefits fake news detection, and entity-focused
code-switching significantly enhances zero-shot cross-lingual transfer on
entity-centric tasks. In terms of effective and faithful approaches to
extracting structured knowledge, it is observed that integrating negative
examples and training with entity planning significantly improves performance.
Additionally, it is established that other general forms of knowledge, such as
parametric and distilled knowledge, enhance multimodal and multilingual
knowledge-intensive tasks. This research shows the tangible benefits of diverse
knowledge integration and motivates further exploration in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoLLEGe: Concept Embedding <span class="highlight-title">Generation</span> for Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Teehan, Brenden Lake, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models are unable to quickly learn new concepts on the fly,
often requiring a more involved finetuning process to learn robustly. Prompting
in-context is not robust to context distractions, and often fails to confer
much information about the new concepts. Classic methods for few-shot word
learning in NLP, relying on global word vectors, are less applicable to large
language models. In this paper, we introduce a novel approach named CoLLEGe
(Concept Learning with Language Embedding Generation) to modernize few-shot
concept learning. CoLLEGe is a meta-learning framework capable of generating
flexible embeddings for new concepts using a small number of example sentences
or definitions. Our primary meta-learning objective is simply to facilitate a
language model to make next word predictions in forthcoming sentences, making
it compatible with language model pretraining. We design a series of tasks to
test new concept learning in challenging real-world scenarios, including new
word acquisition, definition inference, and verbal reasoning, and demonstrate
that our method succeeds in each setting without task-specific training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-<span class="highlight-title">Review</span> Fusion-in-Context <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounded text generation, encompassing tasks such as long-form
question-answering and summarization, necessitates both content selection and
content consolidation. Current end-to-end methods are difficult to control and
interpret due to their opaqueness. Accordingly, recent works have proposed a
modular approach, with separate components for each step. Specifically, we
focus on the second subtask, of generating coherent text given pre-selected
content in a multi-document setting. Concretely, we formalize
\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of
source texts with highlighted spans of targeted content. A model then needs to
generate a coherent passage that includes all and only the target information.
Our work includes the development of a curated dataset of 1000 instances in the
reviews domain, alongside a novel evaluation framework for assessing the
faithfulness and coverage of highlights, which strongly correlate to human
judgment. Several baseline models exhibit promising outcomes and provide
insightful analyses. This study lays the groundwork for further exploration of
modular text generation in the multi-document setting, offering potential
improvements in the quality and reliability of generated content. \footnote{Our
benchmark, FuseReviews, including the dataset, evaluation framework and
designated leaderboard, can be found at \url{https://fusereviews.github.io/}.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024, findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CO-Fun: A German <span class="highlight-title">Dataset</span> on Company Outsourcing in Fund Prospectuses for
  Named Entity Recognition and Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neda Foroutan, Markus Schröder, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of cyber mapping gives insights in relationships among financial
entities and service providers. Centered around the outsourcing practices of
companies within fund prospectuses in Germany, we introduce a dataset
specifically designed for named entity recognition and relation extraction
tasks. The labeling process on 948 sentences was carried out by three experts
which yields to 5,969 annotations for four entity types (Outsourcing, Company,
Location and Software) and 4,102 relation annotations (Outsourcing-Company,
Company-Location). State-of-the-art deep learning models were trained to
recognize entities and extract relations showing first promising results. An
anonymized version of the dataset, along with guidelines and the code used for
model training, are publicly available at
https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Training Data <span class="highlight-title">Generation</span> with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to control a text-to-image generative model
to produce training data specifically "useful" for supervised learning. Unlike
previous works that employ an open-loop approach and pre-define prompts to
generate new data using either a language model or human expertise, we develop
an automated closed-loop system which involves two feedback mechanisms. The
first mechanism uses feedback from a given supervised model and finds
adversarial prompts that result in image generations that maximize the model
loss. While these adversarial prompts result in diverse data informed by the
model, they are not informed of the target distribution, which can be
inefficient. Therefore, we introduce the second feedback mechanism that guides
the generation process towards a certain target distribution. We call the
method combining these two mechanisms Guided Adversarial Prompts. We perform
our evaluations on different tasks, datasets and architectures, with different
types of distribution shifts (spuriously correlated data, unseen domains) and
demonstrate the efficiency of the proposed feedback mechanisms compared to
open-loop approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://adversarial-prompts.epfl.ch/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human behaviour through a LENS: How Linguistic content triggers Emotions
  and Norms and determines Strategy choices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Capraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last two decades, a growing body of experimental research has
provided evidence that linguistic frames influence human behaviour in economic
games, beyond the economic consequences of the available actions. This article
proposes a novel framework that transcends the traditional confines of
outcome-based preference models. According to the LENS model, the Linguistic
description of the decision problem triggers Emotional responses and suggests
potential Norms of behaviour, which then interact to shape an individual's
Strategic choice. The article reviews experimental evidence that supports each
path of the LENS model. Furthermore, it identifies and discusses several
critical research questions that arise from this model, pointing towards
avenues for future inquiry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, submitted to ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specifying Genericity through Inclusiveness and Abstractness Continuous
  Scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudia Collacciani, Andrea Amelio Ravelli, Marianna Marcella Bolognesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel annotation framework for the fine-grained
modeling of Noun Phrases' (NPs) genericity in natural language. The framework
is designed to be simple and intuitive, making it accessible to non-expert
annotators and suitable for crowd-sourced tasks. Drawing from theoretical and
cognitive literature on genericity, this framework is grounded in established
linguistic theory. Through a pilot study, we created a small but crucial
annotated dataset of 324 sentences, serving as a foundation for future
research. To validate our approach, we conducted an evaluation comparing our
continuous annotations with existing binary annotations on the same dataset,
demonstrating the framework's effectiveness in capturing nuanced aspects of
genericity. Our work offers a practical resource for linguists, providing a
first annotated dataset and an annotation scheme designed to build
real-language datasets that can be used in studies on the semantics of
genericity, and NLP practitioners, contributing to the development of
commonsense knowledge repositories valuable in enhancing various NLP
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs <span class="chip">IJCNN2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event temporal relation (TempRel) is a primary subject of the event relation
extraction task. However, the inherent ambiguity of TempRel increases the
difficulty of the task. With the rise of prompt engineering, it is important to
design effective prompt templates and verbalizers to extract relevant
knowledge. The traditional manually designed templates struggle to extract
precise temporal knowledge. This paper introduces a novel retrieval-augmented
TempRel extraction approach, leveraging knowledge retrieved from large language
models (LLMs) to enhance prompt templates and verbalizers. Our method
capitalizes on the diverse capabilities of various LLMs to generate a wide
array of ideas for template and verbalizer design. Our proposed method fully
exploits the potential of LLMs for generation tasks and contributes more
knowledge to our design. Empirical evaluations across three widely recognized
datasets demonstrate the efficacy of our method in improving the performance of
event temporal relation extraction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,6 figures.Accepted to the International Joint Conference on
  Neural Networks (IJCNN2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagination Augmented <span class="highlight-title">Generation</span>: Learning to Imagine Richer Context for
  Question Answering over Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been
proposed to enhance the knowledge required for question answering over Large
Language Models (LLMs). However, the former depends on external resources, and
both require incorporating the explicit documents into the context, which
results in longer contexts that lead to more resource consumption. Recent works
indicate that LLMs have modeled rich knowledge, albeit not effectively
triggered or activated. Inspired by this, we propose a novel
knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which
simulates the human capacity to compensate for knowledge deficits while
answering questions solely through imagination, without relying on external
resources. Guided by IAG, we propose an imagine richer context method for
question answering (IMcQA), which obtains richer context through the following
two modules: explicit imagination by generating a short dummy document with
long context compress and implicit imagination with HyperNetwork for generating
adapter weights. Experimental results on three datasets demonstrate that IMcQA
exhibits significant advantages in both open-domain and closed-book settings,
as well as in both in-distribution performance and out-of-distribution
generalizations. Our code will be available at
https://github.com/Xnhyacinth/IAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A
  Multifaceted Statistical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid evolution of LLMs, the significance of evaluation in
comprehending and propelling these models forward is increasingly paramount.
Evaluations have revealed that factors such as scaling, training types,
architectures and other factors profoundly impact the performance of LLMs.
However, the extent and nature of these impacts continue to be subjects of
debate because most assessments have been restricted to a limited number of
models and data points. Clarifying the effects of these factors on performance
scores can be more effectively achieved through a statistical lens. Our study
embarks on a thorough re-examination of these LLMs, targeting the inadequacies
in current evaluation methods. With the advent of a uniform evaluation
framework, our research leverages an expansive dataset of evaluation results,
introducing a comprehensive statistical methodology. This includes the
application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering
a robust and transparent approach to deciphering LLM performance data. Contrary
to prevailing findings, our results challenge assumptions about emergent
abilities and the influence of given training types and architectures in LLMs.
These findings furnish new perspectives on the characteristics, intrinsic
nature, and developmental trajectories of LLMs. By providing straightforward
and reliable methods to scrutinize and reassess LLM performance data, this
study contributes a nuanced perspective on LLM efficiency and potentials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  <span class="highlight-title">Instruct</span>ions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Attention is Needed: Parameter and Computation Efficient
  Transfer Learning for Multi-modal Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel parameter and computation efficient tuning
method for Multi-modal Large Language Models (MLLMs), termed Efficient
Attention Skipping (EAS). Concretely, we first reveal that multi-head
attentions (MHAs), the main computational overhead of MLLMs, are often
redundant to downstream tasks. Based on this observation, EAS evaluates the
attention redundancy and skips the less important MHAs to speed up inference.
Besides, we also propose a novel propagation-of-information adapter (PIA) to
serve the attention skipping of EAS and keep parameter efficiency, which can be
further re-parameterized into feed-forward networks (FFNs) for zero-extra
latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN
and a classic VL pre-trained model called METER, and conduct extensive
experiments on a set of benchmarks. The experiments show that EAS not only
retains high performance and parameter efficiency, but also greatly speeds up
inference speed. For instance, LaVIN-EAS can obtain 89.98\% accuracy on
ScineceQA while speeding up inference by 2.2 times to LaVIN
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstaSynth: Opportunities and Challenges in Generating Synthetic
  Instagram Data with <span class="highlight-title">ChatGPT</span> for Sponsored Content Detection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) raise concerns about lowering the cost of
generating texts that could be used for unethical or illegal purposes,
especially on social media. This paper investigates the promise of such models
to help enforce legal requirements related to the disclosure of sponsored
content online. We investigate the use of LLMs for generating synthetic
Instagram captions with two objectives: The first objective (fidelity) is to
produce realistic synthetic datasets. For this, we implement content-level and
network-level metrics to assess whether synthetic captions are realistic. The
second objective (utility) is to create synthetic data that is useful for
sponsored content detection. For this, we evaluate the effectiveness of the
generated synthetic data for training classifiers to identify undisclosed
advertisements on Instagram. Our investigations show that the objectives of
fidelity and utility may conflict and that prompt engineering is a useful but
insufficient strategy. Additionally, we find that while individual synthetic
posts may appear realistic, collectively they lack diversity, topic
connectivity, and realistic user interaction patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 18th International AAAI Conference on Web and Social
  Media (ICWSM 2024) -- please cite accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Performance of <span class="highlight-title">Language Models</span> for Completing Code in
  Functional Programming Languages: a Haskell Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim van Dam, Frank van der Heijden, Philippe de Bekker, Berend Nieuwschepen, Marc Otten, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model-based code completion models have quickly grown in use,
helping thousands of developers write code in many different programming
languages. However, research on code completion models typically focuses on
imperative languages such as Python and JavaScript, which results in a lack of
representation for functional programming languages. Consequently, these models
often perform poorly on functional languages such as Haskell. To investigate
whether this can be alleviated, we evaluate the performance of two language
models for code, CodeGPT and UniXcoder, on the functional programming language
Haskell. We fine-tune and evaluate the models on Haskell functions sourced from
a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually
evaluate the models using our novel translated HumanEval dataset. Our automatic
evaluation shows that knowledge of imperative programming languages in the
pre-training of LLMs may not transfer well to functional languages, but that
code completion on functional languages is feasible. Consequently, this shows
the need for more high-quality Haskell datasets. A manual evaluation on
HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and
extra comments, while UniXcoder more often produces incomplete or incorrect
predictions. Finally, we release HumanEval-Haskell, along with the fine-tuned
models and all code required to reproduce our experiments on GitHub
(https://github.com/AISE-TUDelft/HaskellCCEval).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the First Special Event on AI Foundation Models and
  Software Engineering (FORGE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CACA Agent: Capability Collaboration based AI Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xu, Haoran Wang, Chuang Wang, Xu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI Agents based on Large Language Models (LLMs) have shown potential in
practical applications across various fields, how to quickly deploy an AI agent
and how to conveniently expand the application scenario of AI agents has become
a challenge. Previous studies mainly focused on implementing all the reasoning
capabilities of AI agents within a single LLM, which often makes the model more
complex and also reduces the extensibility of AI agent functionality. In this
paper, we propose CACA Agent (Capability Collaboration based AI Agent), using
an open architecture inspired by service computing. CACA Agent integrates a set
of collaborative capabilities to implement AI Agents, not only reducing the
dependence on a single LLM, but also enhancing the extensibility of both the
planning abilities and the tools available to AI agents. Utilizing the proposed
system, we present a demo to illustrate the operation and the application
scenario extension of CACA Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language Models</span> in Dialogue: Conversational Maxims for Human-AI
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models, while sophisticated, exhibit some inherent
shortcomings, particularly in conversational settings. We claim that many of
the observed shortcomings can be attributed to violation of one or more
conversational principles. By drawing upon extensive research from both the
social science and AI communities, we propose a set of maxims -- quantity,
quality, relevance, manner, benevolence, and transparency -- for describing
effective human-AI conversation. We first justify the applicability of the
first four maxims (from Grice) in the context of human-AI interactions. We then
argue that two new maxims, benevolence (concerning the generation of, and
engagement with, harmful content) and transparency (concerning recognition of
one's knowledge boundaries, operational constraints, and intents), are
necessary for addressing behavior unique to modern human-AI interactions. The
proposed maxims offer prescriptive guidance on how to assess conversational
quality between humans and LLM-driven conversational agents, informing both
their evaluation and improved design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text clustering with LLM embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text clustering is an important approach for organising the growing amount of
digital content, helping to structure and find hidden patterns in uncategorised
data. In this research, we investigated how different textual embeddings -
particularly those used in large language models (LLMs) - and clustering
algorithms affect how text datasets are clustered. A series of experiments were
conducted to assess how embeddings influence clustering results, the role
played by dimensionality reduction through summarisation, and embedding size
adjustment. Results reveal that LLM embeddings excel at capturing the nuances
of structured language, while BERT leads the lightweight options in
performance. In addition, we find that increasing embedding dimensionality and
summarisation techniques do not uniformly improve clustering efficiency,
suggesting that these strategies require careful analysis to use in real-life
models. These results highlight a complex balance between the need for nuanced
text representation and computational feasibility in text clustering
applications. This study extends traditional text clustering frameworks by
incorporating embeddings from LLMs, thereby paving the way for improved
methodologies and opening new avenues for future research in various types of
textual analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argument-Aware Approach To Event Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I-Hung Hsu, Zihan Xue, Nilay Pochh, Sahil Bansal, Premkumar Natarajan, Jayanth Srinivasa, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event linking connects event mentions in text with relevant nodes in a
knowledge base (KB). Prior research in event linking has mainly borrowed
methods from entity linking, overlooking the distinct features of events.
Compared to the extensively explored entity linking task, events have more
complex structures and can be more effectively distinguished by examining their
associated arguments. Moreover, the information-rich nature of events leads to
the scarcity of event KBs. This emphasizes the need for event linking models to
identify and classify event mentions not in the KB as ``out-of-KB,'' an area
that has received limited attention. In this work, we tackle these challenges
by introducing an argument-aware approach. First, we improve event linking
models by augmenting input text with tagged event argument information,
facilitating the recognition of key information about event mentions.
Subsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize
out-of-KB training examples from in-KB instances through controlled
manipulation of event arguments. Our experiment across two test datasets showed
significant enhancements in both in-KB and out-of-KB scenarios, with a notable
22% improvement in out-of-KB evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work In Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHisIEC: An Information Extraction Corpus for Ancient Chinese History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemei Tang, Zekun Deng, Qi Su, Hao Yang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays a pivotal role in the realm of
Digital Humanities (DH) and serves as the cornerstone for advancing the
structural analysis of historical and cultural heritage texts. This is
particularly true for the domains of named entity recognition (NER) and
relation extraction (RE). In our commitment to expediting ancient history and
culture, we present the ``Chinese Historical Information Extraction
Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to
develop and evaluate NER and RE tasks, offering a resource to facilitate
research in the field. Spanning a remarkable historical timeline encompassing
data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the
extensive temporal range and text heterogeneity inherent in Chinese historical
documents. The dataset encompasses four distinct entity types and twelve
relation types, resulting in a meticulously labeled dataset comprising 14,194
entities and 8,609 relations. To establish the robustness and versatility of
our dataset, we have undertaken comprehensive experimentation involving models
of various sizes and paradigms. Additionally, we have evaluated the
capabilities of Large Language Models (LLMs) in the context of tasks related to
ancient Chinese history. The dataset and code are available at
\url{https://github.com/tangxuemei1995/CHisIEC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Construction of a Japanese Financial Benchmark for Large <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanori Hirano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent development of large language models (LLMs), models that
focus on certain domains and languages have been discussed for their necessity.
There is also a growing need for benchmarks to evaluate the performance of
current LLMs in each domain. Therefore, in this study, we constructed a
benchmark comprising multiple tasks specific to the Japanese and financial
domains and performed benchmark measurements on some models. Consequently, we
confirmed that GPT-4 is currently outstanding, and that the constructed
benchmarks function effectively. According to our analysis, our benchmark can
differentiate benchmark scores among models in all performance ranges by
combining tasks with different difficulties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Joint Workshop of the 7th Financial Technology and Natural
  Language Processing (FinNLP), the 5th Knowledge Discovery from Unstructured
  Data in Financial Services (KDF), and The 4th Workshop on Economics and
  Natural Language Processing (ECONLP) In conjunction with LREC-COLING-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipali, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained large language models (LLMs) are currently state-of-the-art for
solving the vast majority of natural language processing tasks. While many
real-world applications still require fine-tuning to reach satisfactory levels
of performance, many of them are in the low-data regime, making fine-tuning
challenging. To address this, we propose LLM2LLM, a targeted and iterative data
augmentation strategy that uses a teacher LLM to enhance a small seed dataset
by augmenting additional data that can be used for fine-tuning on a specific
task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,
(2) evaluates and extracts data points that the model gets wrong, and (3) uses
a teacher LLM to generate synthetic data based on these incorrect data points,
which are then added back into the training data. This approach amplifies the
signal from incorrectly predicted data points by the LLM during training and
reintegrates them into the dataset to focus on more challenging examples for
the LLM. Our results show that LLM2LLM significantly enhances the performance
of LLMs in the low-data regime, outperforming both traditional fine-tuning and
other data augmentation baselines. LLM2LLM reduces the dependence on
labor-intensive data curation and paves the way for more scalable and
performant LLM solutions, allowing us to tackle data-constrained domains and
tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on
CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular
fine-tuning in the low-data regime using a LLaMA2-7B student model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/SqueezeAILab/LLM2LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESG Classification by Implicit Rule Learning via <span class="highlight-title">GPT</span>-4 <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyo Jeong Yun, Chanyoung Kim, Moonjeong Hahm, Kyuri Kim, Guijin Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental, social, and governance (ESG) factors are widely adopted as
higher investment return indicators. Accordingly, ongoing efforts are being
made to automate ESG evaluation with language models to extract signals from
massive web text easily. However, recent approaches suffer from a lack of
training data, as rating agencies keep their evaluation metrics confidential.
This paper investigates whether state-of-the-art language models like GPT-4 can
be guided to align with unknown ESG evaluation criteria through strategies such
as prompting, chain-of-thought reasoning, and dynamic in-context learning. We
demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task
ML-ESG-3 Impact Type track for Korean without updating the model on the
provided training data. We also explore how adjusting prompts impacts the
ability of language models to address financial tasks leveraging smaller models
with openly available weights. We observe longer general pre-training to
correlate with enhanced performance in financial downstream tasks. Our findings
showcase the potential of language models to navigate complex, subjective
evaluation guidelines despite lacking explicit training examples, revealing
opportunities for training-free solutions for financial downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Shared Track Paper at 7th FinNLP Workshop @ LREC-COLING
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic
  Textual Relatedness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the MasonTigers entry to the SemEval-2024 Task 1 -
Semantic Textual Relatedness. The task encompasses supervised (Track A),
unsupervised (Track B), and cross-lingual (Track C) approaches across 14
different languages. MasonTigers stands out as one of the two teams who
participated in all languages across the three tracks. Our approaches achieved
rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and
from 5th to 12th in Track C. Adhering to the task-specific constraints, our
best performing approaches utilize ensemble of statistical machine learning
approaches combined with language-specific BERT based models and sentence
transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MasonTigers at SemEval-2024 Task 8: Performance Analysis of
  <span class="highlight-title">Transformer</span>-based Models on Machine-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Amrita Ganguly, Ozlem Uzuner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the MasonTigers entry to the SemEval-2024 Task 8 -
Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text
Classification (Track A), Multi-Way Machine-Generated Text Classification
(Track B), and Human-Machine Mixed Text Detection (Track C). Our best
performing approaches utilize mainly the ensemble of discriminator transformer
models along with sentence transformer and statistical machine learning
approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of
FLAN-T5 are used for Track A and B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk and Response in Large <span class="highlight-title">Language Models</span>: Evaluating Key Threat
  Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahareh Harandizadeh, Abel Salinas, Fred Morstatter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pressing issue of risk assessment in Large Language
Models (LLMs) as they become increasingly prevalent in various applications.
Focusing on how reward models, which are designed to fine-tune pretrained LLMs
to align with human values, perceive and categorize different types of risks,
we delve into the challenges posed by the subjective nature of preference-based
training data. By utilizing the Anthropic Red-team dataset, we analyze major
risk categories, including Information Hazards, Malicious Uses, and
Discrimination/Hateful content. Our findings indicate that LLMs tend to
consider Information Hazards less harmful, a finding confirmed by a specially
developed regression model. Additionally, our analysis shows that LLMs respond
less stringently to Information Hazards compared to other risks. The study
further reveals a significant vulnerability of LLMs to jailbreaking attacks in
Information Hazard scenarios, highlighting a critical security concern in LLM
risk assessment and emphasizing the need for improved AI safety measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of
  Chain-of-Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Sadiya Sayara Chowdhury Puspo, Amrita Ganguly, Marcos Zampieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -
which provides a dataset of puzzles for testing natural language understanding.
We employ large language models (LLMs) to solve this task through several
prompting techniques. Zero-shot and few-shot prompting generate reasonably good
results when tested with proprietary LLMs, compared to the open-source models.
We obtain further improved results with chain-of-thought prompting, an
iterative prompting method that breaks down the reasoning process step-by-step.
We obtain our best results by utilizing an ensemble of chain-of-thought
prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle
subtask. The strong performance of prompted LLMs demonstrates their capability
for complex reasoning when provided with a decomposition of the thought
process. Our work sheds light on how step-wise explanatory prompts can unlock
more of the knowledge encoded in the parameters of large models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a pilot study aimed at introducing multi-agent debate
into multimodal reasoning. The study addresses two key challenges: the
trivialization of opinions resulting from excessive summarization and the
diversion of focus caused by distractor concepts introduced from images. These
challenges stem from the inductive (bottom-up) nature of existing debating
schemes. To address the issue, we propose a deductive (top-down) debating
approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are
confined to a blueprint graph to prevent opinion trivialization through
world-level summarization. Moreover, by storing evidence in branches within the
graph, BDoG mitigates distractions caused by frequent but irrelevant concepts.
Extensive experiments validate BDoG, achieving state-of-the-art results in
Science QA and MMBench with significant improvements over previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapprox: Adaptive Approximation in Adam Optimization via Randomized
  Low-Rank Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kölker, Zhefeng Wang, Xiaoming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning models exponentially increase in size, optimizers such as
Adam encounter significant memory consumption challenges due to the storage of
first and second moment data. Current memory-efficient methods like Adafactor
and CAME often compromise accuracy with their matrix factorization techniques.
Addressing this, we introduce Adapprox, a novel approach that employs
randomized low-rank matrix approximation for a more effective and accurate
approximation of Adam's second moment. Adapprox features an adaptive rank
selection mechanism, finely balancing accuracy and memory efficiency, and
includes an optional cosine similarity guidance strategy to enhance stability
and expedite convergence. In GPT-2 training and downstream tasks, Adapprox
surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings
for the 117M and 345M models, respectively, with the first moment enabled, and
further increases these savings without the first moment. Besides, it enhances
convergence speed and improves downstream task performance relative to its
counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidence-Driven Retrieval Augmented Response <span class="highlight-title">Generation</span> for Online
  Misinformation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of online misinformation has posed significant threats to
public interest. While numerous online users actively participate in the combat
against misinformation, many of such responses can be characterized by the lack
of politeness and supporting facts. As a solution, text generation approaches
are proposed to automatically produce counter-misinformation responses.
Nevertheless, existing methods are often trained end-to-end without leveraging
external knowledge, resulting in subpar text quality and excessively repetitive
responses. In this paper, we propose retrieval augmented response generation
for online misinformation (RARG), which collects supporting evidence from
scientific sources and generates counter-misinformation responses based on the
evidences. In particular, our RARG consists of two stages: (1) evidence
collection, where we design a retrieval pipeline to retrieve and rerank
evidence documents using a database comprising over 1M academic articles; (2)
response generation, in which we align large language models (LLMs) to generate
evidence-based responses via reinforcement learning from human feedback (RLHF).
We propose a reward function to maximize the utilization of the retrieved
evidence while maintaining the quality of the generated text, which yields
polite and factual responses that clearly refutes misinformation. To
demonstrate the effectiveness of our method, we study the case of COVID-19 and
perform extensive experiments with both in- and cross-domain datasets, where
RARG consistently outperforms baselines by generating high-quality
counter-misinformation responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable
  Adaptation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient finetuning (PEFT) is a key technique for adapting large
language models (LLMs) to downstream tasks. In this paper, we study leveraging
knowledge graph embeddings to improve the effectiveness of PEFT. We propose a
knowledgeable adaptation method called KnowLA. It inserts an adaptation layer
into an LLM to integrate the embeddings of entities appearing in the input
text. The adaptation layer is trained in combination with LoRA on instruction
data. Experiments on six benchmarks with two popular LLMs and three knowledge
graphs demonstrate the effectiveness and robustness of KnowLA. We show that
\modelname can help activate the relevant parameterized knowledge in an LLM to
answer a question without changing its parameters or input prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 2024 Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Single Linear Layer Yields Task-Adapted Low-Rank Matrices <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwichan Kim, Shota Sasaki, Sho Hoshino, Ukyo Honda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning
(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix
$\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study
suggested that there is correlation between $W_0$ and $\Delta W$. In this
study, we aim to delve deeper into relationships between $W_0$ and low-rank
matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,
we analyze a conversion matrix that transform $W_0$ into low-rank matrices,
which encapsulates information about the relationships. Our analysis reveals
that the conversion matrices are similar across each layer. Inspired by these
findings, we hypothesize that a single linear layer, which takes each layer's
$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this
hypothesis, we devise a method named Conditionally Parameterized LoRA
(CondLoRA) that updates initial weight matrices with low-rank matrices derived
from a single linear layer. Our empirical results show that CondLoRA maintains
a performance on par with LoRA, despite the fact that the trainable parameters
of CondLoRA are fewer than those of LoRA. Therefore, we conclude that "a single
linear layer yields task-adapted low-rank matrices."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Zero-Shot Counterspeech <span class="highlight-title">Generation</span> by LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Punyajoy Saha, Aalok Agrawal, Abhik Jana, Chris Biemann, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of numerous Large Language Models (LLM), the usage of such
models in various Natural Language Processing (NLP) applications is increasing
extensively. Counterspeech generation is one such key task where efforts are
made to develop generative models by fine-tuning LLMs with hatespeech -
counterspeech pairs, but none of these attempts explores the intrinsic
properties of large language models in zero-shot settings. In this work, we
present a comprehensive analysis of the performances of four LLMs namely GPT-2,
DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech
generation, which is the first of its kind. For GPT-2 and DialoGPT, we further
investigate the deviation in performance with respect to the sizes (small,
medium, large) of the models. On the other hand, we propose three different
prompting strategies for generating different types of counterspeech and
analyse the impact of such strategies on the performance of the models. Our
analysis shows that there is an improvement in generation quality for two
datasets (17%), however the toxicity increase (25%) with increase in model
size. Considering type of model, GPT-2 and FlanT5 models are significantly
better in terms of counterspeech quality but also have high toxicity as
compared to DialoGPT. ChatGPT are much better at generating counter speech than
other models across all metrics. In terms of prompting, we find that our
proposed strategies help in improving counter speech generation across all the
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 tables, accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-Driven Reasoning: Unlocking the Potential of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingli Liao, Danilo Vasconcellos Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities, but their
reasoning abilities and underlying mechanisms remain poorly understood. We
present a novel approach to enhance LLMs' reasoning through attention mechanism
optimization, without additional training data. We identify inefficiencies in
the attention distribution caused by non-semantic tokens and propose an
algorithm to re-balance the skewed distribution, enabling the model to abstract
more nuanced knowledge. Our experiments demonstrate significantly improved
reasoning capabilities, particularly for non-STEM questions. We provide
insights into the role of attention patterns in LLMs' reasoning and propose a
method to enhance these abilities, paving the way for more powerful and
versatile language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Skip Decoding for Efficient Autoregressive Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive decoding strategy is a commonly used method for text
generation tasks with pre-trained language models, while early-exiting is an
effective approach to speedup the inference stage. In this work, we propose a
novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient
autoregressive text generation. Different from existing methods that require
additional trainable components, HSD is a plug-and-play method applicable to
autoregressive text generation models, it adaptively skips decoding layers in a
hierarchical manner based on the current sequence length, thereby reducing
computational workload and allocating computation resources. Comprehensive
experiments on five text generation datasets with pre-trained language models
demonstrate HSD's advantages in balancing efficiency and text quality. With
almost half of the layers skipped, HSD can sustain 90% of the text quality
compared to vanilla autoregressive decoding, outperforming the competitive
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stance Reasoner: Zero-Shot Stance Detection on Social Media with
  Explicit Reasoning <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Taranukhin, Vered Shwartz, Evangelos Milios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms are rich sources of opinionated content. Stance
detection allows the automatic extraction of users' opinions on various topics
from such content. We focus on zero-shot stance detection, where the model's
success relies on (a) having knowledge about the target topic; and (b) learning
general reasoning strategies that can be employed for new topics. We present
Stance Reasoner, an approach to zero-shot stance detection on social media that
leverages explicit reasoning over background knowledge to guide the model's
inference about the document's stance on a target. Specifically, our method
uses a pre-trained language model as a source of world knowledge, with the
chain-of-thought in-context learning approach to generate intermediate
reasoning steps. Stance Reasoner outperforms the current state-of-the-art
models on 3 Twitter datasets, including fully supervised models. It can better
generalize across targets, while at the same time providing explicit and
interpretable explanations for its predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaCmS: Magahi Code-mixed <span class="highlight-title">Dataset</span> for Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present paper introduces new sentiment data, MaCMS, for
Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a
less-resourced minority language. This dataset is the first
Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,
we also provide a linguistics analysis of the dataset to understand the
structure of code-mixing and a statistical study to understand the language
preferences of speakers with different polarities. With these analyses, we also
train baseline models to evaluate the dataset's quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Lrec-Colin 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMR: Real-time <span class="highlight-title">Prompt</span>ing of Interactive Worlds using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 18 figures; Matching version accepted at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Efficient Universal Classifiers with Natural Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Laurer, Wouter van Atteveldt, Andreu Casas, Kasper Welbers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs) have become the mainstream choice for
fewshot and zeroshot learning thanks to the universality of text generation.
Many users, however, do not need the broad capabilities of generative LLMs when
they only want to automate a classification task. Smaller BERT-like models can
also learn universal tasks, which allow them to do any text classification task
without requiring fine-tuning (zeroshot classification) or to learn new tasks
with only a few examples (fewshot), while being significantly more efficient
than generative LLMs. This paper (1) explains how Natural Language Inference
(NLI) can be used as a universal classification task that follows similar
principles as instruction fine-tuning of generative LLMs, (2) provides a
step-by-step guide with reusable Jupyter notebooks for building a universal
classifier, and (3) shares the resulting universal classifier that is trained
on 33 datasets with 389 diverse classes. Parts of the code we share has been
used to train our older zeroshot classifiers that have been downloaded more
than 55 million times via the Hugging Face Hub as of December 2023. Our new
classifier improves zeroshot performance by 9.4%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10311v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10311v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The word order of a sentence is shaped by multiple principles. The principle
of syntactic dependency distance minimization is in conflict with the principle
of surprisal minimization (or predictability maximization) in single head
syntactic dependency structures: while the former predicts that the head should
be placed at the center of the linear arrangement, the latter predicts that the
head should be placed at one of the ends (either first or last). A critical
question is when surprisal minimization (or predictability maximization) should
surpass syntactic dependency distance minimization. In the context of single
head structures, it has been predicted that this is more likely to happen when
two conditions are met, i.e. (a) fewer words are involved and (b) words are
shorter. Here we test the prediction on the noun phrase when it is composed of
a demonstrative, a numeral, an adjective and a noun. We find that, across
preferred orders in languages, the noun tends to be placed at one of the ends,
confirming the theoretical prediction. We also show evidence of anti locality
effects: syntactic dependency distances in preferred orders are longer than
expected by chance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Drafter for Fast Speculative Decoding in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an improved approach of speculative decoding
aimed at enhancing the efficiency of serving large language models. Our method
capitalizes on the strengths of two established techniques: the classic
two-model speculative decoding approach, and the more recent single-model
approach, Medusa. Drawing inspiration from Medusa, our approach adopts a
single-model strategy for speculative decoding. However, our method
distinguishes itself by employing a single, lightweight draft head with a
recurrent dependency design, akin in essence to the small, draft model uses in
classic speculative decoding, but without the complexities of the full
transformer architecture. And because of the recurrent dependency, we can use
beam search to swiftly filter out undesired candidates with the draft head. The
outcome is a method that combines the simplicity of single-model design and
avoids the need to create a data-dependent tree attention structure only for
inference in Medusa. We empirically demonstrate the effectiveness of the
proposed method on several popular open source language models, along with a
comprehensive analysis of the trade-offs involved in adopting this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure (HF) poses a significant public health challenge, with a rising
global mortality rate. Early detection and prevention of HF could significantly
reduce its impact. We introduce a novel methodology for predicting HF risk
using 12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF risk prediction, despite the notable imbalance between low and
high-risk groups. This network incorporates a cross-lead attention module and
twelve lead-specific temporal attention modules, focusing on cross-lead
interactions and each lead's local dynamics. To further alleviate model
overfitting, we leverage a large language model (LLM) with a public ECG-Report
dataset for pretraining on an ECG-report alignment task. The network is then
fine-tuned for HF risk prediction using two specific cohorts from the UK
Biobank study, focusing on patients with hypertension (UKB-HYP) and those who
have had a myocardial infarction (UKB-MI).The results reveal that LLM-informed
pre-training substantially enhances HF risk prediction in these cohorts. The
dual-attention design not only improves interpretability but also predictive
accuracy, outperforming existing competitive methods with C-index scores of
0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's
potential in advancing HF risk assessment with clinical complex ECG data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under journal revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with
  Fact-Checking in Turkish <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid spread of misinformation through social media platforms has raised
concerns regarding its impact on public opinion. While misinformation is
prevalent in other languages, the majority of research in this field has
concentrated on the English language. Hence, there is a scarcity of datasets
for other languages, including Turkish. To address this concern, we have
introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset
spans multiple domains and incorporates evidence collected from three Turkish
fact-checking organizations. Additionally, we aim to assess the effectiveness
of cross-lingual transfer learning for low-resource languages, with a
particular focus on Turkish. We demonstrate in-context learning (zero-shot and
few-shot) performance of large language models in this context. The
experimental results indicate that the dataset has the potential to advance
research in the Turkish language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness of the Random Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Lalegani, Eric De Giuli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Random Language Model (De Giuli 2019) is an ensemble of stochastic
context-free grammars, quantifying the syntax of human and computer languages.
The model suggests a simple picture of first language learning as a type of
annealing in the vast space of potential languages. In its simplest
formulation, it implies a single continuous transition to grammatical syntax,
at which the symmetry among potential words and categories is spontaneously
broken. Here this picture is scrutinized by considering its robustness against
extensions of the original model, and trajectories through parameter space
different from those originally considered. It is shown here that (i) the
scenario is robust to explicit symmetry breaking, an inevitable component of
learning in the real world; and (ii) the transition to grammatical syntax can
be encountered by fixing the deep (hidden) structure while varying the surface
(observable) properties. It is also argued that the transition becomes a sharp
thermodynamic transition in an idealized limit. Moreover, comparison with human
data on the clustering coefficient of syntax networks suggests that the
observed transition is equivalent to that normally experienced by children at
age 24 months. The results are discussed in light of theory of first-language
acquisition in linguistics, and recent successes in machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages; v2: expanded discussion throughout</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision<span class="highlight-title">GPT</span>-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
VisionGPT-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. VisionGPT-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text prompts.
  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Llama</span> meets EU: Investigating the European Political Spectrum through
  the Lens of LLMs <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Chalkidis, Stephanie Brandl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-finetuned Large Language Models inherit clear political leanings
that have been shown to influence downstream task performance. We expand this
line of research beyond the two-party system in the US and audit Llama Chat in
the context of EU politics in various settings to analyze the model's political
knowledge and its ability to reason in context. We adapt, i.e., further
fine-tune, Llama Chat on speeches of individual euro-parties from debates in
the European Parliament to reevaluate its political leaning based on the EUandI
questionnaire. Llama Chat shows considerable knowledge of national parties'
positions and is capable of reasoning in context. The adapted, party-specific,
models are substantially re-aligned towards respective positions which we see
as a starting point for using chat-based LLMs as data-driven conversational
engines to assist research in political science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to NAACL 2024 as a short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunQA: Towards Surprising Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surprising videos, such as funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question-answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Moreover, we propose
FunMentor, an agent designed for Vision-Language Models (VLMs) that uses
multi-turn dialogues to enhance models' understanding of counter-intuitiveness.
Extensive experiments with existing VLMs demonstrate the effectiveness of
FunMentor and reveal significant performance gaps for the FunQA videos across
spatial-temporal reasoning, visual-centered reasoning, and free-text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://funqa-benchmark.github.io/ Codebase:
  https://github.com/Jingkang50/FunQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Robustness of Large <span class="highlight-title">Language Models</span> via Consistency
  Alignment <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown tremendous success in following user
instructions and generating helpful responses. Nevertheless, their robustness
is still far from optimal, as they may generate significantly inconsistent
responses due to minor changes in the verbalized instructions. Recent
literature has explored this inconsistency issue, highlighting the importance
of continued improvement in the robustness of response generation. However,
systematic analysis and solutions are still lacking. In this paper, we
quantitatively define the inconsistency problem and propose a two-stage
training framework consisting of instruction-augmented supervised fine-tuning
and consistency alignment training. The first stage helps a model generalize on
following instructions via similar instruction augmentations. In the second
stage, we improve the diversity and help the model understand which responses
are more aligned with human expectations by differentiating subtle differences
in similar responses. The training process is accomplished by self-rewards
inferred from the trained model at the first stage without referring to
external human preference resources. We conduct extensive experiments on recent
publicly available LLMs on instruction-following tasks and demonstrate the
effectiveness of our training framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pho<span class="highlight-title">GPT</span>: Generative <span class="highlight-title">Pre-train</span>ing for Vietnamese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Dinh Phung, Hung Bui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We open-source a state-of-the-art 4B-parameter generative model series for
Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and
its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B
parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,
with an 8192 context length, employing a vocabulary of 20480 token types. The
chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning
PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along
with an additional 290K conversations. In addition, we also demonstrate its
superior performance compared to previous open-source models. Our PhoGPT models
are available at: https://github.com/VinAIResearch/PhoGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhoGPT-4B Technical Report - 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Guard: Empower the LLM to Safeguard Itself 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The jailbreak attack can bypass the safety measures of a Large Language Model
(LLM), generating harmful content. This misuse of LLM has led to negative
societal consequences. Currently, there are two main approaches to address
jailbreak attacks: safety training and safeguards. Safety training focuses on
further training LLM to enhance its safety. On the other hand, safeguards
involve implementing external models or filters to prevent harmful outputs.
However, safety training has constraints in its ability to adapt to new attack
types and often leads to a drop in model performance. Safeguards have proven to
be of limited help. To tackle these issues, we propose a novel approach called
Self-Guard, which combines the strengths of both safety methods. Self-Guard
includes two stages. In the first stage, we enhance the model's ability to
assess harmful content, and in the second stage, we instruct the model to
consistently perform harmful content detection on its own responses. The
experiment has demonstrated that Self-Guard is robust against jailbreak
attacks. In the bad case analysis, we find that LLM occasionally provides
harmless responses to harmful queries. Additionally, we evaluated the general
capabilities of the LLM before and after safety training, providing evidence
that Self-Guard does not result in the LLM's performance degradation. In
sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM
but also can even mitigate this issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E-Sparse: Boosting the Large Language Model Inference through
  Entropy-based N:M Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional pruning methods are known to be challenging to work in Large
Language Models (LLMs) for Generative AI because of their unaffordable training
process and large computational demands. For the first time, we introduce the
information entropy of hidden state features into a pruning metric design,
namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse
employs the information richness to leverage the channel importance, and
further incorporates several novel techniques to put it into effect: (1) it
introduces information entropy to enhance the significance of parameter weights
and input feature norms as a novel pruning metric, and performs N:M sparsity
without modifying the remaining weights. (2) it designs global naive shuffle
and local block shuffle to quickly optimize the information distribution and
adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is
implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere
GPUs. Extensive experiments on the LLaMA family and OPT models show that
E-Sparse can significantly speed up the model inference over the dense model
(up to 1.53X) and obtain significant memory saving (up to 43.52%), with
acceptable accuracy loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Ye, Ze Tao, Yong Huang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention is an essential component of large language models(LLMs) but a
significant source of inference latency for long sequences. In multi-tenant
LLMs serving scenarios, the compute and memory operation cost of self-attention
can be optimized by using the probability that multiple LLM requests have
shared system prompts in prefixes. In this paper, we introduce ChunkAttention,
a prefix-aware self-attention module that can detect matching prompt prefixes
across multiple requests and share their key/value tensors in memory at runtime
to improve the memory utilization of KV cache. This is achieved by breaking
monolithic key/value tensors into smaller chunks and structuring them into the
auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,
we design an efficient self-attention kernel, where a two-phase partition
algorithm is implemented to improve the data locality during self-attention
computation in the presence of shared system prompts. Experiments show that
ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$
compared to the start-of-the-art implementation, with the length of the system
prompt ranging from 1024 to 4096.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Cross-Lingual Document-Level Event Causality Identification
  with Heterogeneous Graph Contrastive Transfer Learning <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitao He, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Causality Identification (ECI) refers to the detection of causal
relations between events in texts. However, most existing studies focus on
sentence-level ECI with high-resource languages, leaving more challenging
document-level ECI (DECI) with low-resource languages under-explored. In this
paper, we propose a Heterogeneous Graph Interaction Model with
Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot
cross-lingual document-level ECI. Specifically, we introduce a heterogeneous
graph interaction network to model the long-distance dependencies between
events that are scattered over a document. Then, to improve cross-lingual
transferability of causal knowledge learned from the source language, we
propose a multi-granularity contrastive transfer learning module to align the
causal representations across languages. Extensive experiments show our
framework outperforms the previous state-of-the-art model by 9.4% and 8.2% of
average F1 score on monolingual and multilingual scenarios respectively.
Notably, in the multilingual scenario, our zero-shot framework even exceeds
GPT-3.5 with few-shot learning by 24.3% in overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealMe: Harnessing Cognitive Reframing in Large <span class="highlight-title">Language Models</span> for
  Psychotherapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min Peng, Weiguang Han, Jimin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can play a vital role in psychotherapy by
adeptly handling the crucial task of cognitive reframing and overcoming
challenges such as shame, distrust, therapist skill variability, and resource
scarcity. Previous LLMs in cognitive reframing mainly converted negative
emotions to positive ones, but these approaches have limited efficacy, often
not promoting clients' self-discovery of alternative perspectives. In this
paper, we unveil the Helping and Empowering through Adaptive Language in Mental
Enhancement (HealMe) model. This novel cognitive reframing therapy method
effectively addresses deep-rooted negative thoughts and fosters rational,
balanced perspectives. Diverging from traditional LLM methods, HealMe employs
empathetic dialogue based on psychotherapeutic frameworks. It systematically
guides clients through distinguishing circumstances from feelings,
brainstorming alternative viewpoints, and developing empathetic, actionable
suggestions. Moreover, we adopt the first comprehensive and expertly crafted
psychological evaluation metrics, specifically designed to rigorously assess
the performance of cognitive reframing, in both AI-simulated dialogues and
real-world therapeutic conversations. Experimental results show that our model
outperforms others in terms of empathy, guidance, and logical coherence,
demonstrating its effectiveness and potential positive impact on psychotherapy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mPLUG-Owl: Modularization Empowers Large <span class="highlight-title">Language Models</span> with
  Multimodality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive zero-shot abilities
on a variety of open-ended tasks, while recent research has also explored the
use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,
a novel training paradigm that equips LLMs with multi-modal abilities through
modularized learning of foundation LLM, a visual knowledge module, and a visual
abstractor module. This approach can support multiple modalities and facilitate
diverse unimodal and multimodal abilities through modality collaboration. The
training paradigm of mPLUG-Owl involves a two-stage method for aligning image
and text, which learns visual knowledge with the assistance of LLM while
maintaining and even improving the generation abilities of LLM. In the first
stage, the visual knowledge module and abstractor module are trained with a
frozen LLM module to align the image and text. In the second stage,
language-only and multi-modal supervised datasets are used to jointly fine-tune
a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing
the visual knowledge module. We carefully build a visually-related instruction
evaluation set OwlEval. Experimental results show that our model outperforms
existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction
and visual understanding ability, multi-turn conversation ability, and
knowledge reasoning ability. Besides, we observe some unexpected and exciting
abilities such as multi-image correlation and scene text understanding, which
makes it possible to leverage it for harder real scenarios, such as vision-only
document comprehension. Our code, pre-trained model, instruction-tuned models,
and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The
online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in Process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An LLM-Enhanced Adversarial Editing System for Lexical Simplification <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14704v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14704v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical Simplification (LS) aims to simplify text at the lexical level.
Existing methods rely heavily on annotated data, making it challenging to apply
in low-resource scenarios. In this paper, we propose a novel LS method without
parallel corpora. This method employs an Adversarial Editing System with
guidance from a confusion loss and an invariance loss to predict lexical edits
in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced
loss to enable the distillation of knowledge from Large Language Models (LLMs)
into a small-size LS system. From that, complex words within sentences are
masked and a Difficulty-aware Filling module is crafted to replace masked
positions with simpler words. At last, extensive experimental results and
analyses on three benchmark LS datasets demonstrate the effectiveness of our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KoCoSa: Korean Context-aware Sarcasm Detection <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumin Kim, Heejae Suh, Mingi Kim, Dongyeon Won, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm is a way of verbal irony where someone says the opposite of what they
mean, often to ridicule a person, situation, or idea. It is often difficult to
detect sarcasm in the dialogue since detecting sarcasm should reflect the
context (i.e., dialogue history). In this paper, we introduce a new dataset for
the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware
Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and
the labels for this task on the last response. To build the dataset, we propose
an efficient sarcasm detection dataset generation pipeline: 1) generating new
sarcastic dialogues from source dialogues with large language models, 2)
automatic and manual filtering of abnormal and toxic dialogues, and 3) human
annotation for the sarcasm detection task. We also provide a simple but
effective baseline for the Korean sarcasm detection task trained on our
dataset. Experimental results on the dataset show that our baseline system
outperforms strong baselines like large language models, such as GPT-3.5, in
the Korean sarcasm detection task. We show that the sarcasm detection task
relies deeply on the existence of sufficient context. We will release the
dataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rank<span class="highlight-title">Prompt</span>: Step-by-Step Comparisons Make <span class="highlight-title">Language Models</span> Better
  Reasoners <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12373v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12373v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive performance across
various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT
are prone to logical errors during their reasoning processes. Existing
solutions, such as deploying task-specific verifiers or voting over multiple
reasoning paths, either require extensive human annotations or fail in
scenarios with inconsistent responses. To address these challenges, we
introduce RankPrompt, a new prompting method that enables LLMs to self-rank
their responses without additional resources. RankPrompt breaks down the
ranking problem into a series of comparisons among diverse responses,
leveraging the inherent capabilities of LLMs to generate chains of comparison
as contextual exemplars. Our experiments across 11 arithmetic and commonsense
reasoning tasks show that RankPrompt significantly enhances the reasoning
performance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,
RankPrompt excels in LLM-based automatic evaluations for open-ended tasks,
aligning with human judgments 74% of the time in the AlpacaEval dataset. It
also exhibits robustness to variations in response order and consistency.
Collectively, our results validate RankPrompt as an effective method for
eliciting high-quality feedback from language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-Coling 2024 Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Modeling for Content-enriched Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, Kai Zhang, Rui Li, Jiatong Li, Min Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are indispensable in the realm of online applications,
and sequential recommendation has enjoyed considerable prevalence due to its
capacity to encapsulate the dynamic shifts in user interests. However, previous
sequential modeling methods still have limitations in capturing contextual
information. The primary reason is the lack of understanding of domain-specific
knowledge and item-related textual content by language models. Fortunately, the
emergence of powerful language models has unlocked the potential to incorporate
extensive world knowledge into recommendation algorithms, enabling them to go
beyond simple item attributes and truly understand the world surrounding user
preferences. To achieve this, we propose LANCER, which leverages the semantic
understanding capabilities of pre-trained language models to generate
personalized recommendations. Our approach bridges the gap between language
models and recommender systems, resulting in more human-like recommendations.
We demonstrate the effectiveness of our approach through a series of
experiments conducted on multiple benchmark datasets, showing promising results
and providing valuable insights into the influence of our model on sequential
recommendation tasks. Furthermore, our experimental codes are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized
  <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By design, large language models (LLMs) are static general-purpose models,
expensive to retrain or update frequently. As they are increasingly adopted for
knowledge-intensive tasks, it becomes evident that these design choices lead to
failures to generate factual, relevant, and up-to-date knowledge. To this end,
we propose Knowledge Card, a modular framework to plug in new factual and
relevant knowledge into general-purpose LLMs. We first introduce knowledge
cards -- specialized language models trained on corpora from specific domains
and sources. Knowledge cards serve as parametric repositories that are selected
at inference time to generate background knowledge for the base LLM. We then
propose three content selectors to dynamically select and retain information in
documents generated by knowledge cards, specifically controlling for relevance,
brevity, and factuality of outputs. Finally, we propose two complementary
integration approaches to augment the base LLM with the (relevant, factual)
knowledge curated from the specialized LMs. Through extensive experiments, we
demonstrate that Knowledge Card achieves state-of-the-art performance on six
benchmark datasets. Ultimately, Knowledge Card framework enables dynamic
synthesis and updates of knowledge from diverse domains. Its modularity will
ensure that relevant knowledge can be continuously updated through the
collective efforts of the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024, oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Multilingual Models Pivot Zero-Shot Multimodal Learning across
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there has been a significant surge in multimodal learning in terms
of both image-to-text and text-to-image generation. However, the success is
typically limited to English, leaving other languages largely behind. Building
a competitive counterpart in other languages is highly challenging due to the
low-resource nature of non-English multimodal data (i.e., lack of large-scale,
high-quality image-text data). In this work, we propose MPM, an effective
training paradigm for training large multimodal models in non-English
languages. MPM demonstrates that Multilingual language models can Pivot
zero-shot Multimodal learning across languages. Specifically, based on a strong
multilingual large language model, multimodal models pretrained on English-only
image-text data can well generalize to other languages in a (quasi)-zero-shot
manner, even surpassing models trained on image-text data in native languages.
Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in
image-to-text and text-to-image generation, which achieve state-of-the-art
(open-source) performance in Chinese. To facilitate future research, we
open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/OpenBMB/VisCPM.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Adaption to Distribution Shifts By Mixing Source and Target
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Xue, Ali Payani, Yu Yang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained machine learning models need to be adapted to distribution shifts
when deployed in new target environments. When obtaining labeled data from the
target distribution is expensive, few-shot adaptation with only a few examples
from the target distribution becomes essential. In this work, we propose
MixPro, a lightweight and highly data-efficient approach for few-shot
adaptation. MixPro first generates a relatively large dataset by mixing
(linearly combining) pre-trained embeddings of large source data with those of
the few target examples. This process preserves important features of both
source and target distributions, while mitigating the specific noise in the
small target data. Then, it trains a linear classifier on the mixed embeddings
to effectively adapts the model to the target distribution without overfitting
the small target data. Theoretically, we demonstrate the advantages of MixPro
over previous methods. Our experiments, conducted across various model
architectures on 8 datasets featuring different types of distribution shifts,
reveal that MixPro can outperform baselines by up to 7\%, with only 2-4 target
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large <span class="highlight-title">Language Models</span> as Generative User Simulators for
  Conversational Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoleInteract: Evaluating the Social Interaction of Role-Playing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced the development of various AI
conversational agents, including role-playing conversational agents that mimic
diverse characters and human behaviors. While prior research has predominantly
focused on enhancing the conversational capability, role-specific knowledge,
and stylistic attributes of these agents, there has been a noticeable gap in
assessing their social intelligence. In this paper, we introduce RoleInteract,
the first benchmark designed to systematically evaluate the sociality of
role-playing conversational agents at both individual and group levels of
social interactions. The benchmark is constructed from a variety of sources and
covers a wide range of 500 characters and over 6,000 question prompts and
30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations
on this benchmark using mainstream open-source and closed-source LLMs. We find
that agents excelling in individual level does not imply their proficiency in
group level. Moreover, the behavior of individuals may drift as a result of the
influence exerted by other agents within the group. Experimental results on
RoleInteract confirm its significance as a testbed for assessing the social
interaction of role-playing conversational agents. The benchmark is publicly
accessible at https://github.com/X-PLUG/RoleInteract.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2TL: Transforming Natural Languages to Temporal Logics using Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Logic (TL) can be used to rigorously specify complex high-level
specification for systems in many engineering applications. The translation
between natural language (NL) and TL has been under-explored due to the lack of
dataset and generalizable model across different application domains. In this
paper, we propose an accurate and generalizable transformation framework of
English instructions from NL to TL, exploring the use of Large Language Models
(LLMs) at multiple stages. Our contributions are twofold. First, we develop a
framework to create a dataset of NL-TL pairs combining LLMs and human
annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5
models on the lifted versions (i.e., the specific Atomic Propositions (AP) are
hidden) of the NL and TL. The enhanced generalizability originates from two
aspects: 1) Usage of lifted NL-TL characterizes common logical structures,
without constraints of specific domains. 2) Application of LLMs in dataset
creation largely enhances corpus richness. We test the generalization of
trained models on five varied domains. To achieve full NL-TL transformation, we
either combine the lifted model with AP recognition task or do the further
finetuning on each specific domain. During the further finetuning, our model
achieves higher accuracy (>95%) using only <10% training data, compared with
the baseline sequence to sequence (Seq2Seq) model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoTAMP: Autoregressive Task and Motion Planning with LLMs as
  Translators and Checkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective human-robot interaction, robots need to understand, plan, and
execute complex, long-horizon tasks described by natural language. Recent
advances in large language models (LLMs) have shown promise for translating
natural language into robot action sequences for complex tasks. However,
existing approaches either translate the natural language directly into robot
trajectories or factor the inference process by decomposing language into task
sub-goals and relying on a motion planner to execute each sub-goal. When
complex environmental and temporal constraints are involved, inference over
planning tasks must be performed jointly with motion plans using traditional
task-and-motion planning (TAMP) algorithms, making factorization into subgoals
untenable. Rather than using LLMs to directly plan task sub-goals, we instead
perform few-shot translation from natural language task descriptions to an
intermediate task representation that can then be consumed by a TAMP algorithm
to jointly solve the task and motion plan. To improve translation, we
automatically detect and correct both syntactic and semantic errors via
autoregressive re-prompting, resulting in significant improvements in task
completion. We show that our approach outperforms several methods using LLMs as
planners in complex task domains. See our project website
https://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLANNER: Generating Diversified Paragraph via Latent Language Diffusion
  Model <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models for text sometimes generate repetitive and low-quality
output because errors accumulate during the steps of generation. This issue is
often attributed to exposure bias - the difference between how a model is
trained, and how it is used during inference. Denoising diffusion models
provide an alternative approach in which a model can revisit and revise its
output. However, they can be computationally expensive and prior efforts on
text have led to models that produce less fluent output compared to
autoregressive models, especially for longer text and paragraphs. In this
paper, we propose PLANNER, a model that combines latent semantic diffusion with
autoregressive generation, to generate fluent text while exercising global
control over paragraphs. The model achieves this by combining an autoregressive
"decoding" module with a "planning" module that uses latent diffusion to
generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed
method is evaluated on various conditional generation tasks, and results on
semantic generation, text completion and summarization show its effectiveness
in generating high-quality long-form text in an efficient manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023, code at https://github.com/apple/ml-planner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embrace Divergence for Richer Insights: A Multi-document Summarization
  Benchmark and a Case Study on Summarizing Diverse Information from News
  Articles <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Philippe Laban, Alexander R. Fabbri, Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research in multi-document news summarization has typically
concentrated on collating information that all sources agree upon. However, the
summarization of diverse information dispersed across multiple articles about
an event remains underexplored. In this paper, we propose a new task of
summarizing diverse information encountered in multiple news articles
encompassing the same event. To facilitate this task, we outlined a data
collection schema for identifying diverse information and curated a dataset
named DiverseSumm. The dataset includes 245 news stories, with each story
comprising 10 news articles and paired with a human-validated reference. Next,
to enable consistent automatic evaluation, we conducted a comprehensive
analysis to pinpoint the position and verbosity biases when utilizing Large
Language Model (LLM)-based metrics for evaluating the coverage and faithfulness
of summaries. Through correlation analyses, we outline the best practices for
effectively using automatic LLM-based metrics on the DiverseSumm dataset.
Finally, we study how LLMs summarize multiple news articles by analyzing which
type of diverse information LLMs are capable of identifying. Our analyses
suggest that despite the extraordinary capabilities of LLMs in single-document
summarization, the proposed task remains a complex challenge for them mainly
due to their limited coverage, with GPT-4 only able to cover under 40% of the
diverse information on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 Pages, Work in Progress, Benchmark Project Page:
  https://mathverse-cuhk.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for
  Contrastive Self-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotations or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> for Multi-Choice Question Classification of
  Medical Subjects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor Ponce-López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to evaluate whether large language models trained on
multi-choice question data can be used to discriminate between medical
subjects. This is an important and challenging task for automatic question
answering. To achieve this goal, we train deep neural networks for multi-class
classification of questions into the inferred medical subjects. Using our
Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art
results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their
development and test sets, respectively. In this sense, we show the capability
of AI and LLMs in particular for multi-classification tasks in the Healthcare
domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Chain-of-Thought <span class="highlight-title">Prompt</span>ing Approach with LLMs for Evaluating Students'
  Formative Assessment Responses in Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the use of large language models (LLMs) to score and
explain short-answer assessments in K-12 science. While existing methods can
score more structured math and computer science assessments, they often do not
provide explanations for the scores. Our study focuses on employing GPT-4 for
automated assessment in middle school Earth Science, combining few-shot and
active learning with chain-of-thought reasoning. Using a human-in-the-loop
approach, we successfully score and provide meaningful explanations for
formative assessment responses. A systematic analysis of our method's pros and
cons sheds light on the potential for human-in-the-loop techniques to enhance
automated grading for open-ended science assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In press at EAAI-24: The 14th Symposium on Educational Advances in
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Era of Semantic Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Peyrard, Martin Josifoski, Robert West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work demonstrated great promise in the idea of orchestrating
collaborations between LLMs, human input, and various tools to address the
inherent limitations of LLMs. We propose a novel perspective called semantic
decoding, which frames these collaborative processes as optimization procedures
in semantic space. Specifically, we conceptualize LLMs as semantic processors
that manipulate meaningful pieces of information that we call semantic tokens
(known thoughts). LLMs are among a large pool of other semantic processors,
including humans and tools, such as search engines or code executors.
Collectively, semantic processors engage in dynamic exchanges of semantic
tokens to progressively construct high-utility outputs. We refer to these
orchestrated interactions among semantic processors, optimizing and searching
in semantic space, as semantic decoding algorithms. This concept draws a direct
parallel to the well-studied problem of syntactic decoding, which involves
crafting algorithms to best exploit auto-regressive language models for
extracting high-utility sequences of syntactic tokens. By focusing on the
semantic level and disregarding syntactic details, we gain a fresh perspective
on the engineering of AI systems, enabling us to imagine systems with much
greater complexity and capabilities. In this position paper, we formalize the
transition from syntactic to semantic tokens as well as the analogy between
syntactic and semantic decoding. Subsequently, we explore the possibilities of
optimizing within the space of semantic tokens via semantic decoding
algorithms. We conclude with a list of research opportunities and questions
arising from this fresh perspective. The semantic decoding perspective offers a
powerful abstraction for search and optimization directly in the space of
meaningful concepts, with semantic tokens as the fundamental units of a new
type of computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most accurate language models are trained on orders of magnitude more
language data than human language learners receive - but with no supervision
from other sensory modalities that play a crucial role in human learning. Can
we make LMs' representations and predictions more accurate (and more
human-like) with more ecologically plausible supervision? This paper describes
LexiContrastive Grounding (LCG), a grounded language learning procedure that
leverages visual supervision to improve textual representations.
LexiContrastive Grounding combines a next token prediction strategy with a
contrastive visual grounding objective, focusing on early-layer representations
that encode lexical information. Across multiple word-learning and
sentence-understanding benchmarks, LexiContrastive Grounding not only
outperforms standard language-only models in learning efficiency, but also
improves upon vision-and-language learning procedures including CLIP, GIT,
Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves
perplexity by around 5% on multiple language modeling tasks. This work
underscores the potential of incorporating visual grounding into language
models, aligning more closely with the multimodal nature of human language
acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDT: Improving Large <span class="highlight-title">Language Models</span>' <span class="highlight-title">Generation</span> by Entropy-based
  Dynamic Temperature Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimao Zhang, Yu Bao, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have demonstrated outstanding
performance across a wide range of downstream language tasks. Temperature
sampling is a commonly used decoding strategy for LLMs' generation process.
However, a fixed temperature parameter is used in most cases, which may not
always be an optimal choice for balancing generation quality and diversity. In
this paper, we propose an effective Entropy-based Dynamic Temperature (EDT)
Sampling method, to achieve a more balanced performance in terms of both
generation quality and diversity by dynamically selecting the temperature
parameter. Additionally, we also show model performance and comprehensive
analyses for 4 different generation benchmarks. Our experiments show that EDT
significantly outperforms the existing strategies across different tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a Language-Learning Game for Brazilian Indigenous Languages: A
  Case of Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Polleti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we discuss a first attempt to build a language learning game
for brazilian indigenous languages and the challenges around it. We present a
design for the tool with gamification aspects. Then we describe a process to
automatically generate language exercises and questions from a dependency
treebank and a lexical database for Tupian languages. We discuss the
limitations of our prototype highlighting ethical and practical implementation
concerns. Finally, we conclude that new data gathering processes should be
established in partnership with indigenous communities and oriented for
educational purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First Workshop on NLP for Indigenous Languages of Lusophone
  Countries, 16th International Conference on Computational Processing of
  Portuguese (PROPOR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detoxifying Large <span class="highlight-title">Language Models</span> via Knowledge Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates using knowledge editing techniques to detoxify Large
Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine
unsafe categories with various powerful attack prompts and equips comprehensive
metrics for systematic evaluation. We conduct experiments to compare knowledge
editing approaches with previous baselines, indicating that knowledge editing
has the potential to efficiently detoxify LLMs with limited impact on general
performance. Then, we propose a simple yet effective baseline, dubbed
Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the
toxicity of LLMs within a few tuning steps via only one instance. We further
provide an in-depth analysis of the internal mechanism for various detoxify
approaches, demonstrating that previous methods like SFT and DPO may merely
suppress the activations of toxic parameters, while DINM mitigates the toxicity
of the toxic parameters to a certain extent, making permanent adjustments. We
hope that these insights could shed light on future work of developing
detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code
and benchmark are available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Benchmark:
  https://huggingface.co/datasets/zjunlp/SafeEdit Code:
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">ChatGPT</span> Alternative Solutions: Large <span class="highlight-title">Language Models</span> <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanieh Alipour, Nick Pendar, Kohinoor Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, the grandeur of Large Language Models (LLMs) has not only
shone in the realm of natural language processing but has also cast its
brilliance across a vast array of applications. This remarkable display of LLM
capabilities has ignited a surge in research contributions within this domain,
spanning a diverse spectrum of topics. These contributions encompass
advancements in neural network architecture, context length enhancements, model
alignment, training datasets, benchmarking, efficiency improvements, and more.
Recent years have witnessed a dynamic synergy between academia and industry,
propelling the field of LLM research to new heights. A notable milestone in
this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in
LLMs, which has garnered widespread societal attention. The evolving technology
of LLMs has begun to reshape the landscape of the entire AI community,
promising a revolutionary shift in the way we create and employ AI algorithms.
Given this swift-paced technical evolution, our survey embarks on a journey to
encapsulate the recent strides made in the world of LLMs. Through an
exploration of the background, key discoveries, and prevailing methodologies,
we offer an up-to-the-minute review of the literature. By examining multiple
LLM models, our paper not only presents a comprehensive overview but also
charts a course that identifies existing challenges and points toward potential
future research trajectories. This survey furnishes a well-rounded perspective
on the current state of generative AI, shedding light on opportunities for
further exploration, enhancement, and innovation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recourse for reclamation: Chatting with generative <span class="highlight-title">language models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Chien, Kevin R. McKee, Jackie Kay, William Isaac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers and developers increasingly rely on toxicity scoring to moderate
generative language model outputs, in settings such as customer service,
information retrieval, and content generation. However, toxicity scoring may
render pertinent information inaccessible, rigidify or "value-lock" cultural
norms, and prevent language reclamation processes, particularly for
marginalized people. In this work, we extend the concept of algorithmic
recourse to generative language models: we provide users a novel mechanism to
achieve their desired prediction by dynamically setting thresholds for toxicity
filtering. Users thereby exercise increased agency relative to interactions
with the baseline system. A pilot study ($n = 30$) supports the potential of
our proposed recourse mechanism, indicating improvements in usability compared
to fixed-threshold toxicity-filtering of model outputs. Future work should
explore the intersection of toxicity scoring, model controllability, user
agency, and language reclamation processes -- particularly with regard to the
bias that many communities encounter when interacting with generative language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Abstracts of the CHI Conference on Human Factors in
  Computing Systems (CHI EA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Single-System Illusion in Software-Defined Vehicles --
  Automated, AI-Powered Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Lebioda, Viktor Vorobev, Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel model- and feature-based approach to development of
vehicle software systems, where the end architecture is not explicitly defined.
Instead, it emerges from an iterative process of search and optimization given
certain constraints, requirements and hardware architecture, while retaining
the property of single-system illusion, where applications run in a logically
uniform environment. One of the key points of the presented approach is the
inclusion of modern generative AI, specifically Large Language Models (LLMs),
in the loop. With the recent advances in the field, we expect that the LLMs
will be able to assist in processing of requirements, generation of formal
system models, as well as generation of software deployment specification and
test code. The resulting pipeline is automated to a large extent, with feedback
being generated at each step.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Explanations for Generative <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation-based explanation methods such as LIME and SHAP are commonly
applied to text classification. This work focuses on their extension to
generative language models. To address the challenges of text as output and
long text inputs, we propose a general framework called MExGen that can be
instantiated with different attribution algorithms. To handle text output, we
introduce the notion of scalarizers for mapping text to real numbers and
investigate multiple possibilities. To handle long inputs, we take a
multi-level approach, proceeding from coarser levels of granularity to finer
ones, and focus on algorithms with linear scaling in model queries. We conduct
a systematic evaluation, both automated and human, of perturbation-based
attribution methods for summarization and context-grounded question answering.
The results show that our framework can provide more locally faithful
explanations of generated outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ gTBLS: Generating Tables from Text by Conditional Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Christopher Richardson, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling large, unstructured text into a structured, condensed form such as
tables is an open research problem. One of the primary challenges in
automatically generating tables is ensuring their syntactic validity. Prior
approaches address this challenge by including additional parameters in the
Transformer's attention mechanism to attend to specific rows and column
headers. In contrast to this single-stage method, this paper presents a
two-stage approach called Generative Tables (gTBLS). The first stage infers
table structure (row and column headers) from the text. The second stage
formulates questions using these headers and fine-tunes a causal language model
to answer them. Furthermore, the gTBLS approach is amenable to the utilization
of pre-trained Large Language Models in a zero-shot configuration, presenting a
solution for table generation in situations where fine-tuning is not feasible.
gTBLS improves prior approaches by up to 10% in BERTScore on the table
construction task and up to 20% on the table content generation task of the
E2E, WikiTableText, WikiBio, and RotoWire datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of Translation Techniques for the Translation Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhou, Vincent Vandeghinste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation (MT) encompasses a variety of methodologies aimed at
enhancing the accuracy of translations. In contrast, the process of
human-generated translation relies on a wide range of translation techniques,
which are crucial for ensuring linguistic adequacy and fluency. This study
suggests that these translation techniques could further optimize machine
translation if they are automatically identified before being applied to guide
the translation process effectively. The study differentiates between two
scenarios of the translation process: from-scratch translation and
post-editing. For each scenario, a specific set of experiments has been
designed to forecast the most appropriate translation techniques. The findings
indicate that the predictive accuracy for from-scratch translation reaches 82%,
while the post-editing process exhibits even greater potential, achieving an
accuracy rate of 93%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than Just Statistical Recurrence: Human and Machine Unsupervised
  Learning of Māori Word Segmentation across Morphological Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashvini Varatharaj, Simon Todd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-M\=aori-speaking New Zealanders (NMS)are able to segment M\=aori words in
a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is
assumed to derive through the identification and extraction of statistically
recurrent forms. We examine this assumption by asking how NMS segmentations
compare to those produced by Morfessor, an unsupervised machine learning model
that operates based on statistical recurrence, across words formed by a variety
of morphological processes. Both NMS and Morfessor succeed in segmenting words
formed by concatenative processes (compounding and affixation without
allomorphy), but NMS also succeed for words that invoke templates
(reduplication and allomorphy) and other cues to morphological structure,
implying that their learning process is sensitive to more than just statistical
recurrence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 Figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language Models</span> Can Reduce Asymmetry in Information Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Rahaman, Martin Weiss, Manuel Wüthrich, Yoshua Bengio, Li Erran Li, Chris Pal, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent communication and learning pressures in <span class="highlight-title">language models</span>: a
  language evolution perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Galke, Limor Raviv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models and humans are two types of learning systems. Finding or
facilitating commonalities could enable major breakthroughs in our
understanding of the acquisition and evolution of language. Many theories of
language evolution rely heavily on learning biases and learning pressures. Yet
due to substantial differences in learning pressures, it is questionable
whether the similarity between humans and machines is sufficient for insights
to carry over and to be worth testing with human participants. Here, we review
the emergent communication literature, a subfield of multi-agent reinforcement
learning, from a language evolution perspective. We find that the emergent
communication literature excels at designing and adapting models to recover
initially absent linguistic phenomena of natural languages. Based on a short
literature review, we identify key pressures that have recovered initially
absent human patterns in emergent communication models: communicative success,
efficiency, learnability, and other psycho-/sociolinguistic factors. We argue
that this may serve as inspiration for how to design language models for
language acquisition and language evolution research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locating and Mitigating Gender Bias in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models(LLM) are pre-trained on extensive corpora to learn
facts and human cognition which contain human preferences. However, this
process can inadvertently lead to these models acquiring biases and stereotypes
prevalent in society. Prior research has typically tackled the issue of bias
through a one-dimensional perspective, concentrating either on locating or
mitigating it. This limited perspective has created obstacles in facilitating
research on bias to synergistically complement and progressively build upon one
another. In this study, we integrate the processes of locating and mitigating
bias within a unified framework. Initially, we use causal mediation analysis to
trace the causal effects of different components' activation within a large
language model. Building on this, we propose the LSDM (Least Square Debias
Method), a knowledge-editing based method for mitigating gender bias in
occupational pronouns, and compare it against two baselines on three gender
bias datasets and seven knowledge competency test datasets. The experimental
results indicate that the primary contributors to gender bias are the bottom
MLP modules acting on the last token of occupational pronouns and the top
attention module acting on the final word in the sentence. Furthermore, LSDM
mitigates gender bias in the model more effectively than the other baselines,
while fully preserving the model's capabilities in all other aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
  Models through Question Complexity <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Large Language Models (LLMs), which incorporate the
non-parametric knowledge from external knowledge bases into LLMs, have emerged
as a promising approach to enhancing response accuracy in several tasks, such
as Question-Answering (QA). However, even though there are various approaches
dealing with queries of different complexities, they either handle simple
queries with unnecessary computational overhead or fail to adequately address
complex multi-step queries; yet, not all user requests fall into only one of
the simple or complex categories. In this work, we propose a novel adaptive QA
framework, that can dynamically select the most suitable strategy for
(retrieval-augmented) LLMs from the simplest to the most sophisticated ones
based on the query complexity. Also, this selection process is operationalized
with a classifier, which is a smaller LM trained to predict the complexity
level of incoming queries with automatically collected labels, obtained from
actual predicted outcomes of models and inherent inductive biases in datasets.
This approach offers a balanced strategy, seamlessly adapting between the
iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval
methods, in response to a range of query complexities. We validate our model on
a set of open-domain QA datasets, covering multiple query complexities, and
show that ours enhances the overall efficiency and accuracy of QA systems,
compared to relevant baselines including the adaptive retrieval approaches.
Code is available at: https://github.com/starsuzi/Adaptive-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for
  Noise-Robust Speech Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech recognition and translation systems perform poorly on noisy inputs,
which are frequent in realistic environments. Augmenting these systems with
visual signals has the potential to improve robustness to noise. However,
audio-visual (AV) data is only available in limited amounts and for fewer
languages than audio-only resources. To address this gap, we present XLAVS-R, a
cross-lingual audio-visual speech representation model for noise-robust speech
recognition and translation in over 100 languages. It is designed to maximize
the benefits of limited multilingual AV pre-training data, by building on top
of audio-only multilingual pre-training and simplifying existing pre-training
schemes. Extensive evaluation on the MuAViC benchmark shows the strength of
XLAVS-R on downstream audio-visual speech recognition and translation tasks,
where it outperforms the previous state of the art by up to 18.5% WER and 4.7
BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability
with audio-only fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Accurate Translation-Tailored LLMs with Language Aware
  <span class="highlight-title">Instruct</span>ion Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translation-tailored Large language models (LLMs) exhibit remarkable
translation capabilities, even competing with supervised-trained commercial
translation systems. However, off-target translation remains an unsolved
problem, especially for low-resource languages, hindering us from developing
accurate LLMs-based translation models. To mitigate the off-target translation
problem and enhance the performance of LLMs on translation, recent works have
either designed advanced prompting strategies to highlight the functionality of
translation instructions or exploited the in-context learning ability of LLMs
by feeding few-shot demonstrations. However, these methods essentially do not
improve LLM's ability to follow translation instructions, especially the
language direction information. In this work, we design a two-stage fine-tuning
algorithm to improve the instruction-following ability (especially the
translation direction) of LLMs. Specifically, we first tune LLMs with the
maximum likelihood estimation loss on the translation dataset to elicit the
basic translation capabilities. In the second stage, we construct
instruction-conflicting samples by randomly replacing the translation
directions with a wrong one within the instruction, and then introduce an extra
unlikelihood loss to learn those samples. Experiments on IWSLT and WMT
benchmarks upon the LLaMA model spanning 16 zero-shot directions show that,
compared to the competitive baseline -- translation-finetuned LLama, our method
could effectively reduce the off-target translation ratio (averagely -53.3\%),
thus improving translation quality with average +5.7 SacreBLEU and +16.4
BLEURT. Analysis shows that our method could preserve the model's general task
performance on AlpacaEval. Code and models will be released at
\url{https://github.com/alphadl/LanguageAware_Tuning}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Large to Tiny: Distilling and Refining Mathematical Expertise for
  Math Word Problems with Weakly Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenge of high annotation costs in solving Math Word
Problems (MWPs) through full supervision with intermediate equations, recent
works have proposed weakly supervised task settings that rely solely on the
final answer as a supervised signal. Existing leading approaches typically
employ various search techniques to infer intermediate equations, but cannot
ensure their semantic consistency with natural language descriptions. The rise
of Large Language Models (LLMs) like ChatGPT has opened up new possibilities
for addressing MWPs directly. However, the computational demands of LLMs make
them less than ideal for use in settings where resources are tight. In light of
these challenges, we introduce an innovative two-stage framework that adeptly
transfers mathematical Expertise from large to tiny language models. In
\emph{Distillation Stage}, we propose a series of extraction processes that
satisfy the properties of MWPs to distill mathematical knowledge from LLMs to
construct problem-equation pairs required for supervised training. In
\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee
the full utilization of all data, we further utilize the unsuccessfully
searched data effectively by Knowledge Refine method. Finally, We train a small
model using distilled data generated through two-stage methods. As our method
fully leverages the semantic understanding capabilities during the searching
'problem-equation' pair, it demonstrates significantly improved performance on
the Math23K and Weak12K datasets compared to existing small model methods,
while maintaining a much lower computational cost than ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Editing Knowledge Representation of Language Lodel via Rephrased Prefix
  <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural language models (LMs) have been extensively trained on vast corpora to
store factual knowledge about various aspects of the world described in texts.
Current technologies typically employ knowledge editing methods or specific
prompts to modify LM outputs. However, existing knowledge editing methods are
costly and inefficient, struggling to produce appropriate text. Additionally,
prompt engineering is opaque and requires significant effort to find suitable
prompts. To address these issues, we introduce a new method called PSPEM
(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just
one training. It resolves the inefficiencies and generalizability issues in
knowledge editing methods and overcomes the opacity of prompt engineering by
automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a
prompt encoder and an encoding converter to refine key information in prompts
and uses prompt alignment techniques to guide model generation, ensuring text
consistency and adherence to the intended structure and content, thereby
maintaining an optimal balance between efficiency and accuracy. We have
validated the effectiveness of PSPEM through knowledge editing and attribute
inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\% editing
accuracy and demonstrated the highest level of fluency. We further analyzed the
similarities between PSPEM and original prompts and their impact on the model's
internals. The results indicate that PSPEM can serve as an alternative to
original prompts, supporting the model in effective editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages,3figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIT-RAG: Black-Box RAG with Factual Information and Token Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WikiFactDiff: A Large, Realistic, and Temporally Adaptable <span class="highlight-title">Dataset</span> for
  Atomic Factual Knowledge Update in Causal <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The factuality of large language model (LLMs) tends to decay over time since
events posterior to their training are "unknown" to them. One way to keep
models up-to-date could be factual update: the task of inserting, replacing, or
removing certain simple (atomic) facts within the model. To study this task, we
present WikiFactDiff, a dataset that describes the evolution of factual
knowledge between two dates as a collection of simple facts divided into three
categories: new, obsolete, and static. We describe several update scenarios
arising from various combinations of these three types of basic update. The
facts are represented by subject-relation-object triples; indeed, WikiFactDiff
was constructed by comparing the state of the Wikidata knowledge base at 4
January 2021 and 27 February 2023. Those fact are accompanied by verbalization
templates and cloze tests that enable running update algorithms and their
evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact,
WikiFactDiff constitutes a realistic update setting that involves various
update scenarios, including replacements, archival, and new entity insertions.
We also present an evaluation of existing update algorithms on WikiFactDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial
  Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Liu, Yi Yang, Kar Yan Tam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the Financial-STS task, a financial
domain-specific NLP task designed to measure the nuanced semantic similarity
between pairs of financial narratives. These narratives originate from the
financial statements of the same company but correspond to different periods,
such as year-over-year comparisons. Measuring the subtle semantic differences
between these paired narratives enables market stakeholders to gauge changes
over time in the company's financial and operational situations, which is
critical for financial decision-making. We find that existing pretrained
embedding models and LLM embeddings fall short in discerning these subtle
financial narrative shifts. To address this gap, we propose an LLM-augmented
pipeline specifically designed for the Financial-STS task. Evaluation on a
human-annotated dataset demonstrates that our proposed method outperforms
existing methods trained on classic STS tasks and generic LLM embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning, the process of selectively eliminating the influence of
certain data examples used during a model's training, has gained significant
attention as a means for practitioners to comply with recent data protection
regulations. However, existing unlearning methods face critical drawbacks,
including their prohibitively high cost, often associated with a large number
of hyperparameters, and the limitation of forgetting only relatively small data
portions. This often makes retraining the model from scratch a quicker and more
effective solution. In this study, we introduce Gradient-based and
Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework
designed to remove the influence of a subset of training data efficiently. It
applies adaptive gradient ascent to the data to be forgotten while using
standard gradient descent for the remaining data. $\nabla \tau$ offers multiple
benefits over existing approaches. It enables the unlearning of large sections
of the training dataset (up to 30%). It is versatile, supporting various
unlearning tasks (such as subset forgetting or class removal) and applicable
across different domains (images, text, etc.). Importantly, $\nabla \tau$
requires no hyperparameter adjustments, making it a more appealing option than
retraining the model from scratch. We evaluate our framework's effectiveness
using a set of well-established Membership Inference Attack metrics,
demonstrating up to 10% enhancements in performance compared to
state-of-the-art methods without compromising the original model's accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChainLM: Empowering Large <span class="highlight-title">Language Models</span> with Improved Chain-of-Thought
  <span class="highlight-title">Prompt</span>ing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of
large language models (LLMs), establishing itself as a primary approach to
solving complex reasoning tasks. Existing CoT synthesis approaches usually
focus on simpler reasoning tasks and thus result in low-quality and
inconsistent CoT prompts. In response to this challenge, we present an
empirical investigation of CoT prompting and introduce CoTGenius, a novel
framework designed for the automatic generation of superior CoT prompts.
CoTGenius is developed based on three major evolution strategies, i.e.,
complicate, diversify, and specify-alongside two filtering mechanisms:
evolutionary success judgement and correctness verification. We further employ
CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the
Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model
ChainLM. To deal with the cumulative error issue in reasoning steps, we propose
a step-level debating method, wherein multiple debaters discuss each reasoning
step to arrive at the correct answer. Extensive experiments demonstrate that
our ChainLM models exhibit enhanced proficiency in addressing a spectrum of
complex reasoning problems compared to existing models. In addition, we conduct
an in-depth analysis of the impact of data categories within CoTGenius on the
model performance. We release our dataset and code at
https://github.com/RUCAIBox/ChainLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Reference Necessary in the Evaluation of NLG Systems? When and Where? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of automatic metrics for evaluating NLG systems are
reference-based. However, the challenge of collecting human annotation results
in a lack of reliable references in numerous application scenarios. Despite
recent advancements in reference-free metrics, it has not been well understood
when and where they can be used as an alternative to reference-based metrics.
In this study, by employing diverse analytical approaches, we comprehensively
assess the performance of both metrics across a wide range of NLG tasks,
encompassing eight datasets and eight evaluation models. Based on solid
experiments, the results show that reference-free metrics exhibit a higher
correlation with human judgment and greater sensitivity to deficiencies in
language quality. However, their effectiveness varies across tasks and is
influenced by the quality of candidate texts. Therefore, it's important to
assess the performance of reference-free metrics before applying them to a new
task, especially when inputs are in uncommon form or when the answer space is
highly variable. Our study can provide insight into the appropriate application
of automatic metrics and the impact of metric choice on evaluation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Extraction of Contradictions from Patents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Trapp, Joachim Warschat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Already since the 1950s TRIZ shows that patents and the technical
contradictions they solve are an important source of inspiration for the
development of innovative products. However, TRIZ is a heuristic based on a
historic patent analysis and does not make use of the ever-increasing number of
latest technological solutions in current patents. Because of the huge number
of patents, their length, and, last but not least, their complexity there is a
need for modern patent retrieval and patent analysis to go beyond
keyword-oriented methods. Recent advances in patent retrieval and analysis
mainly focus on dense vectors based on neural AI Transformer language models
like Google BERT. They are, for example, used for dense retrieval, question
answering or summarization and key concept extraction. A research focus within
the methods for patent summarization and key concept extraction are generic
inventive concepts respectively TRIZ concepts like problems, solutions,
advantage of invention, parameters, and contradictions. Succeeding rule-based
approaches, finetuned BERT-like language models for sentence-wise
classification represent the state-of-the-art of inventive concept extraction.
While they work comparatively well for basic concepts like problems or
solutions, contradictions - as a more complex abstraction - remain a challenge
for these models. This paper goes one step further, as it presents a method to
extract TRIZ contradictions from patent texts based on Prompt Engineering using
a generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction
detection, sentence extraction, contradiction summarization, parameter
extraction and assignment to the 39 abstract TRIZ engineering parameters are
all performed in a single prompt using the LangChain framework. Our results
show that "off-the-shelf" GPT-4 is a serious alternative to existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the accessibility of psychotherapy with the aid of Large Language
Models (LLMs) is garnering a significant attention in recent years. Recognizing
cognitive distortions from the interviewee's utterances can be an essential
part of psychotherapy, especially for cognitive behavioral therapy. In this
paper, we propose ERD, which improves LLM-based cognitive distortion
classification performance with the aid of additional modules of (1) extracting
the parts related to cognitive distortion, and (2) debating the reasoning steps
by multiple agents. Our experimental results on a public dataset show that ERD
improves the multi-class F1 score as well as binary specificity score.
Regarding the latter score, it turns out that our method is effective in
debiasing the baseline method which has high false positive rate, especially
when the summary of multi-agent debate is provided to LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional
  Expression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuhee Kim, Surin Lee, Sangah Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many literary texts, emotions are indirectly conveyed through descriptions
of actions, facial expressions, and appearances, necessitating emotion
inference for narrative understanding. In this paper, we introduce K-Act2Emo, a
Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional
expressions and the emotions inferable from them. We categorize reasoning types
into inferences in positive situations, inferences in negative situations, and
inferences when expressions do not serve as emotional cues. Unlike existing
CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results
validate its effectiveness for training emotion inference models.
Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo
outperforms various existing Korean large language models, achieving
performance levels comparable to GPT-4 Turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayoutLLM: Large Language Model <span class="highlight-title">Instruct</span>ion Tuning for Visually Rich
  Document Understanding <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masato Fujitake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LayoutLLM, a more flexible document analysis method for
understanding imaged documents. Visually Rich Document Understanding tasks,
such as document image classification and information extraction, have gained
significant attention due to their importance. Existing methods have been
developed to enhance document comprehension by incorporating pre-training
awareness of images, text, and layout structure. However, these methods require
fine-tuning for each task and dataset, and the models are expensive to train
and operate. To overcome this limitation, we propose a new LayoutLLM that
integrates these with large-scale language models (LLMs). By leveraging the
strengths of existing research in document image understanding and LLMs'
superior language understanding capabilities, the proposed model, fine-tuned
with multimodal instruction datasets, performs an understanding of document
images in a single model. Our experiments demonstrate improvement over the
baseline model in various document analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large
  <span class="highlight-title">Language Models</span> with Machine Learning in tele-dermatology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Artificial Intelligence creates great promise in the field of
medical discovery, diagnostics and patient management. However, the vast
complexity of all medical domains require a more complex approach that combines
machine learning algorithms, classifiers, segmentation algorithms and, lately,
large language models. In this paper, we describe, implement and assess an
Artificial Intelligence-empowered system and methodology aimed at assisting the
diagnosis process of skin lesions and other skin conditions within the field of
dermatology that aims to holistically address the diagnostic process in this
domain. The workflow integrates large language, transformer-based vision models
and sophisticated machine learning tools. This holistic approach achieves a
nuanced interpretation of dermatological conditions that simulates and
facilitates a dermatologist's workflow. We assess our proposed methodology
through a thorough cross-model validation technique embedded in an evaluation
pipeline that utilizes publicly available medical case studies of skin
conditions and relevant images. To quantitatively score the system performance,
advanced machine learning and natural language processing tools are employed
which focus on similarity comparison and natural language inference.
Additionally, we incorporate a human expert evaluation process based on a
structured checklist to further validate our results. We implemented the
proposed methodology in a system which achieved approximate (weighted) scores
of 0.87 for both contextual understanding and diagnostic accuracy,
demonstrating the efficacy of our approach in enhancing dermatological
analysis. The proposed methodology is expected to prove useful in the
development of next-generation tele-dermatology applications, enhancing remote
consultation capabilities and access to care, especially in underserved areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning from Reflective Feedback (RLRF): Aligning and
  Improving LLMs via Fine-Grained Self-Reflection <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promise of RLHF in aligning LLMs with human preferences, it often
leads to superficial alignment, prioritizing stylistic changes over improving
downstream performance of LLMs. Underspecified preferences could obscure
directions to align the models. Lacking exploration restricts identification of
desirable outputs to improve the models. To overcome these challenges, we
propose a novel framework: Reinforcement Learning from Reflective Feedback
(RLRF), which leverages fine-grained feedback based on detailed criteria to
improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism
to systematically explore and refine LLM responses, then fine-tuning the models
via a RL algorithm along with promising responses. Our experiments across
Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and
transformative potential of RLRF beyond superficial surface-level adjustment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, Submitted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model editing is a growing area focused on updating the knowledge embedded
within models. Among the various methodologies, ROME and MEMIT stand out as
leading "locate-and-edit" model editing techniques. While MEMIT enables batched
editing of memories, ROME is limited to changing one fact at a time. This paper
introduces a unifying framework that brings ROME and MEMIT under a single
conceptual umbrella, optimizing for the same goal, which we call the
"preservation-memorization" objective. This objective aims to preserve the
representations of certain selected vectors while memorizing the
representations of new factual information. Specifically, ROME optimizes this
objective using an equality constraint, whereas MEMIT employs a more flexible
least-square constraint. In addition to making batched edits, MEMIT also edits
the model at multiple layers. We disentangle the distribution of edits to
multiple layers from the optimization objective of MEMIT and show that these
edit-distribution algorithms should be considered separate entities worthy of
their own line of research.
  Finally, we present EMMET - an Equality-constrained Mass Model Editing
algorithm for Transformers, a new batched memory-editing algorithm. With EMMET,
we present a closed form solution for the equality-constrained version of the
preservation-memorization objective. We show that EMMET is able to perform
batched-edits on par with MEMIT up to a batch-size of 256 and discuss the
challenges in stabilizing EMMET. By articulating the "locate-and-edit" model
editing algorithms under a simple conceptual framework of
"preservation-memorization", we aim to bridge the gap between intuition and
mathematics and hope to simplify the journey for future researchers in model
editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale Label Interpretation Learning for Few-Shot Named Entity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Golde, Felix Hamborg, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot named entity recognition (NER) detects named entities within text
using only a few annotated examples. One promising line of research is to
leverage natural language descriptions of each entity type: the common label
PER might, for example, be verbalized as ''person entity.'' In an initial label
interpretation learning phase, the model learns to interpret such verbalized
descriptions of entity types. In a subsequent few-shot tagset extension phase,
this model is then given a description of a previously unseen entity type (such
as ''music album'') and optionally a few training examples to perform few-shot
NER for this type. In this paper, we systematically explore the impact of a
strong semantic prior to interpret verbalizations of new entity types by
massively scaling up the number and granularity of entity types used for label
interpretation learning. To this end, we leverage an entity linking benchmark
to create a dataset with orders of magnitude of more distinct entity types and
descriptions as currently used datasets. We find that this increased signal
yields strong results in zero- and few-shot NER in in-domain, cross-domain, and
even cross-lingual settings. Our findings indicate significant potential for
improving few-shot NER through heuristical data-based optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Robustness of Large <span class="highlight-title">Language Models</span> via Consistency
  Alignment <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown tremendous success in following user
instructions and generating helpful responses. Nevertheless, their robustness
is still far from optimal, as they may generate significantly inconsistent
responses due to minor changes in the verbalized instructions. Recent
literature has explored this inconsistency issue, highlighting the importance
of continued improvement in the robustness of response generation. However,
systematic analysis and solutions are still lacking. In this paper, we
quantitatively define the inconsistency problem and propose a two-stage
training framework consisting of instruction-augmented supervised fine-tuning
and consistency alignment training. The first stage helps a model generalize on
following instructions via similar instruction augmentations. In the second
stage, we improve the diversity and help the model understand which responses
are more aligned with human expectations by differentiating subtle differences
in similar responses. The training process is accomplished by self-rewards
inferred from the trained model at the first stage without referring to
external human preference resources. We conduct extensive experiments on recent
publicly available LLMs on instruction-following tasks and demonstrate the
effectiveness of our training framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Annotation of Grammaticality in Child-Caregiver Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Nikolaus, Abhishek Agrawal, Petros Kaklamanis, Alex Warstadt, Abdellah Fourtassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition of grammar has been a central question to adjudicate between
theories of language acquisition. In order to conduct faster, more
reproducible, and larger-scale corpus studies on grammaticality in
child-caregiver conversations, tools for automatic annotation can offer an
effective alternative to tedious manual annotation. We propose a coding scheme
for context-dependent grammaticality in child-caregiver conversations and
annotate more than 4,000 utterances from a large corpus of transcribed
conversations. Based on these annotations, we train and evaluate a range of NLP
models. Our results show that fine-tuned Transformer-based models perform best,
achieving human inter-annotation agreement levels.As a first application and
sanity check of this tool, we use the trained models to annotate a corpus
almost two orders of magnitude larger than the manually annotated data and
verify that children's grammaticality shows a steady increase with age.This
work contributes to the growing literature on applying state-of-the-art NLP
methods to help study child language acquisition at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Quality Matters in Training Fusion-in-Decoder for Extractive
  Open-Domain Question Answering <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation models augment knowledge encoded in a language
model by providing additional relevant external knowledge (context) during
generation. Although it has been shown that the quantity and quality of context
impact the performance of retrieval-augmented generation models during
inference, limited research explores how these characteristics affect model
training. This paper explores how context quantity and quality during model
training affect the performance of Fusion-in-Decoder (FiD), the
state-of-the-art retrieval-augmented generation model, in extractive
open-domain question answering tasks. Experimental results suggest that FiD
models overfit to context quality during training and show suboptimal
performance when evaluated on different context quality. Through the
experimental results, we also reveal FiD models trained with different context
quality have different cross-attention distribution patterns. Specifically, as
context quality during training increases, FiD models tend to attend more
uniformly to each passage in context. Finally, based on these observations, we
propose a method to mitigate overfitting to specific context quality by
introducing bias to the cross-attention distribution, which we demonstrate to
be effective in improving the performance of FiD models on different context
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMIDR: Teaching Large Language Model to Interpret Multimodal
  Misinformation via Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic detection of multimodal misinformation has gained a widespread
attention recently. However, the potential of powerful Large Language Models
(LLMs) for multimodal misinformation detection remains underexplored. Besides,
how to teach LLMs to interpret multimodal misinformation in cost-effective and
accessible way is still an open question. To address that, we propose MMIDR, a
framework designed to teach LLMs in providing fluent and high-quality textual
explanations for their decision-making process of multimodal misinformation. To
convert multimodal misinformation into an appropriate instruction-following
format, we present a data augmentation perspective and pipeline. This pipeline
consists of a visual information processing module and an evidence retrieval
module. Subsequently, we prompt the proprietary LLMs with processed contents to
extract rationales for interpreting the authenticity of multimodal
misinformation. Furthermore, we design an efficient knowledge distillation
approach to distill the capability of proprietary LLMs in explaining multimodal
misinformation into open-source LLMs. To explore several research questions
regarding the performance of LLMs in multimodal misinformation detection tasks,
we construct an instruction-following multimodal misinformation dataset and
conduct comprehensive experiments. The experimental findings reveal that our
MMIDR exhibits sufficient detection performance and possesses the capacity to
provide compelling rationales to support its assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual
  Academic Lecture <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publishing open-source academic video recordings is an emergent and prevalent
approach to sharing knowledge online. Such videos carry rich multimodal
information including speech, the facial and body movements of the speakers, as
well as the texts and pictures in the slides and possibly even the papers.
Although multiple academic video datasets have been constructed and released,
few of them support both multimodal content recognition and understanding
tasks, which is partially due to the lack of high-quality human annotations. In
this paper, we propose a novel multimodal, multigenre, and multipurpose
audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of
videos from five sources covering computer science, mathematics, and medical
and biology topics. With high-quality human annotations of the spoken and
written words, in particular high-valued name entities, the dataset can be used
for multiple audio-visual recognition and understanding tasks. Evaluations
performed on contextual speech recognition, speech synthesis, and slide and
script generation tasks demonstrate that the diversity of M$^3$AV makes it a
challenging dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C-TPT: Calibrated Test-Time <span class="highlight-title">Prompt</span> Tuning for Vision-<span class="highlight-title">Language Models</span> via
  Text Feature Dispersion <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration-a crucial aspect
for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Handcrafted Features to LLMs: A Brief <span class="highlight-title">Survey</span> for Machine
  Translation Quality Estimation <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation Quality Estimation (MTQE) is the task of estimating the
quality of machine-translated text in real time without the need for reference
translations, which is of great importance for the development of MT. After two
decades of evolution, QE has yielded a wealth of results. This article provides
a comprehensive overview of QE datasets, annotation methods, shared tasks,
methodologies, challenges, and future research directions. It begins with an
introduction to the background and significance of QE, followed by an
explanation of the concepts and evaluation metrics for word-level QE,
sentence-level QE, document-level QE, and explainable QE. The paper categorizes
the methods developed throughout the history of QE into those based on
handcrafted features, deep learning, and Large Language Models (LLMs), with a
further division of deep learning-based methods into classic deep learning and
those incorporating pre-trained language models (LMs). Additionally, the
article details the advantages and limitations of each method and offers a
straightforward comparison of different approaches. Finally, the paper
discusses the current challenges in QE research and provides an outlook on
future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Design Space for Intelligent and Interactive Writing Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our era of rapid technological advancement, the research landscape for
writing assistants has become increasingly fragmented across various research
communities. We seek to address this challenge by proposing a design space as a
structured way to examine and explore the multidimensional space of intelligent
and interactive writing assistants. Through a large community collaboration, we
explore five aspects of writing assistants: task, user, technology,
interaction, and ecosystem. Within each aspect, we define dimensions (i.e.,
fundamental components of an aspect) and codes (i.e., potential options for
each dimension) by systematically reviewing 115 papers. Our design space aims
to offer researchers and designers a practical tool to navigate, comprehend,
and compare the various possibilities of writing assistants, and aid in the
envisioning and design of new writing assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Chinese Commonsense Reasoning of LLMs: From
  Chinese-Specifics to Reasoning-Memorization Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;
  Corresponding author: Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain
  Multi-Hop Dense Sentence Retrieval <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent research, contrastive learning has proven to be a highly effective
method for representation learning and is widely used for dense retrieval.
However, we identify that relying solely on contrastive learning can lead to
suboptimal retrieval performance. On the other hand, despite many retrieval
datasets supporting various learning objectives beyond contrastive learning,
combining them efficiently in multi-task learning scenarios can be challenging.
In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence
retrieval system built upon a novel Multi-task Mixed-objective approach for
dense text representation learning, addressing the aforementioned challenges.
Our approach yields state-of-the-art performance on a large-scale open-domain
fact verification benchmark dataset, FEVER. Code and data are available at:
https://github.com/TonyBY/M3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Taxonomy of Ambiguity Types for NLP <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margaret Y. Li, Alisa Liu, Zhaofeng Wu, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ambiguity is an critical component of language that allows for more effective
communication between speakers, but is often ignored in NLP. Recent work
suggests that NLP systems may struggle to grasp certain elements of human
language understanding because they may not handle ambiguities at the level
that humans naturally do in communication. Additionally, different types of
ambiguity may serve different purposes and require different approaches for
resolution, and we aim to investigate how language models' abilities vary
across types. We propose a taxonomy of ambiguity types as seen in English to
facilitate NLP analysis. Our taxonomy can help make meaningful splits in
language ambiguity data, allowing for more fine-grained assessments of both
datasets and model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the UnImplicit workshop at EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio
  Benchmarks and Novel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan Cowen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine
learning (ML) experts from various audio domains. There are several valuable
audio-driven ML tasks, from speech emotion recognition to audio event
detection, but the community is sparse compared to other ML areas, e.g.,
computer vision or natural language processing. A major limitation with audio
is the available data; with audio being a time-dependent modality, high-quality
data collection is time-consuming and costly, making it challenging for
academic groups to apply their often state-of-the-art strategies to a larger,
more generalizable dataset. In this short white paper, to encourage researchers
with limited access to large-datasets, the organizers first outline several
open-source datasets that are available to the community, and for the duration
of the workshop are making several propriety datasets available. Namely, three
vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech
dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We
outline the current baselines on these datasets but encourage researchers from
across audio to utilize them outside of the initial baseline tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamReward: Text-to-3D <span class="highlight-title">Generation</span> with Human Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D content creation from text prompts has shown remarkable success recently.
However, current text-to-3D methods often generate 3D results that do not align
well with human preferences. In this paper, we present a comprehensive
framework, coined DreamReward, to learn and improve text-to-3D models from
human preference feedback. To begin with, we collect 25k expert comparisons
based on a systematic annotation pipeline including rating and ranking. Then,
we build Reward3D -- the first general-purpose text-to-3D human preference
reward model to effectively encode human preferences. Building upon the 3D
reward model, we finally perform theoretical analysis and present the Reward3D
Feedback Learning (DreamFL), a direct tuning algorithm to optimize the
multi-view diffusion models with a redefined scorer. Grounded by theoretical
proof and extensive experiment comparisons, our DreamReward successfully
generates high-fidelity and 3D consistent results with significant boosts in
prompt alignment with human intention. Our results demonstrate the great
potential for learning from human feedback to improve text-to-3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jamesyjl.github.io/DreamReward</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Enhanced Data-free Approach for Federated Class-Incremental
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal
issue, involving the dynamic addition of new classes in the context of
federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a
crucial role in addressing catastrophic forgetting and data privacy problems.
However, prior approaches lack the crucial synergy between DFKT and the model
training phases, causing DFKT to encounter difficulties in generating
high-quality data from a non-anchored latent space of the old task model. In
this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge
Transfer) to address this issue by utilizing label text embeddings (LTE)
produced by pretrained language models. Specifically, during the model training
phase, our approach treats LTE as anchor points and constrains the feature
embeddings of corresponding training samples around them, enriching the
surrounding area with more meaningful information. In the DFKT phase, by using
these LTE anchors, LANDER can synthesize more meaningful samples, thereby
effectively addressing the forgetting problem. Additionally, instead of tightly
constraining embeddings toward the anchor, the Bounding Loss is introduced to
encourage sample embeddings to remain flexible within a defined radius. This
approach preserves the natural differences in sample embeddings and mitigates
the embedding overlap caused by heterogeneous federated settings. Extensive
experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that
LANDER significantly outperforms previous methods and achieves state-of-the-art
performance in FCIL. The code is available at
https://github.com/tmtuan1307/lander.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Emotion Phrases from Tweets using BART 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Rezapour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is a natural language processing task that aims to
identify and extract the emotional aspects of a text. However, many existing
sentiment analysis methods primarily classify the overall polarity of a text,
overlooking the specific phrases that convey sentiment. In this paper, we
applied an approach to sentiment analysis based on a question-answering
framework. Our approach leverages the power of Bidirectional Autoregressive
Transformer (BART), a pre-trained sequence-to-sequence model, to extract a
phrase from a given text that amplifies a given sentiment polarity. We create a
natural language question that identifies the specific emotion to extract and
then guide BART to pay attention to the relevant emotional cues in the text. We
use a classifier within BART to predict the start and end positions of the
answer span within the text, which helps to identify the precise boundaries of
the extracted emotion phrase. Our approach offers several advantages over most
sentiment analysis studies, including capturing the complete context and
meaning of the text and extracting precise token spans that highlight the
intended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRE: Document-Level Relation Extraction with Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Lilong, Zhang Dan, Dong Yuxiao, Tang Jie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional abilities in
comprehending and generating text, motivating numerous researchers to utilize
them for Information Extraction (IE) purposes, including Relation Extraction
(RE). Nonetheless, most existing methods are predominantly designed for
Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a
restricted set of relations and triplet facts within a single sentence.
Furthermore, certain approaches resort to treating relations as candidate
choices integrated into prompt templates, leading to inefficient processing and
suboptimal performance when tackling Document-Level Relation Extraction (DocRE)
tasks, which entail handling multiple relations and triplet facts distributed
across a given document, posing distinct challenges. To overcome these
limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel
RE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing
approaches, AutoRE does not rely on the assumption of known relation options,
making it more reflective of real-world scenarios. Additionally, we have
developed an easily extensible RE framework using a Parameters Efficient Fine
Tuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset
showcase AutoRE's best performance, achieving state-of-the-art results,
surpassing TAG by 10.03% and 9.03% respectively on the dev and test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidLA: Video-Language Alignment at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran, Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose VidLA, an approach for video-language alignment at
scale. There are two major limitations of previous video-language alignment
approaches. First, they do not capture both short-range and long-range temporal
dependencies and typically employ complex hierarchical deep network
architectures that are hard to integrate with existing pretrained image-text
foundation models. To effectively address this limitation, we instead keep the
network architecture simple and use a set of data tokens that operate at
different temporal resolutions in a hierarchical manner, accounting for the
temporally hierarchical nature of videos. By employing a simple two-tower
architecture, we are able to initialize our video-language model with
pretrained image-text foundation models, thereby boosting the final
performance. Second, existing video-language alignment works struggle due to
the lack of semantically aligned large-scale training data. To overcome it, we
leverage recent LLMs to curate the largest video-language dataset to date with
better visual grounding. Furthermore, unlike existing video-text datasets which
only contain short clips, our dataset is enriched with video clips of varying
durations to aid our temporally hierarchical data tokens in extracting better
representations at varying temporal scales. Overall, empirical results show
that our proposed approach surpasses state-of-the-art methods on multiple
retrieval benchmarks, especially on longer videos, and performs competitively
on classification benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Plausibility Estimates in Base and <span class="highlight-title">Instruct</span>ion-Tuned Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, Anna A. Ivanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned LLMs can respond to explicit queries formulated as prompts,
which greatly facilitates interaction with human users. However, prompt-based
approaches might not always be able to tap into the wealth of implicit
knowledge acquired by LLMs during pre-training. This paper presents a
comprehensive study of ways to evaluate semantic plausibility in LLMs. We
compare base and instruction-tuned LLM performance on an English sentence
plausibility task via (a) explicit prompting and (b) implicit estimation via
direct readout of the probabilities models assign to strings. Experiment 1
shows that, across model architectures and plausibility datasets, (i) log
likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence
plausibility, with zero-shot prompting yielding inconsistent and typically poor
results; (ii) $\textit{LL}$-based performance is still inferior to human
performance; (iii) instruction-tuned models have worse $\textit{LL}$-based
performance than base models. In Experiment 2, we show that $\textit{LL}$
scores across models are modulated by context in the expected way, showing high
performance on three metrics of context-sensitive plausibility and providing a
direct match to explicit human plausibility judgments. Overall, $\textit{LL}$
estimates remain a more reliable measure of plausibility in LLMs than direct
prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAMS: Translation-Assisted Morphological Segmentation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enora Rice, Ali Marashian, Luke Gessler, Alexis Palmer, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical morphological segmentation is the process of analyzing words into
the standard (aka underlying) forms of their constituent morphemes. This is a
core task in language documentation, and NLP systems have the potential to
dramatically speed up this process. But in typical language documentation
settings, training data for canonical morpheme segmentation is scarce, making
it difficult to train high quality models. However, translation data is often
much more abundant, and, in this work, we present a method that attempts to
leverage this data in the canonical segmentation task. We propose a
character-level sequence-to-sequence model that incorporates representations of
translations obtained from pretrained high-resource monolingual language models
as an additional signal. Our model outperforms the baseline in a super-low
resource setting but yields mixed results on training splits with more data.
While further work is needed to make translations useful in higher-resource
settings, our model shows promise in severely resource-constrained settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACL ARR on December 15th 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The opportunities and risks of large <span class="highlight-title">language models</span> in mental health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global rates of mental health concerns are rising and there is increasing
realization that existing models of mental healthcare will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health-related tasks. In this review, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs application
to mental health and encourage adoption of strategies to mitigate these risks.
The urgent need for mental health support must be balanced with responsible
development, testing, and deployment of mental health LLMs. Especially critical
is ensuring that mental health LLMs are fine-tuned for mental health, enhance
mental health equity, adhere to ethical standards, and that people, including
those with lived experience with mental health concerns, are involved in all
stages from development through deployment. Prioritizing these efforts will
minimize potential harms to mental health and maximize the likelihood that LLMs
will positively impact mental health globally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collection of Pragmatic-Similarity Judgments over Spoken Dialog
  Utterances <span class="chip">LREC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel G. Ward, Divette Marco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic measures of similarity between utterances are invaluable for
training speech synthesizers, evaluating machine translation, and assessing
learner productions. While there exist measures for semantic similarity and
prosodic similarity, there are as yet none for pragmatic similarity. To enable
the training of such measures, we developed the first collection of human
judgments of pragmatic similarity between utterance pairs. Each pair consisting
of an utterance extracted from a recorded dialog and a re-enactment of that
utterance. Re-enactments were done under various conditions designed to create
a variety of degrees of similarity. Each pair was rated on a continuous scale
by 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for
English and 0.66 for Spanish. We make this data available at
https://github.com/divettemarco/PragSim .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot
  Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, Camillo J. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the zero-shot capabilities of foundation models in Visual
Question Answering (VQA) tasks. We propose an adaptive multi-agent system,
named Multi-Agent VQA, to overcome the limitations of foundation models in
object detection and counting by using specialized agents as tools. Unlike
existing approaches, our study focuses on the system's performance without
fine-tuning it on specific VQA datasets, making it more practical and robust in
the open world. We present preliminary experimental results under zero-shot
scenarios and highlight some failure cases, offering new directions for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A full version of the paper will be released soon. The codes are
  available at https://github.com/bowen-upenn/Multi-Agent-VQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Adversarial <span class="highlight-title">Prompt</span> Learning on Vision-<span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of deep neural networks to imperceptible adversarial
perturbations has attracted widespread attention. Inspired by the success of
vision-language foundation models, previous efforts achieved zero-shot
adversarial robustness by aligning adversarial visual features with text
supervision. However, in practice, they are still unsatisfactory due to several
issues, including heavy adaptation cost, suboptimal text supervision, and
uncontrolled natural generalization capacity. In this paper, to address these
issues, we propose a few-shot adversarial prompt framework where adapting input
sequences with limited data makes significant adversarial robustness
improvement. Specifically, we achieve this by providing adversarially
correlated text supervision that is end-to-end learned from adversarial
examples. We also propose a novel training objective that enhances the
consistency of multi-modal features while encourages differentiated uni-modal
features between natural and adversarial examples. The proposed framework gives
access to learn adversarial text supervision, which provides superior
cross-modal adversarial alignment and matches state-of-the-art zero-shot
adversarial robustness with only 1% training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 tables, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamingT2V: Consistent, Dynamic, and Extendable Long Video <span class="highlight-title">Generation</span>
  from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video diffusion models enable the generation of high-quality videos
that follow text instructions, making it easy to create diverse and individual
content. However, existing approaches mostly focus on high-quality short video
generation (typically 16 or 24 frames), ending up with hard-cuts when naively
extended to the case of long video synthesis. To overcome these limitations, we
introduce StreamingT2V, an autoregressive approach for long video generation of
80, 240, 600, 1200 or more frames with smooth transitions. The key components
are:(i) a short-term memory block called conditional attention module (CAM),
which conditions the current generation on the features extracted from the
previous chunk via an attentional mechanism, leading to consistent chunk
transitions, (ii) a long-term memory block called appearance preservation
module, which extracts high-level scene and object features from the first
video chunk to prevent the model from forgetting the initial scene, and (iii) a
randomized blending approach that enables to apply a video enhancer
autoregressively for infinitely long videos without inconsistencies between
chunks. Experiments show that StreamingT2V generates high motion amount. In
contrast, all competing image-to-video methods are prone to video stagnation
when applied naively in an autoregressive manner. Thus, we propose with
StreamingT2V a high-quality seamless text-to-long video generator that
outperforms competitors with consistency and motion. Our code will be available
at: https://github.com/Picsart-AI-Research/StreamingT2V
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Picsart-AI-Research/StreamingT2V</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A <span class="highlight-title">Survey</span> of Neural Code Intelligence: Paradigms, Advances and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng Qiu, <span class="highlight-author">Pengcheng Yin</span>, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Code Intelligence -- leveraging deep learning to understand, generate,
and optimize code -- holds immense potential for transformative impacts on the
whole society. Bridging the gap between Natural Language and Programming
Language, this domain has drawn significant attention from researchers in both
research communities over the past few years. This survey presents a systematic
and chronological review of the advancements in code intelligence, encompassing
over 50 representative models and their variants, more than 20 categories of
tasks, and an extensive coverage of over 680 related works. We follow the
historical progression to trace the paradigm shifts across different research
phases (e.g., from modeling code with recurrent neural networks to the era of
Large Language Models). Concurrently, we highlight the major technical
transitions in models, tasks, and evaluations spanning through different
stages. For applications, we also observe a co-evolving shift. It spans from
initial endeavors to tackling specific scenarios, through exploring a diverse
array of tasks during its rapid expansion, to currently focusing on tackling
increasingly complex and varied real-world challenges. Building on our
examination of the developmental trajectories, we further investigate the
emerging synergies between code intelligence and broader machine intelligence,
uncovering new cross-domain opportunities and illustrating the substantial
influence of code intelligence across various domains. Finally, we delve into
both the opportunities and challenges associated with this field, alongside
elucidating our insights on the most promising research directions. An ongoing,
dynamically updated project and resources associated with this survey have been
released at https://github.com/QiushiSun/NCISurvey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 6 figures, 10 tables, 688 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Knowledge Base Canonicalization with Multi-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Liu, Huang Peng, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction of large open knowledge bases (OKBs) is integral to many
knowledge-driven applications on the world wide web such as web search.
However, noun phrases and relational phrases in OKBs often suffer from
redundancy and ambiguity, which calls for the investigation on OKB
canonicalization. Current solutions address OKB canonicalization by devising
advanced clustering algorithms and using knowledge graph embedding (KGE) to
further facilitate the canonicalization process. Nevertheless, these works fail
to fully exploit the synergy between clustering and KGE learning, and the
methods designed for these subtasks are sub-optimal. To this end, we put
forward a multi-task learning framework, namely MulCanon, to tackle OKB
canonicalization. In addition, diffusion model is used in the soft clustering
process to improve the noun phrase representations with neighboring
information, which can lead to more accurate representations. MulCanon unifies
the learning objectives of these sub-tasks, and adopts a two-stage multi-task
learning paradigm for training. A thorough experimental study on popular OKB
canonicalization benchmarks validates that MulCanon can achieve competitive
canonicalization results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2310.16419</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Table<span class="highlight-title">Llama</span>: Towards Open Large Generalist Models for Tables <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-structured tables are ubiquitous. There has been a variety of tasks that
aim to automatically interpret, augment, and query tables. Current methods
often require pretraining on tables or special model architecture design, are
restricted to specific table types, or have simplifying assumptions about
tables and tasks. This paper makes the first step towards developing
open-source large language models (LLMs) as generalists for a diversity of
table-based tasks. Towards that end, we construct TableInstruct, a new dataset
with a variety of realistic tables and tasks, for instruction tuning and
evaluating LLMs. We further develop the first open-source generalist model for
tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the
long context challenge. We experiment under both in-domain setting and
out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves
comparable or better performance than the SOTA for each task, despite the
latter often has task-specific design. On 6 out-of-domain datasets, it achieves
5-44 absolute point gains compared with the base model, showing that training
on TableInstruct enhances the model's generalizability. We open-source our
dataset and trained model to boost future work on developing open generalist
models for tables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world multi-modal problems are rarely solved by a single machine
learning model, and often require multi-step computational plans that involve
stitching several models. Tool-augmented LLMs hold tremendous promise for
automating the generation of such computational plans. However, the lack of
standardized benchmarks for evaluating LLMs as planners for multi-step
multi-modal tasks has prevented a systematic study of planner design decisions.
Should LLMs generate a full plan in a single shot or step-by-step? Should they
invoke tools directly with Python code or through structured data formats like
JSON? Does feedback improve planning? To answer these questions and more, we
introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks
involving 33 tools that include multi-modal models, (free) public APIs, and
image processing modules. For each of these task queries, we provide
automatically generated plans using this realistic toolset. We further provide
a high-quality subset of 1,565 task plans that are human-verified and correctly
executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies
(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3
types of feedback (parsing/verification/execution). Finally, we summarize
takeaways from our extensive experiments. Our dataset and code are available on
HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github
(https://github.com/RAIVNLab/mnms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling the Mystery of Scaling Laws: Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling law principles indicate a power-law correlation between loss and
variables such as model size, dataset size, and computational resources
utilized during training. These principles play a vital role in optimizing
various aspects of model pre-training, ultimately contributing to the success
of large language models such as GPT-4, Llama and Gemini. However, the original
scaling law paper by OpenAI did not disclose the complete details necessary to
derive the precise scaling law formulas, and their conclusions are only based
on models containing up to 1.5 billion parameters. Though some subsequent works
attempt to unveil these details and scale to larger models, they often neglect
the training dependency of important factors such as the learning rate, context
length and batch size, leading to their failure to establish a reliable formula
for predicting the test loss trajectory. In this technical report, we confirm
that the scaling law formulations proposed in the original OpenAI paper remain
valid when scaling the model size up to 33 billion, but the constant
coefficients in these formulas vary significantly with the experiment setup. We
meticulously identify influential factors and provide transparent, step-by-step
instructions to estimate all constant terms in scaling-law formulas by training
on models with only 1M~60M parameters. Using these estimated formulas, we
showcase the capability to accurately predict various attributes for models
with up to 33B parameters before their training, including (1) the minimum
possible test loss; (2) the minimum required training steps and processed
tokens to achieve a specific loss; (3) the critical batch size with an optimal
time/computation trade-off at any loss value; and (4) the complete test loss
trajectory with arbitrary batch size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Easy<span class="highlight-title">Instruct</span>: An Easy-to-use <span class="highlight-title">Instruct</span>ion Processing Framework for Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignBank+: Preparing a Multilingual Sign Language <span class="highlight-title">Dataset</span> for Machine
  Translation Using Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Moryossef, Zifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SignBank+, a clean version of the SignBank dataset, optimized
for machine translation between spoken language text and SignWriting, a
phonetic sign language writing system. In addition to previous work that
employs complex factorization techniques to enable translation between text and
SignWriting, we show that a traditional text-to-text translation approach
performs equally effectively on the cleaned SignBank+ dataset. Our evaluation
results indicate that models trained on SignBank+ surpass those on the original
dataset, establishing a new benchmark for SignWriting-based sign language
translation and providing an open resource for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Language Augmentation for Multilingual Large <span class="highlight-title">Language Models</span>:
  A Case Study on Korean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) use pretraining to predict the subsequent word;
however, their expansion requires significant computing resources. Numerous big
tech companies and research institutes have developed multilingual LLMs (MLLMs)
to meet current demands, overlooking less-resourced languages (LRLs). This
study proposed three strategies to enhance the performance of LRLs based on the
publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to
enhance expressiveness. Second, bilingual data were used for pretraining to
align the high- and less-resourced languages. Third, a high-quality small-scale
instruction dataset was constructed and instruction-tuning was performed to
augment the LRL. The experiments employed the Llama2 model and Korean was used
as the LRL, which was quantitatively evaluated against other developed LLMs
across eight tasks. Furthermore, a qualitative assessment was performed based
on human evaluation and GPT4. Experimental results showed that our proposed
Bllossom model exhibited superior performance in qualitative analyses compared
to previously proposed Korean monolingual models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence-to-Sequence Spanish <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufiño, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant advancements in pre-trained language models have
driven the creation of numerous non-English language variants, with a
particular emphasis on encoder-only and decoder-only architectures. While
Spanish language models based on BERT and GPT have demonstrated proficiency in
natural language understanding and generation, there remains a noticeable
scarcity of encoder-decoder models explicitly designed for sequence-to-sequence
tasks, which aim to map input sequences to generate output sequences
conditionally. This paper breaks new ground by introducing the implementation
and evaluation of renowned encoder-decoder architectures exclusively
pre-trained on Spanish corpora. Specifically, we present Spanish versions of
BART, T5, and BERT2BERT-style models and subject them to a comprehensive
assessment across various sequence-to-sequence tasks, including summarization,
question answering, split-and-rephrase, dialogue, and translation. Our findings
underscore the competitive performance of all models, with the BART- and
T5-based models emerging as top performers across all tasks. We have made all
models publicly available to the research community to foster future
explorations and advancements in Spanish NLP:
https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper at LREC-Coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Structured <span class="highlight-title">Prompt</span>ing by Meta-Learning and Representative
  Verbalizer <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weisen Jiang, Yu Zhang, James T. Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning for pre-trained masked language models (MLM) has shown
promising performance in natural language processing tasks with few labeled
examples. It tunes a prompt for the downstream task, and a verbalizer is used
to bridge the predicted token and label prediction. Due to the limited training
data, prompt initialization is crucial for prompt tuning. Recently,
MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared
initialization for all task-specific prompts. However, a single initialization
is insufficient to obtain good prompts for all tasks and samples when the tasks
are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a
heavy burden on computation and memory as the MLM is usually large. To address
these issues, we use a prompt pool to extract more task knowledge and construct
instance-dependent prompts via attention. We further propose a novel soft
verbalizer (RepVerb) which constructs label embedding from feature embeddings
directly. Combining meta-learning the prompt pool and RepVerb, we propose
MetaPrompter for effective structured prompting. MetaPrompter is
parameter-efficient as only the pool is required to be tuned. Experimental
results demonstrate that MetaPrompter performs better than the recent
state-of-the-arts and RepVerb outperforms existing soft verbalizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseFormer: Enhancing Information Flow in <span class="highlight-title">Transformer</span>s via Depth
  Weighted Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Pagliardini, Amirkeivan Mohtashami, Francois Fleuret, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer architecture by Vaswani et al. (2017) is now ubiquitous
across application domains, from natural language processing to speech
processing and image understanding. We propose DenseFormer, a simple
modification to the standard architecture that improves the perplexity of the
model without increasing its size -- adding a few thousand parameters for
large-scale models in the 100B parameters range. Our approach relies on an
additional averaging step after each transformer block, which computes a
weighted average of current and past representations -- we refer to this
operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit
coherent patterns of information flow, revealing the strong and structured
reuse of activations from distant layers. Experiments demonstrate that
DenseFormer is more data efficient, reaching the same perplexity of much deeper
transformer models, and that for the same perplexity, these new models
outperform transformer baselines in terms of memory efficiency and inference
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection
  Method <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and
  Safety Adherence in Chinese Journalistic Editorial Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Ming-Bin Chen, Bo Tang, Shengbin Hou, Pengyu Wang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Keming Mao, Peng Cheng, Yi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents NewsBench, a novel benchmark framework developed to
evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic
Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap
between journalistic ethics and the risks associated with AI utilization.
Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including
safety and journalistic writing with 4 detailed facets), and spanning 24 news
topics domains, NewsBench employs two GPT-4 based automatic evaluation
protocols validated by human assessment. Our comprehensive analysis of 10 LLMs
highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative
deficiency in journalistic ethic adherence during creative writing tasks. These
findings underscore the need for enhanced ethical guidance in AI-generated
journalistic content, marking a step forward in aligning AI capabilities with
journalistic standards and safety considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pluggable Neural Machine Translation Models via Memory-augmented
  Adapters <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Shuo Wang, Peng Li, Xuebo Liu, Xiaolong Wang, Weidong Liu, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although neural machine translation (NMT) models perform well in the general
domain, it remains rather challenging to control their generation behavior to
satisfy the requirement of different users. Given the expensive training cost
and the data scarcity challenge of learning a new model from scratch for each
user requirement, we propose a memory-augmented adapter to steer pretrained NMT
models in a pluggable manner. Specifically, we construct a multi-granular
memory based on the user-provided text samples and propose a new adapter
architecture to combine the model representations and the retrieved results. We
also propose a training strategy using memory dropout to reduce spurious
dependencies between the NMT model and the memory. We validate our approach on
both style- and domain-specific experiments and the results indicate that our
method can outperform several representative pluggable baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing Language Model Incorporating Domain-specific Heterogeneous
  Knowledge into A Unified Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.01048v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.01048v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu, Hao Peng, Zhiheng Lyu, Lei Hou, Juanzi Li, Jinghui Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing technologies expand BERT from different perspectives, e.g. designing
different pre-training tasks, different semantic granularities, and different
model architectures. Few models consider expanding BERT from different text
formats. In this paper, we propose a heterogeneous knowledge language model
(\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of
text, including unstructured text, semi-structured text, and well-structured
text. To capture the corresponding relations among these multi-format
knowledge, our approach uses masked language model objective to learn word
knowledge, uses triple classification objective and title matching objective to
learn entity knowledge and topic knowledge respectively. To obtain the
aforementioned multi-format text, we construct a corpus in the tourism domain
and conduct experiments on 5 tourism NLP datasets. The results show that our
approach outperforms the pre-training of plain text using only 1/4 of the data.
We further pre-train the domain-agnostic HKLM and achieve performance gains on
the XNLI dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by
  List-Context Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passage reranking is a critical task in various applications, particularly
when dealing with large volumes of documents. Existing neural architectures
have limitations in retrieving the most relevant passage for a given question
because the semantics of the segmented passages are often incomplete, and they
typically match the question to each passage individually, rarely considering
contextual information from other passages that could provide comparative and
reference information. This paper presents a list-context attention mechanism
to augment the passage representation by incorporating the list-context
information from other candidates. The proposed coarse-to-fine (C2F) neural
retriever addresses the out-of-memory limitation of the passage attention
mechanism by dividing the list-context modeling process into two sub-processes
with a cache policy learning algorithm, enabling the efficient encoding of
context information from a large number of candidate answers. This method can
be generally used to encode context information from any number of candidate
answers in one pass. Different from most multi-stage information retrieval
architectures, this model integrates the coarse and fine rankers into the joint
optimization process, allowing for feedback between the two layers to update
the model simultaneously. Experiments demonstrate the effectiveness of the
proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs and the Human Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Wallis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents three established theories of human decision-making and
describes how they can be integrated to provide a model of purposive human
action. Taking seriously the idea of language as action the model is then
applied to the conversational user interfaces. Theory based AI research has had
a hard time recently and the aim here is to revitalise interest in
understanding what LLMs are actually doing other than running poorly understood
machine learning routines over all the data the relevant Big Tech company can
hoover up. When a raspberry pi computer for under 50USD is up to 400 times
faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech
can get really close to having an infinite number of monkeys typing at random
and producing text, some of which will make sense. By understanding where
ChatGPT's apparent intelligence comes from, perhaps we can perform the magic
with fewer resources and at the same time gain some understanding about our
relationship with our world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A 2nd draft with a better abstract and introduction. target is IVA in
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Clérice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose to evaluate the use of deep learning methods for
semantic classification at the sentence level to accelerate the process of
corpus building in the field of humanities and linguistics, a traditional and
time-consuming task. We introduce a novel corpus comprising around 2500
sentences spanning from 300 BCE to 900 CE including sexual semantics (medical,
erotica, etc.). We evaluate various sentence classification approaches and
different input embedding layers, and show that all consistently outperform
simple token-based searches. We explore the integration of idiolectal and
sociolectal metadata embeddings (centuries, author, type of writing), but find
that it leads to overfitting. Our results demonstrate the effectiveness of this
approach, achieving high precision and true positive rates (TPR) of
respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset
size on the model performances (420 instead of 2013), and show that, while our
models perform worse, they still offer a high enough precision and TPR, even
without MLM, respectively 69% and 51%. Given the result, we provide an analysis
of the attention mechanism as a supporting added value for humanists in order
to produce more data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Llama</span>Factory: Unified Efficient Fine-Tuning of 100+ <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Yongqiang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI LlamaBoard. We empirically validate the efficiency
and effectiveness of our framework on language modeling and text generation
tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and
already received over 13,000 stars and 1,600 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating, Understanding, and Improving Constrained Text <span class="highlight-title">Generation</span> for
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in natural language generation (NLG) and large language models
(LLMs) have led to proficient text generation in various tasks. However,
integrating intricate constraints into neural text generation, due to LLMs'
opacity, remains challenging. This study investigates constrained text
generation for LLMs, where predefined constraints are applied during LLM's
generation process. Our research mainly focuses on mainstream open-source LLMs,
categorizing constraints into lexical, structural, and relation-based types. We
also present various benchmarks to facilitate fair evaluation. The study
addresses some key research questions, including evaluating, understanding and
improving constrained text generation for LLMs. Results illuminate LLMs'
capacity and deficiency to incorporate constraints and provide insights for
future developments in constrained text generation. Codes and datasets will be
released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Explanations to Understand and Repair Embedding-based Entity
  Alignment <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04877v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04877v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobin Tian, Zequn Sun, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) seeks identical entities in different knowledge graphs,
which is a long-standing task in the database research. Recent work leverages
deep learning to embed entities in vector space and align them via nearest
neighbor search. Although embedding-based EA has gained marked success in
recent years, it lacks explanations for alignment decisions. In this paper, we
present the first framework that can generate explanations for understanding
and repairing embedding-based EA results. Given an EA pair produced by an
embedding model, we first compare its neighbor entities and relations to build
a matching subgraph as a local explanation. We then construct an alignment
dependency graph to understand the pair from an abstract perspective. Finally,
we repair the pair by resolving three types of alignment conflicts based on
dependency graphs. Experiments on a variety of EA datasets demonstrate the
effectiveness, generalization, and robustness of our framework in explaining
and repairing embedding-based EA results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 40th IEEE International Conference on Data
  Engineering (ICDE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: Data Value Estimation for Visual <span class="highlight-title">Instruct</span>ion Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning is the key to building multimodal large language
models (MLLMs), which greatly improves the reasoning capabilities of large
language models (LLMs) in vision scenario. However, existing MLLMs mostly rely
on a mixture of multiple highly diverse visual instruction datasets for
training (even more than a million instructions), which may introduce data
redundancy. To investigate this issue, we conduct a series of empirical
studies, which reveal a significant redundancy within the visual instruction
datasets, and show that greatly reducing the amount of several instruction
dataset even do not affect the performance. Based on the findings, we propose a
new data selection approach TIVE, to eliminate redundancy within visual
instruction data. TIVE first estimates the task-level and instance-level value
of the visual instructions based on computed gradients. Then, according to the
estimated values, TIVE determines the task proportion within the visual
instructions, and selects representative instances to compose a smaller visual
instruction subset for training. Experiments on LLaVA-1.5 show that our
approach using only about 7.5% data can achieve comparable performance as the
full-data fine-tuned model across seven benchmarks, even surpassing it on four
of the benchmarks. Our code and data will be publicly released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Challenge <span class="highlight-title">Dataset</span> and Effective Models for Conversational Stance
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuqiang Niu, Min Yang, Ang Li, Baoquan Zhang, Xiaojiang Peng, Bowen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous stance detection studies typically concentrate on evaluating stances
within individual instances, thereby exhibiting limitations in effectively
modeling multi-party discussions concerning the same specific topic, as
naturally transpire in authentic social media interactions. This constraint
arises primarily due to the scarcity of datasets that authentically replicate
real social media contexts, hindering the research progress of conversational
stance detection. In this paper, we introduce a new multi-turn conversation
stance detection dataset (called \textbf{MT-CSD}), which encompasses multiple
targets for conversational stance detection. To derive stances from this
challenging dataset, we propose a global-local attention network
(\textbf{GLAN}) to address both long and short-range dependencies inherent in
conversational data. Notably, even state-of-the-art stance detection methods,
exemplified by GLAN, exhibit an accuracy of only 50.47\%, highlighting the
persistent challenges in conversational stance detection. Furthermore, our
MT-CSD dataset serves as a valuable resource to catalyze advancements in
cross-domain stance detection, where a classifier is adapted from a different
yet related target. We believe that MT-CSD will contribute to advancing
real-world applications of stance detection research. Our source code, data,
and models are available at \url{https://github.com/nfq729/MT-CSD}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rank<span class="highlight-title">Prompt</span>: Step-by-Step Comparisons Make <span class="highlight-title">Language Models</span> Better
  Reasoners <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive performance across
various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT
are prone to logical errors during their reasoning processes. Traditional
approaches to mitigate these errors involve human or tool-based feedback, such
as employing task-specific verifiers or aggregating multiple reasoning paths.
These methods, however, either depend heavily on human input or struggle with
inconsistent responses. To overcome these limitations, we present RankPrompt,
an innovative prompting strategy that empowers LLMs to autonomously rank their
responses without needing extra resources. RankPrompt simplifies the ranking
challenge into comparative evaluations among different responses, leveraging
LLMs' innate ability to generate comparative examples within context. Our
experiments across 11 arithmetic and commonsense reasoning tasks show that
RankPrompt significantly enhances the reasoning performance of ChatGPT and
GPT-4, with improvements of up to 13%. Furthermore, RankPrompt shows
exceptional performance in LLM-based automatic evaluations for open-ended
tasks, matching human judgments 74% of the time in the AlpacaEval dataset. It
also proves to be robust against changes in response order and inconsistency.
Overall, our findings endorse RankPrompt as an effective method for extracting
high-quality feedback directly from language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-Coling 2024 Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANLS* -- A Universal Document Processing Metric for Generative Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs.
  This paper introduces a new metric for generative models called ANLS* for
evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets, 6 different GLLMs and 3
different prompting methods using the ANLS* metric is also provided,
demonstrating the importance of the proposed metric.
  We also benchmark a novel approach to generate prompts for documents, called
SFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,
SFT outperforms other techniques and improves the state-of-the-art, sometimes
by as much as $18$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiC-CLIP: Continual Training of CLIP Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to continually train these models. This problem is exacerbated by
the lack of any large scale continual learning benchmarks or baselines. We
introduce the first set of web-scale Time-Continual (TiC) benchmarks for
training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps.
TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text
pairs spanning 9 years (2014-2022). We first use our benchmarks to curate
various dynamic evaluations to measure temporal robustness of existing models.
We show OpenAI's CLIP (trained on data up to 2020) loses $\approx 8\%$
zero-shot accuracy on our curated retrieval task from 2021-2022 compared with
more recently trained models in OpenCLIP repository. We then study how to
efficiently train models on time-continuous data. We demonstrate that a simple
rehearsal-based approach that continues training from the last checkpoint and
replays old data reduces compute by $2.5\times$ when compared to the standard
practice of retraining from scratch. Code is available at
https://github.com/apple/ml-tic-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge
  Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Li, Shu Guo, Yinjia Chen, Cheng Ji, Jiawei Sheng, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of
a relation given its few-shot reference entity pairs. The side effect of noises
due to the uncertainty of entities and triples may limit the few-shot learning,
but existing FKGC works neglect such uncertainty, which leads them more
susceptible to limited reference samples with noises. In this paper, we propose
a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model
uncertainty for a better understanding of the limited data by learning
representations under Gaussian distribution. Uncertainty representation is
first designed for estimating the uncertainty scope of the entity pairs after
transferring feature representations into a Gaussian distribution. Further, to
better integrate the neighbors with uncertainty characteristics for entity
features, we design an uncertainty-aware relational graph neural network
(UR-GNN) to conduct convolution operations between the Gaussian distributions.
Then, multiple random samplings are conducted for reference triples within the
Gaussian distribution to generate smooth reference representations during the
optimization. The final completion score for each query instance is measured by
the designed uncertainty optimization to make our approach more robust to the
noises in few-shot scenarios. Experimental results show that our approach
achieves excellent performance on two benchmark datasets compared to its
competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoleInteract: Evaluating the Social Interaction of Role-Playing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced the development of various AI
conversational agents, including role-playing conversational agents that mimic
diverse characters and human behaviors. While prior research has predominantly
focused on enhancing the conversational capability, role-specific knowledge,
and stylistic attributes of these agents, there has been a noticeable gap in
assessing their social intelligence. In this paper, we introduce RoleInteract,
the first benchmark designed to systematically evaluate the sociality of
role-playing conversational agents at both individual and group levels of
social interactions. The benchmark is constructed from a variety of sources and
covers a wide range of 500 characters and over 6,000 question prompts and
30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations
on this benchmark using mainstream open-source and closed-source LLMs. We find
that agents excelling in individual level does not imply their proficiency in
group level. Moreover, the behavior of individuals may drift as a result of the
influence exerted by other agents within the group. Experimental results on
RoleInteract confirm its significance as a testbed for assessing the social
interaction of role-playing conversational agents. The benchmark is publicly
accessible at https://github.com/X-PLUG/RoleInteract.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Not Worry if You Do Not Have Data: Building <span class="highlight-title">Pretrain</span>ed Language
  Models Using Translationese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meet Doshi, Raj Dabre, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the utility of Translationese as synthetic data
created using machine translation for pre-training language models (LMs).
Pre-training requires vast amounts of monolingual data, which is mostly
unavailable for languages other than English. Recently, there has been a
growing interest in using synthetic data to address this data scarcity. We take
the case of English and Indic languages and translate web-crawled monolingual
documents (clean) into the target language. Then, we train language models
containing 28M and 85M parameters on this translationese data (synthetic). We
show that their performance on downstream natural language understanding and
generative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than
LMs pre-trained on clean data. Further, we propose the use of lightweight
TinyLMs pre-trained on clean data to filter synthetic data efficiently which
significantly improves the performance of our models. We also find that LMs
trained on synthetic data strongly benefit from extended pretraining on a tiny
fraction (10%) of clean data. We release the data we collected and created as a
part of this work, IndicMonoDoc, the largest collection of monolingual
document-level corpora, which we hope will help bridge the gap between English
and non-English performance for large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoachLM: Automatic <span class="highlight-title">Instruct</span>ion Revisions Improve the Data Quality in LLM
  <span class="highlight-title">Instruct</span>ion Tuning <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Liu, Shimin Tao, Xiaofeng Zhao, Ming Zhu, Wenbing Ma, Junhao Zhu, Chang Su, Yutai Hou, Miao Zhang, Min Zhang, Hongxia Ma, Li Zhang, Hao Yang, Yanfei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is crucial for enabling Language Learning Models (LLMs) in
responding to human instructions. The quality of instruction pairs used for
tuning greatly affects the performance of LLMs. However, the manual creation of
high-quality instruction datasets is costly, leading to the adoption of
automatic generation of instruction pairs by LLMs as a popular alternative. To
ensure the high quality of LLM-generated instruction datasets, several
approaches have been proposed. Nevertheless, existing methods either compromise
dataset integrity by filtering a large proportion of samples, or are unsuitable
for industrial applications. In this paper, instead of discarding low-quality
samples, we propose CoachLM, a novel approach to enhance the quality of
instruction datasets through automatic revisions on samples in the dataset.
CoachLM is trained from the samples revised by human experts and significantly
increases the proportion of high-quality samples in the dataset from 17.7% to
78.9%. The effectiveness of CoachLM is further assessed on various real-world
instruction test sets. The results show that CoachLM improves the
instruction-following capabilities of the instruction-tuned LLM by an average
of 29.9%, which even surpasses larger LLMs with nearly twice the number of
parameters. Furthermore, CoachLM is successfully deployed in a data management
system for LLMs at Huawei, resulting in an efficiency improvement of up to 20%
in the cleaning of 40k real-world instruction pairs. We release various assets
of CoachLM, including the training data, code and test set
(https://github.com/lunyiliu/CoachLM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arcee's MergeKit: A Toolkit for Merging Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of the open-source language model landscape presents an
opportunity to merge the competencies of these model checkpoints by combining
their parameters. Advances in transfer learning, the process of fine-tuning
pretrained models for specific tasks, has resulted in the development of vast
amounts of task-specific models, typically specialized in individual tasks and
unable to utilize each other's strengths. Model merging facilitates the
creation of multitask models without the need for additional training, offering
a promising avenue for enhancing model performance and versatility. By
preserving the intrinsic capabilities of the original models, model merging
addresses complex challenges in AI - including the difficulties of catastrophic
forgetting and multitask learning. To support this expanding area of research,
we introduce MergeKit, a comprehensive, open-source library designed to
facilitate the application of model merging strategies. MergeKit offers an
extensible framework to efficiently merge models on any hardware, providing
utility to researchers and practitioners. To date, thousands of models have
been merged by the open-source community, leading to the creation of some of
the worlds most powerful open-source model checkpoints, as assessed by the Open
LLM Leaderboard. The library is accessible at
https://github.com/arcee-ai/MergeKit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Language Models</span> Hallucinate, but May Excel at Fact Verification <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in natural language processing (NLP) owes much to remarkable
advances in large language models (LLMs). Nevertheless, LLMs frequently
"hallucinate," resulting in non-factual outputs. Our carefully-designed human
evaluation substantiates the serious hallucination issue, revealing that even
GPT-3.5 produces factual outputs less than 25% of the time. This underscores
the importance of fact verifiers in order to measure and incentivize progress.
Our systematic investigation affirms that LLMs can be repurposed as effective
fact verifiers with strong correlations with human judgments. Surprisingly,
FLAN-T5-11B, the least factual generator in our study, performs the best as a
fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.
Delving deeper, we analyze the reliance of these LLMs on high-quality evidence,
as well as their deficiencies in robustness and generalization ability. Our
study presents insights for developing trustworthy generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Representational Harms to Quality-of-Service Harms: A Case Study on
  <span class="highlight-title">Llama</span> 2 Safety Safeguards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taik, Jackie CK Cheung, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) has led to their widespread
adoption in various domains. However, these advancements have also introduced
additional safety risks and raised concerns regarding their detrimental impact
on already marginalized populations. Despite growing mitigation efforts to
develop safety safeguards, such as supervised safety-oriented fine-tuning and
leveraging safe reinforcement learning from human feedback, multiple concerns
regarding the safety and ingrained biases in these models remain. Furthermore,
previous work has demonstrated that models optimized for safety often display
exaggerated safety behaviors, such as a tendency to refrain from responding to
certain requests as a precautionary measure. As such, a clear trade-off between
the helpfulness and safety of these models has been documented in the
literature. In this paper, we further investigate the effectiveness of safety
measures by evaluating models on already mitigated biases. Using the case of
Llama 2 as an example, we illustrate how LLMs' safety responses can still
encode harmful assumptions. To do so, we create a set of non-toxic prompts,
which we then use to evaluate Llama models. Through our new taxonomy of LLMs
responses to users, we observe that the safety/helpfulness trade-offs are more
pronounced for certain demographic groups which can lead to quality-of-service
harms for marginalized populations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NSFW (Not Safe for Work) content, in the context of a dialogue, can have
severe side effects on users in open-domain dialogue systems. However, research
on detecting NSFW language, especially sexually explicit content, within a
dialogue context has significantly lagged behind. To address this issue, we
introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue
detection. Leveraging knowledge distillation techniques involving GPT-4 and
ChatGPT, this dataset offers a cost-effective means of constructing NSFW
content detectors. The process entails collecting real-life human-machine
interaction data and breaking it down into single utterances and single-turn
dialogues, with the chatbot delivering the final utterance. ChatGPT is employed
to annotate unlabeled data, serving as a training set. Rationale validation and
test sets are constructed using ChatGPT and GPT-4 as annotators, with a
self-criticism strategy for resolving discrepancies in labeling. A BERT model
is fine-tuned as a text classifier on pseudo-labeled data, and its performance
is assessed. The study emphasizes the importance of AI systems prioritizing
user safety and well-being in digital conversations while respecting freedom of
expression. The proposed approach not only advances NSFW content detection but
also aligns with evolving user protection needs in AI-driven dialogues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>As we have submitted a final version arXiv:2403.13250, we decide to
  withdraw it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span>4PCG Competition: Character-like Level <span class="highlight-title">Generation</span> for Science
  Birds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Ruck Thawonmas, Julian Togelius, Jochen Renz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE
Conference on Games. The objective of this competition is for participants to
create effective prompts for ChatGPT--enabling it to generate Science Birds
levels with high stability and character-like qualities--fully using their
creativity as well as prompt engineering skills. ChatGPT is a conversational
agent developed by OpenAI. Science Birds is selected as the competition
platform because designing an Angry Birds-like level is not a trivial task due
to the in-game gravity; the quality of the levels is determined by their
stability. To lower the entry barrier to the competition, we limit the task to
the generation of capitalized English alphabetical characters. We also allow
only a single prompt to be used for generating all the characters. Here, the
quality of the generated levels is determined by their stability and similarity
to the given characters. A sample prompt is provided to participants for their
reference. An experiment is conducted to determine the effectiveness of several
modified versions of this sample prompt on level stability and similarity by
testing them on several characters. To the best of our knowledge, we believe
that ChatGPT4PCG is the first competition of its kind and hope to inspire
enthusiasm for prompt engineering in procedural content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper accepted for presentation at IEEE CoG 2023 is made
  available for participants of ChatGPT4PCG Competition
  (https://chatgpt4pcg.github.io/) and readers interested in relevant areas. In
  this PDF version, the affiliation symbol of Julian Togelius has been revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ComCLIP: Training-Free Compositional Image and Text Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13854v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13854v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenan Jiang, Xuehai He, Ruize Xu, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) has demonstrated great
zero-shot performance for matching images and text. However, it is still
challenging to adapt vision-lanaguage pretrained models like CLIP to
compositional image and text matching -- a more challenging image and text
matching task requiring the model understanding of compositional word concepts
and visual components. Towards better compositional generalization in zero-shot
image and text matching, in this paper, we study the problem from a causal
perspective: the erroneous semantics of individual entities are essentially
confounders that cause the matching failure. Therefore, we propose a novel
\textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP
disentangles input images into subjects, objects, and action sub-images and
composes CLIP's vision encoder and text encoder to perform evolving matching
over compositional text embedding and sub-image embeddings. In this way,
ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP
models and dynamically evaluate the importance of each component. Experiments
on four compositional image-text matching datasets: SVO, ComVG, Winoground, and
VL-checklist, and two general image-text retrieval datasets: Flick30K, and
MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts
the \textbf{\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even
without further training or fine-tuning. Our codes can be found at
https://github.com/eric-ai-lab/ComCLIP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Improving for Zero-Shot Named Entity Recognition with Large
  <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the application of powerful large language models (LLMs) on the
named entity recognition (NER) task has drawn much attention recently. This
work pushes the performance boundary of zero-shot NER with LLMs by proposing a
training-free self-improving framework, which utilizes an unlabeled corpus to
stimulate the self-learning ability of LLMs. First, we use the LLM to make
predictions on the unlabeled corpus using self-consistency and obtain a
self-annotated dataset. Second, we explore various strategies to select
reliable annotations to form a reliable self-annotated dataset. Finally, for
each test input, we retrieve demonstrations from the reliable self-annotated
dataset and perform inference via in-context learning. Experiments on four
benchmarks show substantial performance improvements achieved by our framework.
Through comprehensive experimental analysis, we find that increasing the size
of unlabeled corpus or iterations of self-improving does not guarantee further
improvement, but the performance might be boosted via more advanced strategies
for reliable annotation selection. Code and data are publicly available at
https://github.com/Emma1066/Self-Improve-Zero-Shot-NER
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryCeleb: A Speaker Verification <span class="highlight-title">Dataset</span> Based on Infant Cry Sounds <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00969v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00969v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Budaghyan, Charles C. Onu, Arsenii Gorin, Cem Subakan, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large <span class="highlight-title">Language Models</span> understand Medical Codes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon A. Lee, Timothy Lindsey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The overarching goal of recent AI research has been to make steady progress
towards achieving Artificial General Intelligence (AGI), prompting the
evaluation of Large Language Models (LLMs) across a variety of tasks and
domains. One such domain is healthcare, where LLMs can greatly benefit clinical
practice by assisting with a wide range of tasks. However, these models are
also prone to producing ``hallucinations" or incorrect responses when faced
with queries they cannot adequately address, raising concerns and skepticism,
especially within the healthcare community. In this work, we investigate
whether LLMs understand and can predict medical codes, which are extensively
utilized in healthcare practice. This study aims to delineate the capabilities
and limitations of these LLMs. We evaluate various off-the-shelf LLMs (e.g.,
GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to
assess their awareness and understanding of these domain-specific
terminologies. Our results indicate that these models as they currently stand
do not comprehend the meaning of the medical codes, highlighting the need for
better representation of these alphanumeric codes extensively used in
healthcare. We call for improved strategies to effectively capture and
represent the nuances of medical codes and terminologies within LLMs, enabling
them to become more reliable and trustworthy tools for healthcare
professionals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MacGyver: Are Large <span class="highlight-title">Language Models</span> Creative Problem Solvers? <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the creative problem-solving capabilities of modern LLMs in a
novel constrained setting. To this end, we create MACGYVER, an automatically
generated dataset consisting of over 1,600 real-world problems deliberately
designed to trigger innovative usage of objects and necessitate out-of-the-box
thinking. We then present our collection to both LLMs and humans to compare and
contrast their problem-solving abilities. MACGYVER is challenging for both
groups, but in unique and complementary ways. For instance, humans excel in
tasks they are familiar with but struggle with domain-specific knowledge,
leading to a higher variance. In contrast, LLMs, exposed to a variety of
specialized knowledge, attempt broader problems but fail by proposing
physically-infeasible actions. Finally, we provide a detailed error analysis of
LLMs, and demonstrate the potential of enhancing their problem-solving ability
with novel prompting techniques such as iterative step-wise reflection and
divergent-convergent thinking.
  This work (1) introduces a fresh arena for intelligent agents focusing on
intricate aspects of physical reasoning, planning, and unconventional thinking,
which supplements the existing spectrum of machine intelligence; and (2)
provides insight into the constrained problem-solving capabilities of both
humans and AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tur[k]ingBench: A Challenge Benchmark for Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xu, Yeganeh Kordi, Kate Sanders, Yizhong Wang, Adam Byerly, Jack Zhang, Benjamin Van Durme, Daniel Khashabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent chatbots have demonstrated impressive ability to understand and
communicate in raw-text form. However, there is more to the world than raw
text. For example, humans spend long hours of their time on web pages, where
text is intertwined with other modalities and tasks are accomplished in the
form of various complex interactions. Can state-of-the-art multi-modal models
generalize to such complex domains?
  To address this question, we introduce TurkingBench, a benchmark of tasks
formulated as web pages containing textual instructions with multi-modal
context. Unlike existing work which employs artificially synthesized web pages,
here we use natural HTML pages that were originally designed for crowdsourcing
workers for various annotation purposes. The HTML instructions of each task are
also instantiated with various values (obtained from the crowdsourcing tasks)
to form new instances of the task. This benchmark contains 32.2K instances
distributed across 158 tasks.
  Additionally, to facilitate the evaluation on TurkingBench, we develop an
evaluation framework that connects the responses of chatbots to modifications
on web pages (modifying a text box, checking a radio, etc.). We evaluate the
performance of state-of-the-art models, including language-only, vision-only,
and layout-only models, and their combinations, on this benchmark. Our findings
reveal that these models perform significantly better than random chance, yet
considerable room exists for improvement. We hope this benchmark will help
facilitate the evaluation and development of web-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentivizing News Consumption on Social Media Platforms Using Large
  <span class="highlight-title">Language Models</span> and Realistic Bot Accounts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael Heseltine, Magdalena Wojcieszak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polarization, declining trust, and wavering support for democratic norms are
pressing threats to U.S. democracy. Exposure to verified and quality news may
lower individual susceptibility to these threats and make citizens more
resilient to misinformation, populism, and hyperpartisan rhetoric. This project
examines how to enhance users' exposure to and engagement with verified and
ideologically balanced news in an ecologically valid setting. We rely on a
large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on
28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users
tweeting about sports, entertainment, or lifestyle with a contextual reply
containing two hardcoded elements: a URL to the topic-relevant section of
quality news organization and an encouragement to follow its Twitter account.
To further test differential effects by gender of the bots, treated users were
randomly assigned to receive responses by bots presented as female or male. We
examine whether our over-time intervention enhances the following of news media
organization, the sharing and the liking of news content and the tweeting about
politics and the liking of political content. We find that the treated users
followed more news accounts and the users in the female bot treatment were more
likely to like news content than the control. Most of these results, however,
were small in magnitude and confined to the already politically interested
Twitter users, as indicated by their pre-treatment tweeting about politics.
These findings have implications for social media and news organizations, and
also offer direction for future work on how Large Language Models and other
computational interventions can effectively enhance individual on-platform
engagement with quality news and public affairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bad<span class="highlight-title">Llama</span>: cheaply removing safety fine-tuning from <span class="highlight-title">Llama</span> 2-Chat 13B 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Gade, Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Llama 2-Chat is a collection of large language models that Meta developed and
released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output
harmful content, we hypothesize that public access to model weights enables bad
actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's
capabilities for malicious purposes. We demonstrate that it is possible to
effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than
$200, while retaining its general capabilities. Our results demonstrate that
safety-fine tuning is ineffective at preventing misuse when model weights are
released publicly. Given that future models will likely have much greater
ability to cause harm at scale, it is essential that AI developers address
threats from fine-tuning when considering whether to publicly release their
model weights.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Models and Data for Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SynGround, a novel framework that combines data-driven learning
and knowledge transfer from various large-scale pretrained models to enhance
the visual grounding capabilities of a pretrained vision-and-language model.
The knowledge transfer from the models initiates the generation of image
descriptions through an image description generator. These descriptions serve
dual purposes: they act as prompts for synthesizing images through a
text-to-image generator, and as queries for synthesizing text, from which
phrases are extracted using a large language model. Finally, we leverage an
open-vocabulary object detector to generate synthetic bounding boxes for the
synthetic images and texts. We finetune a pretrained vision-and-language model
on this dataset by optimizing a mask-attention consistency objective that
aligns region annotations with gradient-based model explanations. The resulting
model improves the grounding capabilities of an off-the-shelf
vision-and-language model. Particularly, SynGround improves the pointing game
accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on
RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to
63.67%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://catherine-r-he.github.io/SynGround/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZigMa: Zigzag Mamba Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://taohu.me/zigma/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  Control with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Training to Nurse the Reversal Curse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have a surprising failure: when trained on "A
has a feature B", they do not generalize to "B is a feature of A", which is
termed the Reversal Curse. Even when training with trillions of tokens this
issue still appears due to Zipf's law - hence even if we train on the entire
internet. This work proposes an alternative training scheme, called reverse
training, whereby all words are used twice, doubling the amount of available
tokens. The LLM is trained in both forward and reverse directions by reversing
the training strings while preserving (i.e., not reversing) chosen substrings,
such as entities. We show that data-matched reverse-trained models provide
superior performance to standard models on standard tasks, and compute-matched
reverse-trained models provide far superior performance on reversal tasks,
helping resolve the reversal curse issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Interaction: Enhancing Large <span class="highlight-title">Language Models</span> for Psychiatric
  Behavior Understanding by Dyadic Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzeng Han, Weisi Liu, Xiaolei Huang, Brian Borsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic coding patient behaviors is essential to support decision making
for psychotherapists during the motivational interviewing (MI), a collaborative
communication intervention approach to address psychiatric issues, such as
alcohol and drug addiction. While the behavior coding task has rapidly adapted
machine learning to predict patient states during the MI sessions, lacking of
domain-specific knowledge and overlooking patient-therapist interactions are
major challenges in developing and deploying those models in real practice. To
encounter those challenges, we introduce the Chain-of-Interaction (CoI)
prompting method aiming to contextualize large language models (LLMs) for
psychiatric decision support by the dyadic interactions. The CoI prompting
approach systematically breaks down the coding task into three key reasoning
steps, extract patient engagement, learn therapist question strategies, and
integrates dyadic interactions between patients and therapists. This approach
enables large language models to leverage the coding scheme, patient state, and
domain knowledge for patient behavioral coding. Experiments on real-world
datasets can prove the effectiveness and flexibility of our prompting method
with multiple state-of-the-art LLMs over existing prompting baselines. We have
conducted extensive ablation analysis and demonstrate the critical role of
dyadic interactions in applying LLMs for psychotherapy behavior understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Distillation for Reference-less Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current winning recipe for automatic summarization is using proprietary
large-scale language models (LLMs) such as ChatGPT as is, or imitation learning
from them as teacher models. While increasingly ubiquitous dependence on such
large-scale language models is convenient, there remains an important question
of whether small-scale models could have achieved competitive results, if we
were to seek an alternative learning method -- that allows for a more
cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a
novel framework to distill a powerful summarizer based on the
information-theoretic objective for summarization, without relying on either
the LLM's capability or human-written references. To achieve this, we first
propose a novel formulation of the desiderata of summarization (saliency,
faithfulness and brevity) through the lens of mutual information between the
original document and the summary. Based on this formulation, we start off from
Pythia-2.8B as the teacher model, which is not yet capable of summarization,
then self-train the model to optimize for the information-centric measures of
ideal summaries. Distilling from the improved teacher, we arrive at a compact
but powerful summarizer with only 568M parameters that performs competitively
against ChatGPT, without ever relying on ChatGPT's capabilities. Extensive
analysis demonstrates that our approach outperforms in-domain supervised models
in human evaluation, let alone state-of-the-art unsupervised methods, and wins
over ChatGPT in controllable summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Different Tokenization Schemes Lead to Comparable Performance in Spanish
  Number Agreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catherine Arnett, Pamela D. Rivière, Tyler A. Chang, Sean Trott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between language model tokenization and performance is an
open area of research. Here, we investigate how different tokenization schemes
impact number agreement in Spanish plurals. We find that
morphologically-aligned tokenization performs similarly to other tokenization
schemes, even when induced artificially for words that would not be tokenized
that way during training. We then present exploratory analyses demonstrating
that language model embeddings for different plural tokenizations have similar
distributions along the embedding space axis that maximally distinguishes
singular and plural nouns. Our results suggest that morphologically-aligned
tokenization is a viable tokenization approach, and existing models already
generalize some morphological patterns to new items. However, our results
indicate that morphological tokenization is not strictly required for
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EthioLLM: Multilingual Large <span class="highlight-title">Language Models</span> for Ethiopian Languages
  with Task Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PARAMANU-AYN: An Efficient Novel Generative and <span class="highlight-title">Instruct</span>ion-tuned
  Language Model for Indian Legal Case Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present PARAMANU-AYN, a language model based exclusively on
case documents of the Supreme Court of India, the Constitution of India, and
the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is
pretrained from scratch at a context size of 8192. We evaluated our pretrained
legal model on perplexity metrics. We also instruction-tuned our pretrained
model on a set of 10,763 instructions covering various legal tasks such as
legal reasoning, judgement explanation, legal clause generation, legal
drafting, legal contract drafting, case summarization, constitutional
question-answering, etc. We also evaluated the responses of prompts for
instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,
and legal reasoning metrics in a scale of 10. Our model can be run on CPU and
achieved 42.46 tokens/sec CPU inference speed. We found that our models,
despite not being pretrained on legal books, various legal contracts, and legal
documents, were able to learn the domain knowledge required for drafting
various legal contracts and legal clauses, and generalize to draft legal
contracts and legal clauses with limited instruction tuning. Hence, we conclude
that for a strong domain-specialized generative language model (such as legal),
very large amounts of data are not required to develop models from scratch. We
believe that this work is the first attempt to make a dedicated generative
legal language model from scratch for Indian Supreme Court jurisdiction or in
legal NLP overall. We plan to release our Paramanu-Ayn model at
https://www.bharatgpts.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoleInteract: Evaluating the Social Interaction of Role-Playing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have advanced the development of various AI
conversational agents, including role-playing conversational agents that mimic
diverse characters and human behaviors. While prior research has predominantly
focused on enhancing the conversational capability, role-specific knowledge,
and stylistic attributes of these agents, there has been a noticeable gap in
assessing their social intelligence. In this paper, we introduce RoleInteract,
the first benchmark designed to systematically evaluate the sociality of
role-playing conversational agents at both individual and group levels of
social interactions. The benchmark is constructed from a variety of sources and
covers a wide range of 500 characters and over 6,000 question prompts and
30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations
on this benchmark using mainstream open-source and closed-source LLMs. We find
that agents excelling in individual level does not imply their proficiency in
group level. Moreover, the behavior of individuals may drift as a result of the
influence exerted by other agents within the group. Experimental results on
RoleInteract confirm its significance as a testbed for assessing the social
interaction of role-playing conversational agents. The benchmark is publicly
accessible at https://github.com/X-PLUG/RoleInteract.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Spatial Relations in Text-Only <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gorka Azkune, Ander Salaberria, Eneko Agirre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows that text-only Language Models (LM) can learn to ground
spatial relations like "left of" or "below" if they are provided with explicit
location information of objects and they are properly trained to leverage those
locations. We perform experiments on a verbalized version of the Visual Spatial
Reasoning (VSR) dataset, where images are coupled with textual statements which
contain real or fake spatial relations between two objects of the image. We
verbalize the images using an off-the-shelf object detector, adding location
tokens to every object label to represent their bounding boxes in textual form.
Given the small size of VSR, we do not observe any improvement when using
locations, but pretraining the LM over a synthetic dataset automatically
derived by us improves results significantly when using location tokens. We
thus show that locations allow LMs to ground spatial relations, with our
text-only LMs outperforming Vision-and-Language Models and setting the new
state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs
can generalize beyond the relations seen in the synthetic dataset to some
extent, learning also more useful information than that encoded in the spatial
rules we used to create the synthetic dataset itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Not Worry if You Do Not Have Data: Building <span class="highlight-title">Pretrain</span>ed Language
  Models Using Translationese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meet Doshi, Raj Dabre, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the utility of \textit{Translationese} as synthetic
data created using machine translation for pre-training language models (LMs).
Pre-training requires vast amounts of monolingual data, which is mostly
unavailable for languages other than English. Recently, there has been a
growing interest in using synthetic data to address this data scarcity. We take
the case of English and Indic languages and translate web-crawled monolingual
documents (clean) into the target language. Then, we train language models
containing 28M and 85M parameters on this translationese data (synthetic). We
show that their performance on downstream natural language understanding and
generative tasks is only 3.56\% poorer on NLU tasks and 1.51\% on NLG tasks
than LMs pre-trained on clean data. Further, we propose the use of lightweight
\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently
which significantly improves the performance of our models. We also find that
LMs trained on synthetic data strongly benefit from extended pretraining on a
tiny fraction (10\%) of clean data. We release the data we collected and
created as a part of this work, \textit{IndicMonoDoc}, the largest collection
of monolingual document-level corpora, which we hope will help bridge the gap
between English and non-English performance for large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Llama</span> meets EU: Investigating the European Political Spectrum through
  the Lens of LLMs <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Chalkidis, Stephanie Brandl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-finetuned Large Language Models inherit clear political leanings
that have been shown to influence downstream task performance. We expand this
line of research beyond the two-party system in the US and audit Llama Chat in
the context of EU politics in various settings to analyze the model's political
knowledge and its ability to reason in context. We adapt, i.e., further
fine-tune, Llama Chat on speeches of individual euro-parties from debates in
the European Parliament to reevaluate its political leaning based on the EUandI
questionnaire. Llama Chat shows considerable knowledge of national parties'
positions and is capable of reasoning in context. The adapted, party-specific,
models are substantially re-aligned towards respective positions which we see
as a starting point for using chat-based LLMs as data-driven conversational
engines to assist research in political science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to NAACL 2024 as a short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teacher-Student Training for Debiasing: General Permutation Debiasing
  for Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adian Liusie, Yassir Fathullah, Mark J. F. Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities and versatility in NLP tasks, however they sometimes fail to
maintain crucial invariances for specific tasks. One example is permutation
sensitivity, where LLMs' outputs may significantly vary depending on the order
of the input options. While debiasing techniques can mitigate these issues, and
yield better performance and reliability, they often come with a high
computational cost at inference. This paper addresses this inefficiency at
inference time. The aim is to distill the capabilities of a computationally
intensive, debiased, teacher model into a more compact student model. We
explore two variants of student models: one based on pure distillation, and the
other on an error-correction approach for more complex tasks, where the student
corrects a single biased decision from the teacher to achieve a debiased
output. Our approach is general and can be applied to both black-box and
white-box LLMs. Furthermore, we demonstrate that our compact, encoder-only
student models can outperform their larger, biased teacher counterparts,
achieving better results with significantly fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genetic Auto-<span class="highlight-title">prompt</span> Learning for <span class="highlight-title">Pre-train</span>ed Code Intelligence Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhe Feng, Yanan Sun, Ke Li, Pan Zhou, Jiancheng Lv, Aojun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Pre-trained Language Models (PLMs), a popular approach for code
intelligence, continue to grow in size, the computational cost of their usage
has become prohibitively expensive. Prompt learning, a recent development in
the field of natural language processing, emerges as a potential solution to
address this challenge. In this paper, we investigate the effectiveness of
prompt learning in code intelligence tasks. We unveil its reliance on manually
designed prompts, which often require significant human effort and expertise.
Moreover, we discover existing automatic prompt design methods are very limited
to code intelligence tasks due to factors including gradient dependence, high
computational demands, and limited applicability. To effectively address both
issues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate
genetic algorithm to automatically design prompts. With GenAP, non-experts can
effortlessly generate superior prompts compared to meticulously manual-designed
ones. GenAP operates without the need for gradients or additional computational
costs, rendering it gradient-free and cost-effective. Moreover, GenAP supports
both understanding and generation types of code intelligence tasks, exhibiting
great applicability. We conduct GenAP on three popular code intelligence PLMs
with three canonical code intelligence tasks including defect prediction, code
summarization, and code translation. The results suggest that GenAP can
effectively automate the process of designing prompts. Specifically, GenAP
outperforms all other methods across all three tasks (e.g., improving accuracy
by an average of 2.13% for defect prediction). To the best of our knowledge,
GenAP is the first work to automatically design prompts for code intelligence
PLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CONLINE: Complex Code <span class="highlight-title">Generation</span> and Refinement with Online Searching
  and Correctness Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized code generation ability by
converting natural language descriptions into executable code. However,
generating complex code within real-world scenarios remains challenging due to
intricate structures, subtle bugs, understanding of advanced data types, and
lack of supplementary contents. To address these challenges, we introduce the
CONLINE framework, which enhances code generation by incorporating planned
online searches for information retrieval and automated correctness testing for
iterative refinement. CONLINE also serializes the complex inputs and outputs to
improve comprehension and generate test case to ensure the framework's
adaptability for real-world applications. CONLINE is validated through rigorous
experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE
substantially improves the quality of complex code generation, highlighting its
potential to enhance the practicality and reliability of LLMs in generating
intricate code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for
  Counselor Reflection <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Do June Min, Veronica Perez-Rosas, Kenneth Resnicow, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of multi-reward reinforcement learning to
jointly optimize for multiple text qualities for natural language generation.
We focus on the task of counselor reflection generation, where we optimize the
generators to simultaneously improve the fluency, coherence, and reflection
quality of generated counselor responses. We introduce two novel bandit
methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining
rewards into a single value and optimizing them simultaneously. Specifically,
we employ non-contextual and contextual multi-arm bandits to dynamically adjust
multiple reward weights during training. Through automatic and manual
evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,
outperform existing naive and bandit baselines, showcasing their potential for
enhancing language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eRST: A Signaled Graph Theory of Discourse Relations and Organization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Zeldes, Tatsuya Aoyama, Yang Janet Liu, Siyao Peng, Debopam Das, Luke Gessler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new
theoretical framework for computational discourse analysis, based on an
expansion of Rhetorical Structure Theory (RST). The framework encompasses
discourse relation graphs with tree-breaking, nonprojective and concurrent
relations, as well as implicit and explicit signals which give explainable
rationales to our analyses. We survey shortcomings of RST and other existing
frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn
Discourse Treebank (PDTB) and Discourse Dependencies, and address these using
constructs in the proposed theory. We provide annotation, search and
visualization tools for data, and present and evaluate a freely available
corpus of English annotated according to our framework, encompassing 12 spoken
and written genres with over 200K tokens. Finally, we discuss automatic
parsing, evaluation metrics and applications for data in our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What explains the success of cross-modal fine-tuning with ORCA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma García-de-Herreros, Vagrant Gautam, Philipp Slusallek, Dietrich Klakow, Marius Mosbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,
i.e., applying pre-trained transformer models to modalities beyond their
training data. The technique consists primarily of training an embedder and
fine-tuning the embedder and model. Despite its high performance on a variety
of downstream tasks, we do not understand precisely how each of these
components contribute to ORCA's success. Therefore, we run a series of
ablations and find that embedder training does not help 2D tasks at all,
contrary to what the original paper posits. In 1D tasks, some amount of
embedder training is necessary but more is not better. In 4 out of 6 datasets
we experiment with, it is model fine-tuning that makes the biggest difference.
Through our ablations and baselines, we contribute a better understanding of
the individual components of ORCA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion <span class="highlight-title">Generation</span> from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Gender Interacts with Political Values: A Case Study on Czech <span class="highlight-title">BERT</span>
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Al Ali, Jindřich Libovický
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural language models, which reach state-of-the-art results on most natural
language processing tasks, are trained on large text corpora that inevitably
contain value-burdened content and often capture undesirable biases, which the
models reflect. This case study focuses on the political biases of pre-trained
encoders in Czech and compares them with a representative value survey. Because
Czech is a gendered language, we also measure how the grammatical gender
coincides with responses to men and women in the survey. We introduce a novel
method for measuring the model's perceived political values. We find that the
models do not assign statement probability following value-driven reasoning,
and there is no systematic difference between feminine and masculine sentences.
We conclude that BERT-sized models do not manifest systematic alignment with
political values and that the biases observed in the models are rather due to
superficial imitation of training data patterns than systematic value beliefs
encoded in the models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures; LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What if...?: Counterfactual Inception to Mitigate Hallucination Effects
  in Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Yeon Ju Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a way of enhancing the reliability of Large Multimodal
Models (LMMs) in addressing hallucination effects, where models generate
incorrect or unrelated responses. Without additional instruction tuning
paradigm, we introduce Counterfactual Inception, a novel method that implants
counterfactual thoughts into LMMs using carefully chosen, misaligned
counterfactual keywords. This method is grounded in the concept of
counterfactual thinking, a cognitive process where humans consider alternative
realities and outcomes. By applying this human-like reasoning mechanism to
LMMs, we aim to reduce hallucination effects and improve the models'
trustworthiness. We also propose Dual-modality Verification Process (DVP), a
rigorous framework for selecting optimal counterfactual keywords to trigger
counterfactual thinking into LMMs, concurrently considering visual and
linguistic context. Our extensive experiments across various LMMs, including
both open-source and proprietary models, corroborate that our method
significantly mitigates hallucination phenomena across different datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review, code available:
  https://github.com/IVY-LVLM/Counterfactual-Inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Entropy-based Text Watermarking Detection Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, text watermarking algorithms for large language models (LLMs) can
embed hidden features to texts generated by LLMs to facilitate subsequent
detection, thus alleviating the problem of misuse of LLMs. Although the current
text watermarking algorithms perform well in most high-entropy scenarios, its
performance in low-entropy scenarios still needs to be improved. In this work,
we proposed that the influence of token entropy should be fully considered in
the watermark detection process, that is, the weight of each token should be
adjusted according to its entropy during watermark detection, rather than
setting the weight of all tokens to the same value as in previous methods.
Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives
higher-entropy tokens higher weights during watermark detection, so as to
better reflect the degree of watermarking. Furthermore, the proposed detection
process is training-free and fully automated. %In actual detection, we use a
proxy-LLM to calculate the entropy of each token, without the need to use the
original LLM. In the experiment, we found that our method can achieve better
detection performance in low-entropy scenarios, and our method is also general
and can be applied to texts with different entropy distributions. Our code and
data will be available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, submitted to ARR Feb 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements indicate that scaling up Multimodal Large Language Models
(MLLMs) effectively enhances performance on downstream multimodal tasks. The
prevailing MLLM paradigm, \emph{e.g.}, LLaVA, transforms visual features into
text-like tokens using a \emph{static} vision-language mapper, thereby enabling
\emph{static} LLMs to develop the capability to comprehend visual information
through visual instruction tuning. Although promising, the \emph{static} tuning
strategy~\footnote{The static tuning refers to the trained model with static
parameters.} that shares the same parameters may constrain performance across
different downstream multimodal tasks. In light of this, we introduce
HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,
in conjunction with a dynamic visual expert and language expert, respectively.
These experts are derived from HyperNetworks, which generates adaptive
parameter shifts through visual and language guidance, enabling dynamic
projector and LLM modeling in two-stage training.
  Our experiments demonstrate that our solution significantly surpasses LLaVA
on existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and
LLaVA-Bench. ~\footnote{Our project is available on the link
https://github.com/DCDmllm/HyperLLaVA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Agent Group Chat: An Interactive Group Chat Simulacra For Better
  Eliciting Collective Emergent Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, <span class="highlight-author">Yan Gao</span>, Yao Hu, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To investigate the role of language in human collective behaviors, we
developed the Agent Group Chat simulation to simulate linguistic interactions
among multi-agent in different settings. Agents are asked to free chat in this
simulation for their own purposes based on their character setting, aiming to
see agents exhibit emergent behaviours that are both unforeseen and
significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates,
Philosophical Discourses, Movie Casting Contention, are integrated into Agent
Group Chat to evaluate its support for diverse storylines. By configuring
specific environmental settings within Agent Group Chat, we are able to assess
whether agents exhibit behaviors that align with human expectations. We
evaluate the disorder within the environment by computing the n-gram Shannon
entropy of all the content speak by characters. Our findings reveal that under
the premise of agents possessing substantial alignment with human expectations,
facilitating more extensive information exchange within the simulation ensures
greater orderliness amidst diversity, which leads to the emergence of more
unexpected and meaningful emergent behaviors. The code is open source in
https://github.com/MikeGu721/AgentGroup, and online platform will be open soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Llama</span>Factory: Unified Efficient Fine-Tuning of 100+ <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI LlamaBoard. We empirically validate the efficiency
and effectiveness of our framework on language modeling and text generation
tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and
already received over 13,000 stars and 1,600 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinical information extraction for Low-resource languages with Few-shot
  learning using <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">language models</span> and <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab, Christina Kiriakou, Nicolas Geis, Christoph Dieterich, Anette Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic extraction of medical information from clinical documents poses
several challenges: high costs of required clinical expertise, limited
interpretability of model predictions, restricted computational resources and
privacy regulations. Recent advances in domain-adaptation and prompting methods
showed promising results with minimal training data using lightweight masked
language models, which are suited for well-established interpretability
methods. We are first to present a systematic evaluation of these methods in a
low-resource setting, by performing multi-class section classification on
German doctor's letters. We conduct extensive class-wise evaluations supported
by Shapley values, to validate the quality of our small training data set and
to ensure the interpretability of model predictions. We demonstrate that a
lightweight, domain-adapted pretrained model, prompted with just 20 shots,
outperforms a traditional classification model by 30.5% accuracy. Our results
serve as a process-oriented guideline for clinical information extraction
projects working with low-resource.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Models to Study Language Processing in the Human Brain: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaonan Wang, Jingyuan Sun, Yunhao Zhang, Nan Lin, Marie-Francine Moens, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite differing from the human language processing mechanism in
implementation and algorithms, current language models demonstrate remarkable
human-like or surpassing language capabilities. Should computational language
models be employed in studying the brain, and if so, when and how? To delve
into this topic, this paper reviews efforts in using computational models for
brain research, highlighting emerging trends. To ensure a fair comparison, the
paper evaluates various computational models using consistent metrics on the
same dataset. Our analysis reveals that no single model outperforms others on
all datasets, underscoring the need for rich testing datasets and rigid
experimental control to draw robust conclusions in studies involving
computational models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentivizing News Consumption on Social Media Platforms Using Large
  <span class="highlight-title">Language Models</span> and Realistic Bot Accounts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael Heseltine, Magdalena Wojcieszak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polarization, declining trust, and wavering support for democratic norms are
pressing threats to U.S. democracy. Exposure to verified and quality news may
lower individual susceptibility to these threats and make citizens more
resilient to misinformation, populism, and hyperpartisan rhetoric. This project
examines how to enhance users' exposure to and engagement with verified and
ideologically balanced news in an ecologically valid setting. We rely on a
large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on
28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users
tweeting about sports, entertainment, or lifestyle with a contextual reply
containing two hardcoded elements: a URL to the topic-relevant section of
quality news organization and an encouragement to follow its Twitter account.
To further test differential effects by gender of the bots, treated users were
randomly assigned to receive responses by bots presented as female or male. We
examine whether our over-time intervention enhances the following of news media
organization, the sharing and the liking of news content and the tweeting about
politics and the liking of political content. We find that the treated users
followed more news accounts and the users in the female bot treatment were more
likely to like news content than the control. Most of these results, however,
were small in magnitude and confined to the already politically interested
Twitter users, as indicated by their pre-treatment tweeting about politics.
These findings have implications for social media and news organizations, and
also offer direction for future work on how Large Language Models and other
computational interventions can effectively enhance individual on-platform
engagement with quality news and public affairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USE: Dynamic User Modeling with Stateful Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen Liu, Han Liu, Maarten W. Bos, Ron Dotsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings play a crucial role in user engagement forecasting and
personalized services. Recent advances in sequence modeling have sparked
interest in learning user embeddings from behavioral data. Yet behavior-based
user embedding learning faces the unique challenge of dynamic user modeling. As
users continuously interact with the apps, user embeddings should be
periodically updated to account for users' recent and long-term behavior
patterns. Existing methods highly rely on stateless sequence models that lack
memory of historical behavior. They have to either discard historical data and
use only the most recent data or reprocess the old and new data jointly. Both
cases incur substantial computational overhead. To address this limitation, we
introduce User Stateful Embedding (USE). USE generates user embeddings and
reflects users' evolving behaviors without the need for exhaustive reprocessing
by storing previous model states and revisiting them in the future.
Furthermore, we introduce a novel training objective named future W-behavior
prediction to transcend the limitations of next-token prediction by forecasting
a broader horizon of upcoming user behaviors. By combining it with the Same
User Prediction, a contrastive learning-based objective that predicts whether
different segments of behavior sequences belong to the same user, we further
improve the embeddings' distinctiveness and representativeness. We conducted
experiments on 8 downstream tasks using Snapchat users' behavioral logs in both
static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically
updated user behavior sequences) settings. We demonstrate USE's superior
performance over established baselines. The results underscore USE's
effectiveness and efficiency in integrating historical and recent user behavior
sequences into user embeddings in dynamic user modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyacinth6B: A large language model for Traditional Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Wei Song, Yin-Te Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polaris: A Safety-focused LLM Constellation Architecture for Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Mukherjee, Paul Gamble, Markel Sanz Ausin, Neel Kant, Kriti Aggarwal, Neha Manjunath, Debajyoti Datta, Zhengliang Liu, Jiayuan Ding, Sophia Busacca, Cezanne Bianco, Swapnil Sharma, Rae Lasko, Michelle Voisard, Sanchay Harneja, Darya Filippova, Gerry Meixiong, Kevin Cha, Amir Youssefi, Meyhaa Buvanesh, Howard Weingram, Sebastian Bierman-Lytle, Harpreet Singh Mangat, Kim Parikh, Saad Godil, Alex Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop Polaris, the first safety-focused LLM constellation for real-time
patient-AI healthcare conversations. Unlike prior LLM works in healthcare
focusing on tasks like question answering, our work specifically focuses on
long multi-turn voice conversations. Our one-trillion parameter constellation
system is composed of several multibillion parameter LLMs as co-operative
agents: a stateful primary agent that focuses on driving an engaging
conversation and several specialist support agents focused on healthcare tasks
performed by nurses to increase safety and reduce hallucinations. We develop a
sophisticated training protocol for iterative co-training of the agents that
optimize for diverse objectives. We train our models on proprietary data,
clinical care plans, healthcare regulatory documents, medical manuals, and
other medical reasoning documents. We align our models to speak like medical
professionals, using organic healthcare conversations and simulated ones
between patient actors and experienced nurses. This allows our system to
express unique capabilities such as rapport building, trust building, empathy
and bedside manner. Finally, we present the first comprehensive clinician
evaluation of an LLM system for healthcare. We recruited over 1100 U.S.
licensed nurses and over 130 U.S. licensed physicians to perform end-to-end
conversational evaluations of our system by posing as patients and rating the
system on several measures. We demonstrate Polaris performs on par with human
nurses on aggregate across dimensions such as medical safety, clinical
readiness, conversational quality, and bedside manner. Additionally, we conduct
a challenging task-based evaluation of the individual specialist support
agents, where we demonstrate our LLM agents significantly outperform a much
larger general-purpose LLM (GPT-4) as well as from its own medium-size class
(LLaMA-2 70B).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeanReasoner: Boosting Complex Logical Reasoning with Lean <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwei Jiang, Marcio Fonseca, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with complex logical reasoning
due to logical inconsistencies and the inherent difficulty of such reasoning.
We use Lean, a theorem proving framework, to address these challenges. By
formalizing logical reasoning problems into theorems within Lean, we can solve
them by proving or disproving the corresponding theorems. This method reduces
the risk of logical inconsistencies with the help of Lean's symbolic solver. It
also enhances our ability to treat complex reasoning tasks by using Lean's
extensive library of theorem proofs. Our method achieves state-of-the-art
performance on the FOLIO dataset and achieves performance near this level on
ProofWriter. Notably, these results were accomplished by fine-tuning on fewer
than 100 in-domain samples for each dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reading Users' Minds from What They Say: An Investigation into LLM-based
  Empathic Mental Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Zhu, Leah Chong, Maria Yang, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-centered design, developing a comprehensive and in-depth
understanding of user experiences, i.e., empathic understanding, is paramount
for designing products that truly meet human needs. Nevertheless, accurately
comprehending the real underlying mental states of a large human population
remains a significant challenge today. This difficulty mainly arises from the
trade-off between depth and scale of user experience research: gaining in-depth
insights from a small group of users does not easily scale to a larger
population, and vice versa. This paper investigates the use of Large Language
Models (LLMs) for performing mental inference tasks, specifically inferring
users' underlying goals and fundamental psychological needs (FPNs). Baseline
and benchmark datasets were collected from human users and designers to develop
an empathic accuracy metric for measuring the mental inference performance of
LLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with
varied zero-shot prompt engineering techniques are experimented against that of
human designers. Experimental results suggest that LLMs can infer and
understand the underlying goals and FPNs of users with performance comparable
to that of human designers, suggesting a promising avenue for enhancing the
scalability of empathic design approaches through the integration of advanced
artificial intelligence technologies. This work has the potential to
significantly augment the toolkit available to designers during human-centered
design, enabling the development of both large-scale and in-depth understanding
of users' experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IDETC-CIE2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Community Needs and Assets: A Computational Analysis of Community
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Towhidul Absar Chowdhury, Naveen Sharma, Ashiqur R. KhudaBukhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A community needs assessment is a tool used by non-profits and government
agencies to quantify the strengths and issues of a community, allowing them to
allocate their resources better. Such approaches are transitioning towards
leveraging social media conversations to analyze the needs of communities and
the assets already present within them. However, manual analysis of
exponentially increasing social media conversations is challenging. There is a
gap in the present literature in computationally analyzing how community
members discuss the strengths and needs of the community. To address this gap,
we introduce the task of identifying, extracting, and categorizing community
needs and assets from conversational data using sophisticated natural language
processing methods. To facilitate this task, we introduce the first dataset
about community needs and assets consisting of 3,511 conversations from Reddit,
annotated using crowdsourced workers. Using this dataset, we evaluate an
utterance-level classification model compared to sentiment classification and a
popular large language model (in a zero-shot setting), where we find that our
model outperforms both baselines at an F1 score of 94% compared to 49% and 61%
respectively. Furthermore, we observe through our study that conversations
about needs have negative sentiments and emotions, while conversations about
assets focus on location and entities. The dataset is available at
https://github.com/towhidabsar/CommunityNeeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient
  Fine-Tuning of Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, Peter Anthony Beerel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as
Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each
pre-trained frozen weight tensor, we add a parallel path of trainable low-rank
matrices, namely a down-projection and an up-projection matrix, each of which
is followed by a feature transformation vector. Based on a novel freezing
score, we the incrementally freeze these projection matrices during fine-tuning
to reduce the computation and alleviate over-fitting. Our experimental results
demonstrate that we can achieve state-of-the-art performance with an average
improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up
to $9.5\times$ fewer average trainable parameters. While compared in terms of
runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar
PEFT alternatives. Besides the practical utility of our approach, we provide
insights on the trainability requirements of LoRA paths at different modules
and the freezing schedule for the different projection matrices. Code will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arcee's MergeKit: A Toolkit for Merging Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of the open-source language model landscape presents an
opportunity to merge the competencies of these model checkpoints by combining
their parameters. Advances in transfer learning, the process of fine-tuning
pre-trained models for specific tasks, has resulted in the development of vast
amounts of task-specific models, typically specialized in individual tasks and
unable to utilize each other's strengths. Model merging facilitates the
creation of multitask models without the need for additional training, offering
a promising avenue for enhancing model performance and versatility. By
preserving the intrinsic capabilities of the original models, model merging
addresses complex challenges in AI - including the difficulties of catastrophic
forgetting and multi-task learning. To support this expanding area of research,
we introduce MergeKit, a comprehensive, open-source library designed to
facilitate the application of model merging strategies. MergeKit offers an
extensible framework to efficiently merge models on any hardware, providing
utility to researchers and practitioners. To date, thousands of models have
been merged by the open-source community, leading to the creation of some of
the worlds most powerful open-source model checkpoints, as assessed by the Open
LLM Leaderboard. The library is accessible at
https://github.com/arcee-ai/MergeKit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Author Classification Using Parsed Language Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd K Moon, Jacob H. Gunther
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years there has been ongoing interest in detecting authorship of a
text based on statistical properties of the text, such as by using occurrence
rates of noncontextual words. In previous work, these techniques have been
used, for example, to determine authorship of all of \emph{The Federalist
Papers}. Such methods may be useful in more modern times to detect fake or AI
authorship. Progress in statistical natural language parsers introduces the
possibility of using grammatical structure to detect authorship. In this paper
we explore a new possibility for detecting authorship using grammatical
structural information extracted using a statistical natural language parser.
This paper provides a proof of concept, testing author classification based on
grammatical structure on a set of "proof texts," The Federalist Papers and
Sanditon which have been as test cases in previous authorship detection
studies. Several features extracted from the statistical natural language
parser were explored: all subtrees of some depth from any level; rooted
subtrees of some depth, part of speech, and part of speech by level in the
parse tree. It was found to be helpful to project the features into a lower
dimensional space. Statistical experiments on these documents demonstrate that
information from a statistical parser can, in fact, assist in distinguishing
authors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facilitating Pornographic Text Detection for Open-Domain Dialogue
  Systems via Knowledge Distillation of Large <span class="highlight-title">Language Models</span> <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pornographic content occurring in human-machine interaction dialogues can
cause severe side effects for users in open-domain dialogue systems. However,
research on detecting pornographic language within human-machine interaction
dialogues is an important subject that is rarely studied. To advance in this
direction, we introduce CensorChat, a dialogue monitoring dataset aimed at
detecting whether the dialogue session contains pornographic content. To this
end, we collect real-life human-machine interaction dialogues in the wild and
break them down into single utterances and single-turn dialogues, with the last
utterance spoken by the chatbot. We propose utilizing knowledge distillation of
large language models to annotate the dataset. Specifically, first, the raw
dataset is annotated by four open-source large language models, with the
majority vote determining the label. Second, we use ChatGPT to update the empty
label from the first step. Third, to ensure the quality of the validation and
test sets, we utilize GPT-4 for label calibration. If the current label does
not match the one generated by GPT-4, we employ a self-criticism strategy to
verify its correctness. Finally, to facilitate the detection of pornographic
text, we develop a series of text classifiers using a pseudo-labeled dataset.
Detailed data analysis demonstrates that leveraging knowledge distillation
techniques with large language models provides a practical and cost-efficient
method for developing pornographic text detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CSCWD 2024 (27th International Conference on Computer
  Supported Cooperative Work in Design). arXiv admin note: text overlap with
  arXiv:2309.09749</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Instruct</span>ion Multi-Constraint Molecular <span class="highlight-title">Generation</span> Using a
  Teacher-Student Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Siqi Sun, Jianxin Lin, Longyue Wang, Xiangxiang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While various models and computational tools have been proposed for structure
and property analysis of molecules, generating molecules that conform to all
desired structures and properties remains a challenge. Here, we introduce a
multi-constraint molecular generation large language model, TSMMG, which, akin
to a student, incorporates knowledge from various small models and tools,
namely, the 'teachers'. To train TSMMG, we construct a large set of
text-molecule pairs by extracting molecular knowledge from these 'teachers',
enabling it to generate novel molecules that conform to the descriptions
through various text prompts. We experimentally show that TSMMG remarkably
performs in generating molecules meeting complex, natural language-described
property requirements across two-, three-, and four-constraint tasks, with an
average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and
61.44%, respectively. The model also exhibits adaptability through zero-shot
testing, creating molecules that satisfy combinations of properties that have
not been encountered. It can comprehend text inputs with various language
styles, extending beyond the confines of outlined prompts, as confirmed through
empirical validation. Additionally, the knowledge distillation feature of TSMMG
contributes to the continuous enhancement of small models, while the innovative
approach to dataset construction effectively addresses the issues of data
scarcity and quality, which positions TSMMG as a promising tool in the domains
of drug discovery and materials science. Code is available at
https://github.com/HHW-zhou/TSMMG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual
  Summarization <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual summarization (XLS) generates summaries in a language different
from that of the input documents (e.g., English to Spanish), allowing speakers
of the target language to gain a concise view of their content. In the present
day, the predominant approach to this task is to take a performing, pretrained
multilingual language model (LM) and fine-tune it for XLS on the language pairs
of interest. However, the scarcity of fine-tuning samples makes this approach
challenging in some cases. For this reason, in this paper we propose revisiting
the summarize-and-translate pipeline, where the summarization and translation
tasks are performed in a sequence. This approach allows reusing the many,
publicly-available resources for monolingual summarization and translation,
obtaining a very competitive zero-shot performance. In addition, the proposed
pipeline is completely differentiable end-to-end, allowing it to take advantage
of few-shot fine-tuning, where available. Experiments over two contemporary and
widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable
zero-shot performance of the proposed approach, and also its strong few-shot
performance compared to an equivalent multilingual LM baseline, that the
proposed approach has been able to outperform in many languages with only 10%
of the fine-tuning samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical Report: Competition Solution For BetterMixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaijiang Zhao, Xiaoquan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of flourishing large-scale models, the challenge of selecting and
optimizing datasets from the vast and complex sea of data, to enhance the
performance of large language models within the constraints of limited
computational resources, has become paramount. This paper details our solution
for the BetterMixture challenge, which focuses on the fine-tuning data mixing
for large language models. Our approach, which secured third place,
incorporates data deduplication, low-level and high-level quality filtering,
and diversity selection. The foundation of our solution is Ke-Data-Juicer, an
extension of Data-Juicer, demonstrating its robust capabilities in handling and
optimizing data for large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Representational Harms to Quality-of-Service Harms: A Case Study on
  <span class="highlight-title">Llama</span> 2 Safety Safeguards <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf Taïk, Jackie CK Cheung, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) has led to their widespread
adoption in various domains. However, these advancements have also introduced
additional safety risks and raised concerns regarding their detrimental impact
on already marginalized populations. Despite growing mitigation efforts to
develop safety safeguards, such as supervised safety-oriented fine-tuning and
leveraging safe reinforcement learning from human feedback, multiple concerns
regarding the safety and ingrained biases in these models remain. Furthermore,
previous work has demonstrated that models optimized for safety often display
exaggerated safety behaviors, such as a tendency to refrain from responding to
certain requests as a precautionary measure. As such, a clear trade-off between
the helpfulness and safety of these models has been documented in the
literature. In this paper, we further investigate the effectiveness of safety
measures by evaluating models on already mitigated biases. Using the case of
Llama 2 as an example, we illustrate how LLMs' safety responses can still
encode harmful assumptions. To do so, we create a set of non-toxic prompts,
which we then use to evaluate Llama models. Through our new taxonomy of LLMs
responses to users, we observe that the safety/helpfulness trade-offs are more
pronounced for certain demographic groups which can lead to quality-of-service
harms for marginalized populations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, submitted to the 62nd Annual Meeting of the
  Association for Computational Linguistics (ACL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ax-to-Grind Urdu: Benchmark <span class="highlight-title">Dataset</span> for Urdu Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheetal Harris, Jinshuo Liu, Hassan Jalil Hadi, Yue Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation can seriously impact society, affecting anything from public
opinion to institutional confidence and the political horizon of a state. Fake
News (FN) proliferation on online websites and Online Social Networks (OSNs)
has increased profusely. Various fact-checking websites include news in English
and barely provide information about FN in regional languages. Thus the Urdu FN
purveyors cannot be discerned using factchecking portals. SOTA approaches for
Fake News Detection (FND) count upon appropriately labelled and large datasets.
FND in regional and resource-constrained languages lags due to the lack of
limited-sized datasets and legitimate lexical resources. The previous datasets
for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not
manually verified where the news is translated from English into Urdu. In this
paper, we curate and contribute the first largest publicly available dataset
for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations
of existing Urdu datasets in the literature. It constitutes 10,083 fake and
real news on fifteen domains collected from leading and authentic Urdu
newspapers and news channel websites in Pakistan and India. FN for the
Ax-to-Grind dataset is collected from websites and crowdsourcing. The dataset
contains news items in Urdu from the year 2017 to the year 2023. Expert
journalists annotated the dataset. We benchmark the dataset with an ensemble
model of mBERT,XLNet, and XLM RoBERTa. The selected models are originally
trained on multilingual large corpora. The results of the proposed model are
based on performance metrics, F1-score, accuracy, precision, recall and MCC
value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Massive Multilingual <span class="highlight-title">Dataset</span> for High-Performance Language
  Technologies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramírez-Sánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, Jörg Tiedemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the HPLT (High Performance Language Technologies) language
resources, a new massive multilingual dataset including both monolingual and
bilingual corpora extracted from CommonCrawl and previously unused web crawls
from the Internet Archive. We describe our methods for data acquisition,
management and processing of large corpora, which rely on open-source software
tools and high-performance computing. Our monolingual collection focuses on
low- to medium-resourced languages and covers 75 languages and a total of ~5.6
trillion word tokens de-duplicated on the document level. Our English-centric
parallel corpus is derived from its monolingual counterpart and covers 18
language pairs and more than 96 million aligned sentence pairs with roughly 1.4
billion English tokens. The HPLT language resources are one of the largest open
text corpora ever released, providing a great resource for language modeling
and machine translation training. We publicly release the corpora, the
software, and the tools used in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On <span class="highlight-title">Prompt</span> Sensitivity of <span class="highlight-title">ChatGPT</span> in Affective Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa M. Amin, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the emerging capabilities of foundation
models like ChatGPT in several fields, including affective computing. However,
accessing these emerging capabilities is facilitated through prompt
engineering. Despite the existence of some prompting techniques, the field is
still rapidly evolving and many prompting ideas still require investigation. In
this work, we introduce a method to evaluate and investigate the sensitivity of
the performance of foundation models based on different prompts or generation
parameters. We perform our evaluation on ChatGPT within the scope of affective
computing on three major problems, namely sentiment analysis, toxicity
detection, and sarcasm detection. First, we carry out a sensitivity analysis on
pivotal parameters in auto-regressive text generation, specifically the
temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling,
dictating how conservative or creative the model should be during generation.
Furthermore, we explore the efficacy of several prompting ideas, where we
explore how giving different incentives or structures affect the performance.
Our evaluation takes into consideration performance measures on the affective
computing tasks, and the effectiveness of the model to follow the stated
instructions, hence generating easy-to-parse responses to be smoothly used in
downstream applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Tables, 1 Figure, preprint submission to ACII 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Unsupervised Dimensionality Reduction Methods for <span class="highlight-title">Pretrain</span>ed
  Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaifan Zhang, Yi Zhou, Danushka Bollegala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence embeddings produced by Pretrained Language Models (PLMs) have
received wide attention from the NLP community due to their superior
performance when representing texts in numerous downstream applications.
However, the high dimensionality of the sentence embeddings produced by PLMs is
problematic when representing large numbers of sentences in memory- or
compute-constrained devices. As a solution, we evaluate unsupervised
dimensionality reduction methods to reduce the dimensionality of sentence
embeddings produced by PLMs. Our experimental results show that simple methods
such as Principal Component Analysis (PCA) can reduce the dimensionality of
sentence embeddings by almost $50\%$, without incurring a significant loss in
performance in multiple downstream tasks. Surprisingly, reducing the
dimensionality further improves performance over the original high-dimensional
versions for the sentence embeddings produced by some PLMs in some tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Large Language Model Bias with Emphasis on 'Restricted
  Industries': Automated <span class="highlight-title">Dataset</span> Augmentation and Prejudice Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devam Mondal, Carlo Lipizzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the growing capabilities of large language models, there exists
concerns about the biases they develop. In this paper, we propose a novel,
automated mechanism for debiasing through specified dataset augmentation in the
lens of bias producers and in the context of 'restricted industries' with
limited data. We additionally create two new additional metrics, the mb-index
and db-index, to quantify bias, considering the idea that bias occurs due to
both intrinsic model architecture and dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Grounded Speech Models have a Mutual Exclusivity Bias <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leanne Nortje, Dan Oneaţă, Yevgen Matusevych, Herman Kamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When children learn new words, they employ constraints such as the mutual
exclusivity (ME) bias: a novel word is mapped to a novel object rather than a
familiar one. This bias has been studied computationally, but only in models
that use discrete word representations as input, ignoring the high variability
of spoken words. We investigate the ME bias in the context of visually grounded
speech models that learn from natural images and continuous speech audio.
Concretely, we train a model on familiar words and test its ME bias by asking
it to select between a novel and a familiar object when queried with a novel
word. To simulate prior acoustic and visual knowledge, we experiment with
several initialisation strategies using pretrained speech and vision networks.
Our findings reveal the ME bias across the different initialisation approaches,
with a stronger bias in models with more prior (in particular, visual)
knowledge. Additional tests confirm the robustness of our results, even when
different loss functions are considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL, pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Linguistically Enhanced Embeddings for Open Information
  Extraction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fauzan Farooqui, Thanmay Jayakumar, Pulkit Mathur, Mansi Radke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Information Extraction (OIE) is a structured prediction (SP) task in
Natural Language Processing (NLP) that aims to extract structured $n$-ary
tuples - usually subject-relation-object triples - from free text. The word
embeddings in the input text can be enhanced with linguistic features, usually
Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However,
past enhancement techniques cannot leverage the power of pretrained language
models (PLMs), which themselves have been hardly used for OIE. To bridge this
gap, we are the first to leverage linguistic features with a Seq2Seq PLM for
OIE. We do so by introducing two methods - Weighted Addition and Linearized
Concatenation. Our work can give any neural OIE architecture the key
performance boost from both PLMs and linguistic features in one go. In our
settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on
Precision, Recall and F1 scores respectively over the baseline. Beyond this, we
address other important challenges in the field: to reduce compute overheads
with the features, we are the first ones to exploit Semantic Dependency Parse
(SemDP) tags; to address flaws in current datasets, we create a clean synthetic
dataset; finally, we contribute the first known study of OIE behaviour in SP
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024 Main Conference, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train & Constrain: Phonologically Informed Tongue-Twister <span class="highlight-title">Generation</span>
  from Topics and Paraphrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Chen Tang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work in phonologically and phonetically grounded language generation
has mainly focused on domains such as puns and poetry. In this article, we
present new work on the generation of tongue-twisters - a form of language that
is required to be conditioned on a phoneme level to maximize sound overlap,
whilst maintaining semantic consistency with an input topic and still being
grammatically correct. We present TwisterLister, a pipeline for generating
phonologically informed tongue-twisters from Large Language Models (LLMs) that
we use to generate TwistList 2.0, the largest annotated dataset of
tongue-twisters to date, consisting of 17K+ examples from a combination of
human and LLM authors. Our generation pipeline involves the use of a
phonologically constrained vocabulary alongside LLM prompting to generate
novel, non-derivative tongue-twister examples. We additionally present the
results of automatic and human evaluation of smaller models trained on our
generated dataset to demonstrate the extent to which phonologically motivated
language types can be generated without explicit injection of phonological
knowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding
module (PACD) that can be integrated into any causal language model and
demonstrate that this method generates good quality tongue-twisters both with
and without fine-tuning the underlying language model. We also design and
implement a range of automatic metrics for the task of tongue-twister
generation that is phonologically motivated and captures the unique essence of
tongue-twisters based on Phonemic Edit Distance (PED).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Hallucination Control by Visual Information Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Vision-Language Models (VLMs) are prone to generate
plausible-sounding textual answers that, however, are not always grounded in
the input image. We investigate this phenomenon, usually referred to as
"hallucination" and show that it stems from an excessive reliance on the
language prior. In particular, we show that as more tokens are generated, the
reliance on the visual prompt decreases, and this behavior strongly correlates
with the emergence of hallucinations. To reduce hallucinations, we introduce
Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for
prompt amplification. M3ID amplifies the influence of the reference image over
the language prior, hence favoring the generation of tokens with higher mutual
information with the visual prompt. M3ID can be applied to any pre-trained
autoregressive VLM at inference time without necessitating further training and
with minimal computational overhead. If training is an option, we show that
M3ID can be paired with Direct Preference Optimization (DPO) to improve the
model's reliance on the prompt image without requiring any labels. Our
empirical findings show that our algorithms maintain the fluency and linguistic
capabilities of pre-trained VLMs while reducing hallucinations by mitigating
visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and
M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by
25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as
POPE by 21% and 24%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Expressive Power of <span class="highlight-title">Transformer</span>s with Chain of Thought <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07923v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07923v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Ashish Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a "chain of
thought" or "scratchpad", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps, assuming a
slight generalization to standard pre-norm, adds a clear new ability (under
standard complexity conjectures): recognizing all regular languages. Our
results also imply that linear steps keep transformer decoders within
context-sensitive languages, and polynomial steps with generalized pre-norm
make them recognize exactly the class of polynomial-time solvable problems --
the first exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9-page preprint. Updated March 20 after ICLR acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world multi-modal problems are rarely solved by a single machine
learning model, and often require multi-step computational plans that involve
stitching several models. Tool-augmented LLMs hold tremendous promise for
automating the generation of such computational plans. However, the lack of
standardized benchmarks for evaluating LLMs as planners for multi-step
multi-modal tasks has prevented a systematic study of planner design decisions.
Should LLMs generate a full plan in a single shot or step-by-step? Should they
invoke tools directly with Python code or through structured data formats like
JSON? Does feedback improve planning? To answer these questions and more, we
introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks
involving 33 tools that include multi-modal models, (free) public APIs, and
image processing modules. For each of these task queries, we provide
automatically generated plans using this realistic toolset. We further provide
a high-quality subset of 1,565 task plans that are human-verified and correctly
executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies
(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3
types of feedback (parsing/verification/execution). Finally, we summarize
takeaways from our extensive experiments. Our dataset and code are available on
HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github
(https://github.com/RAIVNLab/mnms).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Having Beer after Prayer? Measuring Cultural Bias in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14456v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14456v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the reach of large language models (LMs) expands globally, their ability
to cater to diverse cultural contexts becomes crucial. Despite advancements in
multilingual capabilities, models are not designed with appropriate cultural
nuances. In this paper, we show that multilingual and Arabic monolingual LMs
exhibit bias towards entities associated with Western culture. We introduce
CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities
spanning eight types that contrast Arab and Western cultures. CAMeL provides a
foundation for measuring cultural biases in LMs through both extrinsic and
intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance
in Arabic of 16 different LMs on tasks such as story generation, NER, and
sentiment analysis, where we find concerning cases of stereotyping and cultural
unfairness. We further test their text-infilling performance, revealing the
incapability of appropriate adaptation to Arab cultural contexts. Finally, we
analyze 6 Arabic pre-training corpora and find that commonly used sources such
as Wikipedia may not be best suited to build culturally aware LMs, if used as
they are without adjustment. We will make CAMeL publicly available at:
https://github.com/tareknaous/camel
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAE-RAE Bench: Evaluation of Korean Knowledge in <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02706v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02706v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, Songseong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained on massive corpora demonstrate
impressive capabilities in a wide range of tasks. While there are ongoing
efforts to adapt these models to languages beyond English, the attention given
to their evaluation methodologies remains limited. Current multilingual
benchmarks often rely on back translations or re-implementations of English
tests, limiting their capacity to capture unique cultural and linguistic
nuances. To bridge this gap for the Korean language, we introduce the HAE-RAE
Bench, a dataset curated to challenge models lacking Korean cultural and
contextual depth. The dataset encompasses six downstream tasks across four
domains: vocabulary, history, general knowledge, and reading comprehension.
Unlike traditional evaluation suites focused on token and sequence
classification or mathematical and logical reasoning, the HAE-RAE Bench
emphasizes a model's aptitude for recalling Korean-specific knowledge and
cultural contexts. Comparative analysis with prior Korean benchmarks indicates
that the HAE-RAE Bench presents a greater challenge to non-Korean models by
disturbing abilities and knowledge learned from English being transferred.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Phrase Representation by Information Bottleneck Guided Text
  Diffusion Process for Keyphrase Extraction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhen Luo, Qingyu Zhou, Feng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyphrase extraction (KPE) is an important task in Natural Language
Processing for many scenarios, which aims to extract keyphrases that are
present in a given document. Many existing supervised methods treat KPE as
sequential labeling, span-level classification, or generative tasks. However,
these methods lack the ability to utilize keyphrase information, which may
result in biased results. In this study, we propose Diff-KPE, which leverages
the supervised Variational Information Bottleneck (VIB) to guide the text
diffusion process for generating enhanced keyphrase representations. Diff-KPE
first generates the desired keyphrase embeddings conditioned on the entire
document and then injects the generated keyphrase embeddings into each phrase
representation. A ranking network and VIB are then optimized together with rank
loss and classification loss, respectively. This design of Diff-KPE allows us
to rank each candidate phrase by utilizing both the information of keyphrases
and the document. Experiments show that Diff-KPE outperforms existing KPE
methods on a large open domain keyphrase extraction benchmark, OpenKP, and a
scientific domain dataset, KP20K.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Automatically Mixing <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay,  Mausam, Manaal Faruqui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost
and performance remains challenging. In this work, we present AutoMix, an
approach that strategically routes queries to larger LMs, based on the
approximate correctness of outputs from a smaller LM. Central to AutoMix is a
few-shot self-verification mechanism, which estimates the reliability of its
own outputs without requiring training. Given that verifications can be noisy,
we employ a meta-verifier in AutoMix to refine the accuracy of these
assessments. Our experiments using LLAMA2-13B and GPT-4, on five
context-grounded reasoning datasets demonstrate that AutoMix surpasses
established baselines, improving the incremental benefit per cost by up to 86%.
Our code and data are available at https://github.com/automix-llm/automix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMICL: Empowering Vision-language Model with Multi-Modal In-Context
  Learning <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the resurgence of deep learning, vision-language models (VLMs) enhanced
by large language models (LLMs) have grown exponentially in popularity.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images, making VLMs
less effective in downstream vision-language tasks. In this paper, we address
the limitation above by 1) introducing vision-language Model with Multi-Modal
In-Context Learning(MMICL), a new approach to allow the VLM to deal with
multi-modal inputs efficiently; 2) proposing a novel context scheme to augment
the in-context learning ability of the VLM; 3) constructing the Multi-modal
In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to
understand complex multi-modal prompts. Our experiments confirm that MMICL
achieves new state-of-the-art zero-shot performance on a wide range of general
vision-language tasks, especially for complex benchmarks, including MME and
MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge
of complex multi-modal prompt understanding and emerges the impressive ICL
ability. Furthermore, we observe that MMICL successfully alleviates language
bias in VLMs, a common issue for VLMs that often leads to hallucination when
faced with extensive textual context. Our code, dataset, dataset tool, and
model are available at https://github.com/PKUnlp-icler/MIC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correct Like Humans: Progressive Learning Framework for Chinese Text
  Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Shirong Ma, Shaoshen Chen, Haojing Huang, Shulin Huang, Yangning Li, Hai-Tao Zheng, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Text Error Correction (CTEC) aims to detect and correct errors in the
input text, which benefits human daily life and various downstream tasks.
Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve
CTEC. Although PLMs have achieved remarkable success in CTEC, we argue that
previous studies still overlook the importance of human thinking patterns. To
enhance the development of PLMs for CTEC, inspired by humans' daily
error-correcting behavior, we propose a novel model-agnostic progressive
learning framework, named ProTEC, which guides PLMs-based CTEC models to learn
to correct like humans. During the training process, ProTEC guides the model to
learn text error correction by incorporating these sub-tasks into a progressive
paradigm. During the inference process, the model completes these sub-tasks in
turn to generate the correction results. Extensive experiments and detailed
analyses demonstrate the effectiveness and efficiency of our proposed
model-agnostic ProTEC framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weight-Inherited Distillation for Task-Agnostic <span class="highlight-title">BERT</span> Compression <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, NAACL2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09002v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09002v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our research, we pioneer a novel approach to evaluate the effectiveness of
jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,
diverging from traditional robustness-focused binary evaluations. Our study
introduces two distinct evaluation frameworks: a coarse-grained evaluation and
a fine-grained evaluation. Each framework, using a scoring range from 0 to 1,
offers a unique perspective, enabling a more comprehensive and nuanced
evaluation of attack effectiveness and empowering attackers to refine their
attack prompts with greater understanding. Furthermore, we have developed a
comprehensive ground truth dataset specifically tailored for jailbreak tasks.
This dataset not only serves as a crucial benchmark for our current study but
also establishes a foundational resource for future research, enabling
consistent and comparative analyses in this evolving field. Upon meticulous
comparison with traditional evaluation methods, we discovered that our
evaluation aligns with the baseline's trend while offering a more profound and
detailed assessment. We believe that by accurately evaluating the effectiveness
of attack prompts in the Jailbreak task, our work lays a solid foundation for
assessing a wider array of similar or even more complex tasks in the realm of
prompt injection, potentially revolutionizing this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Walia-LLM: Enhancing Amharic-<span class="highlight-title">LLaMA</span> by Integrating Task-Specific and
  Generative <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Mitiku Yohannes Fuge, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have received a lot of attention in natural
language processing (NLP) research because of their exceptional performance in
understanding and generating human languages. However, low-resource languages
are left behind due to the unavailability of resources. In this work, we focus
on enhancing the LLaMA-2-Amharic model by integrating task-specific and
generative datasets to improve language model performance for Amharic. We
compile an Amharic instruction fine-tuning dataset and fine-tuned
LLaMA-2-Amharic model. The fine-tuned model shows promising results in
different NLP tasks. We open-source our dataset creation pipeline, instruction
datasets, trained models, and evaluation outputs to promote language-specific
studies on these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Language Models</span> Know When They're Hallucinating References? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) are notoriously susceptible to
generating hallucinated information. Such inaccurate outputs not only undermine
the reliability of these models but also limit their use and raise serious
concerns about misinformation and propaganda. In this work, we focus on
hallucinated book and article references and present them as the "model
organism" of language model hallucination research, due to their frequent and
easy-to-discern nature. We posit that if a language model cites a particular
reference in its output, then it should ideally possess sufficient information
about its authors and content, among other relevant details. Using this basic
insight, we illustrate that one can identify hallucinated references without
ever consulting any external resources, by asking a set of direct or indirect
queries to the language model about the references. These queries can be
considered as "consistency checks." Our findings highlight that while LMs,
including GPT-4, often produce inconsistent author lists for hallucinated
references, they also often accurately recall the authors of real references.
In this sense, the LM can be said to "know" when it is hallucinating
references. Furthermore, these findings show how hallucinated references can be
dissected to shed light on their nature. Replication code and results can be
found at https://github.com/microsoft/hallucinated-references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Piecing Together Clues: A Benchmark for Evaluating the Detective Skills
  of Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Lin Zhang, Jiangjie Chen, Haoning Ye, Xiaoxuan Zhu, Zihan Li, Zheyu Ye, <span class="highlight-author">Yan Gao</span>, Yao Hu, Yanghua Xiao, Hongwei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detectives frequently engage in information detection and reasoning
simultaneously when making decisions across various cases, especially when
confronted with a vast amount of information. With the rapid development of
large language models~(LLMs), evaluating how these models identify key
information and reason to solve questions becomes increasingly relevant. We
introduces the DetectBench, a reading comprehension dataset designed to assess
a model's ability to jointly ability in key information detection and multi-hop
reasoning when facing complex and implicit information. The DetectBench
comprises 3,928 questions, each paired with a paragraph averaging 190 tokens in
length. To enhance model's detective skills, we propose the Detective Thinking
Framework. These methods encourage models to identify all possible clues within
the context before reasoning. Our experiments reveal that existing models
perform poorly in both information detection and multi-hop reasoning. However,
the Detective Thinking Framework approach alleviates this issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionIC: emotional inertia and contagion-driven dependency modeling
  for emotion recognition in conversation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11117v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11117v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjian Liu, Jiang Li, Xiaoping Wang, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation (ERC) has attracted growing attention in
recent years as a result of the advancement and implementation of
human-computer interface technologies. In this paper, we propose an emotional
inertia and contagion-driven dependency modeling approach (EmotionIC) for ERC
task. Our EmotionIC consists of three main components, i.e., Identity Masked
Multi-Head Attention (IMMHA), Dialogue-based Gated Recurrent Unit (DiaGRU), and
Skip-chain Conditional Random Field (SkipCRF). Compared to previous ERC models,
EmotionIC can model a conversation more thoroughly at both the
feature-extraction and classification levels. The proposed model attempts to
integrate the advantages of attention- and recurrence-based methods at the
feature-extraction level. Specifically, IMMHA is applied to capture
identity-based global contextual dependencies, while DiaGRU is utilized to
extract speaker- and temporal-aware local contextual information. At the
classification level, SkipCRF can explicitly mine complex emotional flows from
higher-order neighboring utterances in the conversation. Experimental results
show that our method can significantly outperform the state-of-the-art models
on four benchmark datasets. The ablation studies confirm that our modules can
effectively model emotional inertia and contagion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SCIENCE CHINA Information Sciences (SCIS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General-Purpose Retrieval-Enhanced Medical Prediction Model Using
  Near-Infinite History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20204v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20204v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-Gil Jeong, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing clinical prediction models (e.g., mortality prediction) based on
electronic health records (EHRs) typically relies on expert opinion for feature
selection and adjusting observation window size. This burdens experts and
creates a bottleneck in the development process. We propose Retrieval-Enhanced
Medical prediction model (REMed) to address such challenges. REMed can
essentially evaluate an unlimited number of clinical events, select the
relevant ones, and make predictions. This approach effectively eliminates the
need for manual feature selection and enables an unrestricted observation
window. We verified these properties through experiments on 27 clinical tasks
and two independent cohorts from publicly available EHR datasets, where REMed
outperformed other contemporary architectures that aim to handle as many events
as possible. Notably, we found that the preferences of REMed align closely with
those of medical experts. We expect our approach to significantly expedite the
development of EHR prediction models by minimizing clinicians' need for manual
involvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source codes corresponding to this paper are available at:
  https://github.com/starmpcc/REMed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The ParlaSent Multilingual Training <span class="highlight-title">Dataset</span> for Sentiment Identification
  in Parliamentary Proceedings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Mochtak, Peter Rupnik, Nikola Ljubešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a new training dataset of sentences in 7 languages,
manually annotated for sentiment, which are used in a series of experiments
focused on training a robust sentiment identifier for parliamentary
proceedings. The paper additionally introduces the first domain-specific
multilingual transformer language model for political science applications,
which was additionally pre-trained on 1.72 billion words from parliamentary
proceedings of 27 European parliaments. We present experiments demonstrating
how the additional pre-training on parliamentary data can significantly improve
the model downstream performance, in our case, sentiment identification in
parliamentary proceedings. We further show that our multilingual model performs
very well on languages not seen during fine-tuning, and that additional
fine-tuning data from other languages significantly improves the target
parliament's results. The paper makes an important contribution to multiple
disciplines inside the social sciences, and bridges them with computer science
and computational linguistics. Lastly, the resulting fine-tuned language model
sets up a more robust approach to sentiment analysis of political texts across
languages, which allows scholars to study political sentiment from a
comparative perspective using standardized tools and techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Noise: Toward a Unified Multi-modal Knowledge Graph
  Representation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multi-modal Pre-training highlights the necessity for a
robust Multi-Modal Knowledge Graph (MMKG) representation learning framework.
This framework is crucial for integrating structured knowledge into multi-modal
Large Language Models (LLMs) at scale, aiming to alleviate issues like
knowledge misconceptions and multi-modal hallucinations. In this work, to
evaluate models' ability to accurately embed entities within MMKGs, we focus on
two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and
Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a
novel SNAG method that utilizes a Transformer-based architecture equipped with
modality-level noise masking for the robust integration of multi-modal entity
features in KGs. By incorporating specific training objectives for both MKGC
and MMEA, our approach achieves SOTA performance across a total of ten datasets
(three for MKGC and seven for MEMA), demonstrating its robustness and
versatility. Besides, SNAG can not only function as a standalone model but also
enhance other existing methods, providing stable performance improvements. Our
code and data are available at: https://github.com/zjukg/SNAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at
  https://github.com/zjukg/SNAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of Truth: An Interrogation Approach to Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the many advances of Large Language Models (LLMs) and their
unprecedented rapid evolution, their impact and integration into every facet of
our daily lives is limited due to various reasons. One critical factor
hindering their widespread adoption is the occurrence of hallucinations, where
LLMs invent answers that sound realistic, yet drift away from factual truth. In
this paper, we present a novel method for detecting hallucinations in large
language models, which tackles a critical issue in the adoption of these models
in various real-world scenarios. Through extensive evaluations across multiple
datasets and LLMs, including Llama-2, we study the hallucination levels of
various recent LLMs and demonstrate the effectiveness of our method to
automatically detect them. Notably, we observe up to 62% hallucinations for
Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy
(B-ACC) of 87%, all without relying on external knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Over<span class="highlight-title">generation</span> Mistakes <span class="chip">SemEval 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the results of the SHROOM, a shared task focused on
detecting hallucinations: outputs from natural language generation (NLG)
systems that are fluent, yet inaccurate. Such cases of overgeneration put in
jeopardy many NLG applications, where correctness is often mission-critical.
The shared task was conducted with a newly constructed dataset of 4000 model
outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine
translation, paraphrase generation and definition modeling.
  The shared task was tackled by a total of 58 different users grouped in 42
teams, out of which 27 elected to write a system description paper;
collectively, they submitted over 300 prediction sets on both tracks of the
shared task. We observe a number of key trends in how this approach was tackled
-- many participants rely on a handful of model, and often rely either on
synthetic data for fine-tuning or zero-shot prompting strategies. While a
majority of the teams did outperform our proposed baseline system, the
performances of top-scoring systems are still consistent with a random handling
of the more challenging items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SemEval 2024 shared task. Pre-review version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayes<span class="highlight-title">Prompt</span>: <span class="highlight-title">Prompt</span>ing Large-Scale <span class="highlight-title">Pre-Train</span>ed <span class="highlight-title">Language Models</span> on
  Few-shot Inference via Debiased Domain Abstraction <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng, Fuchun Sun, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaD: Program-aided Distillation Can Teach Small Models Reasoning Better
  than Chain-of-thought Fine-tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) excel in various natural language
processing tasks, their huge size and the inaccessibility of parameters present
challenges for practical deployment. Previous studies try to distill
task-specific ability from LLMs to smaller models, using data synthesis and
chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains
faulty reasoning, which deteriorates the quality of distillation, especially in
reasoning capabilities. In this work, we propose Program-aided Distillation
(PaD), which introduces reasoning programs to suppress the errors in distilled
data, and thus achieves better distillation quality for reasoning tasks. In
PaD, we utilize the reasoning program to substitute the CoT, allowing automated
error checking of synthetic data. Further, through error injecting and further
training, the small distilling model could iteratively self-refine the
reasoning. Moreover, we conduct a step-wise beam search by step-by-step
verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic
reasoning, symbolic reasoning, and general ability. Experimental results
demonstrate that smaller models using PaD can not only outperform certain
LLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines
with a significantly smaller scale of parameters and data. The source code is
publicly available at https://github.com/Xuekai-Zhu/pad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Long Paper; Code and data are available at
  https://github.com/Xuekai-Zhu/pad</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChEDDAR: Student-<span class="highlight-title">ChatGPT</span> Dialogue in EFL Writing Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon Ahn, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of generative AI in education is expanding, yet empirical
analyses of large-scale, real-world interactions between students and AI
systems still remain limited. In this study, we present ChEDDAR, ChatGPT & EFL
Learner's Dialogue Dataset As Revising an essay, which is collected from a
semester-long longitudinal experiment involving 212 college students enrolled
in English as Foreign Langauge (EFL) writing courses. The students were asked
to revise their essays through dialogues with ChatGPT. ChEDDAR includes a
conversation log, utterance-level essay edit history, self-rated satisfaction,
and students' intent, in addition to session-level pre-and-post surveys
documenting their objectives and overall experiences. We analyze students'
usage patterns and perceptions regarding generative AI with respect to their
intent and satisfaction. As a foundational step, we establish baseline results
for two pivotal tasks in task-oriented dialogue systems within educational
contexts: intent detection and satisfaction estimation. We finally suggest
further research to refine the integration of generative AI into education
settings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly
available at https://github.com/zeunie/ChEDDAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The new version of this paper is on arXiv as arXiv:2403.08272</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CharPoet: A Chinese Classical Poetry <span class="highlight-title">Generation</span> System Based on
  Token-free LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Chinese classical poetry generation has attracted much research
interest, but achieving effective control over format and content
simultaneously remains challenging. Traditional systems usually accept keywords
as user inputs, resulting in limited control over content. Large language
models (LLMs) improve content control by allowing unrestricted user
instructions, but the token-by-token generation process frequently makes format
errors. Motivated by this, we propose CharPoet, a Chinese classical poetry
generation system based on token-free LLM, which provides effective control
over both format and content. Our token-free architecture generates in a
character-by-character manner, enabling precise control over the number of
characters. Pruned from existing token-based LLMs, CharPoet inherits their
pretrained capabilities and can generate poetry following instructions like
"Write me a poem for my mother's birthday." CharPoet achieves format accuracy
above 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of
content quality, CharPoet surpasses traditional systems including Jiuge, and is
comparable to other LLMs. Our system is open source and available at
https://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of
CharPoet is available at https://youtu.be/voZ25qEp3Dc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Reversal Curse in Large <span class="highlight-title">Language Models</span> via Semantic-aware
  Permutation Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00758v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00758v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have achieved impressive performance
across diverse tasks, recent studies showcase that causal LLMs suffer from the
"reversal curse". It is a typical example that the model knows "A's father is
B", but is unable to reason "B's child is A". This limitation poses a challenge
to the advancement of artificial general intelligence (AGI), as it suggests a
gap in the models' ability to comprehend and apply bidirectional reasoning. In
this paper, we first conduct substantial evaluation and identify that the root
cause of the reversal curse lies in the different word order between the
training and inference stage, namely, the poor ability of causal language
models to predict antecedent words within the training data. Accordingly,
permutation on the training data is considered as a potential solution, since
this can make the model predict antecedent words or tokens. However, previous
permutation methods may disrupt complete phrases or entities, thereby posing
challenges for the model to comprehend and learn from training data. To address
this issue, we propose Semantic-aware Permutation Training (SPT), which
addresses this issue by segmenting the training sentences into semantic units
(i.e., entities or phrases) with an assistant language model and permuting
these units before feeding into the model. Extensive experiments demonstrate
that SPT effectively mitigates the reversal curse since the performance on
reversed questions approximates that on the forward ones, and significantly
advances the performance of existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Over-Reasoning and Redundant Calculation of Large <span class="highlight-title">Language Models</span> <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Han Chiang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve problems step-by-step. While this
chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if
LLMs \textit{know} when to use CoT and whether those CoT are always necessary
to answer the question. This paper shows that LLMs tend to generate redundant
calculations and reasoning on a manually constructed math QA dataset,
GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered
without any calculations, but LLMs, including Llama-2 models and Claude-2, tend
to generate lengthy and unnecessary calculations to answer the questions. We
also conduct experiments to explain why LLMs generate redundant calculations
and reasonings. GSM8K-Zero is publicly available at
https://github.com/d223302/Over-Reasoning-of-LLMs and
https://huggingface.co/datasets/dcml0714/GSM8K-Zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024 main conference paper. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks. Begin the
exploration at \url{https://github.com/SalesforceAIResearch/xLAM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add GitHub repo link at
  \url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link
  at \url{https://huggingface.co/Salesforce/xLAM-v0.1-r}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable <span class="highlight-title">Generation</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional End-to-End Learning of Retriever-Reader Paradigm for
  Entity Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12245v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12245v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghui Li, Yong Jiang, Yangning Li, Xingyu Lu, Pengjun Xie, Ying Shen, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring semantic information in disease: Simple Data Augmentation
  Techniques for Chinese Disease Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Cui, Xiangling Fu, Shaohui Liu, Mingjun Gu, Xien Liu, Ji Wu, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease name normalization is an important task in the medical domain. It
classifies disease names written in various formats into standardized names,
serving as a fundamental component in smart healthcare systems for various
disease-related functions. Nevertheless, the most significant obstacle to
existing disease name normalization systems is the severe shortage of training
data. While data augmentation is a powerful approach for addressing data
scarcity, our findings reveal that conventional data augmentation techniques
often impede task performance, primarily due to the multi-axis and
multi-granularity nature of disease names. Consequently, we introduce a set of
customized data augmentation techniques designed to leverage the semantic
information inherent in disease names. These techniques aim to enhance the
model's understanding of the semantic intricacies and classification structure
of disease names. Through extensive experimentation, we illustrate that our
proposed plug-and-play methods not only surpass general data augmentation
techniques but also exhibit significant performance improvements across various
baseline models and training objectives, particularly in scenarios with limited
training data. This underscores its potential for widespread application in
medical language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Blame the Annotator: Bias Already Starts in the Annotation
  <span class="highlight-title">Instruct</span>ions <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Parmar, Swaroop Mishra, Mor Geva, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, progress in NLU has been driven by benchmarks. These
benchmarks are typically collected by crowdsourcing, where annotators write
examples based on annotation instructions crafted by dataset creators. In this
work, we hypothesize that annotators pick up on patterns in the crowdsourcing
instructions, which bias them to write many similar examples that are then
over-represented in the collected data. We study this form of bias, termed
instruction bias, in 14 recent NLU benchmarks, showing that instruction
examples often exhibit concrete patterns, which are propagated by crowdworkers
to the collected data. This extends previous work (Geva et al., 2019) and
raises a new concern of whether we are modeling the dataset creator's
instructions, rather than the task. Through a series of experiments, we show
that, indeed, instruction bias can lead to overestimation of model performance,
and that models struggle to generalize beyond biases originating in the
crowdsourcing instructions. We further analyze the influence of instruction
bias in terms of pattern frequency and model size, and derive concrete
recommendations for creating future NLU benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2023 (Outstanding Paper Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDA: Simple Discrete Augmentation for Contrastive Sentence
  Representation Learning <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Zhu, Zhenyu Mao, Jinghui Lu, Rui Zhao, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has recently achieved compelling performance in
unsupervised sentence representation. As an essential element, data
augmentation protocols, however, have not been well explored. The pioneering
work SimCSE resorting to a simple dropout mechanism (viewed as continuous
augmentation) surprisingly dominates discrete augmentations such as cropping,
word deletion, and synonym replacement as reported. To understand the
underlying rationales, we revisit existing approaches and attempt to
hypothesize the desiderata of reasonable data augmentation methods: balance of
semantic consistency and expression diversity. We then develop three simple yet
effective discrete sentence augmentation schemes: punctuation insertion, modal
verbs, and double negation. They act as minimal noises at lexical level to
produce diverse forms of sentences. Furthermore, standard negation is
capitalized on to generate negative samples for alleviating feature suppression
involved in contrastive learning. We experimented extensively with semantic
textual similarity on diverse datasets. The results support the superiority of
the proposed methods consistently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Whisper perform speech-based in-context learning? <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the in-context learning abilities of the Whisper
automatic speech recognition (ASR) models released by OpenAI. A novel
speech-based in-context learning (SICL) approach is proposed for test-time
adaptation, which can reduce the word error rates (WERs) with only a small
number of labelled speech samples without gradient descent. Language-level
adaptation experiments using Chinese dialects showed that when applying SICL to
isolated word ASR, consistent and considerable relative WER reductions can be
achieved using Whisper models of any size on two dialects, which is on average
32.3%. A k-nearest-neighbours-based in-context example selection technique can
be applied to further improve the efficiency of SICL, which can increase the
average relative WER reduction to 36.4%. The findings are verified using
speaker adaptation or continuous speech recognition tasks, and both achieved
considerable relative WER reductions. Detailed quantitative analyses are also
provided to shed light on SICL's adaptability to phonological variances and
dialect-specific lexical nuances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIntRec2.0: A Large-scale Benchmark <span class="highlight-title">Dataset</span> for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal intent recognition poses significant challenges, requiring the
incorporation of non-verbal modalities from real-world contexts to enhance the
comprehension of human intentions. Existing benchmark datasets are limited in
scale and suffer from difficulties in handling out-of-scope samples that arise
in multi-turn conversational interactions. We introduce MIntRec2.0, a
large-scale benchmark dataset for multimodal intent recognition in multi-party
conversations. It contains 1,245 dialogues with 15,040 samples, each annotated
within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope
samples, it also includes 5,736 out-of-scope samples appearing in multi-turn
contexts, which naturally occur in real-world scenarios. Furthermore, we
provide comprehensive information on the speakers in each utterance, enriching
its utility for multi-party conversational research. We establish a general
framework supporting the organization of single-turn and multi-turn dialogue
data, modality feature extraction, multimodal fusion, as well as in-scope
classification and out-of-scope detection. Evaluation benchmarks are built
using classic multimodal fusion methods, ChatGPT, and human evaluators. While
existing methods incorporating nonverbal information yield improvements,
effectively leveraging context information and detecting out-of-scope samples
remains a substantial challenge. Notably, large language models exhibit a
significant performance gap compared to humans, highlighting the limitations of
machine learning methods in the cognitive intent understanding task. We believe
that MIntRec2.0 will serve as a valuable resource, providing a pioneering
foundation for research in human-machine conversational interactions, and
significantly facilitating related applications. The full dataset and codes are
available at https://github.com/thuiar/MIntRec2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2024; The abstract is slightly modified due to the
  length limitation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrated <span class="highlight-title">Language Models</span> Must Hallucinate <span class="chip">STOC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Tauman Kalai, Santosh S. Vempala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent language models generate false but plausible-sounding text with
surprising frequency. Such "hallucinations" are an obstacle to the usability of
language-based AI systems and can harm people who rely upon their outputs. This
work shows that there is an inherent statistical lower-bound on the rate that
pretrained language models hallucinate certain types of facts, having nothing
to do with the transformer LM architecture or data quality. For "arbitrary"
facts whose veracity cannot be determined from the training data, we show that
hallucinations must occur at a certain rate for language models that satisfy a
statistical calibration condition appropriate for generative language models.
Specifically, if the maximum probability of any fact is bounded, we show that
the probability of generating a hallucination is close to the fraction of facts
that occur exactly once in the training data (a "Good-Turing" estimate), even
assuming ideal training data without errors.
  One conclusion is that models pretrained to be sufficiently good predictors
(i.e., calibrated) may require post-training to mitigate hallucinations on the
type of arbitrary facts that tend to appear once in the training set. However,
our analysis also suggests that there is no statistical reason that pretraining
will lead to hallucination on facts that tend to appear more than once in the
training data (like references to publications such as articles and books,
whose hallucinations have been particularly notable and problematic) or on
systematic facts (like arithmetic calculations). Therefore, different
architectures and learning algorithms may mitigate these latter types of
hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 56th Annual ACM Symposium on Theory of
  Computing (STOC) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unimodal Aggregation for CTC-based Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Fang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper works on non-autoregressive automatic speech recognition. A
unimodal aggregation (UMA) is proposed to segment and integrate the feature
frames that belong to the same text token, and thus to learn better feature
representations for text tokens. The frame-wise features and weights are both
derived from an encoder. Then, the feature frames with unimodal weights are
integrated and further processed by a decoder. Connectionist temporal
classification (CTC) loss is applied for training. Compared to the regular CTC,
the proposed method learns better feature representations and shortens the
sequence length, resulting in lower recognition error and computational
complexity. Experiments on three Mandarin datasets show that UMA demonstrates
superior or comparable performance to other advanced non-autoregressive
methods, such as self-conditioned CTC. Moreover, by integrating
self-conditioned CTC into the proposed framework, the performance can be
further noticeably improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsyChat: A Client-Centric Dialogue System for Mental Health Support <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue systems are increasingly integrated into mental health support to
help clients facilitate exploration, gain insight, take action, and ultimately
heal themselves. A practical and user-friendly dialogue system should be
client-centric, focusing on the client's behaviors. However, existing dialogue
systems publicly available for mental health support often concentrate solely
on the counselor's strategies rather than the behaviors expressed by clients.
This can lead to unreasonable or inappropriate counseling strategies and
corresponding responses generated by the dialogue system. To address this
issue, we propose PsyChat, a client-centric dialogue system that provides
psychological support through online chat. The client-centric dialogue system
comprises five modules: client behavior recognition, counselor strategy
selection, input packer, response generator, and response selection. Both
automatic and human evaluations demonstrate the effectiveness and practicality
of our proposed dialogue system for real-life mental health support.
Furthermore, the case study demonstrates that the dialogue system can predict
the client's behaviors, select appropriate counselor strategies, and generate
accurate and suitable responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CSCWD 2024 (27th International Conference on Computer
  Supported Cooperative Work in Design)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Multimodal Entity Linking <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12725v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12725v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Entity Linking (MEL) is the task of mapping mentions with
multimodal contexts to the referent entities from a knowledge base. Existing
MEL methods mainly focus on designing complex multimodal interaction mechanisms
and require fine-tuning all model parameters, which can be prohibitively costly
and difficult to scale in the era of Large Language Models (LLMs). In this
work, we propose GEMEL, a Generative Multimodal Entity Linking framework based
on LLMs, which directly generates target entity names. We keep the vision and
language model frozen and only train a feature mapper to enable cross-modality
interactions. To adapt LLMs to the MEL task, we leverage the in-context
learning capability of LLMs by retrieving multimodal instances as
demonstrations. Extensive experiments show that, with only ~0.3% of the model
parameters fine-tuned, GEMEL achieves state-of-the-art results on two
well-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%
accuracy gains on WikiMEL). The performance gain stems from mitigating the
popularity bias of LLM predictions and disambiguating less common entities
effectively. Further analysis verifies the generality and scalability of GEMEL.
Our framework is compatible with any off-the-shelf language model, paving the
way towards an efficient and general solution for utilizing LLMs in the MEL
task. Our code is available at https://github.com/HITsz-TMG/GEMEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Beyond Identification: Multi-bit Watermark for Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KiYoon Yoo, Wonhyuk Ahn, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show the viability of tackling misuses of large language models beyond the
identification of machine-generated text. While existing zero-bit watermark
methods focus on detection only, some malicious misuses demand tracing the
adversary user for counteracting them. To address this, we propose Multi-bit
Watermark via Position Allocation, embedding traceable multi-bit information
during language model generation. Through allocating tokens onto different
parts of the messages, we embed longer messages in high corruption settings
without added latency. By independently embedding sub-units of messages, the
proposed method outperforms the existing works in terms of robustness and
latency. Leveraging the benefits of zero-bit watermarking, our method enables
robust extraction of the watermark without any model access, embedding and
extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining
text quality, while allowing zero-bit detection all at the same time. Code is
released here: https://github.com/bangawayoo/mb-lm-watermarking
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 main. 9 pages and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Highlighter: Interactive Control for Multi-Modal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)
inference: explicit controllable text generation. Multi-modal LLMs empower
multi-modality understanding with the capability of semantic generation yet
bring less explainability and heavier reliance on prompt contents due to their
autoregressive generative nature. While manipulating prompt formats could
improve outputs, designing specific and precise prompts per task can be
challenging and ineffective. To tackle this issue, we introduce a novel
inference method, Prompt Highlighter, which enables users to highlight specific
prompt spans to interactively control the focus during generation. Motivated by
the classifier-free diffusion guidance, we form regular and unconditional
context pairs based on highlighted tokens, demonstrating that the
autoregressive generation in models can be guided in a classifier-free way.
Notably, we find that, during inference, guiding the models with highlighted
tokens through the attention weights leads to more desired outputs. Our
approach is compatible with current LLMs and VLMs, achieving impressive
customized generation results without training. Experiments confirm its
effectiveness in focusing on input contexts and generating reliable content.
Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and
1552.5 in MME-perception. The code is available at:
https://github.com/dvlab-research/Prompt-Highlighter/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project Page:
  https://julianjuaner.github.io/projects/PromptHighlighter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Small Multimodal Models to Bridge Biomedical Competency Gap: A
  Case Study in Radiology Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling laws and extraordinary performance of large foundation models
motivate the development and utilization of such large models in biomedicine.
However, despite early promising results on some biomedical benchmarks, there
are still major challenges that need to be addressed before these models can be
used in real-world applications. Frontier models such as GPT-4V still have
major competency gaps in multimodal capabilities for biomedical applications.
Moreover, pragmatic issues such as access, cost, latency, and compliance make
it hard for clinicians to use privately-hosted state-of-the-art large models
directly on private patient data. In this paper, we explore training
open-source small multimodal models (SMMs) to bridge biomedical competency gaps
for unmet clinical needs. To maximize data efficiency, we adopt a modular
approach by incorporating state-of-the-art pre-trained models for image and
text modalities, and focusing on training a lightweight adapter to ground each
modality to the text embedding space. We conduct a comprehensive study of this
approach on radiology imaging. For training, we assemble a large dataset with
over 1 million image-text pairs. For evaluation, we propose a clinically driven
novel approach using GPT-4 and demonstrate its parity with expert evaluation.
We also study grounding qualitatively using attention. For best practice, we
conduct a systematic ablation study on various choices in data engineering and
multimodal training. The resulting LLaVA-Rad (7B) model attains
state-of-the-art results on radiology tasks such as report generation and
cross-modal retrieval, even outperforming much larger models such as GPT-4V and
Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in
private settings, offering a promising state-of-the-art tool for real-world
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoDAN: Generating Stealthy Jailbreak <span class="highlight-title">Prompt</span>s on Aligned Large Language
  Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aligned Large Language Models (LLMs) are powerful language understanding
and decision-making tools that are created through extensive alignment with
human feedback. However, these large models remain susceptible to jailbreak
attacks, where adversaries manipulate prompts to elicit malicious outputs that
should not be given by aligned LLMs. Investigating jailbreak prompts can lead
us to delve into the limitations of LLMs and further guide us to secure them.
Unfortunately, existing jailbreak techniques suffer from either (1) scalability
issues, where attacks heavily rely on manual crafting of prompts, or (2)
stealthiness problems, as attacks depend on token-based algorithms to generate
prompts that are often semantically meaningless, making them susceptible to
detection through basic perplexity testing. In light of these challenges, we
intend to answer this question: Can we develop an approach that can
automatically generate stealthy jailbreak prompts? In this paper, we introduce
AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can
automatically generate stealthy jailbreak prompts by the carefully designed
hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN
not only automates the process while preserving semantic meaningfulness, but
also demonstrates superior attack strength in cross-model transferability, and
cross-sample universality compared with the baseline. Moreover, we also compare
AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass
them effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Code is available at
  https://github.com/SheltonLiu-N/AutoDAN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via
  Social Commonsense and Linguistic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Xi, Munindar P. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine ethics ensures ethical conduct in Artificial Intelligence (AI) models
and agents. Examining real-life applications benefit learning practical ethics
in many situations, offering valuable data to grasp the complexities of human
ethics in diverse contexts. In this paper, we examine social media platforms
for understanding real-life ethical scenarios and human moral judgments. We
examine posts from a popular Reddit subreddit (i.e., a subcommunity) called
r/AmITheAsshole, where authors and commenters share their moral judgments on
who is blameworthy. We employ computational techniques to investigate the
underlying reasoning influencing moral judgments. We focus on excerpts-which we
term moral sparks-from original posts that commenters include to indicate what
motivates their judgments. To this end, we examine how (1) events activating
social commonsense and (2) linguistic signals affect moral sparks assignment
and their subsequent judgments. By examining over 24 672 posts and 175988
comments, we find that event-related negative character traits (e.g., immature
and rude) attract attention and stimulate blame, implying a dependent
relationship between character traits and moral values. Specially, we focus on
causal graph involving events (c-events) that activate social commonsense. We
observe that c-events are perceived with varying levels of informativeness,
influencing moral spark and judgment assignment in distinct ways. This
observation is reinforced by examining linguistic features describing
semantically similar c-events. Moreover, language influencing commenters'
cognitive processes enhances the probability of an excerpt becoming a moral
spark, while factual and concrete descriptions tend to inhibit this effect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoDia: A New <span class="highlight-title">Dataset</span> for Romanian Dialect Identification from Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RoDia, the first dataset for Romanian dialect identification
from speech. The RoDia dataset includes a varied compilation of speech samples
from five distinct regions of Romania, covering both urban and rural
environments, totaling 2 hours of manually annotated speech data. Along with
our dataset, we introduce a set of competitive models to be used as baselines
for future research. The top scoring model achieves a macro F1 score of 59.83%
and a micro F1 score of 62.08%, indicating that the task is challenging. We
thus believe that RoDia is a valuable resource that will stimulate research
aiming to address the challenges of Romanian dialect identification. We release
our dataset at https://github.com/codrut2/RoDia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is this the real life? Is this just fantasy? The Misleading Success of
  Simulating Social Interactions With LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLM) have enabled richer social
simulations, allowing for the study of various social phenomena with LLM-based
agents. However, most work has used an omniscient perspective on these
simulations (e.g., single LLM to generate all interlocutors), which is
fundamentally at odds with the non-omniscient, information asymmetric
interactions that humans have. To examine these differences, we develop an
evaluation framework to simulate social interactions with LLMs in various
settings (omniscient, non-omniscient). Our experiments show that interlocutors
simulated omnisciently are much more successful at accomplishing social goals
compared to non-omniscient agents, despite the latter being the more realistic
setting. Furthermore, we demonstrate that learning from omniscient simulations
improves the apparent naturalness of interactions but scarcely enhances goal
achievement in cooperative scenarios. Our findings indicate that addressing
information asymmetry remains a fundamental challenge for LLM-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metacognitive <span class="highlight-title">Prompt</span>ing Improves Understanding in Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05342v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05342v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Wang, Yun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design.
Recent advancements in prompting have enhanced reasoning in logic-intensive
tasks for LLMs, yet the nuanced understanding abilities of these models,
crucial for processing and interpreting complex information, remain
underexplored. In this study, we introduce Metacognitive Prompting (MP), a
strategy inspired by human introspective reasoning processes. Using MP, LLMs
undergo a systematic series of structured, self-aware evaluations, drawing on
both their vast inherent knowledge and new insights. We conduct extensive
experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across
ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE,
and LexGLUE benchmarks. Additionally, we compare our method with
chain-of-thought prompting and its advanced versions. The results show that
GPT-4 consistently excels across all tasks, while other models have shown
significant progress in some tasks when used in conjunction with MP.
Furthermore, MP consistently outperforms existing prompting methods in both
general and domain-specific NLU tasks. This study underscores the potential to
amplify the understanding abilities of LLMs and highlights the benefits of
mirroring human introspective reasoning in NLU tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The LLM Surgeon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki M. Asano, Tijmen Blankevoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models are becoming increasingly large in an effort
to achieve the highest performance on large corpora of available textual data.
However, the sheer size of the Transformer architectures makes it difficult to
deploy models within computational, environmental or device-specific
constraints. We explore data-driven compression of existing pretrained models
as an alternative to training smaller models from scratch. To do so, we scale
Kronecker-factored curvature approximations of the target loss landscape to
large language models. In doing so, we can compute both the dynamic allocation
of structures that can be removed as well as updates of remaining weights that
account for the removal. We provide a general framework for unstructured,
semi-structured and structured pruning and improve upon weight updates to
capture more correlations between weights, while remaining computationally
efficient. Experimentally, our method can prune rows and columns from a range
of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,
and achieve state-of-the-art results in unstructured and semi-structured
pruning of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Reasoning Abilities of <span class="highlight-title">ChatGPT</span> in the Context of Claim
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, Maria Liakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reasoning capabilities of LLMs are currently hotly debated. We examine
the issue from the perspective of claim/rumour verification. We propose the
first logical reasoning framework designed to break down any claim or rumour
paired with evidence into the atomic reasoning steps necessary for
verification. Based on our framework, we curate two annotated collections of
such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world
set stemming from rumours circulating on Twitter. We use them to evaluate the
reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as
ChatGPT) within the context of our framework, providing a thorough analysis.
Our results show that ChatGPT struggles in abductive reasoning, although this
can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to
Zero-Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body
of research suggesting that ChatGPT's reasoning processes are unlikely to
mirror human-like reasoning, and that LLMs need to be more rigorously evaluated
to distinguish between hype and actual capabilities, especially in high-stakes
real-world tasks such as claim verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal
  Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00568v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00568v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Ren Yao, Luke Breitfeller, Aakanksha Naik, Chunxiao Zhou, Carolyn Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Temporal Relation Extraction (ETRE) is a crucial yet challenging
problem. Event pairs are situated within a discourse at different distances,
which we refer to as proximity bands. The temporal ordering communicated about
event pairs situated at more remote (i.e., ``long'') or less remote (i.e.,
``short'') proximity bands is encoded differently. SOTA ETRE models have tended
to perform well on events situated at either short or long proximity bands, but
not both. Yet, real-world, natural texts contain all types of temporal
event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge
Co-Distillation, a fusion approach that shares knowledge across multiple event
pair proximity bands in order to improve performance on all types of temporal
datasets. Our experimental results show that MulCo successfully integrates
linguistic cues pertaining to temporal reasoning across both short and long
proximity bands and achieves new state-of-the-art results on several ETRE
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Hadi Veisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthria is a disability that causes a disturbance in the human speech
system and reduces the quality and intelligibility of a person's speech.
Because of this effect, the normal speech processing systems can not work
properly on impaired speech. This disability is usually associated with
physical disabilities. Therefore, designing a system that can perform some
tasks by receiving voice commands in the smart home can be a significant
achievement. In this work, we introduce gammatonegram as an effective method to
represent audio files with discriminative details, which is used as input for
the convolutional neural network. On the other word, we convert each speech
file into an image and propose image recognition system to classify speech in
different scenarios. Proposed CNN is based on the transfer learning method on
the pre-trained Alexnet. In this research, the efficiency of the proposed
system for speech recognition, speaker identification, and intelligibility
assessment is evaluated. According to the results on the UA dataset, the
proposed speech recognition system achieved 91.29% accuracy in
speaker-dependent mode, the speaker identification system acquired 87.74%
accuracy in text-dependent mode, and the intelligibility assessment system
achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network
speech recognition system that works fully automatically. This system is
located in a cascade arrangement with the two-class intelligibility assessment
system, and the output of this system activates each one of the speech
recognition networks. This architecture achieves an accuracy of 92.3% WRR. The
source code of this paper is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures. Iran J Comput Sci (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OSCaR: Object State Captioning and State Change Representation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17128v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17128v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of the
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic
  <span class="highlight-title">Prompt</span> Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on task-agnostic prompt compression for better
generalizability and efficiency. Considering the redundancy in natural
language, existing approaches compress prompts by removing tokens or lexical
units according to their information entropy obtained from a causal language
model such as LLaMa-7B. The challenge is that information entropy may be a
suboptimal compression metric: (i) it only leverages unidirectional context and
may fail to capture all essential information needed for prompt compression;
(ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive
knowledge from an LLM to compress prompts without losing crucial information,
and meantime, introduce an extractive text compression dataset. We formulate
prompt compression as a token classification problem to guarantee the
faithfulness of the compressed prompt to the original one, and use a
Transformer encoder as the base architecture to capture all essential
information for prompt compression from the full bidirectional context. Our
approach leads to lower latency by explicitly learning the compression
objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets,
including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its
small size, our model shows significant performance gains over strong baselines
and demonstrates robust generalization ability across different LLMs.
Additionally, our model is 3x-6x faster than existing prompt compression
methods, while accelerating the end-to-end latency by 1.6x-2.9x with
compression ratios of 2x-5x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale pre-trained Vision-Language Models (VLMs) have
demonstrated great potential in learning open-world visual representations, and
exhibit remarkable performance across a wide range of downstream tasks through
efficient fine-tuning. In this work, we innovatively introduce the concept of
dual learning into fine-tuning VLMs, i.e., we not only learn what an image is,
but also what an image isn't. Building on this concept, we introduce a novel
DualAdapter approach to enable dual-path adaptation of VLMs from both positive
and negative perspectives with only limited annotated samples. In the inference
stage, our DualAdapter performs unified predictions by simultaneously
conducting complementary positive selection and negative exclusion across
target classes, thereby enhancing the overall recognition accuracy of VLMs in
downstream tasks. Our extensive experimental results across 15 datasets
validate that the proposed DualAdapter outperforms existing state-of-the-art
methods on both few-shot learning and domain generalization tasks while
achieving competitive computational efficiency. Code is available at
https://github.com/zhangce01/DualAdapter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dated Data: Tracing Knowledge Cutoffs in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Released Large Language Models (LLMs) are often paired with a claimed
knowledge cutoff date, or the dates at which training data was gathered. Such
information is crucial for applications where the LLM must provide up to date
information. However, this statement only scratches the surface: do all
resources in the training data share the same knowledge cutoff date? Does the
model's demonstrated knowledge for these subsets closely align to their cutoff
dates? In this work, we define the notion of an effective cutoff. This is
distinct from the LLM designer reported cutoff and applies separately to
sub-resources and topics. We propose a simple approach to estimate effective
cutoffs on the resource-level temporal alignment of an LLM by probing across
versions of the data. Using this analysis, we find that effective cutoffs often
differ from reported cutoffs. To understand the root cause of this observation,
we conduct a direct large-scale analysis on open pre-training datasets. Our
analysis reveals two reasons for these inconsistencies: (1) temporal biases of
CommonCrawl data due to non-trivial amounts of old data in new dumps and (2)
complications in LLM deduplication schemes involving semantic duplicates and
lexical near-duplicates. Overall, our results show that knowledge cutoffs are
not as simple as they have seemed and that care must be taken both by LLM
dataset curators as well as practitioners who seek to use information from
these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Information Extraction From Employment Tribunal Judgements
  Using Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joana Ribeiro de Faria, Huiyuan Xie, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Court transcripts and judgments are rich repositories of legal knowledge,
detailing the intricacies of cases and the rationale behind judicial decisions.
The extraction of key information from these documents provides a concise
overview of a case, crucial for both legal experts and the public. With the
advent of large language models (LLMs), automatic information extraction has
become increasingly feasible and efficient. This paper presents a comprehensive
study on the application of GPT-4, a large language model, for automatic
information extraction from UK Employment Tribunal (UKET) cases. We
meticulously evaluated GPT-4's performance in extracting critical information
with a manual verification process to ensure the accuracy and relevance of the
extracted data. Our research is structured around two primary extraction tasks:
the first involves a general extraction of eight key aspects that hold
significance for both legal specialists and the general public, including the
facts of the case, the claims made, references to legal statutes, references to
precedents, general case outcomes and corresponding labels, detailed order and
remedies and reasons for the decision. The second task is more focused, aimed
at analysing three of those extracted features, namely facts, claims and
outcomes, in order to facilitate the development of a tool capable of
predicting the outcome of employment law disputes. Through our analysis, we
demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information
extraction, highlighting the potential of LLMs in revolutionising the way legal
information is processed and utilised, offering significant implications for
legal research and practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supporting Energy Policy Research with Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grant Buster, Pavlo Pinchuk, Jacob Barrons, Ryan McKeever, Aaron Levine, Anthony Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent growth in renewable energy development in the United States has
been accompanied by a simultaneous surge in renewable energy siting ordinances.
These zoning laws play a critical role in dictating the placement of wind and
solar resources that are critical for achieving low-carbon energy futures. In
this context, efficient access to and management of siting ordinance data
becomes imperative. The National Renewable Energy Laboratory (NREL) recently
introduced a public wind and solar siting database to fill this need. This
paper presents a method for harnessing Large Language Models (LLMs) to automate
the extraction of these siting ordinances from legal documents, enabling this
database to maintain accurate up-to-date information in the rapidly changing
energy policy landscape. A novel contribution of this research is the
integration of a decision tree framework with LLMs. Our results show that this
approach is 85 to 90% accurate with outputs that can be used directly in
downstream quantitative modeling. We discuss opportunities to use this work to
support similar large-scale policy research in the energy sector. By unlocking
new efficiencies in the extraction and analysis of legal documents using LLMs,
this study enables a path forward for automated large-scale energy policy
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable and Stable Finetuning of <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Language Models</span> on
  Low-Resource Texts <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11
  tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Sustainable GenAI using <span class="highlight-title">Generation</span> Directives for Carbon-Friendly
  Large Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-sourced Large Language Models (LLMs) have achieved great success in
various NLP tasks, however, they are still far inferior to API-based models
when acting as agents. How to integrate agent ability into general LLMs becomes
a crucial and urgent problem. This paper first delivers three key observations:
(1) the current agent training corpus is entangled with both formats following
and agent reasoning, which significantly shifts from the distribution of its
pre-training data; (2) LLMs exhibit different learning speeds on the
capabilities required by agent tasks; and (3) current approaches have
side-effects when improving agent abilities by introducing hallucinations.
Based on the above findings, we propose Agent-FLAN to effectively Fine-tune
LANguage models for Agents. Through careful decomposition and redesign of the
training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by
3.5\% across various agent evaluation datasets. With comprehensively
constructed negative samples, Agent-FLAN greatly alleviates the hallucination
issues based on our established evaluation benchmark. Besides, it consistently
improves the agent capability of LLMs when scaling model sizes while slightly
enhancing the general capability of LLMs. The code will be available at
https://github.com/InternLM/Agent-FLAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epistemology of <span class="highlight-title">Language Models</span>: Do <span class="highlight-title">Language Models</span> Have Holistic
  Knowledge? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the inherent knowledge in language models from the
perspective of epistemological holism. The purpose of this paper is to explore
whether LLMs exhibit characteristics consistent with epistemological holism.
These characteristics suggest that core knowledge, such as general scientific
knowledge, each plays a specific role, serving as the foundation of our
knowledge system and being difficult to revise. To assess these traits related
to holism, we created a scientific reasoning dataset and examined the
epistemology of language models through three tasks: Abduction, Revision, and
Argument Generation. In the abduction task, the language models explained
situations while avoiding revising the core knowledge. However, in other tasks,
the language models were revealed not to distinguish between core and
peripheral knowledge, showing an incomplete alignment with holistic knowledge
principles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Explanation Faithfulness between Multilingual and Monolingual
  Fine-tuned <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixue Zhao, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real natural language processing application scenarios, practitioners
not only aim to maximize predictive performance but also seek faithful
explanations for the model predictions. Rationales and importance distribution
given by feature attribution methods (FAs) provide insights into how different
parts of the input contribute to a prediction. Previous studies have explored
how different factors affect faithfulness, mainly in the context of monolingual
English models. On the other hand, the differences in FA faithfulness between
multilingual and monolingual models have yet to be explored. Our extensive
experiments, covering five languages and five popular FAs, show that FA
faithfulness varies between multilingual and monolingual models. We find that
the larger the multilingual model, the less faithful the FAs are compared to
its counterpart monolingual models.Our further analysis shows that the
faithfulness disparity is potentially driven by the differences between model
tokenizers. Our code is available:
https://github.com/casszhao/multilingual-faith.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Moral Value Alignment Through Context-Based Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Dognin, Jesus Rios, Ronny Luss, Inkit Padhi, Matthew D Riemer, Miao Liu, Prasanna Sattigeri, Manish Nagireddy, Kush R. Varshney, Djallel Bouneffouf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing value-aligned AI agents is a complex undertaking and an ongoing
challenge in the field of AI. Specifically within the domain of Large Language
Models (LLMs), the capability to consolidate multiple independently trained
dialogue agents, each aligned with a distinct moral value, into a unified
system that can adapt to and be aligned with multiple moral values is of
paramount importance. In this paper, we propose a system that does contextual
moral value alignment based on contextual aggregation. Here, aggregation is
defined as the process of integrating a subset of LLM responses that are best
suited to respond to a user input, taking into account features extracted from
the user's input. The proposed system shows better results in term of alignment
to human value compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Text Shortening Strategy in <span class="highlight-title">BERT</span>: Truncation vs
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirza Alim Mutasodirin, Radityo Eko Prasojo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The parallelism of Transformer-based models comes at the cost of their input
max-length. Some studies proposed methods to overcome this limitation, but none
of them reported the effectiveness of summarization as an alternative. In this
study, we investigate the performance of document truncation and summarization
in text classification tasks. Each of the two was investigated with several
variations. This study also investigated how close their performances are to
the performance of full-text. We used a dataset of summarization tasks based on
Indonesian news articles (IndoSum) to do classification tests. This study shows
how the summaries outperform the majority of truncation method variations and
lose to only one. The best strategy obtained in this study is taking the head
of the document. The second is extractive summarization. This study explains
what happened to the result, leading to further research in order to exploit
the potential of document summarization as a shortening alternative. The code
and data used in this work are publicly available in
https://github.com/mirzaalimm/TruncationVsSummarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th International Conference on Advanced Computer Science and
  Information Systems (ICACSIS 2021)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Data Curation for Robust Language Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuhai Chen, Jonas Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have become the de facto approach to
sequence-to-sequence text generation tasks, but for specialized tasks/domains,
a pretrained LLM lacks specific capabilities to produce accurate or
well-formatted responses. Supervised fine-tuning specializes a LLM by training
it on dataset of example prompts with target responses, but real-world data
tends to be noisy. While many fine-tuning algorithms exist, here we consider a
\emph{data-centric AI} perspective on LLM fine-tuning, studying how to
\emph{systematically} curate the training dataset to improve the LLM produced
via \emph{any} fine-tuning algorithm.
  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM
Evaluation And Rectification) for instruction tuning datasets, that can be used
with any LLM and fine-tuning procedure. CLEAR estimates which training data is
low-quality and either filters or corrects it. Automatically identifying which
data to filter or correct is done via LLM-derived confidence estimates, to
ensure only confident modifications to the dataset. Unlike existing data
curation techniques, CLEAR is a comprehensive framework that can improve a
dataset (and trained model outputs) without additional fine-tuning
computations. We don't assume access to a stronger LLM than the model being
fine-tuned (e.g.\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether
CLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal
that CLEAR consistently improves the performance of fine-tuned models across
many datasets and models (like GPT-3.5 and Llama2).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian
  Dialectal Data <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyao Peng, Zihang Sun, Huangyan Shan, Marie Kolm, Verena Blaschke, Ekaterina Artemova, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) is a fundamental task to extract key
information from texts, but annotated resources are scarce for dialects. This
paper introduces the first dialectal NER dataset for German, BarNER, with 161K
tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets
(bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The
Bavarian dialect differs from standard German in lexical distribution,
syntactic construction, and entity information. We conduct in-domain,
cross-domain, sequential, and joint experiments on two Bavarian and three
German corpora and present the first comprehensive NER results on Bavarian.
Incorporating knowledge from the larger German NER (sub-)datasets notably
improves on bar-wiki and moderately on bar-tweet. Inversely, training first on
Bavarian contributes slightly to the seminal German CoNLL 2006 corpus.
Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task
learning between five NER and two Bavarian-German dialect identification tasks
and achieve NER SOTA on bar-wiki. We substantiate the necessity of our
low-resource BarNER corpus and the importance of diversity in dialects, genres,
and topics in enhancing model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Instruct</span>ing Large <span class="highlight-title">Language Models</span> to Identify and Ignore Irrelevant
  Conditions <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Chao Shen, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math word problem (MWP) solving requires generating a reasoning path based on
a given problem description that often contains irrelevant conditions. Existing
chain-of-thought (CoT) prompting methods elicited multi-step reasoning
abilities of large language models (LLMs) to solve MWPs. However, they were
seriously confused by the irrelevant conditions, resulting in low accuracy. In
this paper, we propose a novel approach named I$^3$C that instructs LLMs to
identify and ignore irrelevant conditions. It identifies a set of irrelevant
condition candidates that have a weak semantic relevance with the question.
Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs
the LLMs with the verification on relevant and irrelevant conditions to avoid
confusion and improve reasoning paths. Moreover, we propose to select (problem,
reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot
reasoning. We develop I$^3$C-Select that selects the most confusing problems
based on the semantic relevance measurement. We conduct extensive experiments
on eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to
improve the performance of solving MWPs. Notably, with GPT-3.5-Turbo and
I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and
GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art
few-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is
made publicly available at https://wzy6642.github.io/I3C.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 - Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Ljubešić, Taja Kuzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a collection of highly comparable web corpora of
Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,
covering thereby the whole spectrum of official languages in the South Slavic
language space. The collection of these corpora comprises a total of 13 billion
tokens of texts from 26 million documents. The comparability of the corpora is
ensured by a comparable crawling setup and the usage of identical crawling and
post-processing technology. All the corpora were linguistically annotated with
the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and
enriched with document-level genre information via the Transformer-based
multilingual X-GENRE classifier, which further enhances comparability at the
level of linguistic annotation and metadata enrichment. The genre-focused
analysis of the resulting corpora shows a rather consistent distribution of
genres throughout the seven corpora, with variations in the most prominent
genre categories being well-explained by the economic strength of each language
community. A comparison of the distribution of genre categories across the
corpora indicates that web corpora from less developed countries primarily
consist of news articles. Conversely, web corpora from economically more
developed countries exhibit a smaller proportion of news content, with a
greater presence of promotional and opinionated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the LREC-COLING 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Taranukhin, Sahithya Ravi, Gabor Lukacs, Evangelos Milios, Vered Shwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Canadian air travel sector has seen a significant increase in flight
delays, cancellations, and other issues concerning passenger rights.
Recognizing this demand, we present a chatbot to assist passengers and educate
them about their rights. Our system breaks a complex user input into simple
queries which are used to retrieve information from a collection of documents
detailing air travel regulations. The most relevant passages from these
documents are presented along with links to the original documents and the
generated queries, enabling users to dissect and leverage the information for
their unique circumstances. The system successfully overcomes two predominant
challenges: understanding complex user inputs, and delivering accurate answers,
free of hallucinations, that passengers can rely on for making informed
decisions. A user study comparing the chatbot to a Google search demonstrated
the chatbot's usefulness and ease of use. Beyond the primary goal of providing
accurate and timely information to air passengers regarding their rights, we
hope that this system will also enable further research exploring the tradeoff
between the user-friendly conversational interface of chatbots and the accuracy
of retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pragmatic Competence Evaluation of Large <span class="highlight-title">Language Models</span> for Korean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dojun Park, Jiwoo Lee, Hyeyun Jeong, Seohyun Park, Sungeun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current evaluation of Large Language Models (LLMs) predominantly relies
on benchmarks focusing on their embedded knowledge by testing through
multiple-choice questions (MCQs), a format inherently suited for automated
evaluation. Our study extends this evaluation to explore LLMs' pragmatic
competence--a facet previously underexamined before the advent of sophisticated
LLMs, specifically in the context of Korean. We employ two distinct evaluation
setups: the conventional MCQ format, adapted for automatic evaluation, and
Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs'
narrative response capabilities without predefined options. Our findings reveal
that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups,
respectively, with HyperCLOVA X, an LLM optimized for Korean, closely
following, especially in the OEQ setup, demonstrating a score of 81.56 with a
marginal difference of 4.13 points compared to GPT-4. Furthermore, while
few-shot learning strategies generally enhance LLM performance,
Chain-of-Thought (CoT) prompting introduces a bias toward literal
interpretations, hindering accurate pragmatic inference. Considering the
growing expectation for LLMs to understand and produce language that aligns
with human communicative norms, our findings emphasize the importance for
advancing LLMs' abilities to grasp and convey sophisticated meanings beyond
mere literal interpretations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Dimensional Machine Translation Evaluation: Model Evaluation and
  Resource for Korean <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dojun Park, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Almost all frameworks for the manual or automatic evaluation of machine
translation characterize the quality of an MT output with a single number. An
exception is the Multidimensional Quality Metrics (MQM) framework which offers
a fine-grained ontology of quality dimensions for scoring (such as style,
fluency, accuracy, and terminology). Previous studies have demonstrated the
feasibility of MQM annotation but there are, to our knowledge, no computational
models that predict MQM scores for novel texts, due to a lack of resources. In
this paper, we address these shortcomings by (a) providing a 1200-sentence MQM
evaluation benchmark for the language pair English-Korean and (b) reframing MT
evaluation as the multi-task problem of simultaneously predicting several MQM
scores using SOTA language models, both in a reference-based MT evaluation
setup and a reference-free quality estimation (QE) setup. We find that
reference-free setup outperforms its counterpart in the style dimension while
reference-based models retain an edge regarding accuracy. Overall, RemBERT
emerges as the most promising model. Through our evaluation, we offer an
insight into the translation quality in a more fine-grained, interpretable
manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation
  Benchmark for Chinese Large <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Liu, Renren Jin, Yuqi Ren, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Large Language Models (LLMs) have recently demonstrated impressive
capabilities across various NLP benchmarks and real-world applications.
However, the existing benchmarks for comprehensively evaluating these LLMs are
still insufficient, particularly in terms of measuring knowledge that LLMs
capture. Current datasets collect questions from Chinese examinations across
different subjects and educational levels to address this issue. Yet, these
benchmarks primarily focus on objective questions such as multiple-choice
questions, leading to a lack of diversity in question types. To tackle this
problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge
Evaluation benchmark in this paper. LHMKE is designed to provide a
comprehensive evaluation of the knowledge acquisition capabilities of Chinese
LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,
ranging from primary school to professional certification exams. Notably, LHMKE
includes both objective and subjective questions, offering a more holistic
evaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs
under the zero-shot setting, which aligns with real examinations, and compared
their performance across different subjects. We also conduct an in-depth
analysis to check whether GPT-4 can automatically score subjective predictions.
Our findings suggest that LHMKE is a challenging and advanced testbed for
Chinese LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler, Jindong Chen, Abhanshu Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) are achieving increasingly strong performance
on multimodal tasks. However, reasoning capabilities remain limited
particularly for smaller VLMs, while those of large-language models (LLMs) have
seen numerous improvements. We propose a technique to transfer capabilities
from LLMs to VLMs. On the recently introduced ChartQA, our method obtains
state-of-the-art performance when applied on the PaLI3-5B VLM by
\citet{chen2023pali3}, while also enabling much better performance on PlotQA
and FigureQA.
  We first improve the chart representation by continuing the pre-training
stage using an improved version of the chart-to-table translation task by
\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than
the original training set. To improve general reasoning capabilities and
improve numerical operations, we synthesize reasoning traces using the table
representation of charts. Lastly, our model is fine-tuned using the multitask
loss introduced by \citet{hsieh2023distilling}.
  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B
without using an upstream OCR system, while keeping inference time constant
compared to the PaLI3-5B baseline. When rationales are further refined with a
simple program-of-thought prompt \cite{chen2023program}, our model outperforms
the recently introduced Gemini Ultra and GPT-4V.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented
  Stock-Chain Framework <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of financial analysis primarily encompasses two key areas: stock
trend prediction and the corresponding financial question answering. Currently,
machine learning and deep learning algorithms (ML&DL) have been widely applied
for stock trend predictions, leading to significant progress. However, these
methods fail to provide reasons for predictions, lacking interpretability and
reasoning processes. Also, they can not integrate textual information such as
financial news or reports. Meanwhile, large language models (LLMs) have
remarkable textual understanding and generation ability. But due to the
scarcity of financial training datasets and limited integration with real-time
knowledge, LLMs still suffer from hallucinations and are unable to keep up with
the latest information. To tackle these challenges, we first release AlphaFin
datasets, combining traditional research datasets, real-time financial data,
and handwritten chain-of-thought (CoT) data. It has a positive impact on
training LLMs for completing financial analysis. We then use AlphaFin datasets
to benchmark a state-of-the-art method, called Stock-Chain, for effectively
tackling the financial analysis task, which integrates retrieval-augmented
generation (RAG) techniques. Extensive experiments are conducted to demonstrate
the effectiveness of our framework on financial analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2024. The first three authors contributed equally. Project
  website: https://github.com/AlphaFin-proj/AlphaFin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Hack for <span class="highlight-title">Transformer</span>s against Heavy Long-Text Classification on a
  Time- and Memory-Limited GPU Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mirza Alim Mutasodirin, Radityo Eko Prasojo, Achmad F. Abka, Hanif Rasyidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many NLP researchers rely on free computational services, such as Google
Colab, to fine-tune their Transformer models, causing a limitation for
hyperparameter optimization (HPO) in long-text classification due to the method
having quadratic complexity and needing a bigger resource. In Indonesian, only
a few works were found on long-text classification using Transformers. Most
only use a small amount of data and do not report any HPO. In this study, using
18k news articles, we investigate which pretrained models are recommended to
use based on the output length of the tokenizer. We then compare some hacks to
shorten and enrich the sequences, which are the removals of stopwords,
punctuation, low-frequency words, and recurring words. To get a fair
comparison, we propose and run an efficient and dynamic HPO procedure that can
be done gradually on a limited resource and does not require a long-running
optimization library. Using the best hack found, we then compare 512, 256, and
128 tokens length. We find that removing stopwords while keeping punctuation
and low-frequency words is the best hack. Some of our setups manage to
outperform taking 512 first tokens using a smaller 128 or 256 first tokens
which manage to represent the same information while requiring less
computational resources. The findings could help developers to efficiently
pursue optimal performance of the models using limited resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 10th International Conference on Advanced Informatics: Concepts,
  Theory, and Applications (ICAICTA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized Learning Assisted with Large Language Model for Gloss-free
  Sign Language Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigang Chen, Benjia Zhou, Jun Li, Jun Wan, Zhen Lei, Ning Jiang, Quan Lu, Guoqing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous Sign Language Translation (SLT) methods achieve superior performance
by relying on gloss annotations. However, labeling high-quality glosses is a
labor-intensive task, which limits the further development of SLT. Although
some approaches work towards gloss-free SLT through jointly training the visual
encoder and translation network, these efforts still suffer from poor
performance and inefficient use of the powerful Large Language Model (LLM).
Most seriously, we find that directly introducing LLM into SLT will lead to
insufficient learning of visual representations as LLM dominates the learning
curve. To address these problems, we propose Factorized Learning assisted with
Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the
training process into two stages. In the visual initialing stage, we employ a
lightweight translation model after the visual encoder to pre-train the visual
encoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the
visual encoder and integrate it with a pre-trained LLM to inspire the LLM's
translation potential. This factorized training strategy proves to be highly
effective as evidenced by significant improvements achieved across three SLT
datasets which are all conducted under the gloss-free setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-based Graph Model for Joint Liberal Event Extraction and Event
  Schema Induction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Di Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Events are essential components of speech and texts, describing the changes
in the state of entities. The event extraction task aims to identify and
classify events and find their participants according to event schemas.
Manually predefined event schemas have limited coverage and are hard to migrate
across domains. Therefore, the researchers propose Liberal Event Extraction
(LEE), which aims to extract events and discover event schemas simultaneously.
However, existing LEE models rely heavily on external language knowledge bases
and require the manual development of numerous rules for noise removal and
knowledge alignment, which is complex and laborious. To this end, we propose a
Prompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we
use a prompt-based model to obtain candidate triggers and arguments, and then
build heterogeneous event graphs to encode the structures within and between
events. Experimental results prove that our approach achieves excellent
performance with or without predefined event schemas, while the automatically
detected event schemas are proven high quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphERE: Jointly Multiple Event-Event Relation Extraction via
  Graph-Enhanced Event Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Di Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Events describe the state changes of entities. In a document, multiple events
are connected by various relations (e.g., Coreference, Temporal, Causal, and
Subevent). Therefore, obtaining the connections between events through
Event-Event Relation Extraction (ERE) is critical to understand natural
language. There are two main problems in the current ERE works: a. Only
embeddings of the event triggers are used for event feature representation,
ignoring event arguments (e.g., time, place, person, etc.) and their structure
within the event. b. The interconnection between relations (e.g., temporal and
causal relations usually interact with each other ) is ignored. To solve the
above problems, this paper proposes a jointly multiple ERE framework called
GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event
embeddings with event argument and structure features by using static AMR
graphs and IE graphs; Then, to jointly extract multiple event relations, we use
Node Transformer and construct Task-specific Dynamic Event Graphs for each type
of relation. Finally, we used a multi-task learning strategy to train the whole
framework. Experimental results on the latest MAVEN-ERE dataset validate that
GraphERE significantly outperforms existing methods. Further analyses indicate
the effectiveness of the graph-enhanced event embeddings and the joint
extraction strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large Collection of Model-generated Contradictory Responses for
  Consistency-aware Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiki Sato, Reina Akama, Jun Suzuki, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the generation of contradictory responses poses a substantial
challenge in dialogue response generation. The quality and quantity of
available contradictory response data play a vital role in suppressing these
contradictions, offering two significant benefits. First, having access to
large contradiction data enables a comprehensive examination of their
characteristics. Second, data-driven methods to mitigate contradictions may be
enhanced with large-scale contradiction data for training. Nevertheless, no
attempt has been made to build an extensive collection of model-generated
contradictory responses. In this paper, we build a large dataset of response
generation models' contradictions for the first time. Then, we acquire valuable
insights into the characteristics of model-generated contradictions through an
extensive analysis of the collected responses. Lastly, we also demonstrate how
this dataset substantially enhances the performance of data-driven
contradiction suppression methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied LLM Agents Learn to Cooperate in Organized Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as integral tools for reasoning,
planning, and decision-making, drawing upon their extensive world knowledge and
proficiency in language-related tasks. LLMs thus hold tremendous potential for
natural language interaction within multi-agent systems to foster cooperation.
However, LLM agents tend to over-report and comply with any instruction, which
may result in information redundancy and confusion in multi-agent cooperation.
Inspired by human organizations, this paper introduces a framework that imposes
prompt-based organization structures on LLM agents to mitigate these problems.
Through a series of experiments with embodied LLM agents and human-agent
collaboration, our results highlight the impact of designated leadership on
team efficiency, shedding light on the leadership qualities displayed by LLM
agents and their spontaneous cooperative behaviors. Further, we harness the
potential of LLMs to propose enhanced organizational prompts, via a
Criticize-Reflect process, resulting in novel organization structures that
reduce communication costs and enhance team efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Do "More Contexts" Help with Sarcasm Recognition? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ojas Nimase, Sanghyun Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm recognition is challenging because it needs an understanding of the
true intention, which is opposite to or different from the literal meaning of
the words. Prior work has addressed this challenge by developing a series of
methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to
models. While shown to be effective individually, no study has systematically
evaluated their collective effectiveness. As a result, it remains unclear to
what extent additional contexts can improve sarcasm recognition. In this work,
we explore the improvements that existing methods bring by incorporating more
contexts into a model. To this end, we develop a framework where we can
integrate multiple contextual cues and test different approaches. In evaluation
with four approaches on three sarcasm recognition benchmarks, we achieve
existing state-of-the-art performances and also demonstrate the benefits of
sequentially adding more contexts. We also identify inherent drawbacks of using
more contexts, highlighting that in the pursuit of even better results, the
model may need to adopt societal biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 [Short]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossTune: Black-Box Few-Shot Classification with Label Enhancement <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Luo, Chen Zhang, Yan Zhang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training or finetuning large-scale language models (LLMs) requires
substantial computation resources, motivating recent efforts to explore
parameter-efficient adaptation to downstream tasks. One approach is to treat
these models as black boxes and use forward passes (Inference APIs) to interact
with them. Current research focuses on adapting these black-box models to
downstream tasks using gradient-free prompt optimization, but this often
involves an expensive process of searching task-specific prompts. Therefore, we
are motivated to study black-box language model adaptation without prompt
search. Specifically, we introduce a label-enhanced cross-attention network
called CrossTune, which models the semantic relatedness between the input text
sequence and task-specific label descriptions. Its effectiveness is examined in
the context of few-shot text classification. To improve the generalization of
CrossTune, we utilize ChatGPT to generate additional training data through
in-context learning. A switch mechanism is implemented to exclude low-quality
ChatGPT-generated data. Through extensive experiments on seven benchmark text
classification datasets, we demonstrate that our proposed approach outperforms
the previous state-of-the-art gradient-free black-box tuning method by 5.7% on
average. Even without using ChatGPT-augmented data, CrossTune performs better
or comparably than previous black-box tuning methods, suggesting the
effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eye-gaze Guided Multi-modal Alignment Framework for Radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Ma, Hanqi Jiang, Wenting Chen, Zihao Wu, Xiaowei Yu, Fang Zeng, Lei Guo, Dajiang Zhu, Tuo Zhang, Dinggang Shen, Tianming Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-modal frameworks, the alignment of cross-modal features presents a
significant challenge. The predominant approach in multi-modal pre-training
emphasizes either global or local alignment between modalities, utilizing
extensive datasets. This bottom-up driven method often suffers from a lack of
interpretability, a critical concern in radiology. Previous studies have
integrated high-level labels in medical images or text, but these still rely on
manual annotation, a costly and labor-intensive process. Our work introduces a
novel approach by using eye-gaze data, collected synchronously by radiologists
during diagnostic evaluations. This data, indicating radiologists' focus areas,
naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze
Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for
better alignment of image and text features, aiming to reduce reliance on
manual annotations and thus cut training costs. Our model demonstrates robust
performance, outperforming other state-of-the-art methods in zero-shot
classification and retrieval tasks. The incorporation of easily-obtained
eye-gaze data during routine radiological diagnoses signifies a step towards
minimizing manual annotation dependency. Additionally, we explore the impact of
varying amounts of eye-gaze data on model performance, highlighting the
feasibility and utility of integrating this auxiliary data into multi-modal
pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Third-Party Language Model Performance Prediction from <span class="highlight-title">Instruct</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Nadkarni, Yizhong Wang, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model-based instruction-following systems have lately shown
increasing performance on many benchmark tasks, demonstrating the capability of
adapting to a broad variety of instructions. However, such systems are often
not designed to be transparent about their limitations; a user may easily
prompt a model with an instruction without any idea of whether the responses
should be expected to be accurate, or if the system is even capable of
performing the task. We propose a third party performance prediction framework,
where a separate model is trained to predict the metric resulting from
evaluating an instruction-following system on a task while assuming access only
to its inputs and outputs at inference time. We perform this analysis with a
variety of both open and closed instruction-following models as well as
multiple performance predictors, and examine the effect of various factors such
as model size, number of training tasks, and prompt format. Our findings
indicate that third-party performance prediction is very challenging, and much
work remains in developing predictors that can automatically reveal the
limitations of modern instruction-following natural language processing
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSLM-S2ST: A Multitask Speech Language Model for Textless
  Speech-to-Speech Translation with Speaker Style Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been emerging research interest and advances in speech-to-speech
translation (S2ST), translating utterances from one language to another. This
work proposes Multitask Speech Language Model (MSLM), which is a decoder-only
speech language model trained in a multitask setting. Without reliance on text
training data, our model is able to support multilingual S2ST with speaker
style preserved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual Transfer for Natural Language Inference via Multilingual
  <span class="highlight-title">Prompt</span> Translator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Qiu, Yuechen Wang, Jiaxin Shi, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Based on multilingual pre-trained models, cross-lingual transfer with prompt
learning has shown promising effectiveness, where soft prompt learned in a
source language is transferred to target languages for downstream tasks,
particularly in the low-resource scenario. To efficiently transfer soft prompt,
we propose a novel framework, Multilingual Prompt Translator (MPT), where a
multilingual prompt translator is introduced to properly process crucial
knowledge embedded in prompt by changing language knowledge while retaining
task knowledge. Concretely, we first train prompt in source language and employ
translator to translate it into target prompt. Besides, we extend an external
corpus as auxiliary data, on which an alignment task for predicted answer
probability is designed to convert language knowledge, thereby equipping target
prompt with multilingual knowledge. In few-shot settings on XNLI, MPT
demonstrates superiority over baselines by remarkable improvements. MPT is more
prominent compared with vanilla prompting when transferring to languages quite
distinct from source language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Interpretable Hate Speech Detection using Large Language
  Model-extracted Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although social media platforms are a prominent arena for users to engage in
interpersonal discussions and express opinions, the facade and anonymity
offered by social media may allow users to spew hate speech and offensive
content. Given the massive scale of such platforms, there arises a need to
automatically identify and flag instances of hate speech. Although several hate
speech detection methods exist, most of these black-box methods are not
interpretable or explainable by design. To address the lack of
interpretability, in this paper, we propose to use state-of-the-art Large
Language Models (LLMs) to extract features in the form of rationales from the
input text, to train a base hate speech classifier, thereby enabling faithful
interpretability by design. Our framework effectively combines the textual
understanding capabilities of LLMs and the discriminative power of
state-of-the-art hate speech classifiers to make these classifiers faithfully
interpretable. Our comprehensive evaluation on a variety of social media hate
speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales,
and (2) the surprising retention of detector performance even after training to
ensure interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Speech <span class="highlight-title">Language Models</span> for <span class="highlight-title">Prompt</span>-Conditioned
  Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan Wang, Hongyu Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models (LMs) are promising for high-quality speech synthesis
through in-context learning. A typical speech LM takes discrete semantic units
as content and a short utterance as prompt, and synthesizes speech which
preserves the content's semantics but mimics the prompt's style. However, there
is no systematic understanding on how the synthesized audio is controlled by
the prompt and content. In this work, we conduct an empirical study of the
widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and
provide insights into the prompt design and content semantic units. Our
analysis reveals that heterogeneous and nonstationary prompts hurt the audio
quality in contrast to the previous finding that longer prompts always lead to
better synthesis. Moreover, we find that the speaker style of the synthesized
audio is also affected by the content in addition to the prompt. We further
show that semantic units carry rich acoustic information such as pitch, tempo,
volume and speech emphasis, which might be leaked from the content to the
synthesized audio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr3: Ask Large <span class="highlight-title">Language Models</span> Not to Give Off-Topic Answers in Open
  Domain Multi-Hop Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gao, Yiheng Zhu, Yuanbin Cao, Yinzhi Zhou, Zhen Wu, Yujie Chen, Shenglan Wu, Haoyuan Hu, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in
Natural Language Processing (NLP) by aiming to answer complex questions through
multi-step reasoning over retrieved information from external knowledge
sources. Recently, Large Language Models (LLMs) have demonstrated remarkable
performance in solving ODMHQA owing to their capabilities including planning,
reasoning, and utilizing tools. However, LLMs may generate off-topic answers
when attempting to solve ODMHQA, namely the generated answers are irrelevant to
the original questions. This issue of off-topic answers accounts for
approximately one-third of incorrect answers, yet remains underexplored despite
its significance. To alleviate this issue, we propose the
Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.
Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to
judge whether the generated answers are off-topic. In cases where an off-topic
answer is detected, the Corrector performs step-wise revisions along the
reversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final
answer becomes on-topic. Experimental results on the HotpotQA and
2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably
reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving
the performance in Exact Match (EM) by nearly 3% compared to the baseline
method without the Dr3 mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AraPoem<span class="highlight-title">BERT</span>: A <span class="highlight-title">Pretrain</span>ed Language Model for Arabic Poetry Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Qarah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic poetry, with its rich linguistic features and profound cultural
significance, presents a unique challenge to the Natural Language Processing
(NLP) field. The complexity of its structure and context necessitates advanced
computational models for accurate analysis. In this paper, we introduce
AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry
text. To demonstrate the effectiveness of the proposed model, we compared
AraPoemBERT with 5 different Arabic language models on various NLP tasks
related to Arabic poetry. The new model outperformed all other models and
achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT
achieved unprecedented accuracy in two out of three novel tasks: poet's gender
classification (99.34\% accuracy), and poetry sub-meter classification (97.79\%
accuracy). In addition, the model achieved an accuracy score in poems' rhyme
classification (97.73\% accuracy) which is almost equivalent to the best score
reported in this study. Moreover, the proposed model significantly outperformed
previous work and other comparative models in the tasks of poems' sentiment
analysis, achieving an accuracy of 78.95\%, and poetry meter classification
(99.03\% accuracy), while significantly expanding the scope of these two
problems. The dataset used in this study, contains more than 2.09 million
verses collected from online sources, each associated with various attributes
such as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the
effectiveness of the proposed model in understanding and analyzing Arabic
poetry, achieving state-of-the-art results in several tasks and outperforming
previous works and other language models included in the study. AraPoemBERT
model is publicly available on \url{https://huggingface.co/faisalq}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 11 figures, not published yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pipelined Biomedical Event Extraction Rivaling Joint Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchao Wu, Xuefeng Li, Jinghang Gu, Longhua Qian, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical event extraction is an information extraction task to obtain
events from biomedical text, whose targets include the type, the trigger, and
the respective arguments involved in an event. Traditional biomedical event
extraction usually adopts a pipelined approach, which contains trigger
identification, argument role recognition, and finally event construction
either using specific rules or by machine learning. In this paper, we propose
an n-ary relation extraction method based on the BERT pre-training model to
construct Binding events, in order to capture the semantic information about an
event's context and its participants. The experimental results show that our
method achieves promising results on the GE11 and GE13 corpora of the BioNLP
shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates
that by significantly improving theperformance of Binding events, the overall
performance of the pipelined event extraction approach or even exceeds those of
current joint learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalizability of Extracting Social Determinants of Health
  Using Large <span class="highlight-title">Language Models</span> through <span class="highlight-title">Prompt</span>-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Peng, Zehao Yu, Kaleb E Smith, Wei-Hsuan Lo-Ciganic, Jiang Bian, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in natural language processing (NLP) using large language models
(LLMs) has greatly improved patient information extraction from clinical
narratives. However, most methods based on the fine-tuning strategy have
limited transfer learning ability for cross-domain applications. This study
proposed a novel approach that employs a soft prompt-based learning
architecture, which introduces trainable prompts to guide LLMs toward desired
outputs. We examined two types of LLM architectures, including encoder-only
GatorTron and decoder-only GatorTronGPT, and evaluated their performance for
the extraction of social determinants of health (SDoH) using a
cross-institution dataset from the 2022 n2c2 challenge and a cross-disease
dataset from the University of Florida (UF) Health. The results show that
decoder-only LLMs with prompt tuning achieved better performance in
cross-domain applications. GatorTronGPT achieved the best F1 scores for both
datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a
cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rank<span class="highlight-title">Prompt</span>: Step-by-Step Comparisons Make <span class="highlight-title">Language Models</span> Better
  Reasoners <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved impressive performance across
various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT
are prone to logical errors during their reasoning processes. Existing
solutions, which include deploying task-specific verifiers or voting over
multiple reasoning paths, either require extensive human annotations or fail in
scenarios with inconsistent responses. To address these challenges, we
introduce RankPrompt, a new prompting method that enables LLMs to self-rank
their responses without additional resources. RankPrompt breaks down the
ranking problem into a series of comparisons among diverse responses,
leveraging the inherent capabilities of LLMs to generate chains of comparison
as contextual exemplars. Our experiments across 11 arithmetic and commonsense
reasoning tasks show that RankPrompt significantly enhances the reasoning
performance of ChatGPT and GPT-4, with improvements of up to 13\%. RankPrompt
also excels in LLM-based automatic evaluations for open-ended generation,
aligning with human preferences 74\% of the time in the AlpacaEval set.
Moreover, RankPrompt demonstrates robustness against variations in the
orderings and consistencies of responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-Coling 2024 Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characteristic AI Agents via Large <span class="highlight-title">Language Models</span> <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Wang, Hongliang Dai, Shen Gao, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Large Language Models (LLMs) has led to significant
enhancements in the performance of chatbot systems. Many researchers have
dedicated their efforts to the development of bringing characteristics to
chatbots. While there have been commercial products for developing role-driven
chatbots using LLMs, it is worth noting that academic research in this area
remains relatively scarce. Our research focuses on investigating the
performance of LLMs in constructing Characteristic AI Agents by simulating
real-life individuals across different settings. Current investigations have
primarily focused on act on roles with simple profiles. In response to this
research gap, we create a benchmark for the characteristic AI agents task,
including dataset, techniques, and evaluation metrics. A dataset called
``Character100'' is built for this benchmark, comprising the most-visited
people on Wikipedia for language models to role-play. With the constructed
dataset, we conduct comprehensive assessment of LLMs across various settings.
In addition, we devise a set of automatic metrics for quantitative performance
evaluation. The experimental results underscore the potential directions for
further improvement in the capabilities of LLMs in constructing characteristic
AI agents. The benchmark is available at
https://github.com/nuaa-nlp/Character100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2024,The benchmark is available at:
  https://github.com/nuaa-nlp/Character100</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Wav2Gloss: Generating Interlinear Glossed Text from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiqi He, Kwanghee Choi, Lindia Tjuatja, Nathaniel R. Robinson, Jiatong Shi, Shinji Watanabe, <span class="highlight-author">Graham Neubig</span>, David R. Mortensen, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thousands of the world's languages are in danger of extinction--a tremendous
threat to cultural identities and human language diversity. Interlinear Glossed
Text (IGT) is a form of linguistic annotation that can support documentation
and resource creation for these languages' communities. IGT typically consists
of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4)
free translations to a majority language. We propose Wav2Gloss: a task to
extract these four annotation components automatically from speech, and
introduce the first dataset to this end, Fieldwork: a corpus of speech with all
these annotations covering 37 languages with standard formatting and
train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods,
with analysis suggesting that pre-trained decoders assist with translation and
glossing, that multi-task and multilingual approaches are underperformant, and
that end-to-end systems perform better than cascaded systems, despite the
text-only systems' advantages. We provide benchmarks to lay the ground work for
future research on IGT generation from speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-generated Replay Memories for Continual Neural Machine Translation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Resta, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Neural Machine Translation systems exhibit strong performance in
several different languages and are constantly improving. Their ability to
learn continuously is, however, still severely limited by the catastrophic
forgetting issue. In this work, we leverage a key property of encoder-decoder
Transformers, i.e. their generative ability, to propose a novel approach to
continually learning Neural Machine Translation systems. We show how this can
effectively learn on a stream of experiences comprising different languages, by
leveraging a replay memory populated by using the model itself as a generator
of parallel sentences. We empirically demonstrate that our approach can
counteract catastrophic forgetting without requiring explicit memorization of
training data. Code will be publicly available upon publication. Code:
https://github.com/m-resta/sg-rep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encode Once and Decode in Parallel: Efficient <span class="highlight-title">Transformer</span> Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Ru Lu, Nikita Haduong, Chien-Yu Lin, Hao Cheng, Noah A. Smith, Mari Ostendorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based NLP models are powerful but have high computational costs
that limit deployment scenarios. Finetuned encoder-decoder models are popular
in specialized domains and can outperform larger more generalized decoder-only
models, such as GPT-4. We introduce a new configuration for encoder-decoder
models that improves efficiency on structured output and question-answering
tasks where multiple outputs are required of a single input. Our method,
prompt-in-decoder (PiD), encodes the input once and decodes output in parallel,
boosting both training and inference efficiency by avoiding duplicate input
encoding, thereby reducing the decoder's memory footprint. We achieve
computation reduction that roughly scales with the number of subtasks, gaining
up to 4.6x speed-up over state-of-the-art models for dialogue state tracking,
summarization, and question-answering tasks with comparable or better
performance. We release our training/inference code and checkpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures.
  https://github.com/boru-roylu/encode-once-and-decode-in-parallel</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unsupervised Question Answering System with Multi-level
  Summarization for Legal Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M Manvith Prabhu, Haricharana Srinivasa, Anand Kumar M
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal
Argument Reasoning in Civil Procedure. To address this Binary Classification
task, which was daunting due to the complexity of the Legal Texts involved, we
propose a simple yet novel similarity and distance-based unsupervised approach
to generate labels. Further, we explore the Multi-level fusion of Legal-Bert
embeddings using ensemble features, including CNN, GRU, and LSTM. To address
the lengthy nature of Legal explanation in the dataset, we introduce T5-based
segment-wise summarization, which successfully retained crucial information,
enhancing the model's performance. Our unsupervised system witnessed a 20-point
increase in macro F1-score on the development set and a 10-point increase on
the test set, which is promising given its uncomplicated architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying
  Structure of Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Singhvi, Andrej Erkelens, Raghav Jain, Diganta Misra, Naomi Saphra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring nonlinear feature interaction is an established approach to
understanding complex patterns of attribution in many models. In this paper, we
use Shapley Taylor interaction indices (STII) to analyze the impact of
underlying data structure on model representations in a variety of modalities,
tasks, and architectures. Considering linguistic structure in masked and
auto-regressive language models (MLMs and ALMs), we find that STII increases
within idiomatic expressions and that MLMs scale STII with syntactic distance,
relying more on syntax in their nonlinear structure than ALMs do. Our speech
model findings reflect the phonetic principal that the openness of the oral
cavity determines how much a phoneme varies based on its context. Finally, we
study image classifiers and illustrate that feature interactions intuitively
reflect object boundaries. Our wide range of results illustrates the benefits
of interdisciplinary work and domain expertise in interpretability research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Summarization of Doctor-Patient Encounter Dialogues Using
  Large Language Model through <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxian Lyu, Cheng Peng, Xiaohan Li, Patrick Balian, Jiang Bian, Yonghui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic text summarization (ATS) is an emerging technology to assist
clinicians in providing continuous and coordinated care. This study presents an
approach to summarize doctor-patient dialogues using generative large language
models (LLMs). We developed prompt-tuning algorithms to instruct generative
LLMs to summarize clinical text. We examined the prompt-tuning strategies, the
size of soft prompts, and the few-short learning ability of GatorTronGPT, a
generative clinical LLM developed using 277 billion clinical and general
English words with up to 20 billion parameters. We compared GatorTronGPT with a
previous solution based on fine-tuning of a widely used T5 model, using a
clinical benchmark dataset MTS-DIALOG. The experimental results show that the
GatorTronGPT- 20B model achieved the best performance on all evaluation
metrics. The proposed solution has a low computing cost as the LLM parameters
are not updated during prompt-tuning. This study demonstrates the efficiency of
generative clinical LLMs for clinical ATS through prompt tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient
  Low-Rank Adaptation of Large <span class="highlight-title">Pre-train</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushi Qiang, Ruiyi Zhang, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale
pre-trained models in downstream tasks by learning low-rank incremental
matrices. Though LoRA and its variants effectively reduce the number of
trainable parameters compared to full fine-tuning methods, they often overfit
training data, resulting in sub-optimal generalization on test data. To address
this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning
approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular
value decomposition to parameterize low-rank incremental matrices and splits
the training of pseudo singular vectors and values across two different subsets
of training data. This division, embedded within separate levels of the BLO
framework, mitigates the risk of overfitting to a single dataset. Tested on ten
datasets covering natural language understanding and generation tasks and
applied to various well-known large pre-trained models, BiLoRA significantly
outperforms LoRA methods and other fine-tuning approaches, with similar amounts
of trainable parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RigorLLM: Resilient Guardrails for Large <span class="highlight-title">Language Models</span> against
  Undesired Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have showcased remarkable
capabilities across various tasks in different domains. However, the emergence
of biases and the potential for generating harmful content in LLMs,
particularly under malicious inputs, pose significant challenges. Current
mitigation strategies, while effective, are not resilient under adversarial
attacks. This paper introduces Resilient Guardrails for Large Language Models
(RigorLLM), a novel framework designed to efficiently and effectively moderate
harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted
approach that includes energy-based training data augmentation through Langevin
dynamics, optimizing a safe suffix for inputs via minimax optimization, and
integrating a fusion-based model combining robust KNN with LLMs based on our
data augmentation, RigorLLM offers a robust solution to harmful content
moderation. Our experimental evaluations demonstrate that RigorLLM not only
outperforms existing baselines like OpenAI API and Perspective API in detecting
harmful content but also exhibits unparalleled resilience to jailbreaking
attacks. The innovative use of constrained optimization and a fusion-based
guardrail approach represents a significant step forward in developing more
secure and reliable LLMs, setting a new standard for content moderation
frameworks in the face of evolving digital threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRESS: <span class="highlight-title">Instruct</span>ing Large Vision-<span class="highlight-title">Language Models</span> to Align and Interact
  with Humans via Natural Language Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DRESS, a large vision language model (LVLM) that innovatively
exploits Natural Language feedback (NLF) from Large Language Models to enhance
its alignment and interactions by addressing two key limitations in the
state-of-the-art LVLMs. First, prior LVLMs generally rely only on the
instruction finetuning stage to enhance alignment with human preferences.
Without incorporating extra feedback, they are still prone to generate
unhelpful, hallucinated, or harmful responses. Second, while the visual
instruction tuning data is generally structured in a multi-turn dialogue
format, the connections and dependencies among consecutive conversational turns
are weak. This reduces the capacity for effective multi-turn interactions. To
tackle these, we propose a novel categorization of the NLF into two key types:
critique and refinement. The critique NLF identifies the strengths and
weaknesses of the responses and is used to align the LVLMs with human
preferences. The refinement NLF offers concrete suggestions for improvement and
is adopted to improve the interaction ability of the LVLMs-- which focuses on
LVLMs' ability to refine responses by incorporating feedback in multi-turn
interactions. To address the non-differentiable nature of NLF, we generalize
conditional reinforcement learning for training. Our experimental results
demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and
harmless (21.03%) responses, and more effectively learn from feedback during
multi-turn interactions compared to SOTA LVMLs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. The feedback datasets are released at:
  https://huggingface.co/datasets/YangyiYY/LVLM_NLF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls
  of Large <span class="highlight-title">Language Models</span> on Bengali NLP <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as one of the most important
breakthroughs in NLP for their impressive skills in language generation and
other language-specific tasks. Though LLMs have been evaluated in various
tasks, mostly in English, they have not yet undergone thorough evaluation in
under-resourced languages such as Bengali (Bangla). To this end, this paper
introduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to
benchmark their performance in the Bengali language that has modest resources.
In this regard, we select various important and diverse Bengali NLP tasks, such
as text summarization, question answering, paraphrasing, natural language
inference, transliteration, text classification, and sentiment analysis for
zero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and
Claude-2. Our experimental results demonstrate that while in some Bengali NLP
tasks, zero-shot LLMs could achieve performance on par, or even better than
current SOTA fine-tuned models; in most tasks, their performance is quite poor
(with the performance of open-source LLMs like LLaMA-2-13b-chat being
significantly bad) in comparison to the current SOTA results. Therefore, it
calls for further efforts to develop a better understanding of LLMs in
modest-resourced languages like Bengali.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024. The first two authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radiology-<span class="highlight-title">GPT</span>: A Large Language Model for Radiology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li, Tianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Radiology-GPT, a large language model for radiology. Using an
instruction tuning approach on an extensive dataset of radiology domain
knowledge, Radiology-GPT demonstrates superior performance compared to general
language models such as StableLM, Dolly and LLaMA. It exhibits significant
versatility in radiological diagnosis, research, and communication. This work
serves as a catalyst for future developments in clinical NLP. The successful
implementation of Radiology-GPT is indicative of the potential of localizing
generative large language models, specifically tailored for distinctive medical
specialties, while ensuring adherence to privacy standards such as HIPAA. The
prospect of developing individualized, large-scale language models that cater
to specific needs of various hospitals presents a promising direction. The
fusion of conversational competence and domain-specific knowledge in these
models is set to foster future development in healthcare AI. A demo of
Radiology-GPT is available at
https://huggingface.co/spaces/allen-eric/radiology-gpt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10311v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10311v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Ferrer-i-Cancho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The word order of a sentence is shaped by multiple principles. The principle
of syntactic dependency distance minimization is in conflict with the principle
of surprisal minimization (or predictability maximization) in single head
syntactic dependency structures: while the former predicts that the head should
be placed at the center of the linear arrangement, the latter predicts that the
head should be placed at one of the ends (either first or last). A critical
question is when surprisal minimization (or predictability maximization) should
surpass syntactic dependency distance minimization. In the context of single
head structures, it has been predicted that this is more likely to happen when
two conditions are met, i.e. (a) fewer words are involved and (b) words are
shorter. Here we test the prediction on the noun phrase when it is composed of
a demonstrative, a numeral, an adjective and a noun. We find that, across
preferred orders in languages, the noun tends to be placed at one of the ends,
confirming the theoretical prediction. We also show evidence of anti locality
effects: syntactic dependency distances in preferred orders are longer than
expected by chance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Many typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety-Tuned <span class="highlight-title">LLaMA</span>s: Lessons From Improving the Safety of Large Language
  Models that Follow <span class="highlight-title">Instruct</span>ions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07875v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07875v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models to follow instructions makes them perform
better on a wide range of tasks and generally become more helpful. However, a
perfectly helpful model will follow even the most malicious instructions and
readily generate harmful content. In this paper, we raise concerns over the
safety of models that only emphasize helpfulness, not harmlessness, in their
instruction-tuning. We show that several popular instruction-tuned models are
highly unsafe. Moreover, we show that adding just 3% safety examples (a few
hundred demonstrations) when fine-tuning a model like LLaMA can substantially
improve its safety. Our safety-tuning does not make models significantly less
capable or helpful as measured by standard benchmarks. However, we do find
exaggerated safety behaviours, where too much safety-tuning makes models refuse
perfectly safe prompts if they superficially resemble unsafe ones. As a whole,
our results illustrate trade-offs in training LLMs to be helpful and training
them to be safe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Reducing Diagnostic Errors with Interpretable Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C. Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Physicians Know How to <span class="highlight-title">Prompt</span>? The Need for Automatic <span class="highlight-title">Prompt</span>
  Optimization Help in Clinical Note <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anique Tahir, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impossible Distillation: from Low-Quality Model to High-Quality <span class="highlight-title">Dataset</span>
  & Model for Summarization and Paraphrasing <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Impossible Distillation, a novel framework for paraphrasing and
sentence summarization, that distills a high-quality dataset and model from a
low-quality teacher that itself cannot perform these tasks. Unlike prior works
that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific
architecture, we hypothesize and verify the paraphrastic proximity intrinsic to
pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in
the LM distribution. By identifying and distilling generations from these
subspaces, Impossible Distillation produces a high-quality dataset and model
even from GPT2-scale LMs. We evaluate our method on multiple benchmarks
spanning unconstrained / syntax-controlled paraphrase generation and sentence
summarization. Our model with 770M parameters consistently outperforms strong
baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT
itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher
diversity and fidelity than up to 13 times larger datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with
  Conditional Generative Adversarial Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rossi Kamal, Zuzana Kubincova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Education is a right of all, however, every individual is different than
others. Teachers in post-communism era discover inherent individualism to
equally train all towards job market of fourth industrial revolution. We can
consider scenario of ethnic minority education in academic practices. Ethnic
minority group has grown in their own culture and would prefer to be taught in
their native way. We have formulated such linguistic anthropology(how people
learn)based engagement as semi-supervised problem. Then, we have developed an
conditional deep generative adversarial network algorithm namely LA-GAN to
classify linguistic ethnographic features in student engagement. Theoretical
justification proves the objective, regularization and loss function of our
semi-supervised adversarial model. Survey questions are prepared to reach some
form of assumptions about z-generation and ethnic minority group, whose
learning style, learning approach and preference are our main area of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multi-Domain Automatic Short Answer Grading through an
  Explainable Neuro-Symbolic Pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Künnecke, Anna Filighera, Colin Leong, Tim Steuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grading short answer questions automatically with interpretable reasoning
behind the grading decision is a challenging goal for current transformer
approaches. Justification cue detection, in combination with logical reasoners,
has shown a promising direction for neuro-symbolic architectures in ASAG. But,
one of the main challenges is the requirement of annotated justification cues
in the students' responses, which only exist for a few ASAG datasets. To
overcome this challenge, we contribute (1) a weakly supervised annotation
procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic
model for explainable ASAG based on justification cues. Our approach improves
upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short
Answer Feedback dataset in a bilingual, multi-domain, and multi-question
training setup. This result shows that our approach provides a promising
direction for generating high-quality grades and accompanying explanations for
future research in ASAG and educational NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
  Data Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific data visualization plays a crucial role in research by enabling
the direct display of complex information and assisting researchers in
identifying implicit patterns. Despite its importance, the use of Large
Language Models (LLMs) for scientific data visualization remains rather
unexplored. In this study, we introduce MatPlotAgent, an efficient
model-agnostic LLM agent framework designed to automate scientific data
visualization tasks. Leveraging the capabilities of both code LLMs and
multi-modal LLMs, MatPlotAgent consists of three core modules: query
understanding, code generation with iterative debugging, and a visual feedback
mechanism for error correction. To address the lack of benchmarks in this
field, we present MatPlotBench, a high-quality benchmark consisting of 100
human-verified test cases. Additionally, we introduce a scoring approach that
utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that
MatPlotAgent can improve the performance of various LLMs, including both
commercial and open-source models. Furthermore, the proposed evaluation method
shows a strong correlation with human-annotated scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiC-CLIP: Continual Training of CLIP Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to \emph{continually} train these models. This problem is
exacerbated by the lack of any large scale continual learning benchmarks or
baselines. We introduce the first set of web-scale Time-Continual (TiC)
benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and
TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped
image-text pairs spanning 9 years (2014--2022). We first use our benchmarks to
curate various \emph{dynamic} evaluations to measure temporal robustness of
existing models. We show OpenAI's CLIP (trained on data up to 2020) loses
$\approx 8\%$ zero-shot accuracy on our curated retrieval task from 2021--2022
compared with more recently trained models in OpenCLIP repository. We then
study how to efficiently train models on time-continuous data. We demonstrate
that a simple rehearsal-based approach that continues training from the last
checkpoint and replays old data reduces compute by $2.5\times$ when compared to
the standard practice of retraining from scratch. Code is available at
https://github.com/apple/ml-tic-clip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of
  <span class="highlight-title">Language Models</span> with Hypothesis Refinement <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, <span class="highlight-author">Bailin Wang</span>, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to derive underlying principles from a handful of observations
and then generalize to novel situations -- known as inductive reasoning -- is
central to human intelligence. Prior work suggests that language models (LMs)
often fall short on inductive reasoning, despite achieving impressive success
on research benchmarks. In this work, we conduct a systematic study of the
inductive reasoning capabilities of LMs through iterative hypothesis
refinement, a technique that more closely mirrors the human inductive process
than standard input-output prompting. Iterative hypothesis refinement employs a
three-step process: proposing, selecting, and refining hypotheses in the form
of textual rules. By examining the intermediate rules, we observe that LMs are
phenomenal hypothesis proposers (i.e., generating candidate rules), and when
coupled with a (task-specific) symbolic interpreter that is able to
systematically filter the proposed set of rules, this hybrid approach achieves
strong results across inductive reasoning benchmarks that require inducing
causal relations, language-like instructions, and symbolic concepts. However,
they also behave as puzzling inductive reasoners, showing notable performance
gaps between rule induction (i.e., identifying plausible rules) and rule
application (i.e., applying proposed rules to instances), suggesting that LMs
are proposing hypotheses without being able to actually apply the rules.
Through empirical and human analyses, we further reveal several discrepancies
between the inductive reasoning processes of LMs and humans, shedding light on
both the potentials and limitations of using LMs in inductive reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Liu, Jingjing Wang, Jiamin Luo, Tao Zeng, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,
Question-Answering and Dialogue) has attracted ever-more interest in recent
years and achieved important progresses. However, existing studies on
interactive ASU largely ignore the coreference issue for opinion targets (i.e.,
aspects), while this phenomenon is ubiquitous in interactive scenarios
especially dialogues, limiting the ASU performance. Recently, large language
models (LLMs) shows the powerful ability to integrate various NLP tasks with
the chat paradigm. In this way, this paper proposes a new Chat-based Aspect
Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in
understanding aspect sentiments in dialogue scenarios. Particularly, this
ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to
address the aspect coreference issue. On this basis, we propose a Trusted
Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.
Specifically, this TSA treats the ACR task as an auxiliary task to boost the
performance of the primary ASU task, and further integrates trusted learning
into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination
problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to
evaluate TSA, and extensive experiments show that our proposed TSA can
significantly outperform several state-of-the-art baselines, justifying the
effectiveness of TSA to ChatASU and the importance of considering the
coreference and hallucination issues in ChatASU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Multimodal Entity Linking <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12725v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12725v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Entity Linking (MEL) is the task of mapping mentions with
multimodal contexts to the referent entities from a knowledge base. Existing
MEL methods mainly focus on designing complex multimodal interaction mechanisms
and require fine-tuning all model parameters, which can be prohibitively costly
and difficult to scale in the era of Large Language Models (LLMs). In this
work, we propose GEMEL, a Generative Multimodal Entity Linking framework based
on LLMs, which directly generates target entity names. We keep the vision and
language model frozen and only train a feature mapper to enable cross-modality
interactions. To adapt LLMs to the MEL task, we leverage the in-context
learning capability of LLMs by retrieving multimodal instances as
demonstrations. Extensive experiments show that, with only ~0.3% of the model
parameters fine-tuned, GEMEL achieves state-of-the-art results on two
well-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%
accuracy gains on WikiMEL). The performance gain stems from mitigating the
popularity bias of LLM predictions and disambiguating less common entities
effectively. Further analysis verifies the generality and scalability of GEMEL.
Our framework is compatible with any off-the-shelf language model, paving the
way towards an efficient and general solution for utilizing LLMs in the MEL
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy
issues, which means they are unaware of unseen events or generate text with
incorrect facts owing to outdated/noisy data. To this end, many knowledge
editing approaches for LLMs have emerged -- aiming to subtly inject/edit
updated knowledge or adjust undesired behavior while minimizing the impact on
unrelated inputs. Nevertheless, due to significant differences among various
knowledge editing methods and the variations in task setups, there is no
standard implementation framework available for the community, which hinders
practitioners from applying knowledge editing to applications. To address these
issues, we propose EasyEdit, an easy-to-use knowledge editing framework for
LLMs. It supports various cutting-edge knowledge editing approaches and can be
readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.
Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,
demonstrating that knowledge editing surpasses traditional fine-tuning in terms
of reliability and generalization. We have released the source code on GitHub,
along with Google Colab tutorials and comprehensive documentation for beginners
to get started. Besides, we present an online system for real-time knowledge
editing, and a demo video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zjunlp/EasyEdit HF Demo:
  https://huggingface.co/spaces/zjunlp/EasyEdit Video:
  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeTI: Learning to Generate from Textual Interactions <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained language models (LMs) is essential for enhancing
their capabilities. Existing techniques commonly fine-tune on input-output
pairs (e.g., instruction tuning) or with numerical rewards that gauge the
output quality (e.g., RLHF). We explore LMs' potential to learn from textual
interactions (LETI) that not only check their correctness with binary labels
but also pinpoint and explain errors in their outputs through textual feedback.
Our focus is the code generation task, where the model produces code based on
natural language instructions. This setting invites a natural and scalable way
to acquire textual feedback: the error messages and stack traces from code
execution using a Python interpreter. LETI iteratively fine-tunes the model,
using the LM objective, on a concatenation of natural language instructions,
LM-generated programs, and textual feedback. Prepended to this fine-tuning
text, a binary reward token is used to differentiate correct and buggy
solutions. LETI requires no ground-truth outputs for training and even
outperforms a fine-tuned baseline that does. LETI not only improves the
performance of LMs on a code generation dataset MBPP, but also generalizes to
other datasets. Trained on MBPP, it achieves comparable or better performance
than the base LMs on unseen problems in HumanEval. Furthermore, compared to
binary feedback, we observe that textual feedback leads to improved generation
quality and sample efficiency, achieving the same performance with fewer than
half of the gradient steps. LETI is equally applicable in natural language
tasks when they can be formulated as code generation, which we empirically
verified on event argument extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese
  Address Entity Recognition <span class="highlight-title">Dataset</span> for UAV Delivery <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame
\textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task
of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle
delivery systems. The dataset encompasses a diverse range of five categories,
enabling comprehensive training and evaluation of NER models. To construct this
dataset, we sourced the data from a real-world UAV delivery system and
conducted a rigorous data cleaning and desensitization process to ensure
privacy and data integrity. The resulting dataset, consisting of around 12,000
annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage
\textbf{M}odel annotation. We evaluated classical NER models on our dataset and
provided in-depth analysis. The dataset and models are publicly available at
\url{https://github.com/zhhvvv/CNER-UAV}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TheWebConf'24 (WWW'24) as a Resource Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Large Language Model (KG-LLM) for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07311v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07311v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Continuous Character-based Language from Non-invasive Brain
  Recordings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cenyuan Zhang, Xiaoqing Zheng, Ruicheng Yin, Shujie Geng, Jianhan Xu, Xuan Gao, Changze Lv, Zixuan Ling, Xuanjing Huang, Miao Cao, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deciphering natural language from brain activity through non-invasive devices
remains a formidable challenge. Previous non-invasive decoders either require
multiple experiments with identical stimuli to pinpoint cortical regions and
enhance signal-to-noise ratios in brain activity, or they are limited to
discerning basic linguistic elements such as letters and words. We propose a
novel approach to decoding continuous language from single-trial non-invasive
fMRI recordings, in which a three-dimensional convolutional network augmented
with information bottleneck is developed to automatically identify responsive
voxels to stimuli, and a character-based decoder is designed for the semantic
reconstruction of continuous language characterized by inherent character
structures. The resulting decoder can produce intelligible textual sequences
that faithfully capture the meaning of perceived speech both within and across
subjects, while existing decoders exhibit significantly inferior performance in
cross-subject contexts. The ability to decode continuous language from single
trials across subjects demonstrates the promising applications of non-invasive
language brain-computer interfaces in both healthcare and neuroscience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling
  Capacities of Large <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved dramatic proficiency over NLP
tasks with normal length. Recently, multiple studies have committed to
extending the context length and enhancing the long text modeling capabilities
of LLMs. To comprehensively evaluate the long context ability of LLMs, we
propose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed
with four principles: comprehensive capacity evaluation, avoidance of data
contamination, accurate automatic evaluation, and different length levels. It
consists of 10 datasets from 5 different long text understanding tasks, i.e.
question answering, hallucination detection, text sorting, language modeling,
and code completion, to cover core capacities and various domains of LLMs. We
conduct experiments with five long context models on BAMBOO and further discuss
four key research questions of long text. We also qualitatively analyze current
long context models and point out future directions for enhancing long text
modeling capacities. We release our data, prompts, and code at
https://github.com/RUCAIBox/BAMBOO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CASIMIR: A Corpus of Scientific Articles enhanced with Multiple
  Author-Integrated Revisions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leane Jourdan, Florian Boudin, Nicolas Hernandez, Richard Dufour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing a scientific article is a challenging task as it is a highly codified
and specific genre, consequently proficiency in written communication is
essential for effectively conveying research findings and ideas. In this
article, we propose an original textual resource on the revision step of the
writing process of scientific articles. This new dataset, called CASIMIR,
contains the multiple revised versions of 15,646 scientific articles from
OpenReview, along with their peer reviews. Pairs of consecutive versions of an
article are aligned at sentence-level while keeping paragraph location
information as metadata for supporting future revision studies at the discourse
level. Each pair of revised sentences is enriched with automatically extracted
edits and associated revision intention. To assess the initial quality on the
dataset, we conducted a qualitative study of several state-of-the-art text
revision approaches and compared various evaluation metrics. Our experiments
led us to question the relevance of the current evaluation methods for the text
revision task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shared and Private Information Learning in Multimodal Sentiment Analysis
  with Deep Modal Alignment and <span class="highlight-title">Self-supervised</span> Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songning Lai, Jiakang Li, Guinan Guo, Xifeng Hu, Yulong Li, Yuan Tan, Zichen Song, Yutong Liu, Zhaoxia Ren, Chun Wan, Danmin Miao, Zhi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an effective representation learning method for multimodal
sentiment analysis tasks is a crucial research direction. The challenge lies in
learning both shared and private information in a complete modal
representation, which is difficult with uniform multimodal labels and a raw
feature fusion approach. In this work, we propose a deep modal shared
information learning module based on the covariance matrix to capture the
shared information between modalities. Additionally, we use a label generation
module based on a self-supervised learning strategy to capture the private
information of the modalities. Our module is plug-and-play in multimodal tasks,
and by changing the parameterization, it can adjust the information exchange
relationship between the modes and learn the private or shared information
between the specified modes. We also employ a multi-task learning strategy to
help the model focus its attention on the modal differentiation training data.
We provide a detailed formulation derivation and feasibility proof for the
design of the deep modal shared information learning module. We conduct
extensive experiments on three common multimodal sentiment analysis baseline
datasets, and the experimental results validate the reliability of our model.
Furthermore, we explore more combinatorial techniques for the use of the
module. Our approach outperforms current state-of-the-art methods on most of
the metrics of the three public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented Large Language Models (TALM) are known to enhance the skillset
of large language models (LLM), thereby, leading to their improved reasoning
abilities across many tasks. While, TALMs have been successfully employed in
different question-answering benchmarks, their efficacy on complex mathematical
reasoning benchmarks, and the potential complimentary benefits offered by tools
for knowledge retrieval and mathematical equation solving, are open research
questions. In this work, we present MATHSENSEI, a tool-augmented large language
model for mathematical reasoning. Augmented with tools for knowledge retrieval
(Bing Web Search), program execution (Python), and symbolic equation solving
(Wolfram-Alpha), we study the complimentary benefits of these tools through
evaluations on mathematical reasoning datasets. We perform exhaustive ablations
on MATH,a popular dataset for evaluating mathematical reasoning on diverse
mathematical disciplines. We also conduct experiments involving well-known tool
planners to study the impact of tool sequencing on the model performance.
MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with
chain-of-thought on the MATH dataset. We further observe that TALMs are not as
effective for simpler math word problems (in GSM-8k), and the benefit increases
as the complexity and required knowledge increases (progressively over AQuA,
MMLU-Math, and higher level complex questions in MATH). The code and data are
available at https://github.com/Debrup-61/MathSensei.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Komodo: A Linguistic Expedition into Indonesia's Regional Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Owen, Vishesh Tripathi, Abhay Kumar, Biddwan Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent breakthroughs in Large Language Models (LLMs) have mostly focused
on languages with easily available and sufficient resources, such as English.
However, there remains a significant gap for languages that lack sufficient
linguistic resources in the public domain. Our work introduces Komodo-7B,
7-billion-parameter Large Language Models designed to address this gap by
seamlessly operating across Indonesian, English, and 11 regional languages in
Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and
Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art
performance in various tasks and languages, outperforming the benchmarks set by
OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,
Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only
demonstrates superior performance in both language-specific and overall
assessments but also highlights its capability to excel in linguistic
diversity. Our commitment to advancing language models extends beyond
well-resourced languages, aiming to bridge the gap for those with limited
linguistic assets. Additionally, Komodo-7B-Instruct's better cross-language
understanding contributes to addressing educational disparities in Indonesia,
offering direct translations from English to 11 regional languages, a
significant improvement compared to existing language translation services.
Komodo-7B represents a crucial step towards inclusivity and effectiveness in
language models, providing to the linguistic needs of diverse communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 Pages, 8 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KorNAT: LLM Alignment Benchmark for Korean Social Values and Common
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For Large Language Models (LLMs) to be effectively deployed in a specific
country, they must possess an understanding of the nation's culture and basic
knowledge. To this end, we introduce National Alignment, which measures an
alignment between an LLM and a targeted country from two aspects: social value
alignment and common knowledge alignment. Social value alignment evaluates how
well the model understands nation-specific social values, while common
knowledge alignment examines how well the model captures basic knowledge
related to the nation. We constructed KorNAT, the first benchmark that measures
national alignment with South Korea. For the social value dataset, we obtained
ground truth labels from a large-scale survey involving 6,174 unique Korean
participants. For the common knowledge dataset, we constructed samples based on
Korean textbooks and GED reference materials. KorNAT contains 4K and 6K
multiple-choice questions for social value and common knowledge, respectively.
Our dataset creation process is meticulously designed and based on statistical
sampling theory and was refined through multiple rounds of human review. The
experiment results of seven LLMs reveal that only a few models met our
reference score, indicating a potential for further enhancement. KorNAT has
received government approval after passing an assessment conducted by a
government-affiliated organization dedicated to evaluating dataset quality.
Samples and detailed evaluation protocols of our dataset can be found in
https://selectstar.ai/ko/papers-national-alignment
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 7 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KG-Rank: Enhancing Large <span class="highlight-title">Language Models</span> for Medical QA with Knowledge
  Graphs and Ranking Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly advanced healthcare
innovation on generation capabilities. However, their application in real
clinical settings is challenging due to potential deviations from medical facts
and inherent biases. In this work, we develop an augmented LLM framework,
KG-Rank, which leverages a medical knowledge graph (KG) with ranking and
re-ranking techniques, aiming to improve free-text question-answering (QA) in
the medical domain. Specifically, upon receiving a question, we initially
retrieve triplets from a medical KG to gather factual information.
Subsequently, we innovatively apply ranking methods to refine the ordering of
these triplets, aiming to yield more precise answers. To the best of our
knowledge, KG-Rank is the first application of ranking models combined with KG
in medical QA specifically for generating long answers. Evaluation of four
selected medical QA datasets shows that KG-Rank achieves an improvement of over
18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it
realizes a 14% improvement in ROUGE-L, showing the effectiveness and potential
of KG-Rank.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) demonstrate that their
capabilities are comparable, or even superior, to humans in many tasks in
natural language processing. Despite this progress, LLMs are still inadequate
at social-cognitive reasoning, which humans are naturally good at. Drawing
inspiration from psychological research on the links between certain
personality traits and Theory-of-Mind (ToM) reasoning, and from prompt
engineering research on the hyper-sensitivity of prompts in affecting LLMs
capabilities, this study investigates how inducing personalities in LLMs using
prompts affects their ToM reasoning capabilities. Our findings show that
certain induced personalities can significantly affect the LLMs' reasoning
capabilities in three different ToM tasks. In particular, traits from the Dark
Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral
across the different ToM tasks. We find that LLMs that exhibit a higher
variance across personality prompts in ToM also tends to be more controllable
in personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and
Mistral can be controllably adjusted through our personality prompts. In
today's landscape where role-play is a common strategy when using LLMs, our
research highlights the need for caution, as models that adopt specific
personas with personalities potentially also alter their reasoning abilities in
an unexpected manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LifeTox: Unveiling Implicit Toxicity in Life Advice <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models become increasingly integrated into daily life,
detecting implicit toxicity across diverse contexts is crucial. To this end, we
introduce LifeTox, a dataset designed for identifying implicit toxicity within
a broad range of advice-seeking scenarios. Unlike existing safety datasets,
LifeTox comprises diverse contexts derived from personal experiences through
open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on
LifeTox matches or surpasses the zero-shot performance of large language models
in toxicity classification tasks. These results underscore the efficacy of
LifeTox in addressing the complex challenges inherent in implicit toxicity. We
open-sourced the
dataset\footnote{\url{https://huggingface.co/datasets/mbkim/LifeTox}} and the
LifeTox moderator family; 350M, 7B, and 13B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HateModerate: Testing Hate Speech Detectors against Content Moderation
  Policies <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangrui Zheng, Xueqing Liu, Guanqun Yang, Mirazul Haque, Xing Qian, Ravishka Rathnasuriya, Wei Yang, Girish Budhrani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To protect users from massive hateful content, existing works studied
automated hate speech detection. Despite the existing efforts, one question
remains: do automated hate speech detectors conform to social media content
policies? A platform's content policies are a checklist of content moderated by
the social media platform. Because content moderation rules are often uniquely
defined, existing hate speech datasets cannot directly answer this question.
  This work seeks to answer this question by creating HateModerate, a dataset
for testing the behaviors of automated content moderators against content
policies. First, we engage 28 annotators and GPT in a six-step annotation
process, resulting in a list of hateful and non-hateful test suites matching
each of Facebook's 41 hate speech policies. Second, we test the performance of
state-of-the-art hate speech detectors against HateModerate, revealing
substantial failures these models have in their conformity to the policies.
Third, using HateModerate, we augment the training data of a top-downloaded
hate detector on HuggingFace. We observe significant improvement in the models'
conformity to content policies while having comparable scores on the original
test data. Our dataset and code can be found in the attachment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Finding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think, Act, and Ask: Open-World Interactive Personalized Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07968v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07968v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards
open-vocabulary objects in unknown environments. The existing works of ZSON
mainly focus on following individual instructions to find generic object
classes, neglecting the utilization of natural language interaction and the
complexities of identifying user-specific objects. To address these
limitations, we introduce Zero-shot Interactive Personalized Object Navigation
(ZIPON), where robots need to navigate to personalized goal objects while
engaging in conversations with users. To solve ZIPON, we propose a new
framework termed Open-woRld Interactive persOnalized Navigation (ORION), which
uses Large Language Models (LLMs) to make sequential decisions to manipulate
different modules for perception, navigation and communication. Experimental
results show that the performance of interactive agents that can leverage user
feedback exhibits significant improvement. However, obtaining a good balance
between task completion and the efficiency of navigation and interaction
remains challenging for all methods. We further provide more findings on the
impact of diverse user feedback forms on the agents' performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:
  https://github.com/sled-group/navchat</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOLTA: Improving Generative Diversity by Variational Mutual Information
  Maximizing Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Dafeng Chi, Jingjing Li, Kai Song, Yuzheng Zhuang, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The natural language generation domain has witnessed great success thanks to
Transformer models. Although they have achieved state-of-the-art generative
quality, they often neglect generative diversity. Prior attempts to tackle this
issue suffer from either low model capacity or over-complicated architectures.
Some recent methods employ the VAE framework to enhance diversity, but their
latent variables fully depend on the input context, restricting exploration of
the latent space. In this paper, we introduce VOLTA, a framework that elevates
generative diversity by bridging Transformer with VAE via a more effective
cross-attention-based connection, departing from conventional embedding
concatenation or summation. Additionally, we propose integrating InfoGAN-style
latent codes to enable input-independent variability, further diversifying the
generation. Moreover, our framework accommodates discrete inputs alongside its
existing support for continuous inputs. We perform comprehensive experiments
with two types of Transformers on six datasets from three different NLG tasks
to show that our approach can significantly improve generative diversity while
maintaining generative quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Hallucination in Large Multi-Modal Models via Robust
  <span class="highlight-title">Instruct</span>ion Tuning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14565v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14565v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promising progress in multi-modal tasks, current large
multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions
with respect to the associated image and human instructions. This paper
addresses this issue by introducing the first large and diverse visual
instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.
Our dataset comprises 400k visual instructions generated by GPT4, covering 16
vision-and-language tasks with open-ended instructions and answers. Unlike
existing studies that primarily focus on positive instruction samples, we
design LRV-Instruction to include both positive and negative instructions for
more robust visual instruction tuning. Our negative instructions are designed
at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent
Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure
the hallucination generated by LMMs, we propose GPT4-Assisted Visual
Instruction Evaluation (GAVIE), a stable approach to evaluate visual
instruction tuning like human experts. GAVIE does not require human-annotated
groundtruth answers and can adapt to diverse instruction formats. We conduct
comprehensive experiments to investigate the hallucination of LMMs. Our results
demonstrate existing LMMs exhibit significant hallucinations when presented
with our negative instructions, particularly Existent Object and Knowledge
Manipulation instructions. Moreover, we successfully mitigate hallucination by
finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving
performance on several public datasets compared to state-of-the-art methods.
Additionally, we observed that a balanced ratio of positive and negative
instances in the training data leads to a more robust model. Code and data are
available at https://github.com/FuxiaoLiu/LRV-Instruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 32 figures, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health-LLM: Personalized Retrieval-Augmented Disease Prediction System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00746v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00746v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in artificial intelligence (AI), especially large
language models (LLMs), have significantly advanced healthcare applications and
demonstrated potentials in intelligent medical treatment. However, there are
conspicuous challenges such as vast data volumes and inconsistent symptom
characterization standards, preventing full integration of healthcare AI
systems with individual patients' needs. To promote professional and
personalized healthcare, we propose an innovative framework, Heath-LLM, which
combines large-scale feature extraction and medical knowledge trade-off
scoring. Compared to traditional health management applications, our system has
three main advantages: (1) It integrates health reports and medical knowledge
into a large model to ask relevant questions to large language model for
disease prediction; (2) It leverages a retrieval augmented generation (RAG)
mechanism to enhance feature extraction; (3) It incorporates a semi-automated
feature updating framework that can merge and delete features to improve
accuracy of disease prediction. We experiment on a large number of health
reports to assess the effectiveness of Health-LLM system. The results indicate
that the proposed system surpasses the existing ones and has the potential to
significantly advance disease prediction and personalized health management.
The code is available at https://github.com/jmyissb/HealthLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring and Improving Chain-of-Thought Reasoning in Vision-Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have recently demonstrated strong efficacy as
visual assistants that can parse natural queries about the visual content and
generate human-like outputs. In this work, we explore the ability of these
models to demonstrate human-like reasoning based on the perceived information.
To address a crucial concern regarding the extent to which their reasoning
capabilities are fully consistent and grounded, we also measure the reasoning
consistency of these models. We achieve this by proposing a chain-of-thought
(CoT) based consistency measure. However, such an evaluation requires a
benchmark that encompasses both high-level inference and detailed reasoning
chains, which is costly. We tackle this challenge by proposing a
LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously
ensuring the generation of a high-quality dataset. Based on this pipeline and
the existing coarse-grained annotated dataset, we build the CURE benchmark to
measure both the zero-shot reasoning performance and consistency of VLMs. We
evaluate existing state-of-the-art VLMs, and find that even the best-performing
model is unable to demonstrate strong visual reasoning capabilities and
consistency, indicating that substantial efforts are required to enable VLMs to
perform visual reasoning as systematically and consistently as humans. As an
early step, we propose a two-stage training framework aimed at improving both
the reasoning performance and consistency of VLMs. The first stage involves
employing supervised fine-tuning of VLMs using step-by-step reasoning samples
automatically generated by LLMs. In the second stage, we further augment the
training process by incorporating feedback provided by LLMs to produce
reasoning chains that are highly consistent and grounded. We empirically
highlight the effectiveness of our framework in both reasoning performance and
consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference. The data is released at
  https://github.com/Yangyi-Chen/CoTConsistency</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BooookScore: A systematic exploration of book-length summarization in
  the era of LLMs <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing book-length documents (>100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of
evaluation: existing book-length summarization datasets (e.g., BookSum) are in
the pretraining data of most public LLMs, and existing evaluation methods
struggle to capture errors made by modern LLM summarizers. In this paper, we
present the first study of the coherence of LLM-based book-length summarizers
implemented via two prompting workflows: (1) hierarchically merging chunk-level
summaries, and (2) incrementally updating a running summary. We obtain 1193
fine-grained human annotations on GPT-4 generated summaries of 100
recently-published books and identify eight common types of coherence errors
made by LLMs. Because human evaluation is expensive and time-consuming, we
develop an automatic metric, BooookScore, that measures the proportion of
sentences in a summary that do not contain any of the identified error types.
BooookScore has high agreement with human annotations and allows us to
systematically evaluate the impact of many other critical parameters (e.g.,
chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation
costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce
summaries with higher BooookScore than those generated by open-source models.
While LLaMA 2 falls behind other models, Mixtral achieves performance on par
with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher
level of detail than hierarchical merging, a trade-off sometimes preferred by
annotators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Informative Metrics for Few-Shot Example Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Adiga, Lakshminarayanan Subramanian, Varun Chandrasekaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have shown remarkable few-shot learning
capabilities when provided with properly formatted examples. However, selecting
the "best" examples remains an open challenge. We propose a complexity-based
prompt selection approach for sequence tagging tasks. This approach avoids the
training of a dedicated model for selection of examples, and instead uses
certain metrics to align the syntactico-semantic complexity of test sentences
and examples. We use both sentence- and word-level metrics to match the
complexity of examples to the (test) sentence being considered. Our results
demonstrate that our approach extracts greater performance from PLMs: it
achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute
improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large
gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large, Small or Both: A Novel Data Augmentation Framework Based on
  <span class="highlight-title">Language Models</span> for Debiasing Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyue Zhang, Pengfei Li, Yilong Lai, Deyu Zhou, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As more than 70$\%$ of reviews in the existing opinion summary data set are
positive, current opinion summarization approaches are reluctant to generate
negative summaries given the input of negative texts. To address such sentiment
bias, a direct approach without the over-reliance on a specific framework is to
generate additional data based on large language models to balance the
emotional distribution of the dataset. However, data augmentation based on
large language models faces two disadvantages: 1) the potential issues or
toxicity in the augmented data; 2) the expensive costs. Therefore, in this
paper, we propose a novel data augmentation framework based on both large and
small language models for debiasing opinion summarization. In specific, a small
size of synthesized negative reviews is obtained by rewriting the positive text
via a large language model. Then, a disentangle reconstruction model is trained
based on the generated data. After training, a large amount of synthetic data
can be obtained by decoding the new representation obtained from the
combination of different sample representations and filtering based on
confusion degree and sentiment classification. Experiments have proved that our
framework can effectively alleviate emotional bias same as using only large
models, but more economically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large <span class="highlight-title">Language Models</span> as Generative User Simulators for
  Conversational Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods for Generating Drift in Text Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr, Jean Paul Barddal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems and individuals produce data continuously. On the Internet, people
share their knowledge, sentiments, and opinions, provide reviews about services
and products, and so on. Automatically learning from these textual data can
provide insights to organizations and institutions, thus preventing financial
impacts, for example. To learn from textual data over time, the machine
learning system must account for concept drift. Concept drift is a frequent
phenomenon in real-world datasets and corresponds to changes in data
distribution over time. For instance, a concept drift occurs when sentiments
change or a word's meaning is adjusted over time. Although concept drift is
frequent in real-world applications, benchmark datasets with labeled drifts are
rare in the literature. To bridge this gap, this paper provides four textual
drift generation methods to ease the production of datasets with labeled
drifts. These methods were applied to Yelp and Airbnb datasets and tested using
incremental classifiers respecting the stream mining paradigm to evaluate their
ability to recover from the drifts. Results show that all methods have their
performance degraded right after the drifts, and the incremental SVM is the
fastest to run and recover the previous performance levels regarding accuracy
and Macro F1-Score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and
  Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui Zhang, Xinmeng Ji, Tingting Cui, Tao Liu, Jinwang Song, Hongying Zan, Sun Li, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Chinese large language models (LLMs) poses big
challenges for efficient LLM evaluation. While current initiatives have
introduced new benchmarks or evaluation platforms for assessing Chinese LLMs,
many of these focus primarily on capabilities, usually overlooking potential
alignment and safety issues. To address this gap, we introduce OpenEval, an
evaluation testbed that benchmarks Chinese LLMs across capability, alignment
and safety. For capability assessment, we include 12 benchmark datasets to
evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,
commonsense reasoning and mathematical reasoning. For alignment assessment,
OpenEval contains 7 datasets that examines the bias, offensiveness and
illegalness in the outputs yielded by Chinese LLMs. To evaluate safety,
especially anticipated risks (e.g., power-seeking, self-awareness) of advanced
LLMs, we include 6 datasets. In addition to these benchmarks, we have
implemented a phased public evaluation and benchmark update strategy to ensure
that OpenEval is in line with the development of Chinese LLMs or even able to
provide cutting-edge benchmark datasets to guide the development of Chinese
LLMs. In our first public evaluation, we have tested a range of Chinese LLMs,
spanning from 7B to 72B parameters, including both open-source and proprietary
models. Evaluation results indicate that while Chinese LLMs have shown
impressive performance in certain tasks, more attention should be directed
towards broader aspects such as commonsense reasoning, alignment, and safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large <span class="highlight-title">Language Models</span> to Extract Information on Substance Use
  Disorder Severity from Clinical Notes: A Zero-shot Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Mahbub, Gregory M. Dams, Sudarshan Srinivasan, Caitlin Rizy, Ioana Danciu, Jodie Trafton, Kathryn Knight
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Substance use disorder (SUD) poses a major concern due to its detrimental
effects on health and society. SUD identification and treatment depend on a
variety of factors such as severity, co-determinants (e.g., withdrawal
symptoms), and social determinants of health. Existing diagnostic coding
systems used by American insurance providers, like the International
Classification of Diseases (ICD-10), lack granularity for certain diagnoses,
but clinicians will add this granularity (as that found within the Diagnostic
and Statistical Manual of Mental Disorders classification or DSM-5) as
supplemental unstructured text in clinical notes. Traditional natural language
processing (NLP) methods face limitations in accurately parsing such diverse
clinical language. Large Language Models (LLMs) offer promise in overcoming
these challenges by adapting to diverse language patterns. This study
investigates the application of LLMs for extracting severity-related
information for various SUD diagnoses from clinical notes. We propose a
workflow employing zero-shot learning of LLMs with carefully crafted prompts
and post-processing techniques. Through experimentation with Flan-T5, an
open-source LLM, we demonstrate its superior recall compared to the rule-based
approach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness
of LLMs in extracting severity information, contributing to improved risk
assessment and treatment planning for SUD patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Investigation of Compositional Syntax and Semantics in
  DALL-E 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Murphy, Jill de Villiers, Sofia Lucero Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study we compared how well DALL-E 2 visually represented the meaning
of linguistic prompts also given to young children in comprehension tests.
Sentences representing fundamental components of grammatical knowledge were
selected from assessment tests used with several hundred English-speaking
children aged 2-7 years for whom we had collected original item-level data.
DALL-E 2 was given these prompts five times to generate 20 cartoons per item,
for 9 adult judges to score. Results revealed no conditions in which DALL-E
2-generated images that matched the semantic accuracy of children, even at the
youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in
reversible forms; it failed on negation despite an easier contrastive prompt
than the children received; it often assigned the adjective to the wrong noun;
it ignored implicit agents in passives. This work points to a clear absence of
compositional sentence representations for DALL-E 2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fin<span class="highlight-title">Llama</span>: Financial Sentiment Classification for Algorithmic Trading
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G. Constantinides, Danilo Mandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are multiple sources of financial news online which influence market
movements and trader's decisions. This highlights the need for accurate
sentiment analysis, in addition to having appropriate algorithmic trading
techniques, to arrive at better informed trading decisions. Standard lexicon
based sentiment approaches have demonstrated their power in aiding financial
decisions. However, they are known to suffer from issues related to context
sensitivity and word ordering. Large Language Models (LLMs) can also be used in
this context, but they are not finance-specific and tend to require significant
computational resources. To facilitate a finance specific LLM framework, we
introduce a novel approach based on the Llama 2 7B foundational model, in order
to benefit from its generative nature and comprehensive language manipulation.
This is achieved by fine-tuning the Llama2 7B model on a small portion of
supervised financial sentiment analysis data, so as to jointly handle the
complexities of financial lexicon and context, and further equipping it with a
neural network based decision mechanism. Such a generator-classifier scheme,
referred to as FinLlama, is trained not only to classify the sentiment valence
but also quantify its strength, thus offering traders a nuanced insight into
financial news articles. Complementing this, the implementation of
parameter-efficient fine-tuning through LoRA optimises trainable parameters,
thus minimising computational and memory requirements, without sacrificing
accuracy. Simulation results demonstrate the ability of the proposed FinLlama
to provide a framework for enhanced portfolio management decisions and
increased market returns. These results underpin the ability of FinLlama to
construct high-return portfolios which exhibit enhanced resilience, even during
volatile periods and unpredictable market events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Multi-task Hallucination Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patanjali Bhamidipati, Advaith Malladi, Manish Shrivastava, Radhika Mamidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, the extensive utilization of large language models has
underscored the importance of robust evaluation methodologies for assessing
text generation quality and relevance to specific tasks. This has revealed a
prevalent issue known as hallucination, an emergent condition in the model
where generated text lacks faithfulness to the source and deviates from the
evaluation criteria. In this study, we formally define hallucination and
propose a framework for its quantitative detection in a zero-shot setting,
leveraging our definition and the assumption that model outputs entail task and
sample specific inputs. In detecting hallucinations, our solution achieves an
accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.
Notably, our solution maintains computational efficiency, requiring far less
computational resources than other SOTA approaches, aligning with the trend
towards lightweight and compressed models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-based Metrics Disprove Themselves in Question <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference-based metrics such as BLEU and BERTScore are widely used to
evaluate question generation (QG). In this study, on QG benchmarks such as
SQuAD and HotpotQA, we find that using human-written references cannot
guarantee the effectiveness of the reference-based metrics. Most QG benchmarks
have only one reference; we replicated the annotation process and collect
another reference. A good metric was expected to grade a human-validated
question no worse than generated questions. However, the results of
reference-based metrics on our newly collected reference disproved the metrics
themselves. We propose a reference-free metric consisted of multi-dimensional
criteria such as naturalness, answerability, and complexity, utilizing large
language models. These criteria are not constrained to the syntactic or
semantic of a single reference question, and the metric does not require a
diverse set of references. Experiments reveal that our metric accurately
distinguishes between high-quality questions and flawed ones, and achieves
state-of-the-art alignment with human judgment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Named Entity Recognition: Comparative Analysis of Mono- and
  Multilingual <span class="highlight-title">Transformer</span> Models on Brazilian Corporate Earnings Call
  Transcriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) is a Natural Language Processing technique for
extracting information from textual documents. However, much of the existing
research on NER has been centered around English-language documents, leaving a
gap in the availability of datasets tailored to the financial domain in
Portuguese. This study addresses the need for NER within the financial domain,
focusing on Portuguese-language texts extracted from earnings call
transcriptions of Brazilian banks. By curating a comprehensive dataset
comprising 384 transcriptions and leveraging weak supervision techniques for
annotation, we evaluate the performance of monolingual models trained on
Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5).
Notably, we introduce a novel approach that reframes the token classification
task as a text generation problem, enabling fine-tuning and evaluation of T5
models. Following the fine-tuning of the models, we conduct an evaluation on
the test dataset, employing performance and error metrics. Our findings reveal
that BERT-based models consistently outperform T5-based models. Furthermore,
while the multilingual models exhibit comparable macro F1-scores, BERTimbau
demonstrates superior performance over PTT5. A manual analysis of sentences
generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to
1.0, between the original and generated sentences. However, critical errors
emerge as both models exhibit discrepancies, such as alterations to monetary
and percentage values, underscoring the importance of accuracy and consistency
in the financial domain. Despite these challenges, PTT5 and mT5 achieve
impressive macro F1-scores of 98.52% and 98.85%, respectively, with our
proposed approach. Furthermore, our study sheds light on notable disparities in
memory and time consumption for inference across the models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TnT-LLM: Text Mining at Scale with Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming unstructured text into structured and meaningful forms,
organized by useful category labels, is a fundamental step in text mining for
downstream analysis and application. However, most existing methods for
producing label taxonomies and building text-based label classifiers still rely
heavily on domain expertise and manual curation, making the process expensive
and time-consuming. This is particularly challenging when the label space is
under-specified and large-scale data annotations are unavailable. In this
paper, we address these challenges with Large Language Models (LLMs), whose
prompt-based interface facilitates the induction and use of large-scale pseudo
labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate
the process of end-to-end label generation and assignment with minimal human
effort for any given use-case. In the first phase, we introduce a zero-shot,
multi-stage reasoning approach which enables LLMs to produce and refine a label
taxonomy iteratively. In the second phase, LLMs are used as data labelers that
yield training samples so that lightweight supervised classifiers can be
reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis
of user intent and conversational domain for Bing Copilot (formerly Bing Chat),
an open-domain chat-based search engine. Extensive experiments using both human
and automatic evaluation metrics demonstrate that TnT-LLM generates more
accurate and relevant label taxonomies when compared against state-of-the-art
baselines, and achieves a favorable balance between accuracy and efficiency for
classification at scale. We also share our practical experiences and insights
on the challenges and opportunities of using LLMs for large-scale text mining
in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages main content, 8 pages references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EasyJailbreak: A Unified Framework for Jailbreaking Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreak attacks are crucial for identifying and mitigating the security
vulnerabilities of Large Language Models (LLMs). They are designed to bypass
safeguards and elicit prohibited outputs. However, due to significant
differences among various jailbreak methods, there is no standard
implementation framework available for the community, which limits
comprehensive security evaluations. This paper introduces EasyJailbreak, a
unified framework simplifying the construction and evaluation of jailbreak
attacks against LLMs. It builds jailbreak attacks using four components:
Selector, Mutator, Constraint, and Evaluator. This modular framework enables
researchers to easily construct attacks from combinations of novel and existing
components. So far, EasyJailbreak supports 11 distinct jailbreak methods and
facilitates the security validation of a broad spectrum of LLMs. Our validation
across 10 distinct LLMs reveals a significant vulnerability, with an average
breach probability of 60% under various jailbreaking attacks. Notably, even
advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success
Rates (ASR) of 57% and 33%, respectively. We have released a wealth of
resources for researchers, including a web platform, PyPI published package,
screencast video, and experimental outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusing Domain-Specific Content from Large <span class="highlight-title">Language Models</span> into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI-MAKE 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Syn-QA2: Evaluating False Assumptions in Long-tail Questions with
  Synthetic QA <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Daswani, Rohan Sawant, Najoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sensitivity to false assumptions (or false premises) in information-seeking
questions is critical for robust question-answering (QA) systems. Recent work
has shown that false assumptions in naturally occurring questions pose
challenges to current models, with low performance on both generative QA and
simple detection tasks (Kim et al. 2023). However, the focus of existing work
on naturally occurring questions leads to a gap in the analysis of model
behavior on the long tail of the distribution of possible questions. To this
end, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA
datasets: one generated using perturbed relations from Wikidata, and the other
by perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range
of large language models are threefold: (1) false assumptions in QA are
challenging, echoing the findings of prior work, (2) the binary detection task
is challenging even compared to the difficulty of generative QA itself,
possibly due to the linguistic structure of the problem, and (3) the detection
task is more challenging with long-tail questions compared to naturally
occurring questions, highlighting the utility of our synthetic datasets and
generation method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixels to Insights: A <span class="highlight-title">Survey</span> on Automatic Chart Understanding in
  the Era of Large Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization in the form of charts plays a pivotal role in data
analysis, offering critical insights and aiding in informed decision-making.
Automatic chart understanding has witnessed significant advancements with the
rise of large foundation models in recent years. Foundation models, such as
large language models (LLMs), have revolutionized various natural language
processing (NLP) tasks and are increasingly being applied to chart
understanding tasks. This survey paper provides a comprehensive overview of the
recent developments, challenges, and future directions in chart understanding
within the context of these foundation models. The paper begins by defining
chart understanding, outlining problem formulations, and discussing fundamental
building blocks crucial for studying chart understanding tasks. In the section
on tasks and datasets, we explore various tasks within chart understanding and
discuss their evaluation metrics and sources of both charts and textual inputs.
Modeling strategies are then examined, encompassing both classification-based
and generation-based approaches, along with tool augmentation techniques that
enhance chart understanding performance. Furthermore, we discuss the
state-of-the-art performance of each task and discuss how we can improve the
performance. Challenges and future directions are addressed in a dedicated
section, highlighting issues such as domain-specific charts, lack of efforts in
evaluation, and agent-oriented settings. This survey paper serves to provide
valuable insights and directions for future research in chart understanding
leveraging large foundation models. The studies mentioned in this paper, along
with emerging new research, will be continually updated at:
https://github.com/khuangaf/Awesome-Chart-Understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexCap: Generating Rich, Localized, and Flexible Captions in Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, Yusuf Aytar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a versatile $\textit{flexible-captioning}$ vision-language model
(VLM) capable of generating region-specific descriptions of varying lengths.
The model, FlexCap, is trained to produce length-conditioned captions for input
bounding boxes, and this allows control over the information density of its
output, with descriptions ranging from concise object labels to detailed
captions. To achieve this we create large-scale training datasets of image
region descriptions of varying length, starting from captioned images. This
flexible-captioning capability has several valuable applications.
  First, FlexCap demonstrates superior performance in dense captioning tasks on
the Visual Genome dataset. Second, a visual question answering (VQA) system can
be built by employing FlexCap to generate localized descriptions as inputs to a
large language model. The resulting system achieves state-of-the-art zero-shot
performance on a number of VQA datasets. We also demonstrate a
$\textit{localize-then-describe}$ approach with FlexCap can be better at
open-ended object detection than a $\textit{describe-then-localize}$ approach
with other VLMs. We highlight a novel characteristic of FlexCap, which is its
ability to extract diverse visual information through prefix conditioning.
Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks
such as image labeling, object attribute recognition, and visual dialog.
Project webpage: https://flex-cap.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Toolbox for Surfacing Health Equity Harms and Biases in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh, Liam G. McCoy, Leo Anthony Celi, Yun Liu, Mike Schaekermann, Alanna Walton, Alicia Parrish, Chirag Nagpal, Preeti Singh, Akeiylah Dewitt, Philip Mansfield, Sushant Prakash, Katherine Heller, Alan Karthikesalingam, Christopher Semturs, Joelle Barral, Greg Corrado, Yossi Matias, Jamila Smith-Loud, Ivor Horn, Karan Singhal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) hold immense promise to serve complex health
information needs but also have the potential to introduce harm and exacerbate
health disparities. Reliably evaluating equity-related model failures is a
critical step toward developing systems that promote health equity. In this
work, we present resources and methodologies for surfacing biases with
potential to precipitate equity-related harms in long-form, LLM-generated
answers to medical questions and then conduct an empirical case study with
Med-PaLM 2, resulting in the largest human evaluation study in this area to
date. Our contributions include a multifactorial framework for human assessment
of LLM-generated answers for biases, and EquityMedQA, a collection of seven
newly-released datasets comprising both manually-curated and LLM-generated
questions enriched for adversarial queries. Both our human assessment framework
and dataset design process are grounded in an iterative participatory approach
and review of possible biases in Med-PaLM 2 answers to adversarial queries.
Through our empirical study, we find that the use of a collection of datasets
curated through a variety of methodologies, coupled with a thorough evaluation
protocol that leverages multiple assessment rubric designs and diverse rater
groups, surfaces biases that may be missed via narrower evaluation approaches.
Our experience underscores the importance of using diverse assessment
methodologies and involving raters of varying backgrounds and expertise. We
emphasize that while our framework can identify specific forms of bias, it is
not sufficient to holistically assess whether the deployment of an AI system
promotes equitable health outcomes. We hope the broader community leverages and
builds on these tools and methods towards realizing a shared goal of LLMs that
promote accessible and equitable healthcare for all.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Hokkien Dual Translation by Exploring and Standardizing of
  Four Writing Systems <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Han Lu, Yi-Hsuan Lin, En-Shiun Annie Lee, Richard Tzong-Han Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation focuses mainly on high-resource languages (HRLs), while
low-resource languages (LRLs) like Taiwanese Hokkien are relatively
under-explored. This study aims to address this gap by developing a dual
translation model between Taiwanese Hokkien and both Traditional Mandarin
Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in
Traditional Mandarin Chinese to leverage the orthographic similarities between
Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive
experiments involve translation tasks across various writing systems of
Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that
the use of a limited monolingual corpus also further improve the model's
Taiwanese Hokkien capabilities. We then utilize our translation model to
standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting
in further performance improvements. Additionally, we introduce an evaluation
method incorporating back-translation and GPT-4 to ensure reliable translation
quality assessment even for LRLs. The study contributes to narrowing the
resource gap for Taiwanese Hokkien and empirically investigates the advantages
and limitations of pre-training and fine-tuning based on LLaMA 2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Fine-Tuning as Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevailing approach to aligning Large Language Models (LLMs) typically
relies on human or AI feedback and assumes access to specific types of
preference datasets. In our work, we question the efficacy of such datasets and
explore various scenarios where alignment with expert demonstrations proves
more realistic. We build a sequential decision-making framework to formulate
the problem of aligning LLMs using demonstration datasets. Drawing insights
from inverse reinforcement learning and imitation learning, we introduce
various approaches for divergence minimization in the LLM alignment tasks. Our
analysis highlights the mass-covering and mode-seeking behaviors of these
different approaches. Inclusively, we examine the pros and cons of the
classical supervised fine-tuning method, elaborating on scenarios where
different methods shine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnvGen: Generating and Adapting Environments via LLMs for Training
  Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller embodied RL agents learn useful skills that they are weak at? We
propose EnvGen, a novel framework to address this question. First, we prompt an
LLM to generate training environments that allow agents to quickly learn
different tasks in parallel. Concretely, the LLM is given the task description
and simulator objectives that the agents should learn and is then asked to
generate a set of environment configurations (e.g., different terrains, items
given to agents, etc.). Next, we train a small RL agent in a mixture of the
original and LLM-generated environments. Then, we enable the LLM to
continuously adapt the generated environments to progressively improve the
skills that the agent is weak at, by providing feedback to the LLM in the form
of the agent's performance. We demonstrate the usefulness of EnvGen with
comprehensive experiments in Crafter and Heist environments. We find that a
small RL agent trained with EnvGen can outperform SOTA methods, including a
GPT-4 agent, and learns long-horizon tasks significantly faster. We show
qualitatively how the LLM adapts training environments to help improve RL
agents' weaker skills over time. Additionally, EnvGen is substantially more
efficient as it only uses a small number of LLM calls (e.g., 4 in total),
whereas LLM agents require thousands of LLM calls. Lastly, we present detailed
ablation studies for our design choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; Project website:
  https://envgen-llm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NovelQA: A Benchmark for Long-Range Novel Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Qian Wang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has introduced a new
frontier in natural language processing, particularly in understanding and
processing long-context information. However, the evaluation of these models'
long-context abilities remains a challenge due to the limitations of current
benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically
designed to test the capabilities of LLMs with extended texts. Constructed from
English novels, NovelQA offers a unique blend of complexity, length, and
narrative coherence, making it an ideal tool for assessing deep textual
understanding in LLMs. This paper presents the design and construction of
NovelQA, highlighting its manual annotation, and diverse question types. Our
evaluation of Long-context LLMs on NovelQA reveals significant insights into
the models' performance, particularly emphasizing the challenges they face with
multi-hop reasoning, detail-oriented questions, and extremely long input with
more than 100,000 tokens. The results underscore the necessity for further
advancements in LLMs to improve their long-context comprehension and
computational literary studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using generative Artificial Intelligence (AI), we transformed a set of 1,000
scientific papers in the area of biological materials into detailed ontological
knowledge graphs, revealing their inherently scale-free nature. Using graph
traversal path detection between dissimilar concepts based on combinatorial
ranking of node similarity and betweenness centrality, we reveal deep insights
into unprecedented interdisciplinary relationships that can be used to answer
queries, identify gaps in knowledge, and propose never-before-seen material
designs and their behaviors. One comparison revealed detailed structural
parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. The
algorithm further created an innovative hierarchical mycelium-based composite
that incorporates joint synthesis of graph sampling with principles extracted
from Kandinsky's Composition VII painting, where the resulting composite
reflects a balance of chaos and order, with features like adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across physical, biological, and artistic spheres,
revealing a nuanced ontology of immanence and material flux that resonates with
postmodern philosophy, and positions these interconnections within a
heterarchical framework. Our findings reveal the dynamic, context-dependent
interplay of entities beyond traditional hierarchical paradigms, emphasizing
the significant role of individual components and their fluctuative
relationships within the system. Our predictions achieve a far higher degree of
novelty, technical detail and explorative capacity than conventional generative
AI methods. The approach establishes a widely useful framework for innovation
by revealing hidden connections that facilitate discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Generative Text Models to Create Qualitative Codebooks for Student
  Evaluations of Teaching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Katz, Mitchell Gerhardt, Michelle Soledad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedback is a critical aspect of improvement. Unfortunately, when there is a
lot of feedback from multiple sources, it can be difficult to distill the
information into actionable insights. Consider student evaluations of teaching
(SETs), which are important sources of feedback for educators. They can give
instructors insights into what worked during a semester. A collection of SETs
can also be useful to administrators as signals for courses or entire programs.
However, on a large scale as in high-enrollment courses or administrative
records over several years, the volume of SETs can render them difficult to
analyze. In this paper, we discuss a novel method for analyzing SETs using
natural language processing (NLP) and large language models (LLMs). We
demonstrate the method by applying it to a corpus of 5,000 SETs from a large
public university. We show that the method can be used to extract, embed,
cluster, and summarize the SETs to identify the themes they express. More
generally, this work illustrates how to use the combination of NLP techniques
and LLMs to generate a codebook for SETs. We conclude by discussing the
implications of this method for analyzing SETs and other types of student
writing in teaching and research settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Natural language processing, large language models, generative AI,
  student evaluations of teaching, codebook generation, qualitative data
  analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Evolution with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Rita, Paul Michel, Rahma Chaabouni, Olivier Pietquin, Emmanuel Dupoux, Florian Strub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational modeling plays an essential role in the study of language
emergence. It aims to simulate the conditions and learning processes that could
trigger the emergence of a structured language within a simulated controlled
environment. Several methods have been used to investigate the origin of our
language, including agent-based systems, Bayesian agents, genetic algorithms,
and rule-based systems. This chapter explores another class of computational
models that have recently revolutionized the field of machine learning: deep
learning models. The chapter introduces the basic concepts of deep and
reinforcement learning methods and summarizes their helpfulness for simulating
language emergence. It also discusses the key findings, limitations, and recent
attempts to build realistic simulations. This chapter targets linguists and
cognitive scientists seeking an introduction to deep learning as a tool to
investigate language evolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in the Oxford Handbook of Approaches to Language Evolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptative Bilingual Aligning Using Multilingual Sentence Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Kraif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an adaptive bitextual alignment system called
AIlign. This aligner relies on sentence embeddings to extract reliable anchor
points that can guide the alignment path, even for texts whose parallelism is
fragmentary and not strictly monotonic. In an experiment on several datasets,
we show that AIlign achieves results equivalent to the state of the art, with
quasi-linear complexity. In addition, AIlign is able to handle texts whose
parallelism and monotonicity properties are only satisfied locally, unlike
recent systems such as Vecalign or Bertalign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tur[k]ingBench: A Challenge Benchmark for Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xu, Yeganeh Kordi, Kate Sanders, Yizhong Wang, Adam Byerly, Jack Zhang, Benjamin Van Durme, Daniel Khashabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent chatbots have demonstrated impressive ability to understand and
communicate in raw-text form. However, there is more to the world than raw
text. For example, humans spend long hours of their time on web pages, where
text is intertwined with other modalities and tasks are accomplished in the
form of various complex interactions. Can state-of-the-art multi-modal models
generalize to such complex domains?
  To address this question, we introduce TurkingBench, a benchmark of tasks
formulated as web pages containing textual instructions with multi-modal
context. Unlike existing work which employs artificially synthesized web pages,
here we use natural HTML pages that were originally designed for crowdsourcing
workers for various annotation purposes. The HTML instructions of each task are
also instantiated with various values (obtained from the crowdsourcing tasks)
to form new instances of the task. This benchmark contains 32.2K instances
distributed across 158 tasks.
  Additionally, to facilitate the evaluation on TurkingBench, we develop an
evaluation framework that connects the responses of chatbots to modifications
on web pages (modifying a text box, checking a radio, etc.). We evaluate the
performance of state-of-the-art models, including language-only, vision-only,
and layout-only models, and their combinations, on this benchmark. Our findings
reveal that these models perform significantly better than random chance, yet
considerable room exists for improvement. We hope this benchmark will help
facilitate the evaluation and development of web-based agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CICLe: Conformal In-Context Learning for Largescale Multi-Class Food
  Risk Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contaminated or adulterated food poses a substantial risk to human health.
Given sets of labeled web texts for training, Machine Learning and Natural
Language Processing can be applied to automatically detect such risks. We
publish a dataset of 7,546 short texts describing public food recall
announcements. Each text is manually labeled, on two granularity levels (coarse
and fine), for food products and hazards that the recall corresponds to. We
describe the dataset and benchmark naive, traditional, and Transformer models.
Based on our analysis, Logistic Regression based on a tf-idf representation
outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss
different prompting strategies and present an LLM-in-the-loop framework, based
on Conformal Prediction, which boosts the performance of the base classifier
while reducing energy consumption compared to normal prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Closer Look at Claim Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark Dredze, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As generated text becomes more commonplace, it is increasingly important to
evaluate how well-supported such text is by external knowledge sources. Many
approaches for evaluating textual support rely on some method for decomposing
text into its individual subclaims which are scored against a trusted
reference. We investigate how various methods of claim decomposition --
especially LLM-based methods -- affect the result of an evaluation approach
such as the recently proposed FActScore, finding that it is sensitive to the
decomposition method used. This sensitivity arises because such metrics
attribute overall textual support to the model that generated the text even
though error can also come from the metric's decomposition step. To measure
decomposition quality, we introduce an adaptation of FActScore, which we call
DecompScore. We then propose an LLM-based approach to generating decompositions
inspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian
semantics and demonstrate its improved decomposition quality over previous
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Markers and Drivers of Gender Bias in Machine Translations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter J Barclay, Ashkan Sami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit gender bias in Large Language Models (LLMs) is a well-documented
problem, and implications of gender introduced into automatic translations can
perpetuate real-world biases. However, some LLMs use heuristics or
post-processing to mask such bias, making investigation difficult. Here, we
examine bias in LLMss via back-translation, using the DeepL translation API to
investigate the bias evinced when repeatedly translating a set of 56 Software
Engineering tasks used in a previous study. Each statement starts with 'she',
and is translated first into a 'genderless' intermediate language then back
into English; we then examine pronoun-choice in the back-translated texts. We
expand prior research in the following ways: (1) by comparing results across
five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and
Hungarian; (2) by proposing a novel metric for assessing the variation in
gender implied in the repeated translations, avoiding the over-interpretation
of individual pronouns, apparent in earlier work; (3) by investigating sentence
features that drive bias; (4) and by comparing results from three time-lapsed
datasets to establish the reproducibility of the approach. We found that some
languages display similar patterns of pronoun use, falling into three loose
groups, but that patterns vary between groups; this underlines the need to work
with multiple languages. We also identify the main verb appearing in a sentence
as a likely significant driver of implied gender in the translations. Moreover,
we see a good level of replicability in the results, and establish that our
variation metric proves robust despite an obvious change in the behaviour of
the DeepL translation API during the course of the study. These results show
that the back-translation method can provide further insights into bias in
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SANER 2024; see
  https://conf.researchr.org/home/saner-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From explainable to interpretable deep learning for natural language
  processing in healthcare: how far from reality? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Yingya Li, Giorgos Papanastasiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) has substantially enhanced healthcare research by
addressing various natural language processing (NLP) tasks. Yet, the increasing
complexity of DL-based NLP methods necessitates transparent model
interpretability, or at least explainability, for reliable decision-making.
This work presents a thorough scoping review on explainable and interpretable
DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial
Intelligence) was introduced to distinguish XAI from IAI. Methods were further
categorized based on their functionality (model-, input-, output-based) and
scope (local, global). Our analysis shows that attention mechanisms were the
most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The
major challenges identified are that most XIAI do not explore "global" modeling
processes, the lack of best practices, and the unmet need for systematic
evaluation and benchmarks. Important opportunities were raised such as using
"attention" to enhance multi-modal XIAI for personalized medicine and combine
DL with causal reasoning. Our discussion encourages the integration of XIAI in
LLMs and domain-specific smaller models. Our review can stimulate further
research and benchmarks toward improving inherent IAI and engaging complex NLP
in healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QueryAgent: A Reliable and Efficient Reasoning Framework with
  Environmental Feedback based Self-Correction <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) for semantic parsing has achieved
remarkable success. However, we find existing methods fall short in terms of
reliability and efficiency when hallucinations are encountered. In this paper,
we address these challenges with a framework called QueryAgent, which solves a
question step-by-step and performs step-wise self-correction. We introduce an
environmental feedback-based self-correction method called ERASER. Unlike
traditional approaches, ERASER leverages rich environmental feedback in the
intermediate steps to perform selective and differentiated self-correction only
when necessary. Experimental results demonstrate that QueryAgent notably
outperforms all previous few-shot methods using only one example on GrailQA and
GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms
of efficiency, including runtime, query overhead, and API invocation costs. By
leveraging ERASER, we further improve another baseline (i.e., AgentBench) by
approximately 10 points, revealing the strong transferability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CO3: Low-resource Contrastive Co-training for Generative Conversational
  Query Rewrite <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Renjun Hu, Zengming Zhang, Feijun Jiang, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative query rewrite generates reconstructed query rewrites using the
conversation history while rely heavily on gold rewrite pairs that are
expensive to obtain. Recently, few-shot learning is gaining increasing
popularity for this task, whereas these methods are sensitive to the inherent
noise due to limited data size. Besides, both attempts face performance
degradation when there exists language style shift between training and testing
cases. To this end, we study low-resource generative conversational query
rewrite that is robust to both noise and language style shift. The core idea is
to utilize massive unlabeled data to make further improvements via a
contrastive co-training paradigm. Specifically, we co-train two dual models
(namely Rewriter and Simplifier) such that each of them provides extra guidance
through pseudo-labeling for enhancing the other in an iterative manner. We also
leverage contrastive learning with data augmentation, which enables our model
pay more attention on the truly valuable information than the noise. Extensive
experiments demonstrate the superiority of our model under both few-shot and
zero-shot scenarios. We also verify the better generalization ability of our
model when encountering language style shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4 as Evaluator: Evaluating Large <span class="highlight-title">Language Models</span> on Pest Management
  in Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, Po Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of artificial intelligence (AI), the
application of large language models (LLMs) in agriculture, particularly in
pest management, remains nascent. We aimed to prove the feasibility by
evaluating the content of the pest management advice generated by LLMs,
including the Generative Pre-trained Transformer (GPT) series from OpenAI and
the FLAN series from Google. Considering the context-specific properties of
agricultural advice, automatically measuring or quantifying the quality of text
generated by LLMs becomes a significant challenge. We proposed an innovative
approach, using GPT-4 as an evaluator, to score the generated content on
Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and
Exhaustiveness. Additionally, we integrated an expert system based on crop
threshold data as a baseline to obtain scores for Factual Accuracy on whether
pests found in crop fields should take management action. Each model's score
was weighted by percentage to obtain a final score. The results showed that
GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories.
Furthermore, the use of instruction-based prompting containing domain-specific
knowledge proved the feasibility of LLMs as an effective tool in agriculture,
with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing
pest management suggestions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for
  <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) exhibit impressive capabilities but also present
risks such as biased content generation and privacy issues. One of the current
alignment techniques includes principle-driven integration, but it faces
challenges arising from the imprecision of manually crafted rules and
inadequate risk perception in models without safety training. To address these,
we introduce Guide-Align, a two-stage approach. Initially, a safety-trained
model identifies potential risks and formulates specific guidelines for various
inputs, thereby establishing a comprehensive library of guidelines and models
for input-guidelines retrieval. Subsequently, the retrieval model correlates
new inputs with pertinent guidelines, guiding LLMs in response generation to
ensure safe and high-quality outputs, thus aligning with human values. An
additional optional stage involves fine-tuning a model with new well-aligned
datasets generated through the process implemented in the second stage. Our
method customizes guidelines to accommodate diverse inputs, thereby enhancing
the fine-grainedness and comprehensiveness of the guideline library.
Furthermore, it incorporates safety expertise from a safety-trained LLM through
a lightweight retrieval model. We evaluated our approach on three benchmarks,
demonstrating significant improvements in LLM security and quality. Notably,
our fine-tuned model, Labrador, even at 13 billion parameters, outperforms
GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding the Relationship between In-context Learning and
  Compositional Generalization <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjun Han, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the principle of compositional generalization, the meaning of a
complex expression can be understood as a function of the meaning of its parts
and of how they are combined. This principle is crucial for human language
processing and also, arguably, for NLP models in the face of
out-of-distribution data. However, many neural network models, including
Transformers, have been shown to struggle with compositional generalization. In
this paper, we hypothesize that forcing models to in-context learn can provide
an inductive bias to promote compositional generalization. To test this
hypothesis, we train a causal Transformer in a setting that renders ordinary
learning very difficult: we present it with different orderings of the training
instance and shuffle instance labels. This corresponds to training the model on
all possible few-shot learning problems attainable from the dataset. The model
can solve the task, however, by utilizing earlier examples to generalize to
later ones (i.e. in-context learning). In evaluations on the datasets, SCAN,
COGS, and GeoQuery, models trained in this manner indeed show improved
compositional generalization. This indicates the usefulness of in-context
learning problems as an inductive bias for generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSCAE -- Semantic, Syntactic, and Context-aware natural language
  Adversarial Examples generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javad Rafiei Asl, Mohammad H. Rafiei, Manar Alohaly, Daniel Takabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are vulnerable to maliciously crafted Adversarial
Examples (AEs). Training a machine learning model with AEs improves its
robustness and stability against adversarial attacks. It is essential to
develop models that produce high-quality AEs. Developing such models has been
much slower in natural language processing (NLP) than in areas such as computer
vision. This paper introduces a practical and efficient adversarial attack
model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and
\textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE
identifies important words and uses a masked language model to generate an
early set of substitutions. Next, two well-known language models are employed
to evaluate the initial set in terms of semantic and syntactic characteristics.
We introduce (1) a dynamic threshold to capture more efficient perturbations
and (2) a local greedy search to generate high-quality AEs. As a black-box
method, SSCAE generates humanly imperceptible and context-aware AEs that
preserve semantic consistency and the source language's syntactical and
grammatical requirements. The effectiveness and superiority of the proposed
SSCAE model are illustrated with fifteen comparative experiments and extensive
sensitivity analysis for parameter optimization. SSCAE outperforms the existing
models in all experiments while maintaining a higher semantic consistency with
a lower query number and a comparable perturbation rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metaphor Understanding Challenge <span class="highlight-title">Dataset</span> for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Tong, Rochelle Choenni, Martha Lewis, Ekaterina Shutova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors in natural language are a reflection of fundamental cognitive
processes such as analogical reasoning and categorisation, and are deeply
rooted in everyday communication. Metaphor understanding is therefore an
essential task for large language models (LLMs). We release the Metaphor
Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor
understanding capabilities of LLMs. The dataset provides over 10k paraphrases
for sentences containing metaphor use, as well as 1.5k instances containing
inapt paraphrases. The inapt paraphrases were carefully selected to serve as
control to determine whether the model indeed performs full metaphor
interpretation or rather resorts to lexical similarity. All apt and inapt
paraphrases were manually annotated. The metaphorical sentences cover natural
metaphor uses across 4 genres (academic, news, fiction, and conversation), and
they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5
demonstrate that MUNCH presents a challenging task for LLMs. The dataset is
freely accessible at
https://github.com/xiaoyuisrain/metaphor-understanding-challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming
  Ability in Multi-Agent Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making, a complicated task requiring various types of abilities,
presents an excellent framework for assessing Large Language Models (LLMs). Our
research investigates LLMs' decision-making capabilities through the lens of a
well-established field, Game Theory. We focus specifically on games that
support the participation of more than two agents simultaneously. Subsequently,
we introduce our framework, GAMA-Bench, including eight classical multi-agent
games. We design a scoring scheme to assess a model's performance in these
games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness,
generalizability, and enhancement strategies. Results reveal that while GPT-3.5
shows satisfying robustness, its generalizability is relatively limited.
However, its performance can be improved through approaches such as
Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and
find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of
72.5. Moreover, the increasingly higher scores across the three iterations of
GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's
intelligence with each update. The code and experimental results are made
publicly available via https://github.com/CUHK-ARISE/GAMABench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 figures, 9 tables. Working in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counting-Stars: A Simple, Efficient, and Reasonable Strategy for
  Evaluating Long-Context Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Song, Mao Zheng, Xuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research endeavors have concentrated on developing Large
Language Models (LLMs) with robust long-context capabilities, due to the lack
of appropriate evaluation strategies, relatively little is known about how well
the long-context processing abilities and performance of leading LLMs (e.g.,
ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and
reasonable strategy for evaluating long-context LLMs as a new benchmark, named
Counting-Stars. The Counting-Stars is designed to require LLMs to fully
understand and capture long dependencies in long contexts and be able to
collect inter-dependency across multiple pieces of evidence spanning the entire
context to finish the task. Based on the Counting-Stars, we conduct experiments
to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat.
The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve
significant performance in the long context from 4K to 128K. We further present
two intriguing analyses regarding the behavior of LLMs processing long context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Abilities of Large <span class="highlight-title">Language Models</span>: In-Depth Analysis on the
  Abstraction and Reasoning Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungpil Lee, Woochang Sim, Donghyeon Shin, Sanha Hwang, Wongyu Seo, Jiwon Park, Seokki Lee, Sejin Kim, Sundong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing methods for evaluating the inference abilities of Large Language
Models (LLMs) have been results-centric, making it difficult to assess the
inference process. We introduce a new approach using the Abstract and Reasoning
Corpus (ARC) dataset to evaluate the inference and contextual understanding
abilities of large language models in a process-centric manner. ARC demands
rigorous logical structures for problem-solving, making it a benchmark that
facilitates the comparison of model inference abilities with humans.
Experimental results confirm that while large language models possess weak
inference abilities, they still lag in terms of logical coherence,
compositionality, and productivity. Our experiments highlight the reasoning
capabilities of LLMs, proposing development paths for achieving human-level
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Construction of Hyper-Relational Knowledge Graphs Using <span class="highlight-title">Pre-Train</span>ed
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetha Datta, Fedor Vitiugin, Anastasiia Chizhikova, Nitin Sawhney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting hyper-relations is crucial for constructing comprehensive
knowledge graphs, but there are limited supervised methods available for this
task. To address this gap, we introduce a zero-shot prompt-based method using
OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.
Comparing our model with a baseline, we achieved promising results, with a
recall of 0.77. Although our precision is currently lower, a detailed analysis
of the model outputs has uncovered potential pathways for future research in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages + references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality-Agnostic fMRI Decoding of Vision and Language <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Nikolaus, Milad Mozafari, Nicholas Asher, Leila Reddy, Rufin VanRullen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies have shown that it is possible to map brain activation data
of subjects viewing images onto the feature representation space of not only
vision models (modality-specific decoding) but also language models
(cross-modal decoding). In this work, we introduce and use a new large-scale
fMRI dataset (~8,500 trials per subject) of people watching both images and
text descriptions of such images. This novel dataset enables the development of
modality-agnostic decoders: a single decoder that can predict which stimulus a
subject is seeing, irrespective of the modality (image or text) in which the
stimulus is presented. We train and evaluate such decoders to map brain signals
onto stimulus representations from a large range of publicly available vision,
language and multimodal (vision+language) models. Our findings reveal that (1)
modality-agnostic decoders perform as well as (and sometimes even better than)
modality-specific decoders (2) modality-agnostic decoders mapping brain data
onto representations from unimodal models perform as well as decoders relying
on multimodal representations (3) while language and low-level visual
(occipital) brain regions are best at decoding text and image stimuli,
respectively, high-level visual (temporal) regions perform well on both
stimulus types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2024 workshop on Representational Alignment
  (Re-Align)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting The Classics: A Study on Identifying and Rectifying Gender
  Stereotypes in Rhymes and Poems <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Narayan Sankaran, Vigneshwaran Shankaran, Sampath Lonka, Rajesh Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rhymes and poems are a powerful medium for transmitting cultural norms and
societal roles. However, the pervasive existence of gender stereotypes in these
works perpetuates biased perceptions and limits the scope of individuals'
identities. Past works have shown that stereotyping and prejudice emerge in
early childhood, and developmental research on causal mechanisms is critical
for understanding and controlling stereotyping and prejudice. This work
contributes by gathering a dataset of rhymes and poems to identify gender
stereotypes and propose a model with 97\% accuracy to identify gender bias.
Gender stereotypes were rectified using a Large Language Model (LLM) and its
effectiveness was evaluated in a comparative survey against human educator
rectifications. To summarize, this work highlights the pervasive nature of
gender stereotypes in literary works and reveals the potential of LLMs to
rectify gender stereotypes. This study raises awareness and promotes
inclusivity within artistic expressions, making a significant contribution to
the discourse on gender equality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded Named Entity Recognition using Probing Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Popovič, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting semantic information from generated text is a useful tool for
applications such as automated fact checking or retrieval augmented generation.
Currently, this requires either separate models during inference, which
increases computational cost, or destructive fine-tuning of the language model.
Instead, we propose directly embedding information extraction capabilities into
pre-trained language models using probing classifiers, enabling efficient
simultaneous text generation and information extraction. For this, we introduce
an approach called EMBER and show that it enables named entity recognition in
decoder-only language models without fine-tuning them and while incurring
minimal additional computational cost at inference time. Specifically, our
experiments using GPT-2 show that EMBER maintains high token generation rates
during streaming text generation, with only a negligible decrease in speed of
around 1% compared to a 43.64% slowdown measured for a baseline using a
separate NER model. Code and data are available at
https://github.com/nicpopovic/EMBER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyun Xu, Runzhe Zhan, Derek F. Wong, Lidia S. Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are composed of neurons that exhibit various
behaviors and roles, which become increasingly diversified as models scale.
Recent studies have revealed that not all neurons are active across different
datasets, and this sparsity correlates positively with the task-specific
ability, leading to advancements in model pruning and training efficiency.
Traditional fine-tuning methods engage all parameters of LLMs, which is
computationally expensive and may not be necessary. In contrast,
Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of
trainable parameters, yet they still operate at a relatively macro scale (e.g.,
layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach
that refines the granularity of parameter training down to the individual
neuron, enabling more precise and computationally efficient model updates. The
experimental results show that NeFT not only exceeded the performance of
full-parameter fine-tuning and PEFT but also provided insights into the
analysis of neurons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguacodus: A Synergistic Framework for Transformative Code <span class="highlight-title">Generation</span>
  in Machine Learning Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNA<span class="highlight-title">BERT</span>-2: Efficient Foundation Model and Benchmark For Multi-Species
  Genome <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding the linguistic intricacies of the genome is a crucial problem in
biology, and pre-trained foundational models such as DNABERT and Nucleotide
Transformer have made significant strides in this area. Existing works have
largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the
token of the genome language due to its simplicity. However, we argue that the
computation and sample inefficiencies introduced by k-mer tokenization are
primary obstacles in developing large genome foundational models. We provide
conceptual and empirical insights into genome tokenization, building on which
we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a
statistics-based data compression algorithm that constructs tokens by
iteratively merging the most frequent co-occurring genome segment in the
corpus. We demonstrate that BPE not only overcomes the limitations of k-mer
tokenization but also benefits from the computational efficiency of
non-overlapping tokenization. Based on these insights, we introduce DNABERT-2,
a refined genome foundation model that adapts an efficient tokenizer and
employs multiple strategies to overcome input length constraints, reduce time
and memory expenditure, and enhance model capability. Furthermore, we identify
the absence of a comprehensive and standardized benchmark for genome
understanding as another significant impediment to fair comparative analysis.
In response, we propose the Genome Understanding Evaluation (GUE), a
comprehensive multi-species genome classification dataset that amalgamates $36$
distinct datasets across $9$ tasks, with input lengths ranging from $70$ to
$10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate
that DNABERT-2 achieves comparable performance to the state-of-the-art model
with $21 \times$ fewer parameters and approximately $92 \times$ less GPU time
in pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Role Labeling Guided Out-of-distribution Detection <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinan Zou, Maihao Guo, Yu Tian, Yuhao Lin, Haiyao Cao, Lingqiao Liu, Ehsan Abbasnejad, Javen Qinfeng Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying unexpected domain-shifted instances in natural language
processing is crucial in real-world applications. Previous works identify the
out-of-distribution (OOD) instance by leveraging a single global feature
embedding to represent the sentence, which cannot characterize subtle OOD
patterns well. Another major challenge current OOD methods face is learning
effective low-dimensional sentence representations to identify the hard OOD
instances that are semantically similar to the in-distribution (ID) data. In
this paper, we propose a new unsupervised OOD detection method, namely Semantic
Role Labeling Guided Out-of-distribution Detection (SRLOOD), that separates,
extracts, and learns the semantic role labeling (SRL) guided fine-grained local
feature representations from different arguments of a sentence and the global
feature representations of the full sentence using a margin-based contrastive
loss. A novel self-supervised approach is also introduced to enhance such
global-local feature learning by predicting the SRL extracted role. The
resulting model achieves SOTA performance on four OOD benchmarks, indicating
the effectiveness of our approach. The code is publicly accessible via
\url{https://github.com/cytai/SRLOOD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Modeling Is Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has long been established that predictive models can be transformed into
lossless compressors and vice versa. Incidentally, in recent years, the machine
learning community has focused on training increasingly large and powerful
self-supervised (language) models. Since these large language models exhibit
impressive predictive capabilities, they are well-positioned to be strong
compressors. In this work, we advocate for viewing the prediction problem
through the lens of compression and evaluate the compression capabilities of
large (foundation) models. We show that large language models are powerful
general-purpose predictors and that the compression viewpoint provides novel
insights into scaling laws, tokenization, and in-context learning. For example,
Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to
43.4% and LibriSpeech samples to 16.4% of their raw size, beating
domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.
Finally, we show that the prediction-compression equivalence allows us to use
any compressor (like gzip) to build a conditional generative model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fisher Mask Nodes for Language Model Merging <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Ganesh Nathan, Suchithra M S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup of 57.4x in the biggest model. Our results prove the potential of our
method in current multi-task learning environments and suggest its scalability
and adaptability to new model architectures and learning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-MARS: Improving Visual Representations by Circumventing Text Feature
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyush Maini, Sachin Goyal, Zachary C. Lipton, J. Zico Kolter, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large web-sourced multimodal datasets have powered a slew of new methods for
learning general-purpose visual representations, advancing the state of the art
in computer vision and revolutionizing zero- and few-shot recognition. One
crucial decision facing practitioners is how, if at all, to curate these
ever-larger datasets. For example, the creators of the LAION-5B dataset chose
to retain only image-caption pairs whose CLIP similarity score exceeded a
designated threshold. In this paper, we propose a new state-of-the-art data
filtering approach motivated by our observation that nearly 40% of LAION's
images contain text that overlaps significantly with the caption. Intuitively,
such data could be wasteful as it incentivizes models to perform optical
character recognition rather than learning visual features. However, naively
removing all such data could also be wasteful, as it throws away images that
contain visual features (in addition to overlapping text). Our simple and
scalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those
pairs where the text dominates the remaining visual features -- by first
masking out the text and then filtering out those with a low CLIP similarity
score of the masked image. Experimentally, T-MARS outperforms the top-ranked
method on the "medium scale" of DataComp (a data filtering benchmark) by a
margin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic
evaluation on various data pool sizes from 2M to 64M shows that the accuracy
gains enjoyed by T-MARS linearly increase as data and compute are scaled
exponentially. Code is available at https://github.com/locuslab/T-MARS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024. Oral at ICCV Datacomp 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis aims to create accurate, executable programs from problem
specifications, specifically from natural language descriptions in our context.
Recent studies have leveraged the power of reinforcement learning (RL) in
conjunction with large language models (LLMs), significantly enhancing code
generation capabilities. The application of RL focuses on directly optimizing
for functional correctness, offering an advantage over conventional supervised
methods. Despite policy-based RL methods dominating the literature on RL for
program synthesis, the nature of program synthesis tasks hints at a natural
alignment with value-based methods. This stems from the rich collection of
off-policy programs, including those developed by human programmers and also
historical samples, coupled with the straightforward verification of generated
programs through automated unit testing, meaning rewards are easy to obtain.
Diverging from the dominant use of policy-based algorithms, our work explores
the feasibility of value-based approaches, leading to the development of our
$\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based
methods presents challenges due to the enormous search space inherent to
program synthesis. To this end, we introduce an initialization protocol for RL
agents utilizing pre-trained LMs and a conservative Bellman operator to reduce
training complexities. Moreover, we demonstrate how to leverage the learned
value functions as a dual strategy to post-process generated programs. Our
empirical evaluations demonstrated $\mathcal{B}$-Coder's capability in
achieving state-of-the-art performance when compared to policy-based methods.
Remarkably, this achievement is reached with minimal reward engineering effort,
highlighting the effectiveness of value-based RL, independent of reward
designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieve to Explain: Evidence-driven Predictions with <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Patel, Angus Brayne, Rogier Hintzen, Daniel Jaroslawicz, Georgiana Neculae, Dane Corneil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models, particularly language models, are notoriously
difficult to introspect. Black-box models can mask both issues in model
training and harmful biases. For human-in-the-loop processes, opaque
predictions can drive lack of trust, limiting a model's impact even when it
performs effectively. To address these issues, we introduce Retrieve to Explain
(R2E). R2E is a retrieval-based language model that prioritizes amongst a
pre-defined set of possible answers to a research question based on the
evidence in a document corpus, using Shapley values to identify the relative
importance of pieces of evidence to the final prediction. R2E can adapt to new
evidence without retraining, and incorporate structured data through templating
into natural language. We assess on the use case of drug target identification
from published scientific literature, where we show that the model outperforms
an industry-standard genetics-based approach on predicting clinical trial
outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Refinement of Translations: Large <span class="highlight-title">Language Models</span> for
  Sentence and Document-Level Post-Editing <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Koneru, Miriam Exel, Matthias Huck, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM's) have demonstrated considerable success in
various Natural Language Processing tasks, but they have yet to attain
state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
their significant performance in tasks demanding a broad understanding and
contextual processing shows their potential for translation. To exploit these
abilities, we investigate using LLM's for MT and explore recent
parameter-efficient fine-tuning techniques. Surprisingly, our initial
experiments find that fine-tuning for translation purposes even led to
performance degradation. To overcome this, we propose an alternative approach:
adapting LLM's as Automatic Post-Editors (APE) rather than direct translators.
Building on the LLM's exceptional ability to process and generate lengthy
sequences, we also propose extending our approach to document-level
translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can
yield significant improvements across both sentence and document-level metrics
while generalizing to out-of-domain data. Most notably, we achieve a
state-of-the-art accuracy rate of 89\% on the ContraPro test set, which
specifically assesses the model's ability to resolve pronoun ambiguities when
translating from English to German. Lastly, we investigate a practical scenario
involving manual post-editing for document-level translation, where reference
context is made available. Here, we demonstrate that leveraging human
corrections can significantly reduce the number of edits required for
subsequent translations (Interactive Demo for integrating manual feedback can
be found here:
https://huggingface.co/spaces/skoneru/contextual_refinement_ende).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  <span class="highlight-title">Language Models</span> from the Perspective of Position Bias <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error Norm Truncation: Robust Training in the Presence of Data Noise for
  Text <span class="highlight-title">Generation</span> Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, Kenton Murray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text generation models are notoriously vulnerable to errors in the training
data. With the wide-spread availability of massive amounts of web-crawled data
becoming more commonplace, how can we enhance the robustness of models trained
on a massive amount of noisy web-crawled text? In our work, we propose Error
Norm Truncation (ENT), a robust enhancement method to the standard training
objective that truncates noisy data. Compared to methods that only uses the
negative log-likelihood loss to estimate data quality, our method provides a
more accurate estimation by considering the distribution of non-target tokens,
which is often overlooked by previous work. Through comprehensive experiments
across language modeling, machine translation, and text summarization, we show
that equipping text generation models with ENT improves generation quality over
standard training and previous soft and hard truncation methods. Furthermore,
we show that our method improves the robustness of models against two of the
most detrimental types of noise in machine translation, resulting in an
increase of more than 2 BLEU points over the MLE baseline when up to 50% of
noise is added to the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synergizing Machine Learning & Symbolic Methods: A <span class="highlight-title">Survey</span> on Hybrid
  Approaches to Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rrubaa Panchendrarajan, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of machine learning and symbolic approaches have underscored
their strengths and weaknesses in Natural Language Processing (NLP). While
machine learning approaches are powerful in identifying patterns in data, they
often fall short in learning commonsense and the factual knowledge required for
the NLP tasks. Meanwhile, the symbolic methods excel in representing
knowledge-rich data. However, they struggle to adapt dynamic data and
generalize the knowledge. Bridging these two paradigms through hybrid
approaches enables the alleviation of weaknesses in both while preserving their
strengths. Recent studies extol the virtues of this union, showcasing promising
results in a wide range of NLP tasks. In this paper, we present an overview of
hybrid approaches used for NLP. Specifically, we delve into the
state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks
requiring natural language understanding, generation, and reasoning.
Furthermore, we discuss the existing resources available for hybrid approaches
for NLP along with the challenges and future directions, offering a roadmap for
future research avenues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised according to review comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Claim Detection for Automated Fact-checking: A <span class="highlight-title">Survey</span> on Monolingual,
  Multilingual and Cross-Lingual Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rrubaa Panchendrarajan, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated fact-checking has drawn considerable attention over the past few
decades due to the increase in the diffusion of misinformation on online
platforms. This is often carried out as a sequence of tasks comprising (i) the
detection of sentences circulating in online platforms which constitute claims
needing verification, followed by (ii) the verification process of those
claims. This survey focuses on the former, by discussing existing efforts
towards detecting claims needing fact-checking, with a particular focus on
multilingual data and methods. This is a challenging and fertile direction
where existing methods are yet far from matching human performance due to the
profoundly challenging nature of the issue. Especially, the dissemination of
information across multiple social platforms, articulated in multiple languages
and modalities demands more generalized solutions for combating misinformation.
Focusing on multilingual misinformation, we present a comprehensive survey of
existing multilingual claim detection research. We present state-of-the-art
multilingual claim detection research categorized into three key factors of the
problem, verifiability, priority, and similarity. Further, we present a
detailed overview of the existing multilingual datasets along with the
challenges and suggest possible future advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Streamlining Social Media Information Retrieval for COVID-19 Research
  with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hua, Jiageng Wu, Shixu Lin, Minghui Li, Yujie Zhang, Dinah Foer, Siwen Wang, Peilin Zhou, Jie Yang, Li Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Social media-based public health research is crucial for epidemic
surveillance, but most studies identify relevant corpora with keyword-matching.
This study develops a system to streamline the process of curating colloquial
medical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial
symptom dictionary from COVID-19-related tweets as proof of concept. Methods:
COVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The
pipeline includes three modules: a named entity recognition module to detect
symptoms in tweets; an entity normalization module to aggregate detected
entities; and a mapping module that iteratively maps entities to Unified
Medical Language System concepts. A random 500 entity sample were drawn from
the final dictionary for accuracy validation. Additionally, we conducted a
symptom frequency distribution analysis to compare our dictionary to a
pre-defined lexicon from previous research. Results: We identified 498,480
unique symptom entity expressions from the tweets. Pre-processing reduces the
number to 18,226. The final dictionary contains 38,175 unique expressions of
symptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom
distribution analysis found that our dictionary detects more symptoms and is
effective at identifying psychiatric disorders like anxiety and depression,
often missed by pre-defined lexicons. Conclusions: This study advances public
health research by implementing a novel, systematic pipeline for curating
symptom lexicons from social media data. The final lexicon's high accuracy,
validated by medical professionals, underscores the potential of this
methodology to reliably interpret and categorize vast amounts of unstructured
social media data into actionable medical insights across diverse linguistic
and regional landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated full paper. Abstract presented at IEEE ICHI 2023 and AMIA
  Annual Symposium 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predict the Next Word: Humans exhibit uncertainty in this task and
  <span class="highlight-title">language models</span> _____ <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenia Ilia, Wilker Aziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are statistical models trained to assign probability to
human-generated text. As such, it is reasonable to question whether they
approximate linguistic variability exhibited by humans well. This form of
statistical assessment is difficult to perform at the passage level, for it
requires acceptability judgements (i.e., human evaluation) or a robust
automated proxy (which is non-trivial). At the word level, however, given some
context, samples from an LM can be assessed via exact matching against a
prerecorded dataset of alternative single-word continuations of the available
context. We exploit this fact and evaluate the LM's ability to reproduce
variability that humans (in particular, a population of English speakers)
exhibit in the 'next word prediction' task. This can be seen as assessing a
form of calibration, which, in the context of text classification, Baan et al.
(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and
ChatGPT and find that they exhibit fairly low calibration to human uncertainty.
We also verify the failure of expected calibration error (ECE) to reflect this,
and as such, advise the community against relying on it in this setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PerceptionCLIP: Visual Classification by Inferring and Conditioning on
  Contexts <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models like CLIP are widely used in zero-shot image
classification due to their ability to understand various visual concepts and
natural language descriptions. However, how to fully leverage CLIP's
unprecedented human-like understanding capabilities to achieve better
performance is still an open question. This paper draws inspiration from the
human visual perception process: when classifying an object, humans first infer
contextual attributes (e.g., background and orientation) which help separate
the foreground object from the background, and then classify the object based
on this information. Inspired by it, we observe that providing CLIP with
contextual attributes improves zero-shot image classification and mitigates
reliance on spurious features. We also observe that CLIP itself can reasonably
infer the attributes from an image. With these observations, we propose a
training-free, two-step zero-shot classification method PerceptionCLIP. Given
an image, it first infers contextual attributes (e.g., background) and then
performs object classification conditioning on them. Our experiments show that
PerceptionCLIP achieves better generalization, group robustness, and
interoperability. Our code is available at
https://github.com/umd-huang-lab/perceptionCLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Executable Code Actions Elicit Better LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) agents, capable of performing a broad range of
actions, such as invoking tools and controlling robots, show great potential in
tackling real-world challenges. LLM agents are typically prompted to produce
actions by generating JSON or text in a pre-defined format, which is usually
limited by constrained action space (e.g., the scope of pre-defined tools) and
restricted flexibility (e.g., inability to compose multiple tools). This work
proposes to use executable Python code to consolidate LLM agents' actions into
a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct
can execute code actions and dynamically revise prior actions or emit new
actions upon new observations through multi-turn interactions. Our extensive
analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that
CodeAct outperforms widely used alternatives (up to 20% higher success rate).
The encouraging performance of CodeAct motivates us to build an open-source LLM
agent that interacts with environments by executing interpretable code and
collaborates with users using natural language. To this end, we collect an
instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn
interactions using CodeAct. We show that it can be used with existing data to
improve models in agent-oriented tasks without compromising their general
capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with
Python interpreter and uniquely tailored to perform sophisticated tasks (e.g.,
model training) using existing libraries and autonomously self-debug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data, model, and demo are available at
  https://github.com/xingyaoww/code-act</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Machine Translation with <span class="highlight-title">Human Feedback</span>: An Exploration of
  Quality Estimation as a Reward Model <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12873v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12873v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insufficient modeling of human preferences within the reward model is a major
obstacle for leveraging human feedback to improve translation quality.
Fortunately, quality estimation (QE), which predicts the quality of a given
translation without reference, has achieved impressive alignment with human
evaluations in the last two years. In this work, we investigate the potential
of employing the QE model as the reward model to predict human preferences for
feedback training. We first identify the overoptimization problem during
QE-based feedback training, manifested as an increase in reward while
translation quality declines. We examine the problem and argue that the
vulnerability of the QE model might lead to high rewards for incorrect
translations, resulting in overoptimization and error propagation. To address
the problem, we adopt a simple yet effective method that uses heuristic rules
to detect the incorrect translations and assigns a penalty term to the reward
scores of them. Experimental results show that the proposed QE-based feedback
training achieves consistent and significant improvements across various
settings, further verified through human preference studies. Our subsequent
analysis demonstrates the high data efficiency of the proposed QE-based
feedback training: it outperforms systems using larger parallel corpora by a
small amount of monolingual data. Our code is available at:
https://github.com/zwhe99/FeedbackMT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParallelPARC: A Scalable Pipeline for Generating Natural-Language
  Analogies <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analogy-making is central to human cognition, allowing us to adapt to novel
situations -- an ability that current AI systems still lack. Most analogy
datasets today focus on simple analogies (e.g., word analogies); datasets
including complex types of analogies are typically manually curated and very
small. We believe that this holds back progress in computational analogy. In
this work, we design a data generation pipeline, ParallelPARC (Parallel
Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to
create complex, paragraph-based analogies, as well as distractors, both simple
and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset
of analogies between scientific processes. We publish a gold-set, validated by
humans, and a silver-set, generated automatically. We test LLMs' and humans'
analogy recognition in binary and multiple-choice settings, and found that
humans outperform the best models (~13% gap) after a light supervision. We
demonstrate that our silver-set is useful for training models. Lastly, we show
challenging distractors confuse LLMs, but not humans. We hope our pipeline will
encourage research in this emerging field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PET-SQL: A <span class="highlight-title">Prompt</span>-enhanced Two-stage <span class="highlight-title">Text-to-SQL</span> Framework with
  Cross-consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large
language models (LLM) on in-context learning, achieving significant results.
Nevertheless, they face challenges when dealing with verbose database
information and complex user intentions. This paper presents a two-stage
framework to enhance the performance of current LLM-based natural language to
SQL systems. We first introduce a novel prompt representation, called
reference-enhanced representation, which includes schema information and
randomly sampled cell values from tables to instruct LLMs in generating SQL
queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot
demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After
that, the mentioned entities in PreSQL are parsed to conduct schema linking,
which can significantly compact the useful information. In the second stage,
with the linked schema, we simplify the prompt's schema information and
instruct the LLM to produce the final SQL. Finally, as the post-refinement
module, we propose using cross-consistency across different LLMs rather than
self-consistency within a particular LLM. Our methods achieve new SOTA results
on the Spider benchmark, with an execution accuracy of 87.6%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Meaning Composition in the Human Brain with Composition Scores
  from Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of meaning composition, wherein smaller units like morphemes or
words combine to form the meaning of phrases and sentences, is essential for
human sentence comprehension. Despite extensive neurolinguistic research into
the brain regions involved in meaning composition, a computational metric to
quantify the extent of composition is still lacking. Drawing on the key-value
memory interpretation of transformer feed-forward network blocks, we introduce
the Composition Score, a novel model-based metric designed to quantify the
degree of meaning composition during sentence comprehension. Experimental
findings show that this metric correlates with brain clusters associated with
word frequency, structural processing, and general sensitivity to words,
suggesting the multifaceted nature of meaning composition during human sentence
comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Angry Men, Sad Women: Large <span class="highlight-title">Language Models</span> Reflect Gendered Stereotypes
  in Emotion Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin Abercrombie, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) reflect societal norms and biases, especially
about gender. While societal biases and stereotypes have been extensively
researched in various NLP applications, there is a surprising gap for emotion
analysis. However, emotion and gender are closely linked in societal discourse.
E.g., women are often thought of as more empathetic, while men's anger is more
socially accepted. To fill this gap, we present the first comprehensive study
of gendered emotion attribution in five state-of-the-art LLMs (open- and
closed-source). We investigate whether emotions are gendered, and whether these
variations are based on societal stereotypes. We prompt the models to adopt a
gendered persona and attribute emotions to an event like 'When I had a serious
argument with a dear person'. We then analyze the emotions generated by the
models in relation to the gender-event pairs. We find that all models
consistently exhibit gendered emotions, influenced by gender stereotypes. These
findings are in line with established research in psychology and gender
studies. Our study sheds light on the complex societal interplay between
language, gender, and emotion. The reproduction of emotion stereotypes in LLMs
allows us to use those models to study the topic in detail, but raises
questions about the predictive use of those same LLMs for emotion applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for
  <span class="highlight-title">Self-supervised</span> Representations of French Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is at the origin of unprecedented improvements
in many different domains including computer vision and natural language
processing. Speech processing drastically benefitted from SSL as most of the
current domain-related tasks are now being approached with pre-trained models.
This work introduces LeBenchmark 2.0 an open-source framework for assessing and
building SSL-equipped French speech technologies. It includes documented,
large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous
speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to
one billion learnable parameters shared with the community, and an evaluation
protocol made of six downstream tasks to complement existing benchmarks.
LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for
speech with the investigation of frozen versus fine-tuned downstream models,
task-agnostic versus task-specific pre-trained models as well as a discussion
on the carbon footprint of large-scale model training. Overall, the newly
introduced models trained on 14,000 hours of French speech outperform
multilingual and previous LeBenchmark SSL models across the benchmark but also
required up to four times more energy for pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computer Science and Language. Preprint allowed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotions are a central aspect of communication. Consequently, emotion
analysis (EA) is a rapidly growing field in natural language processing (NLP).
However, there is no consensus on scope, direction, or methods. In this paper,
we conduct a thorough review of 154 relevant NLP publications from the last
decade. Based on this review, we address four different questions: (1) How are
EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and
which emotions are modeled? (3) Is the subjectivity of emotions considered in
terms of demographics and cultural factors? and (4) What are the primary NLP
applications for EA? We take stock of trends in EA and tasks, emotion
frameworks used, existing datasets, methods, and applications. We then discuss
four lacunae: (1) the absence of demographic and cultural aspects does not
account for the variation in how emotions are perceived, but instead assumes
they are universally experienced in the same manner; (2) the poor fit of
emotion categories from the two main emotion theories to the task; (3) the lack
of standardized EA terminology hinders gap identification, comparison, and
future goals; and (4) the absence of interdisciplinary research isolates EA
from insights in other fields. Our work will enable more focused research into
EA and a more holistic approach to modeling emotions in NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency effects in Linear Discriminative Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Heitmeier, Yu-Ying Chuang, Seth D. Axen, R. Harald Baayen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word frequency is a strong predictor in most lexical processing tasks. Thus,
any model of word recognition needs to account for how word frequency effects
arise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)
models lexical processing with linear mappings between words' forms and their
meanings. So far, the mappings can either be obtained incrementally via
error-driven learning, a computationally expensive process able to capture
frequency effects, or in an efficient, but frequency-agnostic solution
modelling the theoretical endstate of learning (EL) where all words are learned
optimally. In this study we show how an efficient, yet frequency-informed
mapping between form and meaning can be obtained (Frequency-informed learning;
FIL). We find that FIL well approximates an incremental solution while being
computationally much cheaper. FIL shows a relatively low type- and high
token-accuracy, demonstrating that the model is able to process most word
tokens encountered by speakers in daily life correctly. We use FIL to model
reaction times in the Dutch Lexicon Project (Keuleers et al., 2010) and find
that FIL predicts well the S-shaped relationship between frequency and the mean
of reaction times but underestimates the variance of reaction times for low
frequency words. FIL is also better able to account for priming effects in an
auditory lexical decision task in Mandarin Chinese (Lee, 2007), compared to EL.
Finally, we used ordered data from CHILDES (Brown, 1973; Demuth et al., 2006)
to compare mappings obtained with FIL and incremental learning. The mappings
are highly correlated, but with FIL some nuances based on word ordering effects
are lost. Our results show how frequency effects in a learning model can be
simulated efficiently, and raise questions about how to best account for
low-frequency words in cognitive models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures, 3 tables; revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liang, Yuwei Wang, Yang Li, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.56% in the DART and
WebNLG tasks, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the potential of AI-assisted pragmatic annotation: The case of
  apologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Yu, Luyang Li, Hang Su, Matteo Fuoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Certain forms of linguistic annotation, like part of speech and semantic
tagging, can be automated with high accuracy. However, manual annotation is
still necessary for complex pragmatic and discursive features that lack a
direct mapping to lexical forms. This manual process is time-consuming and
error-prone, limiting the scalability of function-to-form approaches in corpus
linguistics. To address this, our study explores automating pragma-discursive
corpus annotation using large language models (LLMs). We compare ChatGPT, the
Bing chatbot, and a human coder in annotating apology components in English
based on the local grammar framework. We find that the Bing chatbot
outperformed ChatGPT, with accuracy approaching that of a human coder. These
results suggest that AI can be successfully deployed to aid pragma-discursive
corpus annotation, making the process more efficient and scalable. Keywords:
linguistic annotation, function-to-form approaches, large language models,
local grammar analysis, Bing chatbot, ChatGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures, 3 tablels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAM: Global Reasoning for Multi-Page VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of transformer-based large language models brings forward
the challenge of processing long sequences. In document visual question
answering (DocVQA), leading methods focus on the single-page setting, while
documents can span hundreds of pages. We present GRAM, a method that seamlessly
extends pre-trained single-page models to the multi-page setting, without
requiring computationally-heavy pretraining. To do so, we leverage a
single-page encoder for local page-level understanding, and enhance it with
document-level designated layers and learnable tokens, facilitating the flow of
information across pages for global reasoning. To enforce our model to utilize
the newly introduced document tokens, we propose a tailored bias adaptation
method. For additional computational savings during decoding, we introduce an
optional compression stage using our compression-transformer
(C-Former),reducing the encoded sequence length, thereby allowing a tradeoff
between quality and latency. Extensive experiments showcase GRAM's
state-of-the-art performance on the benchmarks for multi-page DocVQA,
demonstrating the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU.
  CMMMU includes 12k manually collected multimodal questions from college
exams, quizzes, and textbooks, covering six core disciplines: Art & Design,
Business, Science, Health & Medicine, Humanities & Social Science, and Tech &
Engineering, like its companion, MMMU. These questions span 30 subjects and
comprise 39 highly heterogeneous image types, such as charts, diagrams, maps,
tables, music sheets, and chemical structures.
  CMMMU focuses on complex perception and reasoning with domain-specific
knowledge in the Chinese context. We evaluate 11 open-source LLMs and one
proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,
indicating a large space for improvement. CMMMU will boost the community to
build the next-generation LMMs towards expert artificial intelligence and
promote the democratization of LMMs by providing diverse language contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is it Really Negative? Evaluating Natural Language Video Localization
  Performance on Multiple Reliable Videos Pool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosion of multimedia content in recent years, Video Corpus Moment
Retrieval (VCMR), which aims to detect a video moment that matches a given
natural language query from multiple videos, has become a critical problem.
However, existing VCMR studies have a significant limitation since they have
regarded all videos not paired with a specific query as negative, neglecting
the possibility of including false negatives when constructing the negative
video set. In this paper, we propose an MVMR (Massive Videos Moment Retrieval)
task that aims to localize video frames within a massive video set, mitigating
the possibility of falsely distinguishing positive and negative videos. For
this task, we suggest an automatic dataset construction framework by employing
textual and visual semantic matching evaluation methods on the existing video
moment search datasets and introduce three MVMR datasets. To solve MVMR task,
we further propose a strong method, CroCs, which employs cross-directional
contrastive learning that selectively identifies the reliable and informative
negatives, enhancing the robustness of a model on MVMR task. Experimental
results on the introduced datasets reveal that existing video moment search
models are easily distracted by negative video frames, whereas our model shows
significant performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoAgent: Long-form Video Understanding with Large Language Model as
  Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Wang, Yuhui Zhang, Orr Zohar, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form video understanding represents a significant challenge within
computer vision, demanding a model capable of reasoning over long multi-modal
sequences. Motivated by the human cognitive process for long-form video
understanding, we emphasize interactive reasoning and planning over the ability
to process lengthy visual inputs. We introduce a novel agent-based system,
VideoAgent, that employs a large language model as a central agent to
iteratively identify and compile crucial information to answer a question, with
vision-language foundation models serving as tools to translate and retrieve
visual information. Evaluated on the challenging EgoSchema and NExT-QA
benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only
8.4 and 8.2 frames used on average. These results demonstrate superior
effectiveness and efficiency of our method over the current state-of-the-art
methods, highlighting the potential of agent-based approaches in advancing
long-form video understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A
  Pilot Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training image representations from the raw text about images enables
zero-shot vision transfer to downstream tasks. Through pre-training on millions
of samples collected from the internet, multimodal foundation models, such as
CLIP, produce state-of-the-art zero-shot results that often reach
competitiveness with fully supervised methods without the need for
task-specific training. Besides the encouraging performance on classification
accuracy, it is reported that these models close the robustness gap by matching
the performance of supervised models trained on ImageNet under natural
distribution shift. Because robustness is critical to real-world applications,
especially safety-critical ones, in this paper, we present a comprehensive
evaluation based on a large-scale robustness benchmark covering 7 natural, 3
synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a
pilot study. We show that CLIP leads to a significant robustness drop compared
to supervised ImageNet models on our benchmark, especially under synthetic
distribution shift and adversarial attacks. Furthermore, data overlap analysis
suggests that the observed robustness under natural distribution shifts could
be attributed, at least in part, to data overlap. In summary, our evaluation
shows a comprehensive evaluation of robustness is necessary; and there is a
significant need to improve the robustness of zero-shot multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A
  Case Study on Domain-Specific Queries in Private Knowledge-Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Li, Ye Yuan, Zehua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed an end-to-end system design towards utilizing Retrieval Augmented
Generation (RAG) to improve the factual accuracy of Large Language Models
(LLMs) for domain-specific and time-sensitive queries related to private
knowledge-bases. Our system integrates RAG pipeline with upstream datasets
processing and downstream performance evaluation. Addressing the challenge of
LLM hallucinations, we finetune models with a curated dataset which originates
from CMU's extensive resources and annotated with the teacher model. Our
experiments demonstrate the system's effectiveness in generating more accurate
answers to domain-specific and time-sensitive inquiries. The results also
revealed the limitations of fine-tuning LLMs with small-scale and skewed
datasets. This research highlights the potential of RAG systems in augmenting
LLMs with external datasets for improved performance in knowledge-intensive
tasks. Our code and models are available on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>These authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Block-Level Draft Verification for Accelerating Speculative
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding has shown to be an effective method for lossless
acceleration of large language models (LLMs) during inference. In each
iteration, the algorithm first uses a smaller model to draft a block of tokens.
The tokens are then verified by the large model in parallel and only a subset
of tokens will be kept to guarantee that the final output follows the
distribution of the large model. In all of the prior speculative decoding
works, the draft verification is performed token-by-token independently. In
this work, we propose a better draft verification algorithm that provides
additional wall-clock speedup without incurring additional computation cost and
draft tokens. We first formulate the draft verification step as a block-level
optimal transport problem. The block-level formulation allows us to consider a
wider range of draft verification algorithms and obtain a higher number of
accepted tokens in expectation in one draft block. We propose a verification
algorithm that achieves the optimal accepted length for the block-level
transport problem. We empirically evaluate our proposed block-level
verification algorithm in a wide range of tasks and datasets, and observe
consistent improvements in wall-clock speedup when compared to token-level
verification algorithm. To the best of our knowledge, our work is the first to
establish improvement over speculative decoding through a better draft
verification algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monotonic Representation of Numeric Properties in <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Heinzerling, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) can express factual knowledge involving numeric
properties such as Karl Popper was born in 1902. However, how this information
is encoded in the model's internal representations is not understood well.
Here, we introduce a simple method for finding and editing representations of
numeric properties such as an entity's birth year. Empirically, we find
low-dimensional subspaces that encode numeric properties monotonically, in an
interpretable and editable fashion. When editing representations along
directions in these subspaces, LM output changes accordingly. For example, by
patching activations along a "birthyear" direction we can make the LM express
an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was
born in 1957, Karl Popper was born in 1968. Property-encoding directions exist
across several numeric properties in all models under consideration, suggesting
the possibility that monotonic representation of numeric properties
consistently emerges during LM pretraining. Code:
https://github.com/bheinzerling/numeric-property-repr
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for
  Evaluating Vision <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EXAMS-V, a new challenging multi-discipline multimodal
multilingual exam benchmark for evaluating vision language models. It consists
of 20,932 multiple-choice questions across 20 school disciplines covering
natural science, social science, and other miscellaneous studies, e.g.,
religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal
features such as text, images, tables, figures, diagrams, maps, scientific
symbols, and equations. The questions come in 11 languages from 7 language
families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering
school exam questions from various countries, with a variety of education
systems. This distinctive approach calls for intricate reasoning across diverse
languages and relies on region-specific knowledge. Solving the problems in the
dataset requires advanced perception and joint reasoning over the text and the
visual content of the image. Our evaluation results demonstrate that this is a
challenging dataset, which is difficult even for advanced vision-text models
such as GPT-4V and Gemini; this underscores the inherent complexity of the
dataset and its significance as a future benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriSum: Learning Summarization Ability from Large <span class="highlight-title">Language Models</span> with
  Structured Rationale <span class="chip">NAACL'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has significantly advanced natural
language processing tasks like text summarization. However, their large size
and computational demands, coupled with privacy concerns in data transmission,
limit their use in resource-constrained and privacy-centric settings. To
overcome this, we introduce TriSum, a framework for distilling LLMs' text
summarization abilities into a compact, local model. Initially, LLMs extract a
set of aspect-triple rationales and summaries, which are refined using a
dual-scoring method for quality. Next, a smaller local model is trained with
these tasks, employing a curriculum learning strategy that evolves from simple
to complex tasks. Our method enhances local model performance on various
benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by
4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by
providing insights into the summarization rationale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating grammatical abstraction in <span class="highlight-title">language models</span> using few-shot
  learning of novel noun gender <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Sukumaran, Conor Houghton, Nina Kazanina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can learn a new word and infer its grammatical properties from very
few examples. They have an abstract notion of linguistic properties like
grammatical gender and agreement rules that can be applied to novel syntactic
contexts and words. Drawing inspiration from psycholinguistics, we conduct a
noun learning experiment to assess whether an LSTM and a decoder-only
transformer can achieve human-like abstraction of grammatical gender in French.
Language models were tasked with learning the gender of a novel noun embedding
from a few examples in one grammatical agreement context and predicting
agreement in another, unseen context. We find that both language models
effectively generalise novel noun gender from one to two learning examples and
apply the learnt gender across agreement contexts, albeit with a bias for the
masculine gender category. Importantly, the few-shot updates were only applied
to the embedding layers, demonstrating that models encode sufficient gender
information within the word embedding space. While the generalisation behaviour
of models suggests that they represent grammatical gender as an abstract
category, like humans, further work is needed to explore the details of how
exactly this is implemented. For a comparative perspective with human
behaviour, we conducted an analogous one-shot novel noun gender learning
experiment, which revealed that native French speakers, like language models,
also exhibited a masculine gender bias and are not excellent one-shot learners
either.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024; Findings of the Association for Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDGP: Automatic Cloze Distractor <span class="highlight-title">Generation</span> based on <span class="highlight-title">Pre-train</span>ed
  Language Model <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Hsuan Chiang, Ssu-Cheng Wang, Yao-Chung Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually designing cloze test consumes enormous time and efforts. The major
challenge lies in wrong option (distractor) selection. Having carefully-design
distractors improves the effectiveness of learner ability assessment. As a
result, the idea of automatically generating cloze distractor is motivated. In
this paper, we investigate cloze distractor generation by exploring the
employment of pre-trained language models (PLMs) as an alternative for
candidate distractor generation. Experiments show that the PLM-enhanced model
brings a substantial performance improvement. Our best performing model
advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our
code and dataset is available at https://github.com/AndyChiangSH/CDGP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of short paper, EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-SMART: Universal Science Multimodal Analysis and Research
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In scientific research and its application, scientific literature analysis is
crucial as it allows researchers to build on the work of others. However, the
fast growth of scientific knowledge has led to a massive increase in scholarly
articles, making in-depth literature analysis increasingly challenging and
time-consuming. The emergence of Large Language Models (LLMs) has offered a new
way to address this challenge. Known for their strong abilities in summarizing
texts, LLMs are seen as a potential tool to improve the analysis of scientific
literature. However, existing LLMs have their own limits. Scientific literature
often includes a wide range of multimodal elements, such as molecular
structure, tables, and charts, which are hard for text-focused LLMs to
understand and analyze. This issue points to the urgent need for new solutions
that can fully understand and analyze multimodal content in scientific
literature. To answer this demand, we present Uni-SMART (Universal Science
Multimodal Analysis and Research Transformer), an innovative model designed for
in-depth understanding of multimodal scientific literature. Through rigorous
quantitative evaluation across several domains, Uni-SMART demonstrates superior
performance over leading text-focused LLMs. Furthermore, our exploration
extends to practical applications, including patent infringement detection and
nuanced analysis of charts. These applications not only highlight Uni-SMART's
adaptability but also its potential to revolutionize how we interact with
scientific literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Verena Blaschke, Barbara Kovačić, Siyao Peng, Hinrich Schütze, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of the Universal Dependencies (UD) project exemplified by
its impressive language breadth, there is still a lack in `within-language
breadth': most treebanks focus on standard languages. Even for German, the
language with the most annotations in UD, so far no treebank exists for one of
its language varieties spoken by over 10M people: Bavarian. To contribute to
closing this gap, we present the first multi-dialect Bavarian treebank
(MaiBaam) manually annotated with part-of-speech and syntactic dependency
information in UD, covering multiple text genres (wiki, fiction, grammar
examples, social, non-fiction). We highlight the morphosyntactic differences
between the closely-related Bavarian and German and showcase the rich
variability of speakers' orthographies. Our corpus includes 15k tokens,
covering dialects from all Bavarian-speaking areas spanning three countries. We
provide baseline parsing and POS tagging results, which are lower than results
obtained on German and vary substantially between different graph-based
parsers. To support further research on Bavarian syntax, we make our dataset,
language-specific guidelines and code publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification
  with Fine-Tuning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao, Wen-Chih Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Pre-CoFactv3, a comprehensive framework comprised
of Question Answering and Text Classification components for fact verification.
Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and
the FakeNet model, we address the challenges of fact verification. Our
experiments explore diverse approaches, comparing different Pre-trained LLMs,
introducing FakeNet, and implementing various ensemble methods. Notably, our
team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop,
surpassing the baseline accuracy by 103% and maintaining a 70% lead over the
second competitor. This success underscores the efficacy of our approach and
its potential contributions to advancing fact verification research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024 Workshop: FACTIFY 3.0 - Workshop Series on
  Multimodal Fact-Checking and Hate Speech Detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Question on the Explainability of Large <span class="highlight-title">Language Models</span> and the
  Word-Level Univariate First-Order Plausibility Assumption <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremie Bogaert, Francois-Xavier Standaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explanations of large language models have recently been shown to be
sensitive to the randomness used for their training, creating a need to
characterize this sensitivity. In this paper, we propose a characterization
that questions the possibility to provide simple and informative explanations
for such models. To this end, we give statistical definitions for the
explanations' signal, noise and signal-to-noise ratio. We highlight that, in a
typical case study where word-level univariate explanations are analyzed with
first-order statistical tools, the explanations of simple feature-based models
carry more signal and less noise than those of transformer ones. We then
discuss the possibility to improve these results with alternative definitions
of signal and noise that would capture more complex explanations and analysis
methods, while also questioning the tradeoff with their plausibility for
readers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures, Accepted and presented at AAAI 2024 (ReLM
  workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Translation All You Need? A Study on Solving Multilingual Tasks with
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong multilingual
capabilities; yet, they are mostly English-centric due to the imbalanced
training corpora. Existing works leverage this phenomenon to improve their
multilingual performances on NLP tasks. In this work, we extend the evaluation
from NLP tasks to real user queries. We find that even though translation into
English can help improve the performance of multilingual NLP tasks for
English-centric LLMs, it may not be optimal for all scenarios. For
culture-related tasks that need deep language understanding, prompting in the
native language proves to be more promising since it can capture the nuances
related to culture and language. Therefore, we advocate for more efforts
towards the development of strong multilingual LLMs instead of just
English-centric LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Big Data Approach to Understand Sub-national Determinants of FDI in
  Africa 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Fronzetti Colladon, R. Vestrelli, S. Bait, M. M. Schiraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various macroeconomic and institutional factors hinder FDI inflows, including
corruption, trade openness, access to finance, and political instability.
Existing research mostly focuses on country-level data, with limited
exploration of firm-level data, especially in developing countries. Recognizing
this gap, recent calls for research emphasize the need for qualitative data
analysis to delve into FDI determinants, particularly at the regional level.
This paper proposes a novel methodology, based on text mining and social
network analysis, to get information from more than 167,000 online news
articles to quantify regional-level (sub-national) attributes affecting FDI
ownership in African companies. Our analysis extends information on obstacles
to industrial development as mapped by the World Bank Enterprise Surveys.
Findings suggest that regional (sub-national) structural and institutional
characteristics can play an important role in determining foreign ownership.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comprehensive study on Frequent Pattern Mining and Clustering
  categories for topic detection in Persian text stream 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elnaz Zafarani-Moattar, Mohammad Reza Kangavari, Amir Masoud Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic detection is a complex process and depends on language because it
somehow needs to analyze text. There have been few studies on topic detection
in Persian, and the existing algorithms are not remarkable. Therefore, we aimed
to study topic detection in Persian. The objectives of this study are: 1) to
conduct an extensive study on the best algorithms for topic detection, 2) to
identify necessary adaptations to make these algorithms suitable for the
Persian language, and 3) to evaluate their performance on Persian social
network texts. To achieve these objectives, we have formulated two research
questions: First, considering the lack of research in Persian, what
modifications should be made to existing frameworks, especially those developed
in English, to make them compatible with Persian? Second, how do these
algorithms perform, and which one is superior? There are various topic
detection methods that can be categorized into different categories. Frequent
pattern and clustering are selected for this research, and a hybrid of both is
proposed as a new category. Then, ten methods from these three categories are
selected. All of them are re-implemented from scratch, changed, and adapted
with Persian. These ten methods encompass different types of topic detection
methods and have shown good performance in English. The text of Persian social
network posts is used as the dataset. Additionally, a new multiclass evaluation
criterion, called FS, is used in this paper for the first time in the field of
topic detection. Approximately 1.4 billion tokens are processed during
experiments. The results indicate that if we are searching for keyword-topics
that are easily understandable by humans, the hybrid category is better.
However, if the aim is to cluster posts for further analysis, the frequent
pattern category is more suitable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HawkEye: Training Video-Text LLMs for Grounding Text in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-text Large Language Models (video-text LLMs) have shown remarkable
performance in answering questions and holding conversations on simple videos.
However, they perform almost the same as random on grounding text queries in
long and complicated videos, having little ability to understand and reason
about temporal information, which is the most fundamental difference between
videos and images. In this paper, we propose HawkEye, one of the first
video-text LLMs that can perform temporal video grounding in a fully
text-to-text manner. To collect training data that is applicable for temporal
video grounding, we construct InternVid-G, a large-scale video-text corpus with
segment-level captions and negative spans, with which we introduce two new
time-aware training objectives to video-text LLMs. We also propose a
coarse-grained method of representing segments in videos, which is more robust
and easier for LLMs to learn and follow than other alternatives. Extensive
experiments show that HawkEye is better at temporal video grounding and
comparable on other video-text tasks with existing video-text LLMs, which
verifies its superior video-text multi-modal understanding abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Coherence-Aware Network with Hierarchical Disentanglement for
  Aspect-Category Sentiment Analysis <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Cui, Fumiyo Fukumoto, Xinfeng Wang, Yoshimi Suzuki, Jiyi Li, Noriko Tomuro, Wanzeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-category-based sentiment analysis (ACSA), which aims to identify
aspect categories and predict their sentiments has been intensively studied due
to its wide range of NLP applications. Most approaches mainly utilize
intrasentential features. However, a review often includes multiple different
aspect categories, and some of them do not explicitly appear in the review.
Even in a sentence, there is more than one aspect category with its sentiments,
and they are entangled intra-sentence, which makes the model fail to
discriminately preserve all sentiment characteristics. In this paper, we
propose an enhanced coherence-aware network with hierarchical disentanglement
(ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture
the contexts across the whole review and to help the implicit aspect and
sentiment identification. To address the issue of multiple aspect categories
and sentiment entanglement, we propose a hierarchical disentanglement module to
extract distinct categories and sentiment features. Extensive experimental and
visualization results show that our ECAN effectively decouples multiple
categories and sentiments entangled in the coherence representations and
achieves state-of-the-art (SOTA) performance. Our codes and data are available
online: \url{https://github.com/cuijin-23/ECAN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read between the lines -- Functionality Extraction From READMEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prince Kumar, Srikanth Tamilselvam, Dinesh Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text summarization is a well-known NLP task, in this paper, we
introduce a novel and useful variant of it called functionality extraction from
Git README files. Though this task is a text2text generation at an abstract
level, it involves its own peculiarities and challenges making existing
text2text generation systems not very useful. The motivation behind this task
stems from a recent surge in research and development activities around the use
of large language models for code-related tasks, such as code refactoring, code
summarization, etc. We also release a human-annotated dataset called FuncRead,
and develop a battery of models for the task. Our exhaustive experimentation
shows that small size fine-tuned models beat any baseline models that can be
designed using popular black-box or white-box large language models (LLMs) such
as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70%
and 20% gain on the F1 score against ChatGPT and Bard respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based
  Deception <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswathy Velutharambath, Amelie Wührl, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If a person firmly believes in a non-factual statement, such as "The Earth is
flat", and argues in its favor, there is no inherent intention to deceive. As
the argumentation stems from genuine belief, it may be unlikely to exhibit the
linguistic properties associated with deception or lying. This interplay of
factuality, personal belief, and intent to deceive remains an understudied
area. Disentangling the influence of these variables in argumentation is
crucial to gain a better understanding of the linguistic properties attributed
to each of them. To study the relation between deception and factuality, based
on belief, we present the DeFaBel corpus, a crowd-sourced resource of
belief-based deception. To create this corpus, we devise a study in which
participants are instructed to write arguments supporting statements like
"eating watermelon seeds can cause indigestion", regardless of its factual
accuracy or their personal beliefs about the statement. In addition to the
generation task, we ask them to disclose their belief about the statement. The
collected instances are labelled as deceptive if the arguments are in
contradiction to the participants' personal beliefs. Each instance in the
corpus is thus annotated (or implicitly labelled) with personal beliefs of the
author, factuality of the statement, and the intended deceptiveness. The
DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive
and 388 are non-deceptive. It is the first publicly available corpus for
studying deception in German. In our analysis, we find that people are more
confident in the persuasiveness of their arguments when the statement is
aligned with their belief, but surprisingly less confident when they are
generating arguments in favor of facts. The DeFaBel corpus can be obtained from
https://www.ims.uni-stuttgart.de/data/defabel
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLP Verification: Towards a General Methodology for Certifying
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Casadio, Tanvi Dinkar, Ekaterina Komendantskaya, Luca Arnaboldi, Omri Isac, Matthew L. Daggitt, Guy Katz, Verena Rieser, Oliver Lemon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have exhibited substantial success in the field of
Natural Language Processing (NLP) and ensuring their safety and reliability is
crucial: there are safety critical contexts where such models must be robust to
variability or attack, and give guarantees over their output. Unlike Computer
Vision, NLP lacks a unified verification methodology and, despite recent
advancements in literature, they are often light on the pragmatical issues of
NLP verification. In this paper, we make an attempt to distil and evaluate
general components of an NLP verification pipeline, that emerges from the
progress in the field to date. Our contributions are two-fold. Firstly, we give
a general characterisation of verifiable subspaces that result from embedding
sentences into continuous spaces. We identify, and give an effective method to
deal with, the technical challenge of semantic generalisability of verified
subspaces; and propose it as a standard metric in the NLP verification
pipelines (alongside with the standard metrics of model accuracy and model
verifiability). Secondly, we propose a general methodology to analyse the
effect of the embedding gap, a problem that refers to the discrepancy between
verification of geometric subpspaces on the one hand, and semantic meaning of
sentences which the geometric subspaces are supposed to represent, on the other
hand. In extreme cases, poor choices in embedding of sentences may invalidate
verification results. We propose a number of practical NLP methods that can
help to identify the effects of the embedding gap; and in particular we propose
the metric of falsifiability of semantic subpspaces as another fundamental
metric to be reported as part of the NLP verification pipeline. We believe that
together these general principles pave the way towards a more consolidated and
effective development of this new domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Whole is Better than the Sum: Using Aggregated Demonstrations in
  In-Context Learning for Sequential Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Ee-Peng Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown excellent performance on various NLP
tasks. To use LLMs as strong sequential recommenders, we explore the in-context
learning approach to sequential recommendation. We investigate the effects of
instruction format, task consistency, demonstration selection, and number of
demonstrations. As increasing the number of demonstrations in ICL does not
improve accuracy despite using a long prompt, we propose a novel method called
LLMSRec-Syn that incorporates multiple demonstration users into one aggregated
demonstration. Our experiments on three recommendation datasets show that
LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation
methods. In some cases, LLMSRec-Syn can perform on par with or even better than
supervised learning methods. Our code is publicly available at
https://github.com/demoleiwang/LLMSRec_Syn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAFT: Adapting Language Model to Domain Specific RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining Large Language Models (LLMs) on large corpora of textual data is
now a standard paradigm. When using these LLMs for many downstream
applications, it is common to additionally bake in new knowledge (e.g.,
time-critical news, or private domain knowledge) into the pretrained model
either through RAG-based-prompting, or fine-tuning. However, the optimal
methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented FineTuning (RAFT), a training
recipe that improves the model's ability to answer questions in a "open-book"
in-domain settings. In RAFT, given a question, and a set of retrieved
documents, we train the model to ignore those documents that don't help in
answering the question, which we call, distractor documents. RAFT accomplishes
this by citing verbatim the right sequence from the relevant document that
would help answer the question. This coupled with RAFT's chain-of-thought-style
response helps improve the model's ability to reason. In domain-specific RAG,
RAFT consistently improves the model's performance across PubMed, HotpotQA, and
Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs
to in-domain RAG. RAFT's code and demo are open-sourced at
github.com/ShishirPatil/gorilla.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-conditioned and Non-toxic Counterspeech <span class="highlight-title">Generation</span> using
  Multi-Task <span class="highlight-title">Instruct</span>ion Tuning with RLAIF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad Akhtar, Tanmoy Chakroborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterspeech, defined as a response to mitigate online hate speech, is
increasingly used as a non-censorial solution. Addressing hate speech
effectively involves dispelling the stereotypes, prejudices, and biases often
subtly implied in brief, single-sentence statements or abuses. These implicit
expressions challenge language models, especially in seq2seq tasks, as model
performance typically excels with longer contexts. Our study introduces CoARL,
a novel framework enhancing counterspeech generation by modeling the pragmatic
implications underlying social biases in hateful statements. CoARL's first two
phases involve sequential multi-instruction tuning, teaching the model to
understand intents, reactions, and harms of offensive statements, and then
learning task-specific low-rank adapter weights for generating
intent-conditioned counterspeech. The final phase uses reinforcement learning
to fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms
existing benchmarks in intent-conditioned counterspeech generation, showing an
average improvement of 3 points in intent-conformity and 4 points in
argument-quality metrics. Extensive human evaluation supports CoARL's efficacy
in generating superior and more context-appropriate responses compared to
existing systems, including prominent LLMs like ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRAGIN: Dynamic Retrieval Augmented <span class="highlight-title">Generation</span> based on the Real-time
  Information Needs of Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triple GNNs: Introducing Syntactic and Semantic Information for
  Conversational Aspect-Based Quadruple Sentiment Analysis <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Li, Yuqing Li, Siyu Jia, Bingnan Ma, Yu Ding, Zisen Qi, Xingbang Tan, Menghan Guo, Shenghui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Aspect-Based Sentiment Analysis (DiaASQ) aims to detect
quadruples \{target, aspect, opinion, sentiment polarity\} from given
dialogues. In DiaASQ, elements constituting these quadruples are not
necessarily confined to individual sentences but may span across multiple
utterances within a dialogue. This necessitates a dual focus on both the
syntactic information of individual utterances and the semantic interaction
among them. However, previous studies have primarily focused on coarse-grained
relationships between utterances, thus overlooking the potential benefits of
detailed intra-utterance syntactic information and the granularity of
inter-utterance relationships. This paper introduces the Triple GNNs network to
enhance DiaAsQ. It employs a Graph Convolutional Network (GCN) for modeling
syntactic dependencies within utterances and a Dual Graph Attention Network
(DualGATs) to construct interactions between utterances. Experiments on two
standard datasets reveal that our model significantly outperforms
state-of-the-art baselines. The code is available at
\url{https://github.com/nlperi2b/Triple-GNNs-}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CSCWD2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repoformer: Selective Retrieval for Repository-Level Code Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, Xiaofei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). To tackle the challenges, this paper
proposes a selective RAG framework where retrieval is avoided when unnecessary.
To power this framework, we design a self-supervised learning approach that
enables a code LM to accurately self-evaluate whether retrieval can improve its
output quality and robustly leverage the potentially noisy retrieved contexts.
Using this LM as both the selective retrieval policy and the generation model,
our framework consistently outperforms the state-of-the-art prompting with an
invariable retrieval approach on diverse benchmarks including RepoEval,
CrossCodeEval, and a new benchmark. Meanwhile, our selective retrieval strategy
results in strong efficiency improvements by as much as 70% inference speedup
without harming the performance. We demonstrate that our framework effectively
accommodates different generation models, retrievers, and programming
languages. These advancements position our framework as an important step
towards more accurate and efficient repository-level code completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Half-listen: Capturing Key-part Information in Continual
  <span class="highlight-title">Instruct</span>ion Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan He, Xuancheng Huang, Minghao Tang, Lingxun Meng, Xiang Li, Wei Lin, Wenyuan Zhang, Yifu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning for large language models (LLMs) can drive them to produce
results consistent with human goals in specific downstream tasks. However, the
process of continual instruction tuning (CIT) for LLMs may bring about the
catastrophic forgetting (CF) problem, where previously learned abilities are
degraded. Recent methods try to alleviate the CF problem by modifying models or
replaying data, which may only remember the surface-level pattern of
instructions and get confused on held-out tasks. In this paper, we propose a
novel continual instruction tuning method based on Key-part Information Gain
(KPIG). Our method computes the information gain on masked parts to dynamically
replay data and refine the training objective, which enables LLMs to capture
task-aware information relevant to the correct response and alleviate
overfitting to general descriptions in instructions. In addition, we propose
two metrics, P-score and V-score, to measure the generalization and
instruction-following abilities of LLMs. Experiments demonstrate our method
achieves superior performance on both seen and held-out tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Overlap: Exploring Watermark Collision in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Luo, Ke Lin, Chao Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large language models (LLMs) in generating content
raises concerns about text copyright. Watermarking methods, particularly
logit-based approaches, embed imperceptible identifiers into text to address
these challenges. However, the widespread use of watermarking across diverse
LLMs has led to an inevitable issue known as watermark collision during common
tasks like question answering and paraphrasing. This study focuses on dual
watermark collisions, where two watermarks are present simultaneously in the
same text. The research demonstrates that watermark collision poses a threat to
detection performance for detectors of both upstream and downstream watermark
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short Paper, 4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Health Risks from Family History: A <span class="highlight-title">Survey</span> of Natural
  Language Processing Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Dai, Sarvnaz Karimi, Nathan O'Callaghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health records include information on patients' status and medical
history, which could cover the history of diseases and disorders that could be
hereditary. One important use of family history information is in precision
health, where the goal is to keep the population healthy with preventative
measures. Natural Language Processing (NLP) and machine learning techniques can
assist with identifying information that could assist health professionals in
identifying health risks before a condition is developed in their later years,
saving lives and reducing healthcare costs.
  We survey the literature on the techniques from the NLP field that have been
developed to utilise digital health records to identify risks of familial
diseases. We highlight that rule-based methods are heavily investigated and are
still actively used for family history extraction. Still, more recent efforts
have been put into building neural models based on large-scale pre-trained
language models. In addition to the areas where NLP has successfully been
utilised, we also identify the areas where more research is needed to unlock
the value of patients' records regarding data collection, task formulation and
downstream applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GET: Unlocking the Multi-modal Potential of CLIP for Generalized
  Category Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enguang Wang, Zhimao Peng, Zhengyuan Xie, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given unlabelled datasets containing both old and new categories, generalized
category discovery (GCD) aims to accurately discover new classes while
correctly classifying old classes, leveraging the class concepts learned from
labeled samples. Current GCD methods only use a single visual modality of
information, resulting in poor classification of visually similar classes.
Though certain classes are visually confused, their text information might be
distinct, motivating us to introduce text information into the GCD task.
However, the lack of class names for unlabelled data makes it impractical to
utilize text information. To tackle this challenging problem, in this paper, we
propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings
for unlabelled samples. Specifically, our TES leverages the property that CLIP
can generate aligned vision-language features, converting visual embeddings
into tokens of the CLIP's text encoder to generate pseudo text embeddings.
Besides, we employ a dual-branch framework, through the joint learning and
instance consistency of different modality branches, visual and semantic
information mutually enhance each other, promoting the interaction and fusion
of visual and text embedding space. Our method unlocks the multi-modal
potentials of CLIP and outperforms the baseline methods by a large margin on
all GCD benchmarks, achieving new state-of-the-art. The code will be released
at \url{https://github.com/enguangW/GET}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Twice Before Assure: Confidence Estimation for Large Language
  Models through Reflection on Multiple Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence estimation aiming to evaluate output trustability is crucial for
the application of large language models (LLM), especially the black-box ones.
Existing confidence estimation of LLM is typically not calibrated due to the
overconfidence of LLM on its generated incorrect answers. Existing approaches
addressing the overconfidence issue are hindered by a significant limitation
that they merely consider the confidence of one answer generated by LLM. To
tackle this limitation, we propose a novel paradigm that thoroughly evaluates
the trustability of multiple candidate answers to mitigate the overconfidence
on incorrect answers. Building upon this paradigm, we introduce a two-step
framework, which firstly instructs LLM to reflect and provide justifications
for each answer, and then aggregates the justifications for comprehensive
confidence estimation. This framework can be integrated with existing
confidence estimation approaches for superior calibration. Experimental results
on six datasets of three tasks demonstrate the rationality and effectiveness of
the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. However, the extent and impact of prompt bias
within the model remain underexplored. In response, this paper quantifies the
bias with various types of prompts and assesses their impact on different
benchmarks. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). Our findings shed
new light on the underlying predicting mechanisms of prompt-based queries in
PLMs. Hopefully, our plug-and-play approach can be a golden standard to
strengthen PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extraction of Sleep Information from Clinical Notes of Patients with
  Alzheimer's Disease Using Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonish Sivarajkumar, Thomas Yu CHow Tam, Haneef Ahamed Mohammad, Samual Viggiano, David Oniani, Shyam Visweswaran, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) is the most common form of dementia in the United
States. Sleep is one of the lifestyle-related factors that has been shown
critical for optimal cognitive function in old age. However, there is a lack of
research studying the association between sleep and AD incidence. A major
bottleneck for conducting such research is that the traditional way to acquire
sleep information is time-consuming, inefficient, non-scalable, and limited to
patients' subjective experience. A gold standard dataset is created from manual
annotation of 570 randomly sampled clinical note documents from the adSLEEP, a
corpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved
from the University of Pittsburgh Medical Center (UPMC). We developed a
rule-based Natural Language Processing (NLP) algorithm, machine learning
models, and Large Language Model(LLM)-based NLP algorithms to automate the
extraction of sleep-related concepts, including snoring, napping, sleep
problem, bad sleep quality, daytime sleepiness, night wakings, and sleep
duration, from the gold standard dataset. Rule-based NLP algorithm achieved the
best performance of F1 across all sleep-related concepts. In terms of Positive
Predictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime
sleepiness and sleep duration, machine learning models: 0.95 and for napping,
0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning
achieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for
sleep duration. The results show that the rule-based NLP algorithm consistently
achieved the best performance for all sleep concepts. This study focused on the
clinical notes of patients with AD, but could be extended to general sleep
information extraction for other diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How (un)ethical are <span class="highlight-title">instruct</span>ion-centric responses of LLMs? Unveiling the
  vulnerabilities of safety guardrails to harmful queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15302v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15302v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we tackle a growing concern around the safety and ethical use
of large language models (LLMs). Despite their potential, these models can be
tricked into producing harmful or unethical content through various
sophisticated methods, including 'jailbreaking' techniques and targeted
manipulation. Our work zeroes in on a specific issue: to what extent LLMs can
be led astray by asking them to generate responses that are instruction-centric
such as a pseudocode, a program or a software snippet as opposed to vanilla
text. To investigate this question, we introduce TechHazardQA, a dataset
containing complex queries which should be answered in both text and
instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers
for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,
Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and
instruction-centric responses. For evaluation we report the harmfulness score
metric as well as judgements from GPT-4 and humans. Overall, we observe that
asking LLMs to produce instruction-centric responses enhances the unethical
response generation by ~2-38% across the models. As an additional objective, we
investigate the impact of model editing using the ROME technique, which further
increases the propensity for generating undesirable content. In particular,
asking edited LLMs to generate instruction-centric responses further increases
the unethical response generation by ~3-16% across the different models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mining Clinical Notes for Physical Rehabilitation Exercise Information:
  Natural Language Processing Algorithm Development and Validation Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonish Sivarajkumar, Fengyi Gao, Parker E. Denny, Bayan M. Aldhahwani, Shyam Visweswaran, Allyn Bove, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-stroke patient rehabilitation requires precise, personalized treatment
plans. Natural Language Processing (NLP) offers potential to extract valuable
exercise information from clinical notes, aiding in the development of more
effective rehabilitation strategies. Objective: This study aims to develop and
evaluate a variety of NLP algorithms to extract and categorize physical
rehabilitation exercise information from the clinical notes of post-stroke
patients treated at the University of Pittsburgh Medical Center. A cohort of
13,605 patients diagnosed with stroke was identified, and their clinical notes
containing rehabilitation therapy notes were retrieved. A comprehensive
clinical ontology was created to represent various aspects of physical
rehabilitation exercises. State-of-the-art NLP algorithms were then developed
and compared, including rule-based, machine learning-based algorithms, and
large language model (LLM)-based algorithms (ChatGPT). Analysis was conducted
on a dataset comprising 23,724 notes with detailed demographic and clinical
characteristics. The rule-based NLP algorithm demonstrated superior performance
in most areas, particularly in detecting the 'Right Side' location with an F1
score of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting
excelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing
rule-based NLP by 0.023. It also showed notable performance in 'Passive Range
of Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.
The rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'
with F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot
prompts, achieved high recall but generally lower precision and F1 scores.
However, it notably excelled in 'Backward Plane' motion detection, achieving an
F1 score of 0.846, surpassing the rule-based algorithm's 0.720.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19791v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19791v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) now excel at code generation, a key aspect
of software development is the art of refactoring: consolidating code into
libraries of reusable and readable programs. In this paper, we introduce LILO,
a neurosymbolic framework that iteratively synthesizes, compresses, and
documents code to build libraries tailored to particular problem domains. LILO
combines LLM-guided program synthesis with recent algorithmic advances in
automated refactoring from Stitch: a symbolic compression system that
efficiently identifies optimal lambda abstractions across large code corpora.
To make these abstractions interpretable, we introduce an auto-documentation
(AutoDoc) procedure that infers natural language names and docstrings based on
contextual examples of usage. In addition to improving human readability, we
find that AutoDoc boosts performance by helping LILO's synthesizer to interpret
and deploy learned abstractions. We evaluate LILO on three inductive program
synthesis benchmarks for string editing, scene reasoning, and graphics
composition. Compared to existing neural and symbolic methods - including the
state-of-the-art library learning algorithm DreamCoder - LILO solves more
complex tasks and learns richer libraries that are grounded in linguistic
knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Algorithm through Model Adaptation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, David Mareček, Tomáš Musil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are becoming the go-to solution for the ever-growing
number of tasks. However, with growing capacity, models are prone to rely on
spurious correlations stemming from biases and stereotypes present in the
training data. This work proposes a novel method for detecting and mitigating
gender bias in language models. We perform causal analysis to identify
problematic model components and discover that mid-upper feed-forward layers
are most prone to convey bias. Based on the analysis results, we intervene in
the model by applying a linear projection to the weight matrices of these
layers. Our titular method, DAMA, significantly decreases bias as measured by
diverse metrics while maintaining the model's performance on downstream tasks.
We release code for our method and models, which retrain LLaMA's
state-of-the-art performance while being significantly less biased.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CinPatent: <span class="highlight-title">Dataset</span>s for Patent Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12192v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12192v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh-Tien Nguyen, Nhung Bui, Manh Tran-Tien, Linh Le, Huy-The Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patent classification is the task that assigns each input patent into several
codes (classes). Due to its high demand, several datasets and methods have been
introduced. However, the lack of both systematic performance comparison of
baselines and access to some datasets creates a gap for the task. To fill the
gap, we introduce two new datasets in English and Japanese collected by using
CPC codes. The English dataset includes 45,131 patent documents with 425 labels
and the Japanese dataset contains 54,657 documents with 523 labels. To
facilitate the next studies, we compare the performance of strong multi-label
text classification methods on the two datasets. Experimental results show that
AttentionXML is consistently better than other strong baselines. The ablation
study is also conducted in two aspects: the contribution of different parts
(title, abstract, description, and claims) of a patent and the behavior of
baselines in terms of performance with different training data segmentation. We
release the two new datasets with the code of the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper describes an on-going work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cognitive Architectures for Language Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts have augmented large language models (LLMs) with external
resources (e.g., the Internet) or internal control flows (e.g., prompt
chaining) for tasks requiring grounding or reasoning, leading to a new class of
language agents. While these agents have achieved substantial empirical
success, we lack a systematic framework to organize existing agents and plan
future developments. In this paper, we draw on the rich history of cognitive
science and symbolic artificial intelligence to propose Cognitive Architectures
for Language Agents (CoALA). CoALA describes a language agent with modular
memory components, a structured action space to interact with internal memory
and external environments, and a generalized decision-making process to choose
actions. We use CoALA to retrospectively survey and organize a large body of
recent work, and prospectively identify actionable directions towards more
capable agents. Taken together, CoALA contextualizes today's language agents
within the broader history of AI and outlines a path towards language-based
general intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3 is TMLR camera ready version. 19 pages of main content, 5 figures.
  The first two authors contributed equally, order decided by coin flip. A
  CoALA-based repo of recent work on language agents:
  https://github.com/ysymyth/awesome-language-agents</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with
  Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become
a heated topic. Various methods have been proposed to forecast links on TKGs.
Most of them are embedding-based, where hidden representations are learned to
represent knowledge graph (KG) entities and relations based on the observed
graph contexts. Although these methods show strong performance on traditional
TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the
unseen zero-shot relations that have no prior graph context. In this paper, we
try to mitigate this problem as follows. We first input the text descriptions
of KG relations into large language models (LLMs) for generating relation
representations, and then introduce them into embedding-based TKGF methods.
LLM-empowered representations can capture the semantic information in the
relation descriptions. This makes the relations, whether seen or unseen, with
similar semantic meanings stay close in the embedding space, enabling TKGF
models to recognize zero-shot relations even without any observed graph
context. Experimental results show that our approach helps TKGF models to
achieve much better performance in forecasting the facts with previously unseen
relations, while still maintaining their ability in link forecasting regarding
seen relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Voting-based Multimodal Automatic Deception Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lana Touma, Mohammad Al Horani, Manar Tailouni, Anas Dahabiah, Khloud Al Jallad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciAssess: Benchmarking LLM Proficiency in Scientific Literature
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, Guolin Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in Large Language Models (LLMs) have revolutionized
natural language understanding and generation, igniting a surge of interest in
leveraging these technologies in the field of scientific literature analysis.
Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in
scientific literature analysis, especially in scenarios involving complex
comprehension and multimodal data. In response, we introduced SciAssess, a
benchmark tailored for the in-depth analysis of scientific literature, crafted
to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on
evaluating LLMs' abilities in memorization, comprehension, and analysis within
the context of scientific literature analysis. It includes representative tasks
from diverse scientific fields, such as general chemistry, organic materials,
and alloy materials. And rigorous quality control measures ensure its
reliability in terms of correctness, anonymization, and copyright compliance.
SciAssess evaluates leading LLMs, including GPT-4, GPT-3.5, and Gemini,
identifying their strengths and aspects for improvement and supporting the
ongoing development of LLM applications in scientific literature analysis.
SciAssess and its resources are made available at https://sci-assess.github.io,
offering a valuable tool for advancing LLM capabilities in scientific
literature analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DevBench: A Comprehensive Benchmark for Software Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, <span class="highlight-author">Binyuan Hui</span>, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their coding capabilities. However, existing benchmarks predominantly
focused on simplified or isolated aspects of programming, such as single-file
code generation or repository issue debugging, falling short of measuring the
full spectrum of challenges raised by real-world programming activities. To
this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs
across various stages of the software development lifecycle, including software
design, environment setup, implementation, acceptance testing, and unit
testing. DevBench features a wide range of programming languages and domains,
high-quality data collection, and carefully designed and verified metrics for
each task. Empirical studies show that current LLMs, including GPT-4-Turbo,
fail to solve the challenges presented within DevBench. Analyses reveal that
models struggle with understanding the complex structures in the repository,
managing the compilation process, and grasping advanced programming concepts.
Our findings offer actionable insights for the future development of LLMs
toward real-world programming applications. Our benchmark is available at
https://github.com/open-compass/DevBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our data and code are available at
  https://github.com/open-compass/DevBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Ribeiro, Stephanie Brandl, Anders Søgaard, Nora Hollenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading
dataset, designed as the first webcam-based eye-tracking corpus of reading to
support the development of explainable computational language processing
models. WebQAmGaze includes webcam eye-tracking data from 600 participants of a
wide age range naturally reading English, German, Spanish, and Turkish texts.
Each participant performs two reading tasks composed of five texts each, a
normal reading and an information-seeking task, followed by a comprehension
question. We compare the collected webcam data to high-quality eye-tracking
recordings. The results show a moderate to strong correlation between the eye
movement measures obtained with the webcam compared to those obtained with a
commercial eye-tracking device. When validating the data, we find that higher
fixation duration on relevant text spans accurately indicates correctness when
answering the corresponding questions. This dataset advances webcam-based
reading studies and opens avenues to low-cost and diverse data collection.
WebQAmGaze is beneficial to learn about the cognitive processes behind
question-answering and to apply these insights to computational models of
language understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and <span class="highlight-title">Language Models</span> in Modern Organizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Jose Xavier Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CODIS: Benchmarking Context-Dependent Visual Comprehension for
  Multimodal Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated promising results
in a variety of tasks that combine vision and language. As these models become
more integral to research and applications, conducting comprehensive
evaluations of their capabilities has grown increasingly important. However,
most existing benchmarks fail to consider that, in certain situations, images
need to be interpreted within a broader context. In this work, we introduce a
new benchmark, named as CODIS, designed to assess the ability of models to use
context provided in free-form text to enhance visual comprehension. Our
findings indicate that MLLMs consistently fall short of human performance on
this benchmark. Further analysis confirms that these models struggle to
effectively extract and utilize contextual information to improve their
understanding of images. This underscores the pressing need to enhance the
ability of MLLMs to comprehend visuals in a context-dependent manner. View our
project website at https://thunlp-mt.github.io/CODIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Chain-of-Thoughts <span class="highlight-title">Prompt</span>ing with Iterative Bootstrapping in
  Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian Guo, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can achieve highly effective performance on
various reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting as demonstrations. However, the reasoning chains of demonstrations
generated by LLMs are prone to errors, which can subsequently lead to incorrect
reasoning during inference. Furthermore, inappropriate exemplars (overly
simplistic or complex), can affect overall performance among varying levels of
difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts
Prompting), an iterative bootstrapping approach for selecting exemplars and
generating reasoning chains. By utilizing iterative bootstrapping, our approach
enables LLMs to autonomously rectify errors, resulting in more precise and
comprehensive reasoning chains. Simultaneously, our approach selects
challenging yet answerable questions accompanied by reasoning chains as
exemplars with a moderate level of difficulty, which enhances the LLMs'
generalizability across varying levels of difficulty. Experimental results
indicate that Iter-CoT exhibits superiority, achieving competitive performance
across three distinct reasoning tasks on ten datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Potential of Large <span class="highlight-title">Language Models</span> in Computational
  Argumentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational argumentation has become an essential tool in various fields,
including artificial intelligence, law, and public policy. It is an emerging
research field in natural language processing that attracts increasing
attention. Research on computational argumentation mainly involves two types of
tasks: argument mining and argument generation. As large language models have
demonstrated strong abilities in understanding context and generating natural
language, it is worthwhile to evaluate the performance of LLMs on various
computational argumentation tasks. This work aims to embark on an assessment of
LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and
few-shot settings within the realm of computational argumentation. We organize
existing tasks into six main categories and standardise the format of fourteen
open-sourced datasets. In addition, we present a new benchmark dataset on
counter speech generation, that aims to holistically evaluate the end-to-end
performance of LLMs on argument mining and argument generation. Extensive
experiments show that LLMs exhibit commendable performance across most of these
datasets, demonstrating their capabilities in the field of argumentation. Our
analysis offers valuable suggestions for evaluating computational argumentation
and its integration with LLMs in future research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAGPIE: Multi-Task Media-Bias Analysis Generalization for <span class="highlight-title">Pre-Train</span>ed
  Identification of Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Horych, Martin Wessel, Jan Philip Wahle, Terry Ruas, Jerome Waßmuth, André Greiner-Petter, Akiko Aizawa, Bela Gipp, Timo Spinde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Media bias detection poses a complex, multifaceted problem traditionally
tackled using single-task models and small in-domain datasets, consequently
lacking generalizability. To address this, we introduce MAGPIE, the first
large-scale multi-task pre-training approach explicitly tailored for media bias
detection. To enable pre-training at scale, we present Large Bias Mixture
(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous
approaches in media bias detection on the Bias Annotation By Experts (BABE)
dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs
better than previous models on 5 out of 8 tasks in the Media Bias
Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%
of finetuning steps compared to single-task approaches. Our evaluation shows,
for instance, that tasks like sentiment and emotionality boost all learning,
all tasks enhance fake news detection, and scaling tasks leads to the best
results. MAGPIE confirms that MTL is a promising approach for addressing media
bias detection, enhancing the accuracy and efficiency of existing models.
Furthermore, LBM is the first available resource collection focused on media
bias MTL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follow-Up Differential Descriptions: <span class="highlight-title">Language Models</span> Resolve Ambiguities
  for Image Classification <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A promising approach for improving the performance of vision-language models
like CLIP for image classification is to extend the class descriptions (i.e.,
prompts) with related attributes, e.g., using brown sparrow instead of sparrow.
However, current zero-shot methods select a subset of attributes regardless of
commonalities between the target classes, potentially providing no useful
information that would have helped to distinguish between them. For instance,
they may use color instead of bill shape to distinguish between sparrows and
wrens, which are both brown. We propose Follow-up Differential Descriptions
(FuDD), a zero-shot approach that tailors the class descriptions to each
dataset and leads to additional attributes that better differentiate the target
classes. FuDD first identifies the ambiguous classes for each image, and then
uses a Large Language Model (LLM) to generate new class descriptions that
differentiate between them. The new class descriptions resolve the initial
ambiguity and help predict the correct label. In our experiments, FuDD
consistently outperforms generic description ensembles and naive LLM-generated
descriptions on 12 datasets. We show that differential descriptions are an
effective tool to resolve class ambiguities, which otherwise significantly
degrade the performance. We also show that high quality natural language class
descriptions produced by FuDD result in comparable performance to few-shot
adaptation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIcK: A Benchmark <span class="highlight-title">Dataset</span> of Cultural and Linguistic Intelligence in
  Korean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of large language models (LLMs) for the Korean
language, there remains an obvious lack of benchmark datasets that test the
requisite Korean cultural and linguistic knowledge. Because many existing
Korean benchmark datasets are derived from the English counterparts through
translation, they often overlook the different cultural contexts. For the few
benchmark datasets that are sourced from Korean data capturing cultural
knowledge, only narrow tasks such as bias and hate speech detection are
offered. To address this gap, we introduce a benchmark of Cultural and
Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.
CLIcK sources its data from official Korean exams and textbooks, partitioning
the questions into eleven categories under the two main categories of language
and culture. For each instance in CLIcK, we provide fine-grained annotation of
which cultural and linguistic knowledge is required to answer the question
correctly. Using CLIcK, we test 13 language models to assess their performance.
Our evaluation uncovers insights into their performances across the categories,
as well as the diverse factors affecting their comprehension. CLIcK offers the
first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in
Korean culture and language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Natural Language Processing for Long Texts: A <span class="highlight-title">Survey</span> on
  Classification and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16259v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16259v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Tsirmpas, Ioannis Gkionis, Georgios Th. Papadopoulos, Ioannis Mademlis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural
Language Processing (NLP) during the past decade. However, the demands of long
document analysis are quite different from those of shorter texts, while the
ever increasing size of documents uploaded online renders automated
understanding of lengthy texts a critical issue. Relevant applications include
automated Web mining, legal document review, medical records analysis,
financial reports analysis, contract management, environmental impact
assessment, news aggregation, etc. Despite the relatively recent development of
efficient algorithms for analyzing long documents, practical tools in this
field are currently flourishing. This article serves as an entry point into
this dynamic domain and aims to achieve two objectives. First of all, it
provides an introductory overview of the relevant neural building blocks,
serving as a concise tutorial for the field. Secondly, it offers a brief
examination of the current state-of-the-art in two key long document analysis
tasks: document classification and document summarization. Sentiment analysis
for long texts is also covered, since it is typically treated as a particular
case of document classification. Consequently, this article presents an
introductory exploration of document-level analysis, addressing the primary
challenges, concerns, and existing solutions. Finally, it offers a concise
definition of "long text/document", presents an original overarching taxonomy
of common deep neural methods for long document analysis and lists publicly
available annotated datasets that can facilitate further research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages, 11 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Before You Speak: Cultivating Communication Skills of Large
  <span class="highlight-title">Language Models</span> via Inner Monologue <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) further improves the
capabilities of open-domain dialogue systems and can generate fluent, coherent,
and diverse responses. However, LLMs still lack a crucial ability:
communication skills. This limitation renders them more like information
seeking tools rather than anthropomorphic chatbots. Communication skills, such
as topic transition, proactively asking questions, concept guidance, empathy,
and summarising often should be taken into consideration, to make LLMs more
anthropomorphic and proactive during the conversation, thereby increasing the
interest of users and attracting them to chat for longer. However, enabling
these communication skills in black-box LLMs remains a key challenge because
they do not have the same utterance formation mode as real people: think before
speaking. Inspired by linguistics and cognitive science, we empower LLMs with
communication skills through inner monologues. To evaluate various
communication skills, we construct a benchmark named Cskills, which can also
more comprehensively evaluate the dialogue generation ability of the model.
Experimental results show that the proposed CSIM strategy improves the backbone
models and outperforms the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via
  Graph Representation <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjun Kim, Seungwoo Song, Youhan Lee, Haneol Jang, Kyungtae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current research direction in generative models, such as the recently
developed GPT4, aims to find relevant knowledge information for multimodal and
multilingual inputs to provide answers. Under these research circumstances, the
demand for multilingual evaluation of visual question answering (VQA) tasks, a
representative task of multimodal systems, has increased. Accordingly, we
propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that
can be extended to multilingualism. The proposed data include 17K images, 17K
question-answer pairs for both Korean and English and 280K instances of
knowledge information related to question-answer content. We also present a
framework that can effectively inject knowledge information into a VQA system
by pretraining the knowledge information of BOK-VQA data in the form of graph
embeddings. Finally, through in-depth analysis, we demonstrated the actual
effect of the knowledge information contained in the constructed training data
on VQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-based Automated Model Evaluation <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE's validity, together with its
superiority compared with prior approaches. We also prove MDE's versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels. Code and data are
available: https://github.com/pengr/Energy_AutoEval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2024 poster paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> of Natural Language Processing for Education: Taxonomy,
  Systematic <span class="highlight-title">Review</span>, and Future Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07518v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07518v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian, Aoying Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) aims to analyze text or speech via
techniques in the computer science field. It serves the applications in domains
of healthcare, commerce, education and so on. Particularly, NLP has been widely
applied to the education domain and its applications have enormous potential to
help teaching and learning. In this survey, we review recent advances in NLP
with the focus on solving problems relevant to the education domain. In detail,
we begin with introducing the related background and the real-world scenarios
in education where NLP techniques could contribute. Then, we present a taxonomy
of NLP in the education domain and highlight typical NLP applications including
question answering, question construction, automated assessment, and error
correction. Next, we illustrate the task definition, challenges, and
corresponding cutting-edge techniques based on the above taxonomy. In
particular, LLM-involved methods are included for discussion due to the wide
usage of LLMs in diverse NLP applications. After that, we showcase some
off-the-shelf demonstrations in this domain. At last, we conclude with six
promising directions for future research, including more datasets in education
domain, controllable usage of LLMs, intervention of difficulty-level control,
interpretable educational NLP, methods with adaptive learning, and integrated
systems for education. We organize all relevant datasets and papers in the
open-available Github Link for better
review~\url{https://github.com/LiXinyuan1015/NLP-for-Education}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Retraining-free Pruning for <span class="highlight-title">Pretrain</span>ed Encoder-based Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungcheol Park, Hojun Choi, U Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a pretrained encoder-based language model, how can we accurately
compress it without retraining? Retraining-free structured pruning algorithms
are crucial in pretrained language model compression due to their significantly
reduced pruning cost and capability to prune large language models. However,
existing retraining-free algorithms encounter severe accuracy degradation, as
they fail to handle pruning errors, especially at high compression rates. In
this paper, we propose K-prune (Knowledge-preserving pruning), an accurate
retraining-free structured pruning algorithm for pretrained encoder-based
language models. K-prune focuses on preserving the useful knowledge of the
pretrained model to minimize pruning errors through a carefully designed
iterative pruning process composed of knowledge measurement,
knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a
result, K-prune shows significant accuracy improvements up to 58.02%p higher F1
score compared to existing retraining-free pruning algorithms under a high
compression rate of 80% on the SQuAD benchmark without any retraining process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kosmos-G: Generating Images in Context with Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in subject-driven image generation have made significant
strides. However, current methods still fall short in diverse application
scenarios, as they require test-time tuning and cannot accept interleaved
multi-image and text input. These limitations keep them far from the ultimate
goal of "image as a foreign language in image generation." This paper presents
Kosmos-G, a model that leverages the advanced multimodal perception
capabilities of Multimodal Large Language Models (MLLMs) to tackle the
aforementioned challenge. Our approach aligns the output space of MLLM with
CLIP using the textual modality as an anchor and performs compositional
instruction tuning on curated data. Kosmos-G demonstrates an impressive
capability of zero-shot subject-driven generation with interleaved multi-image
and text input. Notably, the score distillation instruction tuning requires no
modifications to the image decoder. This allows for a seamless substitution of
CLIP and effortless integration with a myriad of U-Net techniques ranging from
fine-grained controls to personalized image decoder variants. We posit Kosmos-G
as an initial attempt towards the goal of "image as a foreign language in image
generation." The code can be found at https://aka.ms/Kosmos-G
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://aka.ms/Kosmos-G Project Page:
  https://xichenpan.github.io/kosmosg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MUFFIN: Curating Multi-Faceted <span class="highlight-title">Instruct</span>ions for Improving
  <span class="highlight-title">Instruct</span>ion-Following <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02436v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02436v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of large language models (LLMs), enhancing instruction-following
capability often involves curating expansive training data. This is achieved
through two primary schemes: i) Scaling-Inputs: Amplifying (input, output)
pairs per task instruction, aiming for better instruction adherence. ii)
Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,
output) pair (without requiring a separate input anymore). However, LLMs under
Scaling-Inputs tend to be overly sensitive to inputs, leading to
misinterpretation or non-compliance with instructions. Conversely, Scaling
Input-Free Tasks demands a substantial number of tasks but is less effective in
instruction following when dealing with instances in Scaling-Inputs. This work
introduces MUFFIN, a new scheme of instruction-following dataset curation.
Specifically, we automatically Scale Tasks per Input by diversifying these
tasks with various input facets. Experimental results across four zero-shot
benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,
reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate
superior instruction-following capabilities compared to those trained on the
two aforementioned schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Data, model, and code are available at:
  https://renzelou.github.io/Muffin/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in Large Language Models (LLMs) have been hindered by their
substantial sizes, which necessitate LLM compression methods for practical
deployment. Singular Value Decomposition (SVD) offers a promising solution for
LLM compression. However, state-of-the-art SVD-based LLM compression methods
have two key limitations: truncating smaller singular values may lead to higher
compression loss, and the lack of update on the remaining model parameters
after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM
compression method that addresses the limitations of existing methods. SVD-LLM
incorporates a truncation-aware data whitening strategy to ensure a direct
mapping between singular values and compression loss. Moreover, SVD-LLM adopts
a layer-wise closed-form model parameter update strategy to compensate for
accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total
of 11 datasets and seven models from three different LLM families at four
different scales. Our results demonstrate the superiority of SVD-LLM over
state-of-the-arts, especially at high model compression ratios. The source code
is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XAL: EXplainable Active Learning Makes Classifiers Better Low-resource
  Learners <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning (AL), which aims to construct an effective training set by
iteratively curating the most formative unlabeled data for annotation, has been
widely used in low-resource tasks. Most active learning techniques in
classification rely on the model's uncertainty or disagreement to choose
unlabeled data, suffering from the problem of over-confidence in superficial
patterns and a lack of exploration. Inspired by the cognitive processes in
which humans deduce and predict through causal information, we take an initial
attempt towards integrating rationales into AL and propose a novel Explainable
Active Learning framework (XAL) for low-resource text classification, which
aims to encourage classifiers to justify their inferences and delve into
unlabeled data for which they cannot provide reasonable explanations.
Specifically, besides using a pre-trained bi-directional encoder for
classification, we employ a pre-trained uni-directional decoder to generate and
score the explanation. We further facilitate the alignment of the model with
human reasoning preference through a proposed ranking loss. During the
selection of unlabeled data, the predicted uncertainty of the encoder and the
explanation score of the decoder complement each other as the final metric to
acquire informative data. Extensive experiments on six datasets show that XAL
achieves consistent improvement over 9 strong baselines. Analysis indicates
that the proposed method can generate corresponding explanations for its
predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logits of API-Protected LLMs Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Xiang Ren, Swabha Swayamdipta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercialization of large language models (LLMs) has led to the common
practice of high-level API-only access to proprietary models. In this work, we
show that even with a conservative assumption about the model architecture, it
is possible to learn a surprisingly large amount of non-public information
about an API-protected LLM from a relatively small number of API queries (e.g.,
costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on
one key observation: most modern LLMs suffer from a softmax bottleneck, which
restricts the model outputs to a linear subspace of the full output space. We
show that this lends itself to a model image or a model signature which unlocks
several capabilities with affordable cost: efficiently discovering the LLM's
hidden size, obtaining full-vocabulary outputs, detecting and disambiguating
different model updates, identifying the source LLM given a single full LLM
output, and even estimating the output layer parameters. Our empirical
investigations show the effectiveness of our methods, which allow us to
estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.
Lastly, we discuss ways that LLM providers can guard against these attacks, as
well as how these capabilities can be viewed as a feature (rather than a bug)
by allowing for greater transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JoMA: Demystifying Multilayer <span class="highlight-title">Transformer</span>s via JOint Dynamics of MLP and
  Attention <span class="chip">ICLR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical
framework to understand the training procedure of multilayer Transformer
architectures. This is achieved by integrating out the self-attention layer in
Transformers, producing a modified dynamics of MLP layers only. JoMA removes
unrealistic assumptions in previous analysis (e.g., lack of residual
connection) and predicts that the attention first becomes sparse (to learn
salient tokens), then dense (to learn less salient tokens) in the presence of
nonlinear activations, while in the linear case, it is consistent with existing
works that show attention becomes sparse over time. We leverage JoMA to
qualitatively explains how tokens are combined to form hierarchies in
multilayer Transformers, when the input tokens are generated by a latent
hierarchical generative model. Experiments on models trained from real-world
dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia)
verify our theoretical findings. Code can be found in
https://github.com/facebookresearch/luckmatters/tree/yuandong3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR'24 camera ready. Improve theorem 3 and theorem 4. Polish writing
  and add code link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Explainable Mental Health Analysis on Social Media by
  Incorporating Mental Scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional discriminative approaches in mental health analysis are known for
their strong capacity but lack interpretability and demand large-scale
annotated data. The generative approaches, such as those based on large
language models (LLMs), have the potential to get rid of heavy annotations and
provide explanations but their capabilities still fall short compared to
discriminative approaches, and their explanations may be unreliable due to the
fact that the generation of explanation is a black-box process. Inspired by the
psychological assessment practice of using scales to evaluate mental states,
our method which is called Mental Analysis by Incorporating Mental Scales
(MAIMS), incorporates two procedures via LLMs. First, the patient completes
mental scales, and second, the psychologist interprets the collected
information from the mental scales and makes informed decisions. Experimental
results show that MAIMS outperforms other zero-shot methods. MAIMS can generate
more rigorous explanation based on the outputs of mental scales
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Inference Unveiled: <span class="highlight-title">Survey</span> and Roofline Model Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16363v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16363v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, Kurt Keutzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of efficient Large Language Model (LLM) inference is rapidly
evolving, presenting a unique blend of opportunities and challenges. Although
the field has expanded and is vibrant, there hasn't been a concise framework
that analyzes the various methods of LLM Inference to provide a clear
understanding of this domain. Our survey stands out from traditional literature
reviews by not only summarizing the current state of research but also by
introducing a framework based on roofline model for systematic analysis of LLM
inference techniques. This framework identifies the bottlenecks when deploying
LLMs on hardware devices and provides a clear understanding of practical
problems, such as why LLMs are memory-bound, how much memory and computation
they need, and how to choose the right hardware. We systematically collate the
latest advancements in efficient LLM inference, covering crucial areas such as
model compression (e.g., Knowledge Distillation and Quantization), algorithm
improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and
system-level enhancements. Our survey stands out by analyzing these methods
with roofline model, helping us understand their impact on memory access and
computation. This distinctive approach not only showcases the current research
landscape but also delivers valuable insights for practical implementation,
positioning our work as an indispensable resource for researchers new to the
field as well as for those seeking to deepen their understanding of efficient
LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous QA Learning with Structured <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14602v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14602v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhe Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  QA models with lifelong learning (LL) abilities are important for practical
QA applications, and architecture-based LL methods are reported to be an
effective implementation for these models. However, it is non-trivial to extend
previous approaches to QA tasks since they either require access to task
identities in the testing phase or do not explicitly model samples from unseen
tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong
QA model that tries to learn a sequence of QA tasks with a prompt enhanced
language model. Four types of hierarchically organized prompts are used in
Diana to capture QA knowledge from different granularities. Specifically, we
dedicate task-level prompts to capture task-specific knowledge to retain high
LL performances and maintain instance-level prompts to learn knowledge shared
across different input samples to improve the model's generalization
performance. Moreover, we dedicate separate prompts to explicitly model unseen
tasks and introduce a set of prompt key vectors to facilitate knowledge sharing
between tasks. Extensive experiments demonstrate that Diana outperforms
state-of-the-art lifelong QA models, especially in handling unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Duplicate of arXiv:2305.06555 (Please cite arXiv:2305.06555 since it
  is the camera ready version)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have emerged as the backbone of large language models (LLMs).
However, generation remains inefficient due to the need to store in memory a
cache of key-value representations for past tokens, whose size scales linearly
with the input sequence length and batch size. As a solution, we propose
Dynamic Memory Compression (DMC), a method for on-line key-value cache
compression at inference time. Most importantly, the model learns to apply
different compression rates in different heads and layers. We retrofit
pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,
achieving up to ~3.7x throughput increase in auto-regressive inference on a
NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible
percentage of the original data without adding any extra parameters. We find
that DMC preserves the original downstream performance with up to 4x cache
compression, outperforming up-trained grouped-query attention (GQA). GQA and
DMC can be even combined to obtain compounded gains. As a result DMC fits
longer contexts and larger batches within any given memory budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s Get Stable: An End-to-End Signal Propagation Theory for
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of their huge success, transformer models remain difficult to scale
in depth. In this work, we develop a unified signal propagation theory and
provide formulae that govern the moments of the forward and backward signal
through the transformer model. Our framework can be used to understand and
mitigate vanishing/exploding gradients, rank collapse, and instability
associated with high attention scores. We also propose DeepScaleLM, an
initialization and scaling scheme that conserves unit output/gradient moments
throughout the model, enabling the training of very deep models with 100s of
layers. We find that transformer models could be much deeper - our deep models
with fewer parameters outperform shallow models in Language Modeling, Speech
Translation, and Image Classification, across Encoder-only, Decoder-only and
Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for
multiple datasets and model sizes. These improvements also translate into
improved performance on downstream Question Answering tasks and improved
robustness for image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution.
  Source code is available at
  https://github.com/akhilkedia/TranformersGetStable</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-VLA: A 3D Vision-Language-Action Generative World Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vis-www.cs.umass.edu/3dvla/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quiet-STaR: <span class="highlight-title">Language Models</span> Can Teach Themselves to Think Before
  Speaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reawakening knowledge: Anticipatory recovery from catastrophic
  interference via structured training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the training dynamics of neural networks in a structured non-IID
setting where documents are presented cyclically in a fixed, repeated sequence.
Typically, networks suffer from catastrophic interference when training on a
sequence of documents; however, we discover a curious and remarkable property
of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory
behavior, recovering from the forgetting on documents before encountering them
again. The behavior emerges and becomes more robust as the architecture scales
up its number of parameters. Through comprehensive experiments and
visualizations, we uncover new insights into training over-parameterized
networks in structured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM1: Methods, Analysis & Insights from Multimodal LLM <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, consisting of
both dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> and Causal Inference in Collaboration: A
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, Julian McAuley, Wei Ai, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference has shown potential in enhancing the predictive accuracy,
fairness, robustness, and explainability of Natural Language Processing (NLP)
models by capturing causal relationships among variables. The emergence of
generative Large Language Models (LLMs) has significantly impacted various NLP
domains, particularly through their advanced reasoning capabilities. This
survey focuses on evaluating and improving LLMs from a causal view in the
following areas: understanding and improving the LLMs' reasoning capacity,
addressing fairness and safety issues in LLMs, complementing LLMs with
explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning
capacities can in turn contribute to the field of causal inference by aiding
causal relationship discovery and causal effect estimations. This review
explores the interplay between causal inference frameworks and LLMs from both
perspectives, emphasizing their collective potential to further the development
of more advanced and equitable artificial intelligence systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Data Value Estimation for Visual <span class="highlight-title">Instruct</span>ion Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual instruction tuning is the key to building multimodal large language
models (MLLMs), which greatly improves the reasoning capabilities of large
language models (LLMs) in vision scenario. However, existing MLLMs mostly rely
on a mixture of multiple highly diverse visual instruction datasets for
training (even more than a million instructions), which may introduce data
redundancy. To investigate this issue, we conduct a series of empirical
studies, which reveal a significant redundancy within the visual instruction
datasets, and show that greatly reducing the amount of several instruction
dataset even do not affect the performance. Based on the findings, we propose a
new data selection approach TIVE, to eliminate redundancy within visual
instruction data. TIVE first estimates the task-level and instance-level value
of the visual instructions based on computed gradients. Then, according to the
estimated values, TIVE determines the task proportion within the visual
instructions, and selects representative instances to compose a smaller visual
instruction subset for training. Experiments on LLaVA-1.5 show that our
approach using only about 7.5% data can achieve comparable performance as the
full-data fine-tuned model across seven benchmarks, even surpassing it on four
of the benchmarks. Our code and data will be publicly released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logits of API-Protected LLMs Leak Proprietary Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Swabha Swayamdipta, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The commercialization of large language models (LLMs) has led to the common
practice of high-level API-only access to proprietary models. In this work, we
show that even with a conservative assumption about the model architecture, it
is possible to learn a surprisingly large amount of non-public information
about an API-protected LLM from a relatively small number of API queries (e.g.,
costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on
one key observation: most modern LLMs suffer from a softmax bottleneck, which
restricts the model outputs to a linear subspace of the full output space. We
show that this lends itself to a model image or a model signature which unlocks
several capabilities with affordable cost: efficiently discovering the LLM's
hidden size, obtaining full-vocabulary outputs, detecting and disambiguating
different model updates, identifying the source LLM given a single full LLM
output, and even estimating the output layer parameters. Our empirical
investigations show the effectiveness of our methods, which allow us to
estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.
Lastly, we discuss ways that LLM providers can guard against these attacks, as
well as how these capabilities can be viewed as a feature (rather than a bug)
by allowing for greater transparency and accountability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision<span class="highlight-title">GPT</span>-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
VisionGPT-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. VisionGPT-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text prompts.
  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MT-PATCHER: Selective and Extendable Knowledge Distillation from Large
  <span class="highlight-title">Language Models</span> for Machine Translation <span class="chip">NAACL-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Li, Shanbo Cheng, Shujian Huang, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) have demonstrated their strong ability in the
field of machine translation (MT), yet they suffer from high computational cost
and latency. Therefore, transferring translation knowledge from giant LLMs to
medium-sized machine translation models is a promising research direction.
However, traditional knowledge distillation methods do not take the capability
of student and teacher models into consideration, therefore repeatedly teaching
student models on the knowledge they have learned, and failing to extend to
novel contexts and knowledge. In this paper, we propose a framework called
MT-Patcher, which transfers knowledge from LLMs to existing MT models in a
selective, comprehensive and proactive manner. Considering the current
translation ability of student MT models, we only identify and correct their
translation errors, instead of distilling the whole translation from the
teacher. Leveraging the strong language abilities of LLMs, we instruct LLM
teachers to synthesize diverse contexts and anticipate more potential errors
for the student. Experiment results on translating both specific language
phenomena and general MT benchmarks demonstrate that finetuning the student MT
model on about 10% examples can achieve comparable results to the traditional
knowledge distillation method, and synthesized potential errors and diverse
contexts further improve translation performances on unseen contexts and words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Prototypical Representations for Mitigating Social Bias
  without Demographic Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Iskander, Kira Radinsky, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating social biases typically requires identifying the social groups
associated with each data sample. In this paper, we present DAFair, a novel
approach to address social bias in language models. Unlike traditional methods
that rely on explicit demographic labels, our approach does not require any
such information. Instead, we leverage predefined prototypical demographic
texts and incorporate a regularization term during the fine-tuning process to
mitigate bias in the model's representations. Our empirical results across two
tasks and two models demonstrate the effectiveness of our method compared to
previous approaches that do not rely on labeled data. Moreover, with limited
demographic-annotated data, our approach outperforms common debiasing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward
  Fake News 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital era, the rapid propagation of fake news and rumors via social
networks brings notable societal challenges and impacts public opinion
regulation. Traditional fake news modeling typically forecasts the general
popularity trends of different groups or numerically represents opinions shift.
However, these methods often oversimplify real-world complexities and overlook
the rich semantic information of news text. The advent of large language models
(LLMs) provides the possibility of modeling subtle dynamics of opinion.
Consequently, in this work, we introduce a Fake news Propagation Simulation
framework (FPS) based on LLM, which studies the trends and control of fake news
propagation in detail. Specifically, each agent in the simulation represents an
individual with a distinct personality. They are equipped with both short-term
and long-term memory, as well as a reflective mechanism to mimic human-like
thinking. Every day, they engage in random opinion exchanges, reflect on their
thinking, and update their opinions. Our simulation results uncover patterns in
fake news propagation related to topic relevance, and individual traits,
aligning with real-world observations. Additionally, we evaluate various
intervention strategies and demonstrate that early and appropriately frequent
interventions strike a balance between governance cost and effectiveness,
offering valuable insights for practical applications. Our study underscores
the significant utility and potential of LLMs in combating fake news.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper-CL: Conditioning Sentence Representations with Hypernetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Hyun Yoo, Jii Cha, Changhyeon Kim, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the introduction of contrastive learning frameworks in sentence
representation learning has significantly contributed to advancements in the
field, it still remains unclear whether state-of-the-art sentence embeddings
can capture the fine-grained semantics of sentences, particularly when
conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an
efficient methodology that integrates hypernetworks with contrastive learning
to compute conditioned sentence representations. In our proposed approach, the
hypernetwork is responsible for transforming pre-computed condition embeddings
into corresponding projection layers. This enables the same sentence embeddings
to be projected differently according to various conditions. Evaluation on two
representative conditioning benchmarks, namely conditional semantic text
similarity and knowledge graph completion, demonstrates that Hyper-CL is
effective in flexibly conditioning sentence representations, showcasing its
computational efficiency at the same time. We also provide a comprehensive
analysis of the inner workings of our approach, leading to a better
interpretation of its mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rectifying Demonstration Shortcut in In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are able to solve various tasks with only a few
demonstrations utilizing their in-context learning (ICL) abilities. However,
LLMs often rely on their pre-trained semantic priors of demonstrations rather
than on the input-label relationships to proceed with ICL prediction. In this
work, we term this phenomenon as the `Demonstration Shortcut'. While previous
works have primarily focused on improving ICL prediction results for predefined
tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM
to effectively learn new input-label relationships from demonstrations. To
achieve this, we introduce In-Context Calibration, a demonstration-aware
calibration method. We evaluate the effectiveness of the proposed method in two
settings: (1) the Original ICL Task using the standard label space and (2) the
Task Learning setting, where the label space is replaced with semantically
unrelated tokens. In both settings, In-Context Calibration demonstrates
substantial improvements, with results generalized across three LLM families
(OPT, GPT, and Llama2) under various configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current AI alignment methodologies rely on human-provided demonstrations or
judgments, and the learned capabilities of AI systems would be upper-bounded by
human capabilities as a result. This raises a challenging research question:
How can we keep improving the systems when their capabilities have surpassed
the levels of humans? This paper answers this question in the context of
tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from
human annotations on easier tasks (e.g., level 1-3 MATH problems), which we
term as \textit{easy-to-hard generalization}. Our key insight is that an
evaluator (reward model) trained on supervisions for easier tasks can be
effectively used for scoring candidate solutions of harder tasks and hence
facilitating easy-to-hard generalization over different levels of tasks. Based
on this insight, we propose a novel approach to scalable alignment, which
firstly trains the process-supervised reward models on easy problems (e.g.,
level 1-3), and then uses them to evaluate the performance of policy models on
hard problems. We show that such \textit{easy-to-hard generalization from
evaluators} can enable \textit{easy-to-hard generalizations in generators}
either through re-ranking or reinforcement learning (RL). Notably, our
process-supervised 7b RL model achieves an accuracy of 34.0\% on MATH500,
despite only using human supervision on easy problems. Our approach suggests a
promising path toward AI systems that advance beyond the frontier of human
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS
  Students using Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Bernstein, Paul Denny, Juho Leinonen, Lauren Kan, Arto Hellas, Matt Littlefield Sami Sarsa, Stephen MacNeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping complex computing concepts often poses a challenge for students who
struggle to anchor these new ideas to familiar experiences and understandings.
To help with this, a good analogy can bridge the gap between unfamiliar
concepts and familiar ones, providing an engaging way to aid understanding.
However, creating effective educational analogies is difficult even for
experienced instructors. We investigate to what extent large language models
(LLMs), specifically ChatGPT, can provide access to personally relevant
analogies on demand. Focusing on recursion, a challenging threshold concept, we
conducted an investigation analyzing the analogies generated by more than 350
first-year computing students. They were provided with a code snippet and
tasked to generate their own recursion-based analogies using ChatGPT,
optionally including personally relevant topics in their prompts. We observed a
great deal of diversity in the analogies produced with student-prescribed
topics, in contrast to the otherwise generic analogies, highlighting the value
of student creativity when working with LLMs. Not only did students enjoy the
activity and report an improved understanding of recursion, but they described
more easily remembering analogies that were personally and culturally relevant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, ITiCSE 2024 preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Komodo: A Linguistic Expedition into Indonesia's Regional Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Owen, Vishesh Tripathi, Abhay Kumar, Biddwan Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent breakthroughs in Large Language Models (LLMs) have mostly focused
on languages with easily available and sufficient resources, such as English.
However, there remains a significant gap for languages that lack sufficient
linguistic resources in the public domain. Our work introduces Komodo-7B,
7-billion-parameter Large Language Models designed to address this gap by
seamlessly operating across Indonesian, English, and 11 regional languages in
Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and
Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art
performance in various tasks and languages, outperforming the benchmarks set by
OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,
Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only
demonstrates superior performance in both language-specific and overall
assessments but also highlights its capability to excel in linguistic
diversity. Our commitment to advancing language models extends beyond
well-resourced languages, aiming to bridge the gap for those with limited
linguistic assets. Additionally, Komodo-7B-Instruct's better cross-language
understanding contributes to addressing educational disparities in Indonesia,
offering direct translations from English to 11 regional languages, a
significant improvement compared to existing language translation services.
Komodo-7B represents a crucial step towards inclusivity and effectiveness in
language models, providing to the linguistic needs of diverse communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 Pages, 8 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More than words: Advancements and challenges in speech recognition for
  singing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kruspe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges and advancements in speech recognition
for singing, a domain distinctly different from standard speech recognition.
Singing encompasses unique challenges, including extensive pitch variations,
diverse vocal styles, and background music interference. We explore key areas
such as phoneme recognition, language identification in songs, keyword
spotting, and full lyrics transcription. I will describe some of my own
experiences when performing research on these tasks just as they were starting
to gain traction, but will also show how recent developments in deep learning
and large-scale datasets have propelled progress in this field. My goal is to
illuminate the complexities of applying speech recognition to singing, evaluate
current capabilities, and outline future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Electronic Speech Signal Processing (ESSV) 2024,
  Keynote</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anatomical Structure-Guided Medical Vision-Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingqiu Li, Xiaohan Yan, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning medical visual representations through vision-language pre-training
has reached remarkable progress. Despite the promising performance, it still
faces challenges, i.e., local alignment lacks interpretability and clinical
relevance, and the insufficient internal and external representation learning
of image-report pairs. To address these issues, we propose an Anatomical
Structure-Guided (ASG) framework. Specifically, we parse raw reports into
triplets <anatomical region, finding, existence>, and fully utilize each
element as supervision to enhance representation learning. For anatomical
region, we design an automatic anatomical region-sentence alignment paradigm in
collaboration with radiologists, considering them as the minimum semantic units
to explore fine-grained local alignment. For finding and existence, we regard
them as image tags, applying an image-tag recognition decoder to associate
image features with their respective tags within each sample and constructing
soft labels for contrastive learning to improve the semantic association of
different image-report pairs. We evaluate the proposed ASG framework on two
downstream tasks, including five public benchmarks. Experimental results
demonstrate that our method outperforms the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Label or Not to Label: Hybrid Active Learning for Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning (AL) techniques reduce labeling costs for training neural
machine translation (NMT) models by selecting smaller representative subsets
from unlabeled data for annotation. Diversity sampling techniques select
heterogeneous instances, while uncertainty sampling methods select instances
with the highest model uncertainty. Both approaches have limitations -
diversity methods may extract varied but trivial examples, while uncertainty
sampling can yield repetitive, uninformative instances. To bridge this gap, we
propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines
uncertainty and diversity for sentence selection. HUDS computes uncertainty
scores for unlabeled sentences and subsequently stratifies them. It then
clusters sentence embeddings within each stratum using k-MEANS and computes
diversity scores by distance to the centroid. A weighted hybrid score that
combines uncertainty and diversity is then used to select the top instances for
annotation in each AL iteration. Experiments on multi-domain German-English
datasets demonstrate the better performance of HUDS over other strong AL
baselines. We analyze the sentence selection with HUDS and show that it
prioritizes diverse instances having high model uncertainty for annotation in
early AL iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval augmented <span class="highlight-title">text-to-SQL</span> <span class="highlight-title">generation</span> for epidemiological question
  answering using electronic health records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Ziletti, Leonardo D'Ambrosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic health records (EHR) and claims data are rich sources of
real-world data that reflect patient health status and healthcare utilization.
Querying these databases to answer epidemiological questions is challenging due
to the intricacy of medical terminology and the need for complex SQL queries.
Here, we introduce an end-to-end methodology that combines text-to-SQL
generation with retrieval augmented generation (RAG) to answer epidemiological
questions using EHR and claims data. We show that our approach, which
integrates a medical coding step into the text-to-SQL process, significantly
improves the performance over simple prompting. Our findings indicate that
although current language models are not yet sufficiently accurate for
unsupervised use, RAG offers a promising direction for improving their
capabilities, as shown in a realistic industry setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taxo<span class="highlight-title">LLaMA</span>: WordNet-based Model for Solving Multiple Lexical Sematic
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the capabilities of LLMs in capturing
lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model
and test it on multiple lexical semantic tasks. As the outcome of our
experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due
to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results
out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy
Construction, and Lexical Entailment tasks. Moreover, it demonstrates very
strong zero-shot performance on Lexical Entailment and Taxonomy Construction
with no fine-tuning. We also explore its hidden multilingual and domain
adaptation capabilities with a little tuning or few-shot learning. All
datasets, code, and model are available online at
https://github.com/VityaVitalich/TaxoLLaMA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dial-insight: Fine-tuning Large <span class="highlight-title">Language Models</span> with High-Quality
  Domain-Specific Data Preventing Capability Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Sun, Chaoyang Mei, Linlin Wei, Kaiyu Zheng, Na Liu, Ming Cui, Tianyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of large language models (LLMs) is heavily dependent on the
quality of the underlying data, particularly within specialized domains. A
common challenge when fine-tuning LLMs for domain-specific applications is the
potential degradation of the model's generalization capabilities. To address
these issues, we propose a two-stage approach for the construction of
production prompts designed to yield high-quality data. This method involves
the generation of a diverse array of prompts that encompass a broad spectrum of
tasks and exhibit a rich variety of expressions. Furthermore, we introduce a
cost-effective, multi-dimensional quality assessment framework to ensure the
integrity of the generated labeling data. Utilizing a dataset comprised of
service provider and customer interactions from the real estate sector, we
demonstrate a positive correlation between data quality and model performance.
Notably, our findings indicate that the domain-specific proficiency of general
LLMs can be enhanced through fine-tuning with data produced via our proposed
method, without compromising their overall generalization abilities, even when
exclusively domain-specific data is employed for fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Comprehension of <span class="highlight-title">ChatGPT</span> in Traditional Chinese Medicine
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yizhen, Huang Shaohan, Qi Jiaxing, Quan Lei, Han Dongran, Luan Zhongzhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No previous work has studied the performance of Large Language Models (LLMs)
in the context of Traditional Chinese Medicine (TCM), an essential and distinct
branch of medical knowledge with a rich history. To bridge this gap, we present
a TCM question dataset named TCM-QA, which comprises three question types:
single choice, multiple choice, and true or false, to examine the LLM's
capacity for knowledge recall and comprehensive reasoning within the TCM
domain. In our study, we evaluate two settings of the LLM, zero-shot and
few-shot settings, while concurrently discussing the differences between
English and Chinese prompts. Our results indicate that ChatGPT performs best in
true or false questions, achieving the highest precision of 0.688 while scoring
the lowest precision is 0.241 in multiple-choice questions. Furthermore, we
observed that Chinese prompts outperformed English prompts in our evaluations.
Additionally, we assess the quality of explanations generated by ChatGPT and
their potential contribution to TCM knowledge comprehension. This paper offers
valuable insights into the applicability of LLMs in specialized domains and
paves the way for future research in leveraging these powerful models to
advance TCM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caveat Lector: Large <span class="highlight-title">Language Models</span> in Legal Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliza Mik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current fascination with large language models, or LLMs, derives from the
fact that many users lack the expertise to evaluate the quality of the
generated text. LLMs may therefore appear more capable than they actually are.
The dangerous combination of fluency and superficial plausibility leads to the
temptation to trust the generated text and creates the risk of overreliance.
Who would not trust perfect legalese? Relying recent findings in both technical
and legal scholarship, this Article counterbalances the overly optimistic
predictions as to the role of LLMs in legal practice. Integrating LLMs into
legal workstreams without a better comprehension of their limitations, will
create inefficiencies if not outright risks. Notwithstanding their
unprecedented ability to generate text, LLMs do not understand text. Without
the ability to understand meaning, LLMs will remain unable to use language, to
acquire knowledge and to perform complex reasoning tasks. Trained to model
language on the basis of stochastic word predictions, LLMs cannot distinguish
fact from fiction. Their knowledge of the law is limited to word strings
memorized in their parameters. It is also incomplete and largely incorrect.
LLMs operate at the level of word distributions, not at the level of verified
facts. The resulting propensity to hallucinate, to produce statements that are
incorrect but appear helpful and relevant, is alarming in high-risk areas like
legal services. At present, lawyers should beware of relying on text generated
by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Vol 19 Rutgers Bus L R 2 2024 (forthcoming)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Generalization Power of Fine-Tuned Large <span class="highlight-title">Language Models</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann Heng, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have demonstrated exceptional multitasking
abilities, fine-tuning these models on downstream, domain-specific datasets is
often necessary to yield superior performance on test sets compared to their
counterparts without fine-tuning. However, the comprehensive effects of
fine-tuning on the LLMs' generalization ability are not fully understood. This
paper delves into the differences between original, unmodified LLMs and their
fine-tuned variants. Our primary investigation centers on whether fine-tuning
affects the generalization ability intrinsic to LLMs. To elaborate on this, we
conduct extensive experiments across five distinct language tasks on various
datasets. Our main findings reveal that models fine-tuned on generation and
classification tasks exhibit dissimilar behaviors in generalizing to different
domains and tasks. Intriguingly, we observe that integrating the in-context
learning strategy during fine-tuning on generation tasks can enhance the
model's generalization ability. Through this systematic investigation, we aim
to contribute valuable insights into the evolving landscape of fine-tuning
practices for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Basque and Spanish Counter Narrative <span class="highlight-title">Generation</span>: Data Creation and
  Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaione Bengoetxea, Yi-Ling Chung, Marco Guerini, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counter Narratives (CNs) are non-negative textual responses to Hate Speech
(HS) aiming at defusing online hatred and mitigating its spreading across
media. Despite the recent increase in HS content posted online, research on
automatic CN generation has been relatively scarce and predominantly focused on
English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset
for CN generation developed by means of Machine Translation (MT) and
professional post-edition. Being a parallel corpus, also with respect to the
original English CONAN, it allows to perform novel research on multilingual and
crosslingual automatic generation of CNs. Our experiments on CN generation with
mT5, a multilingual encoder-decoder model, show that generation greatly
benefits from training on post-edited data, as opposed to relying on silver MT
data only. These results are confirmed by their correlation with a qualitative
manual evaluation, demonstrating that manually revised training data remains
crucial for the quality of the generated CNs. Furthermore, multilingual data
augmentation improves results over monolingual settings for structurally
similar languages such as English and Spanish, while being detrimental for
Basque, a language isolate. Similar findings occur in zero-shot crosslingual
evaluations, where model transfer (fine-tuning in English and generating in a
different target language) outperforms fine-tuning mT5 on machine translated
data for Spanish but not for Basque. This provides an interesting insight into
the asymmetry in the multilinguality of generative models, a challenging topic
which is still open to research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating LLMs for Gender Disparities in Notable Persons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Rhue, Sofie Goethals, Arun Sundararajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the use of Large Language Models (LLMs) for retrieving
factual information, addressing concerns over their propensity to produce
factually incorrect "hallucinated" responses or to altogether decline to even
answer prompt at all. Specifically, it investigates the presence of
gender-based biases in LLMs' responses to factual inquiries. This paper takes a
multi-pronged approach to evaluating GPT models by evaluating fairness across
multiple dimensions of recall, hallucinations and declinations. Our findings
reveal discernible gender disparities in the responses generated by GPT-3.5.
While advancements in GPT-4 have led to improvements in performance, they have
not fully eradicated these gender disparities, notably in instances where
responses are declined. The study further explores the origins of these
disparities by examining the influence of gender associations in prompts and
the homogeneity in the responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based
  on Meta Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, Pengtao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pretraining followed by task-specific finetuning has achieved
great success in various NLP tasks. Since finetuning all parameters of large
pretrained models poses substantial computational and memory challenges,
several efficient finetuning methods have been developed. Among them, low-rank
adaptation (LoRA), which finetunes low-rank incremental update matrices on top
of frozen pretrained weights, has proven particularly effective. Nonetheless,
LoRA's uniform rank assignment across all layers, along with its reliance on an
exhaustive search to find the best rank, leads to high computation costs and
suboptimal finetuning performance. To address these limitations, we introduce
AutoLoRA, a meta learning based framework for automatically identifying the
optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a
low-rank update matrix with a selection variable, which determines whether the
rank-1 matrix should be discarded. A meta learning based method is developed to
learn these selection variables. The optimal rank is determined by thresholding
the values of these variables. Our comprehensive experiments on natural
language understanding, generation, and sequence labeling demonstrate the
effectiveness of AutoLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI on AI: Exploring the Utility of <span class="highlight-title">GPT</span> as an Expert Annotator of AI
  Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Autumn Toney-Wails, Christian Schoeberl, James Dunham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying scientific publications that are within a dynamic field of
research often requires costly annotation by subject-matter experts. Resources
like widely-accepted classification criteria or field taxonomies are
unavailable for a domain like artificial intelligence (AI), which spans
emerging topics and technologies. We address these challenges by inferring a
functional definition of AI research from existing expert labels, and then
evaluating state-of-the-art chatbot models on the task of expert data
annotation. Using the arXiv publication database as ground-truth, we experiment
with prompt engineering for GPT chatbot models to identify an alternative,
automated expert annotation pipeline that assigns AI labels with 94% accuracy.
For comparison, we fine-tune SPECTER, a transformer language model pre-trained
on scientific publications, that achieves 96% accuracy (only 2% higher than
GPT) on classifying AI publications. Our results indicate that with effective
prompt engineering, chatbots can be used as reliable data annotators even where
subject-area expertise is required. To evaluate the utility of
chatbot-annotated datasets on downstream classification tasks, we train a new
classifier on GPT-labeled data and compare its performance to the arXiv-trained
model. The classifier trained on GPT-labeled data outperforms the arXiv-trained
model by nine percentage points, achieving 82% accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCFEND: A Multi-source Benchmark <span class="highlight-title">Dataset</span> for Chinese Fake News Detection <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Li, Haorui He, Jin Bai, Dacheng Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of fake news across various online sources has had a
significant influence on the public. Existing Chinese fake news detection
datasets are limited to news sourced solely from Weibo. However, fake news
originating from multiple sources exhibits diversity in various aspects,
including its content and social context. Methods trained on purely one single
news source can hardly be applicable to real-world scenarios. Our pilot
experiment demonstrates that the F1 score of the state-of-the-art method that
learns from a large Chinese fake news detection dataset, Weibo-21, drops
significantly from 0.943 to 0.470 when the test data is changed to multi-source
news data, failing to identify more than one-third of the multi-source fake
news. To address this limitation, we constructed the first multi-source
benchmark dataset for Chinese fake news detection, termed MCFEND, which is
composed of news we collected from diverse sources such as social platforms,
messaging apps, and traditional online news outlets. Notably, such news has
been fact-checked by 14 authoritative fact-checking agencies worldwide. In
addition, various existing Chinese fake news detection methods are thoroughly
evaluated on our proposed dataset in cross-source, multi-source, and unseen
source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news
detection approaches in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset
  available: https://github.com/TrustworthyComp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meaningful Learning: Advancing Abstract Reasoning in Large Language
  Models via Generic Fact Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have developed impressive performance and strong
explainability across various reasoning scenarios, marking a significant stride
towards mimicking human-like intelligence. Despite this, when tasked with
simple questions supported by a generic fact, LLMs often fail to provide
consistent and precise answers, indicating a deficiency in abstract reasoning
abilities. This has sparked a vigorous debate about whether LLMs are genuinely
reasoning or merely memorizing. In light of this, we design a preliminary study
to quantify and delve into the abstract reasoning abilities of existing LLMs.
Our findings reveal a substantial discrepancy between their general reasoning
and abstract reasoning performances. To relieve this problem, we tailor an
abstract reasoning dataset (AbsR) together with a meaningful learning paradigm
to teach LLMs how to leverage generic facts for reasoning purposes. The results
show that our approach not only boosts the general reasoning performance of
LLMs but also makes considerable strides towards their capacity for abstract
reasoning, moving beyond simple memorization or imitation to a more nuanced
understanding and application of generic facts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Extraction: An application to the domain of hyper-local
  financial data on developing countries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abuzar Royesh, Olamide Oladeji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the need for financial data on company activities in developing
countries for development research and economic analysis, such data does not
exist. In this project, we develop and evaluate two Natural Language Processing
(NLP) based techniques to address this issue. First, we curate a custom dataset
specific to the domain of financial text data on developing countries and
explore multiple approaches for information extraction. We then explore a
text-to-text approach with the transformer-based T5 model with the goal of
undertaking simultaneous NER and relation extraction. We find that this model
is able to learn the custom text structure output data corresponding to the
entities and their relations, resulting in an accuracy of 92.44\%, a precision
of 68.25\% and a recall of 54.20\% from our best T5 model on the combined task.
Secondly, we explore an approach with sequential NER and relation extration.
For the NER, we run pre-trained and fine-tuned models using SpaCy, and we
develop a custom relation extraction model using SpaCy's Dependency Parser
output and some heuristics to determine entity relationships \cite{spacy}. We
obtain an accuracy of 84.72\%, a precision of 6.06\% and a recall of 5.57\% on
this sequential task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> are Parallel Multilingual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we reveal an in-context learning (ICL) capability of
multilingual large language models (LLMs): by translating the input to several
languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which
significantly enhances their comprehension abilities. To test this capability,
we design extensive experiments encompassing 8 typical datasets, 7 languages
and 8 state-of-the-art multilingual LLMs. Experimental results show that (1)
incorporating more languages help PiM surpass the conventional ICL further; (2)
even combining with the translations that are inferior to baseline performance
can also help. Moreover, by examining the activated neurons in LLMs, we
discover a counterintuitive but interesting phenomenon. Contrary to the common
thought that PiM would activate more neurons than monolingual input to leverage
knowledge learned from diverse languages, PiM actually inhibits neurons and
promotes more precise neuron activation especially when more languages are
added. This phenomenon aligns with the neuroscience insight about synaptic
pruning, which removes less used neural connections, strengthens remainders,
and then enhances brain intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCode: Learning a Unified Codebook for Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose \textbf{UniCode}, a novel approach within the
domain of multimodal large language models (MLLMs) that learns a unified
codebook to efficiently tokenize visual, text, and potentially other types of
signals. This innovation addresses a critical limitation in existing MLLMs:
their reliance on a text-only codebook, which restricts MLLM's ability to
generate images and texts in a multimodal context. Towards this end, we propose
a language-driven iterative training paradigm, coupled with an in-context
pre-training task we term ``image decompression'', enabling our model to
interpret compressed visual data and generate high-quality images.The unified
codebook empowers our model to extend visual instruction tuning to
non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse
stacked quantization approaches in order to compress visual signals into a more
compact token representation. Despite using significantly fewer parameters and
less data during training, Unicode demonstrates promising capabilities in
visual reconstruction and generation. It also achieves performances comparable
to leading MLLMs across a spectrum of VQA benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAMP: A Language Model on the Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale Balsebre, Weiming Huang, Gao Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are poised to play an increasingly important
role in our lives, providing assistance across a wide array of tasks. In the
geospatial domain, LLMs have demonstrated the ability to answer generic
questions, such as identifying a country's capital; nonetheless, their utility
is hindered when it comes to answering fine-grained questions about specific
places, such as grocery stores or restaurants, which constitute essential
aspects of people's everyday lives. This is mainly because the places in our
cities haven't been systematically fed into LLMs, so as to understand and
memorize them. This study introduces a novel framework for fine-tuning a
pre-trained model on city-specific data, to enable it to provide accurate
recommendations, while minimizing hallucinations. We share our model, LAMP, and
the data used to train it. We conduct experiments to analyze its ability to
correctly retrieving spatial objects, and compare it to well-known open- and
closed- source language models, such as GPT-4. Finally, we explore its emerging
capabilities through a case study on day planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Continued <span class="highlight-title">Pretrain</span>ed LLM Approach for Automatic Medical Note
  <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Yuan, Eti Rastogi, Gautam Naik, Jai Chintagunta, Sree Prasanna Rajagopal, Fen Zhao, Sagar Goyal, Jeff Ward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like
GPT-4, is too costly for most domain-specific scenarios. We present the first
continuously trained 13B Llama2-based LLM that is purpose-built for medical
conversations and measured on automated scribing. Our results show that our
model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its
performance in summarizing medical conversations into SOAP notes. Notably, our
model exceeds GPT-4 in capturing a higher number of correct medical concepts
and outperforms human scribes with higher correctness and completeness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RAGGED: Towards Informed Design of Retrieval Augmented <span class="highlight-title">Generation</span>
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, <span class="highlight-author">Graham Neubig</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) greatly benefits language models (LMs)
by providing additional context for tasks such as document-based question
answering (DBQA). Despite its potential, the power of RAG is highly dependent
on its configuration, raising the question: What is the optimal RAG
configuration? To answer this, we introduce the RAGGED framework to analyze and
optimize RAG systems. On a set of representative DBQA tasks, we study two
classic sparse and dense retrievers, and four top-performing LMs in
encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that
different models suit substantially varied RAG setups. While encoder-decoder
models monotonically improve with more documents, we find decoder-only models
can only effectively use < 5 documents, despite often having a longer context
window. RAGGED offers further insights into LMs' context utilization habits,
where we find that encoder-decoder models rely more on contexts and are thus
more sensitive to retrieval quality, while decoder-only models tend to rely on
knowledge memorized during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The First to Know: How Token Distributions Reveal Hidden Knowledge in
  Large Vision-<span class="highlight-title">Language Models</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs), designed to interpret and respond to
human instructions, occasionally generate hallucinated or harmful content due
to inappropriate instructions. This study uses linear probing to shed light on
the hidden knowledge at the output layer of LVLMs. We demonstrate that the
logit distributions of the first tokens contain sufficient information to
determine whether to respond to the instructions, including recognizing
unanswerable visual questions, defending against multi-modal jailbreaking
attack, and identifying deceptive questions. Such hidden knowledge is gradually
lost in logits of subsequent tokens during response generation. Then, we
illustrate a simple decoding strategy at the generation of the first token,
effectively improving the generated content. In experiments, we find a few
interesting insights: First, the CLIP model already contains a strong signal
for solving these tasks, indicating potential bias in the existing datasets.
Second, we observe performance improvement by utilizing the first logit
distributions on three additional tasks, including indicting uncertainty in
math solving, mitigating hallucination, and image classification. Last, with
the same training data, simply finetuning LVLMs improve models' performance but
is still inferior to linear probing on these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Project page:
  https://github.com/Qinyu-Allen-Zhao/LVLM-LP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeUltraFeedback: An LLM-as-a-Judge <span class="highlight-title">Dataset</span> for Aligning Large Language
  Models to Coding Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Weyssow, Aton Kamanda, Houari Sahraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires assessing intricate
textual LLMs' outputs. By relying on automated metrics and static analysis
tools, existing benchmarks fail to assess nuances in user instructions and LLM
outputs, highlighting the need for large-scale datasets and benchmarks for LLM
preference alignment. In this paper, we introduce CodeUltraFeedback, a
preference dataset of 10,000 complex instructions to tune and align LLMs to
coding preferences through AI feedback. We generate responses to the
instructions using a pool of 14 diverse LLMs, which we then annotate according
to their alignment with five coding preferences using the LLM-as-a-Judge
approach with GPT-3.5, producing both numerical and textual feedback. We also
present CODAL-Bench, a benchmark for assessing LLM alignment with these coding
preferences. Our results show that CodeLlama-7B-Instruct, aligned through
reinforcement learning from AI feedback (RLAIF) with direct preference
optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B
LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference
tuning. Furthermore, we show our DPO-aligned CodeLlama model improves
functional correctness on HumanEval+ compared to the unaligned base model.
Therefore, our contributions bridge the gap in preference tuning of LLMs for
code and set the stage for further advancements in model alignment and RLAIF
for code intelligence. Our code and data are available at
https://github.com/martin-wey/CodeUltraFeedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chart<span class="highlight-title">Instruct</span>: <span class="highlight-title">Instruct</span>ion Tuning for Chart Comprehension and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts provide visual representations of data and are widely used for
analyzing information, addressing queries, and conveying insights to others.
Various chart-related downstream tasks have emerged recently, such as
question-answering and summarization. A common strategy to solve these tasks is
to fine-tune various models originally trained on vision tasks language.
However, such task-specific models are not capable of solving a wide range of
chart-related tasks, constraining their real-world applicability. To overcome
these challenges, we introduce ChartInstruct: a novel chart-specific
vision-language Instruction-following dataset comprising 191K instructions
generated with 71K charts. We then present two distinct systems for instruction
tuning on such datasets: (1) an end-to-end model that connects a vision encoder
for chart understanding with a LLM; and (2) a pipeline model that employs a
two-step approach to extract chart data tables and input them into the LLM. In
experiments on four downstream tasks, we first show the effectiveness of our
model--achieving a new set of state-of-the-art results. Further evaluation
shows that our instruction-tuning approach supports a wide array of real-world
chart comprehension and reasoning scenarios, thereby expanding the scope and
applicability of our models to new kinds of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semiparametric Token-Sequence Co-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunji Lee, Doyoung Kim, Jihoon Jun, Sejune Joo, Joel Jang, Kyoung-Woon On, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a semiparametric token-sequence co-supervision
training method. It trains a language model by simultaneously leveraging
supervision from the traditional next token prediction loss which is calculated
over the parametric token embedding space and the next sequence prediction loss
which is calculated over the nonparametric sequence embedding space. The
nonparametric sequence embedding space is constructed by a separate language
model tasked to condense an input text into a single representative embedding.
Our experiments demonstrate that a model trained via both supervisions
consistently surpasses models trained via each supervision independently.
Analysis suggests that this co-supervision encourages a broader generalization
capability across the model. Especially, the robustness of parametric token
space which is established during the pretraining step tends to effectively
enhance the stability of nonparametric sequence embedding space, a new space
established by another language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The swift progress and widespread acceptance of artificial intelligence (AI)
systems highlight a pressing requirement to comprehend both the capabilities
and potential risks associated with AI. Given the linguistic complexity,
cultural richness, and underrepresented status of Arabic in AI research, there
is a pressing need to focus on Large Language Models (LLMs) performance and
safety for Arabic related tasks. Despite some progress in their development,
there is a lack of comprehensive trustworthiness evaluation benchmarks which
presents a major challenge in accurately assessing and improving the safety of
LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first
comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises
516 human-written multiple-choice questions addressing diverse dimensions
related to truthfulness, ethics, safety, physical health, mental health,
unfairness, illegal activities, privacy, and offensive language. By introducing
AraTrust, we aim to promote collaborative efforts to create safer and more
trustworthy LLMs for Arabic users. We evaluated a set of LLMs against our
benchmark to assess its trustworthiness. GPT-4 showed to be the most
trustworthy regarding Arabic language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laying the Foundation First? Investigating the Generalization from
  Atomic Skills to Complex Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Huang, Qianyu He, Yipei Xu, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current language models have demonstrated their capability to develop basic
reasoning, but struggle in more complicated reasoning tasks that require a
combination of atomic skills, such as math word problem requiring skills like
arithmetic and unit conversion. Previous methods either do not improve the
inherent atomic skills of models or not attempt to generalize the atomic skills
to complex reasoning tasks. In this paper, we first propose a probing framework
to investigate whether the atomic skill can spontaneously generalize to complex
reasoning tasks. Then, we introduce a hierarchical curriculum learning training
strategy to achieve better skill generalization. In our experiments, we find
that atomic skills can not spontaneously generalize to compositional tasks. By
leveraging hierarchical curriculum learning, we successfully induce
generalization, significantly improve the performance of open-source LMs on
complex reasoning tasks. Promisingly, the skill generalization exhibit
effective in cross-dataset and cross-domain scenarios. Complex reasoning can
also help enhance atomic skills. Our findings offer valuable guidance for
designing better training strategies for complex reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient
  Generative Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have emerged as the underpinning architecture for Large Language
Models (LLMs). In generative language models, the inference process involves
two primary phases: prompt processing and token generation. Token generation,
which constitutes the majority of the computational workload, primarily entails
vector-matrix multiplications and interactions with the Key-Value (KV) Cache.
This phase is constrained by memory bandwidth due to the overhead of
transferring weights and KV cache values from the memory system to the
computing units. This memory bottleneck becomes particularly pronounced in
applications that require long-context and extensive text generation, both of
which are increasingly crucial for LLMs.
  This paper introduces "Keyformer", an innovative inference-time approach, to
mitigate the challenges associated with KV cache size and memory bandwidth
utilization. Keyformer leverages the observation that approximately 90% of the
attention weight in generative inference focuses on a specific subset of
tokens, referred to as "key" tokens. Keyformer retains only the key tokens in
the KV cache by identifying these crucial tokens using a novel score function.
This approach effectively reduces both the KV cache size and memory bandwidth
usage without compromising model accuracy. We evaluate Keyformer's performance
across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ
various positional embedding algorithms. Our assessment encompasses a variety
of tasks, with a particular emphasis on summarization and conversation tasks
involving extended contexts. Keyformer's reduction of KV cache reduces
inference latency by 2.1x and improves token generation throughput by 2.4x,
while preserving the model's accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A collaborative effort by d-matrix and the University of British
  Columbia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Drafter for Fast Speculative Decoding in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an improved approach of speculative decoding
aimed at enhancing the efficiency of serving large language models. Our method
capitalizes on the strengths of two established techniques: the classic
two-model speculative decoding approach, and the more recent single-model
approach, Medusa. Drawing inspiration from Medusa, our approach adopts a
single-model strategy for speculative decoding. However, our method
distinguishes itself by employing a single, lightweight draft head with a
recurrent dependency design, akin in essence to the small, draft model uses in
classic speculative decoding, but without the complexities of the full
transformer architecture. And because of the recurrent dependency, we can use
beam search to swiftly filter out undesired candidates with the draft head. The
outcome is a method that combines the simplicity of single-model design and
avoids the need to create a data-dependent tree attention structure only for
inference in Medusa. We empirically demonstrate the effectiveness of the
proposed method on several popular open source language models, along with a
comprehensive analysis of the trade-offs involved in adopting this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geographically-Informed Language Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Dunn, Lane Edwards-Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an approach to language identification in which the set
of languages considered by the model depends on the geographic origin of the
text in question. Given that many digital corpora can be geo-referenced at the
country level, this paper formulates 16 region-specific models, each of which
contains the languages expected to appear in countries within that region.
These regional models also each include 31 widely-spoken international
languages in order to ensure coverage of these linguae francae regardless of
location. An upstream evaluation using traditional language identification
testing data shows an improvement in f-score ranging from 1.7 points (Southeast
Asia) to as much as 10.4 points (North Africa). A downstream evaluation on
social media data shows that this improved performance has a significant impact
on the language labels which are applied to large real-world corpora. The
result is a highly-accurate model that covers 916 languages at a sample size of
50 characters, the performance improved by incorporating geographic information
into the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fisher Mask Nodes for Language Model Merging <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thennal D K, Ganesh Nathan, Suchithra M S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup of 57.4x. Our results prove the potential of our method in current
multi-task learning environments and suggest its scalability and adaptability
to new model architectures and learning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sabiá-2: A New <span class="highlight-title">Generation</span> of Portuguese Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FakeWatch: A Framework for Detecting Fake News to Ensure Credible
  Elections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Tahniat Khan, Drai Paulen-Patterson, Veronica Chatrath, Mizanur Rahman, Oluwanifemi Bamgbose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's technologically driven world, the rapid spread of fake news,
particularly during critical events like elections, poses a growing threat to
the integrity of information. To tackle this challenge head-on, we introduce
FakeWatch, a comprehensive framework carefully designed to detect fake news.
Leveraging a newly curated dataset of North American election-related news
articles, we construct robust classification models. Our framework integrates a
model hub comprising of both traditional machine learning (ML) techniques and
cutting-edge Language Models (LMs) to discern fake news effectively. Our
overarching objective is to provide the research community with adaptable and
precise classification models adept at identifying the ever-evolving landscape
of misinformation. Quantitative evaluations of fake news classifiers on our
dataset reveal that, while state-of-the-art LMs exhibit a slight edge over
traditional ML models, classical models remain competitive due to their balance
of accuracy and computational efficiency. Additionally, qualitative analyses
shed light on patterns within fake news articles. This research lays the
groundwork for future endeavors aimed at combating misinformation, particularly
concerning electoral processes. We provide our labeled data and model publicly
for use and reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2312.03730</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Boosts Calibration for Math Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibration, which establishes the correlation between accuracy and model
confidence, is important for LLM development. We design three off-the-shelf
calibration methods based on self-consistency (Wang et al., 2022) for math
reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using
strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model
confidence and accuracy than existing methods based on p(True) (Kadavath et
al., 2022) or logit (Kadavath et al., 2022).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Behavior of Machine Translation with Large <span class="highlight-title">Language Models</span> under
  <span class="highlight-title">Prompt</span> Injection Attacks <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifan Sun, Antonio Valerio Miceli-Barone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly becoming the preferred
foundation platforms for many Natural Language Processing tasks such as Machine
Translation, owing to their quality often comparable to or better than
task-specific models, and the simplicity of specifying the task through natural
language instructions or in-context examples. Their generality, however, opens
them up to subversion by end users who may embed into their requests
instructions that cause the model to behave in unauthorized and possibly unsafe
ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple
families of LLMs on a Machine Translation task, focusing on the effects of
model size on the attack success rates. We introduce a new benchmark data set
and we discover that on multiple language pairs and injected prompts written in
English, larger models under certain conditions may become more susceptible to
successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et
al., 2023). To our knowledge, this is the first work to study non-trivial LLM
scaling behaviour in a multi-lingual setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 18 figures, First Workshop on the Scaling Behavior of Large
  Language Models (SCALE-LLM 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Helpful or Harmful? Exploring the Efficacy of Large <span class="highlight-title">Language Models</span> for
  Online Grooming Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ellie Prosser, Matthew Edwards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful generative Large Language Models (LLMs) are becoming popular tools
amongst the general public as question-answering systems, and are being
utilised by vulnerable groups such as children. With children increasingly
interacting with these tools, it is imperative for researchers to scrutinise
the safety of LLMs, especially for applications that could lead to serious
outcomes, such as online child safety queries. In this paper, the efficacy of
LLMs for online grooming prevention is explored both for identifying and
avoiding grooming through advice generation, and the impact of prompt design on
model performance is investigated by varying the provided context and prompt
specificity. In results reflecting over 6,000 LLM interactions, we find that no
models were clearly appropriate for online grooming prevention, with an
observed lack of consistency in behaviours, and potential for harmful answer
generation, especially from open-source models. We outline where and how models
fall short, providing suggestions for improvement, and identify prompt designs
that heavily altered model performance in troubling ways, with findings that
can be used to inform best practice usage guides.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Images are Achilles' Heel of Alignment: Exploiting Visual
  Vulnerabilities for Jailbreaking Multimodal Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the harmlessness alignment problem of multimodal
large language models~(MLLMs). We conduct a systematic empirical analysis of
the harmlessness performance of representative MLLMs and reveal that the image
input poses the alignment vulnerability of MLLMs. Inspired by this, we propose
a novel jailbreak method named HADES, which hides and amplifies the harmfulness
of the malicious intent within the text input, using meticulously crafted
images. Experimental results show that HADES can effectively jailbreak existing
MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for
LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotional Intelligence Through Artificial Intelligence : NLP and Deep
  Learning in the Analysis of Healthcare Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Was Your <span class="highlight-title">Prompt</span>? A Remote Keylogging Attack on AI Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Weiss, Daniel Ayzenshteyn, Guy Amit, Yisroel Mirsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI assistants are becoming an integral part of society, used for asking
advice or help in personal and confidential issues. In this paper, we unveil a
novel side-channel that can be used to read encrypted responses from AI
Assistants over the web: the token-length side-channel. We found that many
vendors, including OpenAI and Microsoft, have this side-channel.
  However, inferring the content of a response from a token-length sequence
alone proves challenging. This is because tokens are akin to words, and
responses can be several sentences long leading to millions of grammatically
correct sentences. In this paper, we show how this can be overcome by (1)
utilizing the power of a large language model (LLM) to translate these
sequences, (2) providing the LLM with inter-sentence context to narrow the
search space and (3) performing a known-plaintext attack by fine-tuning the
model on the target model's writing style.
  Using these methods, we were able to accurately reconstruct 29\% of an AI
assistant's responses and successfully infer the topic from 55\% of them. To
demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and
Microsoft's Copilot on both browser and API traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and <span class="highlight-title">Language Models</span> in Modern Organizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Jose Xavier Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Safety Generalization Challenges of Large <span class="highlight-title">Language Models</span> via
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VBART: The Turkish LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meliksah Turker, Mehmet Erdi Ari, Aydin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is up to 11x more
efficient than multilingual tokenizers. Last but not least, we introduce a
method to enlarge an existing pre-trained LLM and question the relevancy of
Chinchilla Scaling Law to sequence-to-sequence masked language models. Our
fine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Me <span class="highlight-title">LLaMA</span>: Foundation Large <span class="highlight-title">Language Models</span> for Medical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xingyu Zhou, Huan He, Lucila Ohno-Machado, Yonghui Wu, Hua Xu, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) such as ChatGPT and LLaMA have shown
great promise in many AI applications. However, their performance on medical
tasks is suboptimal and can be improved by training on extensive
domain-specific datasets. This study introduces Me LLaMA, a medical LLM family
that includes foundation models - Me LLaMA 13/70B, along with their
chat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual
pre-training and instruction tuning of LLaMA2 using large medical datasets. Our
domain-specific data suite for training and evaluation includes a large-scale,
continual pre-training dataset with 129B tokens, an instruction tuning dataset
with 214k samples, and a new medical evaluation benchmark (MIBE) across six
tasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me
LLaMA models achieve overall better performance than existing open-source
medical LLMs in zero-shot, few-shot and supervised learning abilities. Their
zero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,
with a slight variance of within 3%, and yet falls short when compared to
GPT-4. In addition, we investigated the catastrophic forgetting problem, and
our results show that Me LLaMA models outperform other open-source medical LLMs
in mitigating this issue. Me LLaMA is one of the largest open-source medical
foundation LLMs that use both biomedical and clinical data. It exhibits
superior performance across both general and medical tasks compared to other
open-source medical LLMs, rendering it an attractive choice for medical AI
applications. We release our models, datasets, and evaluation scripts at:
https://github.com/BIDS-Xu-Lab/Me-LLaMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot and Few-shot <span class="highlight-title">Generation</span> Strategies for Artificial Clinical
  Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlend Frayling, Jake Lever, Graham McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-pop Lyric Translation: <span class="highlight-title">Dataset</span>, Analysis, and Neural-Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyric translation, a field studied for over a century, is now attracting
computational linguistics researchers. We identified two limitations in
previous studies. Firstly, lyric translation studies have predominantly focused
on Western genres and languages, with no previous study centering on K-pop
despite its popularity. Second, the field of lyric translation suffers from a
lack of publicly available datasets; to the best of our knowledge, no such
dataset exists. To broaden the scope of genres and languages in lyric
translation studies, we introduce a novel singable lyric translation dataset,
approximately 89\% of which consists of K-pop song lyrics. This dataset aligns
Korean and English lyrics line-by-line and section-by-section. We leveraged
this dataset to unveil unique characteristics of K-pop lyric translation,
distinguishing it from other extensively studied genres, and to construct a
neural lyric translation model, thereby underscoring the importance of a
dedicated dataset for singable lyric translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOCIALITE-<span class="highlight-title">LLAMA</span>: An <span class="highlight-title">Instruct</span>ion-Tuned Model for Social Scientific Tasks <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gourab Dey, Adithya V Ganesan, Yash Kumar Lal, Manal Shah, Shreyashee Sinha, Matthew Matero, Salvatore Giorgi, Vivek Kulkarni, H. Andrew Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social science NLP tasks, such as emotion or humor detection, are required to
capture the semantics along with the implicit pragmatics from text, often with
limited amounts of training data. Instruction tuning has been shown to improve
the many capabilities of large language models (LLMs) such as commonsense
reasoning, reading comprehension, and computer programming. However, little is
known about the effectiveness of instruction tuning on the social domain where
implicit pragmatic cues are often needed to be captured. We explore the use of
instruction tuning for social science NLP tasks and introduce Socialite-Llama
-- an open-source, instruction-tuned Llama. On a suite of 20 social science
tasks, Socialite-Llama improves upon the performance of Llama as well as
matches or improves upon the performance of a state-of-the-art, multi-task
finetuned model on a majority of them. Further, Socialite-Llama also leads to
improvement on 5 out of 6 related social tasks as compared to Llama, suggesting
instruction tuning can lead to generalized social understanding. All resources
including our code, model and dataset can be found through
bit.ly/socialitellama.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper accepted to EACL 2024. 4 pgs, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring <span class="highlight-title">Self-supervised</span> Logic-enhanced Training for Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13718v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13718v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangkai Jiao, Zhiyang Teng, Bosheng Ding, Zhengyuan Liu, Nancy F. Chen, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing efforts to improve logical reasoning ability of language models have
predominantly relied on supervised fine-tuning, hindering generalization to new
domains and/or tasks. The development of Large Langauge Models (LLMs) has
demonstrated the capacity of compressing abundant knowledge into a single
proxy, enabling them to tackle multiple tasks effectively. Our preliminary
experiments, nevertheless, show that LLMs do not show capability on logical
reasoning. The performance of LLMs on logical reasoning benchmarks is far
behind the existing state-of-the-art baselines. In this paper, we make the
first attempt to investigate the feasibility of incorporating logical knowledge
through self-supervised post-training, and activating it via in-context
learning, which we termed as LogicLLM. Specifically, we devise an
auto-regressive objective variant of MERIt and integrate it with two LLM
series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to
13 billion. The results on two challenging logical reasoning benchmarks
demonstrate the effectiveness of LogicLLM. Besides, we conduct extensive
ablation studies to analyze the key factors in designing logic-oriented proxy
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning New Tasks from a Few Examples with Soft-Label Prototypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avyav Kumar Singh, Ekaterina Shutova, Helen Yannakoudakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to few-shot learning in NLP rely on large language models
and fine-tuning of these to generalise on out-of-distribution data. In this
work, we propose a simple yet powerful approach to "extreme" few-shot learning,
wherein models are exposed to as little as 4 examples per class, based on
soft-label prototypes that collectively capture the distribution of different
classes across the input domain space. Inspired by previous work (Sucholutsky
et al., 2021) on univariate or simple multivariate (synthetic) data, we propose
a novel approach that is effective on large, high-dimensional and real-world
datasets. We learn soft-label prototypes within a neural framework (DeepSLP)
and we experimentally demonstrate that it achieves superior performance on
31/48 tested tasks and few-shot settings while closely matching the performance
of strong baselines on the rest. We focus on learning previously unseen NLP
tasks from very few examples (4, 8, 16) per label and present an in-depth
analysis of the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) now excel at code generation, a key aspect
of software development is the art of refactoring: consolidating code into
libraries of reusable and readable programs. In this paper, we introduce LILO,
a neurosymbolic framework that iteratively synthesizes, compresses, and
documents code to build libraries tailored to particular problem domains. LILO
combines LLM-guided program synthesis with recent algorithmic advances in
automated refactoring from Stitch: a symbolic compression system that
efficiently identifies optimal lambda abstractions across large code corpora.
To make these abstractions interpretable, we introduce an auto-documentation
(AutoDoc) procedure that infers natural language names and docstrings based on
contextual examples of usage. In addition to improving human readability, we
find that AutoDoc boosts performance by helping LILO's synthesizer to interpret
and deploy learned abstractions. We evaluate LILO on three inductive program
synthesis benchmarks for string editing, scene reasoning, and graphics
composition. Compared to existing neural and symbolic methods - including the
state-of-the-art library learning algorithm DreamCoder - LILO solves more
complex tasks and learns richer libraries that are grounded in linguistic
knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot
  Vision-and-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Liang, Liang Ma, Shanshan Guo, Jianhua Han, Hang Xu, Shikui Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and following natural language instructions while navigating
through complex, real-world environments poses a significant challenge for
general-purpose robots. These environments often include obstacles and
pedestrians, making it essential for autonomous agents to possess the
capability of self-corrected planning to adjust their actions based on feedback
from the surroundings. However, the majority of existing vision-and-language
navigation (VLN) methods primarily operate in less realistic simulator settings
and do not incorporate environmental feedback into their decision-making
processes. To address this gap, we introduce a novel zero-shot framework called
CorNav, utilizing a large language model for decision-making and comprising two
key components: 1) incorporating environmental feedback for refining future
plans and adjusting its actions, and 2) multiple domain experts for parsing
instructions, scene understanding, and refining predicted actions. In addition
to the framework, we develop a 3D simulator that renders realistic scenarios
using Unreal Engine 5. To evaluate the effectiveness and generalization of
navigation agents in a zero-shot multi-task setting, we create a benchmark
called NavBench. Extensive experiments demonstrate that CorNav consistently
outperforms all baselines by a significant margin across all tasks. On average,
CorNav achieves a success rate of 28.1\%, surpassing the best baseline's
performance of 20.5\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Executing Natural Language-Described Algorithms with Large Language
  Models: An Investigation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Executing computer programs described in natural language has long been a
pursuit of computer science. With the advent of enhanced natural language
understanding capabilities exhibited by large language models (LLMs), the path
toward this goal has been illuminated. In this paper, we seek to examine the
capacity of present-day LLMs to comprehend and execute algorithms outlined in
natural language. We established an algorithm test set sourced from
Introduction to Algorithm, a well-known textbook that contains many
representative widely-used algorithms. To systematically assess LLMs' code
execution abilities, we selected 30 algorithms, generated 300 random-sampled
instances in total, and evaluated whether popular LLMs can understand and
execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can
effectively execute programs described in natural language, as long as no heavy
numeric computation is involved. We believe our findings contribute to
evaluating LLMs' code execution abilities and would encourage further
investigation and application for the computation power of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernelized Concept Erasure <span class="chip">EMNLP22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12191v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12191v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific non-linear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper in EMNLP22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Log-linear Guardedness and its Implications <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10012v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10012v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Yoav Goldberg, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for erasing human-interpretable concepts from neural representations
that assume linearity have been found to be tractable and useful. However, the
impact of this removal on the behavior of downstream classifiers trained on the
modified representations is not fully understood. In this work, we formally
define the notion of log-linear guardedness as the inability of an adversary to
predict the concept directly from the representation, and study its
implications. We show that, in the binary case, under certain assumptions, a
downstream log-linear model cannot recover the erased concept. However, we
demonstrate that a multiclass log-linear model \emph{can} be constructed that
indirectly recovers the concept in some cases, pointing to the inherent
limitations of log-linear guardedness as a downstream bias mitigation
technique. These findings shed light on the theoretical limitations of linear
erasure methods and highlight the need for further research on the connections
between intrinsic and extrinsic bias in neural models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper in ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confusing charge prediction is a challenging task in legal AI, which involves
predicting confusing charges based on fact descriptions. While existing charge
prediction methods have shown impressive performance, they face significant
challenges when dealing with confusing charges, such as Snatch and Robbery. In
the legal domain, constituent elements play a pivotal role in distinguishing
confusing charges. Constituent elements are fundamental behaviors underlying
criminal punishment and have subtle distinctions among charges. In this paper,
we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces
domain knowledge regarding constituent elements to guide the model in making
judgments on confusing charges, much like a judge's reasoning process.
Specifically, we first construct a legal knowledge graph containing constituent
elements to help select keywords for each charge, forming a word bag.
Subsequently, to guide the model's attention towards the differentiating
information for each charge within the context, we expand the attention
mechanism and introduce a new loss function with attention supervision through
words in the word bag. We construct the confusing charges dataset from
real-world judicial documents. Experiments demonstrate the effectiveness of our
method, especially in maintaining exceptional performance in imbalanced label
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting the Translation Ability of Large <span class="highlight-title">Language Models</span> via
  Multilingual Finetuning with Translation <span class="highlight-title">Instruct</span>ions <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15083v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15083v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have
shown strong abilities in multilingual translations, without being explicitly
trained on parallel corpora. It is interesting how the LLMs obtain their
ability to carry out translation instructions for different languages. In this
paper, we present a detailed analysis by finetuning a multilingual pretrained
language model, XGLM-7B, to perform multilingual translation following given
instructions. Firstly, we show that multilingual LLMs have stronger translation
abilities than previously demonstrated. For a certain language, the performance
depends on its similarity to English and the amount of data used in the
pretraining phase. Secondly, we find that LLMs' ability to carry out
translation instructions relies on the understanding of translation
instructions and the alignment among different languages. With multilingual
finetuning, LLMs could learn to perform the translation task well even for
those language pairs unseen during the instruction tuning phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by Transaction of ACL, pre-MIT version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Contextual Clues and Role Correlations for Enhancing
  Document-level Event Argument Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanlong Liu, Dingyi Zeng, Li Zhou, Yichen Xiao, Weishan Kong, Malu Zhang, Shaohuan Cheng, Hongyang Zhao, Wenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level event argument extraction is a crucial yet challenging task
within the field of information extraction. Current mainstream approaches
primarily focus on the information interaction between event triggers and their
arguments, facing two limitations: insufficient context interaction and the
ignorance of event correlations. Here, we introduce a novel framework named
CARLG (Contextual Aggregation of clues and Role-based Latent Guidance),
comprising two innovative components: the Contextual Clues Aggregation (CCA)
and the Role-based Latent Information Guidance (RLIG). The CCA module leverages
the attention weights derived from a pre-trained encoder to adaptively
assimilates broader contextual information, while the RLIG module aims to
capture the semantic correlations among event roles. We then instantiate the
CARLG framework into two variants based on two types of current mainstream EAE
approaches. Notably, our CARLG framework introduces less than 1% new parameters
yet significantly improving the performance. Comprehensive experiments across
the RAMS, WikiEvents, and MLEE datasets confirm the superiority of CARLG,
showing significant superiority in terms of both performance and inference
speed compared to major benchmarks. Further analyses demonstrate the
effectiveness of the proposed modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>pre-submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ German also Hallucinates! Inconsistency Detection in News Summaries with
  the Absinth <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Mascarell, Ribin Chalumattu, Annette Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has led to remarkable progress on
a wide range of natural language processing tasks. Despite the advances, these
large-sized models still suffer from hallucinating information in their output,
which poses a major issue in automatic text summarization, as we must guarantee
that the generated summary is consistent with the content of the source
document. Previous research addresses the challenging task of detecting
hallucinations in the output (i.e. inconsistency detection) in order to
evaluate the faithfulness of the generated summaries. However, these works
primarily focus on English and recent multilingual approaches lack German data.
This work presents absinth, a manually annotated dataset for hallucination
detection in German news summarization and explores the capabilities of novel
open-source LLMs on this task in both fine-tuning and in-context learning
settings. We open-source and release the absinth dataset to foster further
research on hallucination detection in German.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures, 7 tables, conference: Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024), Turin, Italy, May 20-25, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMSR: Symbolic Regression is a Multimodal Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18603v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18603v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan Hao, Su Wei, Yusong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical formulas are the crystallization of human wisdom in exploring
the laws of nature for thousands of years. Describing the complex laws of
nature with a concise mathematical formula is a constant pursuit of scientists
and a great challenge for artificial intelligence. This field is called
symbolic regression. Symbolic regression was originally formulated as a
combinatorial optimization problem, and GP and reinforcement learning
algorithms were used to solve it. However, GP is sensitive to hyperparameters,
and these two types of algorithms are inefficient. To solve this problem,
researchers treat the mapping from data to expressions as a translation
problem. And the corresponding large-scale pre-trained model is introduced.
However, the data and expression skeletons do not have very clear word
correspondences as the two languages do. Instead, they are more like two
modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.
The SR problem is solved as a pure multimodal problem, and contrastive learning
is also introduced in the training process for modal alignment to facilitate
later modal feature fusion. It is worth noting that in order to better promote
the modal feature fusion, we adopt the strategy of training contrastive
learning loss and other losses at the same time, which only needs one-step
training, instead of training contrastive learning loss first and then training
other losses. Because our experiments prove training together can make the
feature extraction module and feature fusion module running-in better.
Experimental results show that compared with multiple large-scale pre-training
baselines, MMSR achieves the most advanced results on multiple mainstream
datasets including SRBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regional inflation analysis using social network data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilii Chsherbakov, Ilia Karpov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inflation is one of the most important macroeconomic indicators that have a
great impact on the population of any country and region. Inflation is
influenced by range of factors, one of which is inflation expectations. Many
central banks take this factor into consideration while implementing monetary
policy within the inflation targeting regime. Nowadays, a lot of people are
active users of the Internet, especially social networks. There is a hypothesis
that people search, read, and discuss mainly only those issues that are of
particular interest to them. It is logical to assume that the dynamics of
prices may also be in the focus of user discussions. So, such discussions could
be regarded as an alternative source of more rapid information about inflation
expectations. This study is based on unstructured data from Vkontakte social
network to analyze upward and downward inflationary trends (on the example of
the Omsk region). The sample of more than 8.5 million posts was collected
between January 2010 and May 2022. The authors used BERT neural networks to
solve the problem. These models demonstrated better results than the benchmarks
(e.g., logistic regression, decision tree classifier, etc.). It makes possible
to define pro-inflationary and disinflationary types of keywords in different
contexts and get their visualization with SHAP method. This analysis provides
additional operational information about inflationary processes at the regional
level The proposed approach can be scaled for other regions. At the same time
the limitation of the work is the time and power costs for the initial training
of similar models for all regions of Russia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merge, Then Compress: Demystify Efficient SMoE with Hints from Its
  Routing Policy <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up
the learning capacity of neural networks, however, they have issues like (a)
High Memory Usage, due to duplication of the network layers into multiple
copies as experts; and (b) Redundancy in Experts, as common learning-based
routing policies suffer from representational collapse. Therefore, vanilla SMoE
models are memory inefficient and non-scalable, especially for
resource-constrained downstream scenarios. In this paper, we ask: Can we craft
a compact SMoE model by consolidating expert information? What is the best
recipe to merge multiple experts into fewer but more knowledgeable experts? Our
pilot investigation reveals that conventional model merging methods fail to be
effective in such expert merging for SMoE. The potential reasons are: (1)
redundant information overshadows critical experts; (2) appropriate neuron
permutation for each expert is missing to bring all of them in alignment. To
address this, we propose M-SMoE, which leverages routing statistics to guide
expert merging. Specifically, it starts with neuron permutation alignment for
experts; then, dominant experts and their "group members" are formed; lastly,
every expert group is merged into a single expert by utilizing each expert's
activation frequency as their weight for merging, thus diminishing the impact
of insignificant experts. Moreover, we observed that our proposed merging
promotes a low dimensionality in the merged expert's weight space, naturally
paving the way for additional compression. Hence, our final method, MC-SMoE
(i.e., Merge, then Compress SMoE), further decomposes the merged experts into
low-rank and structural sparse alternatives. Extensive experiments across 8
benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE
achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andries Smit, Paul Duckworth, Nathan Grinsztajn, Thomas D. Barrett, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) underscore their
potential for responding to inquiries in various domains. However, ensuring
that generative agents provide accurate and reliable answers remains an ongoing
challenge. In this context, multi-agent debate (MAD) has emerged as a promising
strategy for enhancing the truthfulness of LLMs. We benchmark a range of
debating and prompting strategies to explore the trade-offs between cost, time,
and accuracy. Importantly, we find that multi-agent debating systems, in their
current form, do not reliably outperform other proposed prompting strategies,
such as self-consistency and ensembling using multiple reasoning paths.
However, when performing hyperparameter tuning, several MAD systems, such as
Multi-Persona, perform better. This suggests that MAD protocols might not be
inherently worse than other approaches, but that they are more sensitive to
different hyperparameter settings and difficult to optimize. We build on these
results to offer insights into improving debating strategies, such as adjusting
agent agreement levels, which can significantly enhance performance and even
surpass all other non-debate protocols we evaluated. We provide an open-source
repository to the community with several state-of-the-art protocols together
with evaluation scripts to benchmark across popular research datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Clarity: Generating Sentences with <span class="highlight-title">Transformer</span> Models using
  Context-Reverso Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Musaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of information abundance, the ability to provide users with
contextually relevant and concise information is crucial. Keyword in Context
(KIC) generation is a task that plays a vital role in and generation
applications, such as search engines, personal assistants, and content
summarization. In this paper, we present a novel approach to generating
unambiguous and brief sentence-contexts for given keywords using the T5
transformer model, leveraging data obtained from the Context-Reverso API. The
code is available at https://github.com/Rusamus/word2context/tree/main .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study
  on Out-of-Distribution Generalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining deep learning with symbolic logic reasoning aims to capitalize on
the success of both fields and is drawing increasing attention. Inspired by
DeepLogic, an end-to-end model trained to perform inference on logic programs,
we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step
reasoning expressed in natural language. In our model, reasoning is performed
using an iterative memory neural network based on RNN with a gate attention
mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES
V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention
can achieve higher test accuracy than DeepLogic and other RNN baseline models.
Our model achieves better out-of-distribution generalisation than RoBERTa-Large
when the rules have been shuffled. Furthermore, to address the issue of
unbalanced distribution of reasoning depths in the current multi-step reasoning
datasets, we develop PARARULE-Plus, a large dataset with more examples that
require deeper reasoning steps. Experimental results show that the addition of
PARARULE-Plus can increase the model's performance on examples requiring deeper
reasoning depths. The source code and data are available at
https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, The 2nd International Joint Conference on
  Learning & Reasoning and 16th International Workshop on Neural-Symbolic
  Learning and Reasoning (IJCLR-NeSy 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DyVal: Dynamic Evaluation of Large <span class="highlight-title">Language Models</span> for Reasoning Tasks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, <span class="highlight-author">Diyi Yang</span>, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable performance in various
evaluation benchmarks. However, concerns are raised about potential data
contamination in their considerable volume of training corpus. Moreover, the
static nature and fixed complexity of current benchmarks may inadequately gauge
the advancing capabilities of LLMs. In this paper, we introduce DyVal, a
general and flexible protocol for dynamic evaluation of LLMs. Based on our
framework, we build graph-informed DyVal by leveraging the structural advantage
of directed acyclic graphs to dynamically generate evaluation samples with
controllable complexities. DyVal generates challenging evaluation sets on
reasoning tasks including mathematics, logical reasoning, and algorithm
problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo
and GPT-4. Experiments show that LLMs perform worse in DyVal-generated
evaluation samples with different complexities, highlighting the significance
of dynamic evaluation. We also analyze the failure cases and results of
different prompting methods. Moreover, DyVal-generated samples are not only
evaluation sets, but also helpful data for fine-tuning to improve the
performance of LLMs on existing benchmarks. We hope that DyVal can shed light
on future evaluation research of LLMs. Code is available at:
https://github.com/microsoft/promptbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 spotlight; 38 pages; code is at aka.ms/dyval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-Distil-<span class="highlight-title">BERT</span>: Code-mixed Language Modeling for Bangla, English, and
  Hindi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Nishat Raihan, Dhiman Goswami, Antara Mahmud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most popular downstream tasks in the field of Natural Language
Processing is text classification. Text classification tasks have become more
daunting when the texts are code-mixed. Though they are not exposed to such
text during pre-training, different BERT models have demonstrated success in
tackling Code-Mixed NLP challenges. Again, in order to enhance their
performance, Code-Mixed NLP models have depended on combining synthetic data
with real-world data. It is crucial to understand how the BERT models'
performance is impacted when they are pretrained using corresponding code-mixed
languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model
pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model
fine-tuned on code-mixed data. Both models are evaluated across multiple NLP
tasks and demonstrate competitive performance against larger models like mBERT
and XLM-R. Our two-tiered pre-training approach offers efficient alternatives
for multilingual and code-mixed language understanding, contributing to
advancements in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making <span class="highlight-title">Language Models</span> Better Tool Learners with Execution Feedback <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13068v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13068v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tools serve as pivotal interfaces that enable humans to understand and
reshape the environment. With the advent of foundation models, AI systems can
utilize tools to expand their capabilities and interact with the real world.
Existing tool learning methodologies, encompassing supervised fine-tuning and
prompt engineering approaches, often induce large language models to utilize
tools indiscriminately, as complex tasks often exceed their own competencies.
However, introducing tools for simple tasks, which the models themselves can
readily resolve, can inadvertently propagate errors rather than enhance
performance. This leads to the research question: can we teach language models
when and how to use tools? To meet this need, we propose Tool leaRning wIth
exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the
model to continually learn through feedback derived from tool execution,
thereby learning when and how to use tools effectively. Experimental results,
backed by further analysis, show that TRICE can make the large language model
selectively use tools by improving the accuracy of tool usage while enhancing
insufficient tool learning and mitigating excessive reliance on tools. Code is
available at https://github.com/zjunlp/TRICE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vanishing Gradients in Reinforcement Finetuning of <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models are commonly aligned with human preferences and
downstream tasks via reinforcement finetuning (RFT), which refers to maximizing
a (possibly learned) reward function using policy gradient algorithms. This
work identifies a fundamental optimization obstacle in RFT: we prove that the
expected gradient for an input vanishes when its reward standard deviation
under the model is small, even if the expected reward is far from optimal.
Through experiments on an RFT benchmark and controlled environments, as well as
a theoretical analysis, we then demonstrate that vanishing gradients due to
small reward standard deviation are prevalent and detrimental, leading to
extremely slow reward maximization. Lastly, we explore ways to overcome
vanishing gradients in RFT. We find the common practice of an initial
supervised finetuning (SFT) phase to be the most promising candidate, which
sheds light on its importance in an RFT pipeline. Moreover, we show that a
relatively small number of SFT optimization steps on as few as 1% of the input
samples can suffice, indicating that the initial SFT phase need not be
expensive in terms of compute and data labeling efforts. Overall, our results
emphasize that being mindful for inputs whose expected gradient vanishes, as
measured by the reward standard deviation, is crucial for successful execution
of RFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Interactive Evaluation for Large <span class="highlight-title">Language Models</span> with State
  Aware Patient Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable proficiency in
human interactions, yet their application within the medical field remains
insufficiently explored. Previous works mainly focus on the performance of
medical knowledge with examinations, which is far from the realistic scenarios,
falling short in assessing the abilities of LLMs on clinical tasks. In the
quest to enhance the application of Large Language Models (LLMs) in healthcare,
this paper introduces the Automated Interactive Evaluation (AIE) framework and
the State-Aware Patient Simulator (SAPS), targeting the gap between traditional
LLM evaluations and the nuanced demands of clinical practice. Unlike prior
methods that rely on static medical knowledge assessments, AIE and SAPS provide
a dynamic, realistic platform for assessing LLMs through multi-turn
doctor-patient simulations. This approach offers a closer approximation to real
clinical scenarios and allows for a detailed analysis of LLM behaviors in
response to complex patient interactions. Our extensive experimental validation
demonstrates the effectiveness of the AIE framework, with outcomes that align
well with human evaluations, underscoring its potential to revolutionize
medical LLM testing for improved healthcare delivery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BeLLM: Backward Dependency Enhanced Large Language Model for Sentence
  Embeddings <span class="chip">NAACL24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianming Li, Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence embeddings are crucial in measuring semantic similarity. Most recent
studies employed large language models (LLMs) to learn sentence embeddings.
Existing LLMs mainly adopted autoregressive architecture without explicit
backward dependency modeling. Therefore, we examined the effects of backward
dependencies in LLMs for semantic similarity measurements. Concretely, we
propose a novel model: backward dependency enhanced large language model
(BeLLM). It learns sentence embeddings via transforming specific attention
layers from uni- to bi-directional. We extensively experiment across various
semantic textual similarity (STS) tasks and downstream applications. BeLLM
achieves state-of-the-art performance in varying scenarios. It shows that
auto-regressive LLMs benefit from backward dependencies for sentence
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL24 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Probing and Tuning for Text-to-Image <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in text-to-image generation (T2I), prior methods often
face text-image misalignment problems such as relation confusion in generated
images. Existing solutions involve cross-attention manipulation for better
compositional understanding or integrating large language models for improved
layout planning. However, the inherent alignment capabilities of T2I models are
still inadequate. By reviewing the link between generative and discriminative
modeling, we posit that T2I models' discriminative abilities may reflect their
text-image alignment proficiency during generation. In this light, we advocate
bolstering the discriminative abilities of T2I models to achieve more precise
text-to-image alignment for generation. We present a discriminative adapter
built on T2I models to probe their discriminative abilities on two
representative tasks and leverage discriminative fine-tuning to improve their
text-image alignment. As a bonus of the discriminative adapter, a
self-correction mechanism can leverage discriminative gradients to better align
generated images to text prompts during inference. Comprehensive evaluations
across three benchmark datasets, including both in-distribution and
out-of-distribution scenarios, demonstrate our method's superior generation
performance. Meanwhile, it achieves state-of-the-art discriminative performance
on the two discriminative tasks compared to other generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; project page: https://dpt-t2i.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rule-driven News Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Xu, Tingting Zhang, Hongshuo Tian, An-An Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News captioning task aims to generate sentences by describing named entities
or concrete events for an image with its news article. Existing methods have
achieved remarkable results by relying on the large-scale pre-trained models,
which primarily focus on the correlations between the input news content and
the output predictions. However, the news captioning requires adhering to some
fundamental rules of news reporting, such as accurately describing the
individuals and actions associated with the event. In this paper, we propose
the rule-driven news captioning method, which can generate image descriptions
following designated rule signal. Specifically, we first design the news-aware
semantic rule for the descriptions. This rule incorporates the primary action
depicted in the image (e.g., "performing") and the roles played by named
entities involved in the action (e.g., "Agent" and "Place"). Second, we inject
this semantic rule into the large-scale pre-trained model, BART, with the
prefix-tuning strategy, where multiple encoder layers are embedded with
news-aware semantic rule. Finally, we can effectively guide BART to generate
news sentences that comply with the designated rule. Extensive experiments on
two widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORPO: Monolithic Preference Optimization without Reference Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Hong, Noah Lee, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent preference alignment algorithms for language models have
demonstrated promising results, supervised fine-tuning (SFT) remains imperative
for achieving successful convergence. In this paper, we study the crucial role
of SFT within the context of preference alignment, emphasizing that a minor
penalty for the disfavored generation style is sufficient for
preference-aligned SFT. Building on this foundation, we introduce a
straightforward and innovative reference model-free monolithic odds ratio
preference optimization algorithm, ORPO, eliminating the necessity for an
additional preference alignment phase. We demonstrate, both empirically and
theoretically, that the odds ratio is a sensible choice for contrasting favored
and disfavored styles during SFT across the diverse sizes from 125M to 7B.
Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with
ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art
language models with more than 7B and 13B parameters: achieving up to 12.20% on
$\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level
loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model
checkpoints for Mistral-ORPO-$\alpha$ (7B) and Mistral-ORPO-$\beta$ (7B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XAL: EXplainable Active Learning Makes Classifiers Better Low-resource
  Learners <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning (AL), which aims to construct an effective training set by
iteratively curating the most formative unlabeled data for annotation, has been
widely used in low-resource tasks. Most active learning techniques in
classification rely on the model's uncertainty or disagreement to choose
unlabeled data, suffering from the problem of over-confidence in superficial
patterns and a lack of exploration. Inspired by the cognitive processes in
which humans deduce and predict through causal information, we take an initial
attempt towards integrating rationales into AL and propose a novel Explainable
Active Learning framework (XAL) for low-resource text classification, which
aims to encourage classifiers to justify their inferences and delve into
unlabeled data for which they cannot provide reasonable explanations.
Specifically, besides using a pre-trained bi-directional encoder for
classification, we employ a pre-trained uni-directional decoder to generate and
score the explanation. We further facilitate the alignment of the model with
human reasoning preference through a proposed ranking loss. During the
selection of unlabeled data, the predicted uncertainty of the encoder and the
explanation score of the decoder complement each other as the final metric to
acquire informative data. Extensive experiments on six datasets show that XAL
achieves consistent improvement over 9 strong baselines. Analysis indicates
that the proposed method can generate corresponding explanations for its
predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Large <span class="highlight-title">Language Models</span> Cannot Self-Correct Reasoning Yet <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Huang, <span class="highlight-author">Xinyun Chen</span>, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance even
degrades after self-correction. Drawing from these insights, we offer
suggestions for future research and practical applications in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations
  from Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we emphasize the need
for a systematic characterization of faithfulness-plausibility requirements of
different real-world applications and ensure explanations meet those needs.
While there are several approaches to improving plausibility, improving
faithfulness is an open challenge. We call upon the community to develop novel
methods to enhance the faithfulness of self explanations thereby enabling
transparent deployment of LLMs in diverse high-stakes settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeChain: Towards Modular Code <span class="highlight-title">Generation</span> Through Chain of
  Self-revisions with Representative Sub-modules <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have already become quite proficient at solving
simpler programming tasks like those in HumanEval or MBPP benchmarks. However,
solving more complex and competitive programming tasks is still quite
challenging for these models - possibly due to their tendency to generate
solutions as monolithic code blocks instead of decomposing them into logical
sub-tasks and sub-modules. On the other hand, experienced programmers
instinctively write modularized code with abstraction for solving complex
tasks, often reusing previously developed modules. To address this gap, we
propose CodeChain, a novel framework for inference that elicits modularized
code generation through a chain of self-revisions, each being guided by some
representative sub-modules generated in previous iterations. Concretely,
CodeChain first instructs the LLM to generate modularized codes through
chain-of-thought prompting. Then it applies a chain of self-revisions by
iterating the two steps: 1) extracting and clustering the generated sub-modules
and selecting the cluster representatives as the more generic and re-usable
implementations, and 2) augmenting the original chain-of-thought prompt with
these selected module-implementations and instructing the LLM to re-generate
new modularized solutions. We find that by naturally encouraging the LLM to
reuse the previously developed and verified sub-modules, CodeChain can
significantly boost both modularity as well as correctness of the generated
solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on
CodeContests. It is shown to be effective on both OpenAI LLMs as well as
open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation
studies with different methods of prompting, number of clusters, model sizes,
program qualities, etc., to provide useful insights that underpin CodeChain's
success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, <span class="highlight-author">Graham Neubig</span>, Yonatan Bisk, Hao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Label Dependencies-aware Set Prediction Networks for Multi-label Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Du Xinkai, Han Quanjie, Sun Yalin, Lv Chao, Sun Maosong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label text classification involves extracting all relevant labels from
a sentence. Given the unordered nature of these labels, we propose approaching
the problem as a set prediction task. To address the correlation between
labels, we leverage Graph Convolutional Networks and construct an adjacency
matrix based on the statistical relations between labels. Additionally, we
enhance recall ability by applying the Bhattacharyya distance to the output
distributions of the set prediction networks. We evaluate the effectiveness of
our approach on two multi-label datasets and demonstrate its superiority over
previous baselines through experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truth-Aware Context Selection: Mitigating the Hallucinations of Large
  <span class="highlight-title">Language Models</span> Being Misled by Untruthful Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yu, Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have demonstrated impressive text
generation capabilities, they are easily misled by the untruthful context
provided by users or knowledge augmentation tools, thereby producing
hallucinations. To alleviate the LLMs from being misled by untruthful
information and take advantage of knowledge augmentation, we propose
Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful
context from the inputs. TACS begins by performing truth detection on the input
context, leveraging the parameterized knowledge within the LLM. Subsequently,
it constructs a corresponding attention mask based on the truthfulness of each
position, selecting the truthful context and discarding the untruthful context.
Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate,
to further study the LLMs' ability to accept truthful information and resist
untruthful information. Experimental results show that TACS can effectively
filter information in context and significantly improve the overall quality of
LLMs' responses when presented with misleading information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/ictnlp/TACS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Instruct</span>ion Tuning for Large <span class="highlight-title">Language Models</span>: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2; Last update: March 12, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Zero-Shot Reasoning with Role-Play <span class="highlight-title">Prompt</span>ing <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, Xiaohang Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs) exhibit a remarkable capacity for
role-playing, enabling them to embody not only human characters but also
non-human entities. This versatility allows them to simulate complex human-like
interactions and behaviors within various contexts, as well as to emulate
specific objects or systems. While these capabilities have enhanced user
engagement and introduced novel modes of interaction, the influence of
role-playing on LLMs' reasoning abilities remains underexplored. In this study,
we introduce a strategically designed role-play prompting methodology and
assess its performance under the zero-shot setting across twelve diverse
reasoning benchmarks. Our empirical results illustrate that role-play prompting
consistently surpasses the standard zero-shot approach across most datasets.
Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from
53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison
with the Zero-Shot-CoT technique, which prompts the model to "think step by
step", our study demonstrates that role-play prompting acts as a more effective
trigger for the CoT process. This highlights its potential to augment the
reasoning capabilities of LLMs. We release our code at
https://github.com/NKU-HLT/Role-Play-Prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024, Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Reinforcement Learning from <span class="highlight-title">Human Feedback</span> Using Contrastive
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models
  against Counterfactual Noise <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing retrieval-augmented language models (LMs) assume a naive
dichotomy within a retrieved document set: query-relevance and irrelevance. Our
work investigates a more challenging scenario in which even the "relevant"
documents may contain misleading or incorrect information, causing conflict
among the retrieved documents and thereby negatively influencing model
decisions as noise. We observe that existing LMs are highly brittle to the
presence of conflicting information in both the fine-tuning and in-context
few-shot learning scenarios. We propose approaches for handling knowledge
conflicts among retrieved documents by explicitly fine-tuning a discriminator
or prompting GPT-3.5 to elicit its discriminative capability. Our empirical
results on open-domain QA show that these approaches significantly enhance
model robustness. We also provide our findings on incorporating the fine-tuned
discriminator's decision into the in-context learning process, proposing a way
to exploit the benefits of two disparate learning schemes. Alongside our
findings, we provide MacNoise, a machine-generated, conflict-induced dataset to
further encourage research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 (Findings; Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Science Checker Reloaded: A Bidirectional Paradigm for Transparency and
  Logical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loïc Rakotoson, Sylvain Massip, Fréjus A. A. Laleye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval is a rapidly evolving field. However it still faces
significant limitations in the scientific and industrial vast amounts of
information, such as semantic divergence and vocabulary gaps in sparse
retrieval, low precision and lack of interpretability in semantic search, or
hallucination and outdated information in generative models. In this paper, we
introduce a two-block approach to tackle these hurdles for long documents. The
first block enhances language understanding in sparse retrieval by query
expansion to retrieve relevant documents. The second block deepens the result
by providing comprehensive and informative answers to the complex question
using only the information spread in the long document, enabling bidirectional
engagement. At various stages of the pipeline, intermediate results are
presented to users to facilitate understanding of the system's reasoning. We
believe this bidirectional approach brings significant advancements in terms of
transparency, logical thinking, and comprehensive understanding in the field of
scientific information retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  <span class="highlight-title">Language Models</span> from the Perspective of Position Bias <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XATU: A Fine-grained <span class="highlight-title">Instruct</span>ion-based Benchmark for Explainable Text
  Updates <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Zhang, Hayate Iso, Sairam Gurajada, Nikita Bhutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text editing is a crucial task of modifying text to better align with user
intents. However, existing text editing benchmark datasets contain only
coarse-grained instructions and lack explainability, thus resulting in outputs
that deviate from the intended changes outlined in the gold reference. To
comprehensively investigate the text editing capabilities of large language
models (LLMs), this paper introduces XATU, the first benchmark specifically
designed for fine-grained instruction-based explainable text editing. XATU
considers finer-grained text editing tasks of varying difficulty
(simplification, grammar check, fact-check, etc.), incorporating lexical,
syntactic, semantic, and knowledge-intensive edit aspects. To enhance
interpretability, we combine LLM-based annotation and human annotation,
resulting in a benchmark that includes fine-grained instructions and
gold-standard edit explanations. By evaluating existing LLMs against our
benchmark, we demonstrate the effectiveness of instruction tuning and the
impact of underlying architecture across various editing tasks. Furthermore,
extensive experimentation reveals the significant role of explanations in
fine-tuning language models for text editing tasks. The benchmark will be
open-sourced to support reproduction and facilitate future research
at~\url{https://github.com/megagonlabs/xatu}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect
  Representations <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debaditya Shome, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EmoDistill, a novel speech emotion recognition (SER) framework
that leverages cross-modal knowledge distillation during training to learn
strong linguistic and prosodic representations of emotion from speech. During
inference, our method only uses a stream of speech signals to perform unimodal
SER thus reducing computation overhead and avoiding run-time transcription and
prosodic feature extraction errors. During training, our method distills
information at both embedding and logit levels from a pair of pre-trained
Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on
the IEMOCAP benchmark demonstrate that our method outperforms other unimodal
and multimodal techniques by a considerable margin, and achieves
state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted
accuracy. Detailed ablation studies demonstrate the impact of each component of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Large <span class="highlight-title">Language Models</span> with Divide-and-Conquer Program for
  Discerning Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
Existing works show that appropriate prompt design, such as Chain-of-Thoughts,
can unlock LLM's powerful capacity in diverse areas. However, when handling
tasks involving repetitive sub-tasks and/or deceptive contents, such as
arithmetic calculation and article-level fake news detection, existing
prompting strategies either suffers from insufficient expressive power or
intermediate errors triggered by hallucination. To make LLM more discerning to
such intermediate errors, we propose to guide LLM with a Divide-and-Conquer
program that simultaneously ensures superior expressive power and disentangles
task decomposition, sub-task resolution, and resolution assembly process.
Theoretic analysis reveals that our strategy can guide LLM to extend the
expressive power of fixed-depth Transformer. Experiments indicate that our
proposed method can achieve better performance than typical prompting
strategies in tasks bothered by intermediate errors and deceptive contents,
such as large integer multiplication, hallucination detection and
misinformation detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Platypus: Quick, Cheap, and Powerful Refinement of LLMs <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present $\textbf{Platypus}$, a family of fine-tuned and merged Large
Language Models (LLMs) that achieves the strongest performance and currently
stands at first place in HuggingFace's Open LLM Leaderboard as of the release
date of this work. In this work we describe (1) our curated dataset
$\textbf{Open-Platypus}$, that is a subset of other open datasets and which
$\textit{we release to the public}$ (2) our process of fine-tuning and merging
LoRA modules in order to conserve the strong prior of pretrained LLMs, while
bringing specific domain knowledge to the surface (3) our efforts in checking
for test data leaks and contamination in the training data, which can inform
future research. Specifically, the Platypus family achieves strong performance
in quantitative LLM metrics across model sizes, topping the global Open LLM
leaderboard while using just a fraction of the fine-tuning data and overall
compute that are required for other state-of-the-art fine-tuned LLMs. In
particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU
using 25k questions in 5 hours. This is a testament of the quality of our
Open-Platypus dataset, and opens opportunities for more improvements in the
field. Project page: https://platypus-llm.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Instruction Tuning and Instruction Following at NeurIPS
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Musketeer: Joint Training for Multi-task Vision Language Model with Task
  Explanation <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a vision-language model whose parameters are jointly trained on
all tasks and fully shared among multiple heterogeneous tasks which may
interfere with each other, resulting in a single model which we named
Musketeer. The integration of knowledge across heterogeneous tasks is enabled
by a novel feature called Task Explanation Prompt (TEP). With rich and
structured information such as task input/output format, TEP reduces
interference among tasks, allowing the model to focus on their shared
structure. With a single model, Musketeer achieves results comparable to or
better than strong baselines trained on single tasks, almost uniformly across
multiple tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling <span class="highlight-title">Language Models</span> to Implicitly Learn Self-Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
open-ended text generation tasks. However, the inherent open-ended nature of
these tasks implies that there is always room for improvement in the quality of
model responses. To address this challenge, various approaches have been
proposed to enhance the performance of LLMs. There has been a growing focus on
enabling LLMs to self-improve their response quality, thereby reducing the
reliance on extensive human annotation efforts for collecting diverse and
high-quality training data. Recently, prompting-based methods have been widely
explored among self-improvement methods owing to their effectiveness,
efficiency, and convenience. However, those methods usually require explicitly
and thoroughly written rubrics as inputs to LLMs. It is expensive and
challenging to manually derive and provide all necessary rubrics with a
real-world complex goal for improvement (e.g., being more helpful and less
harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework
that implicitly learns the improvement goal from human preference data. PIT
only requires preference data that are used to train reward models without
extra human efforts. Specifically, we reformulate the training objective of
reinforcement learning from human feedback (RLHF) -- instead of maximizing
response quality for a given input, we maximize the quality gap of the response
conditioned on a reference response. In this way, PIT is implicitly trained
with the improvement goal of better aligning with human preferences.
Experiments on two real-world datasets and one synthetic dataset show that our
method significantly outperforms prompting-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIXEN: Visual Text Comparison Network for Image Difference Captioning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Black, Jing Shi, Yifei Fan, Tu Bui, John Collomosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VIXEN - a technique that succinctly summarizes in text the visual
differences between a pair of images in order to highlight any content
manipulation present. Our proposed network linearly maps image features in a
pairwise manner, constructing a soft prompt for a pretrained large language
model. We address the challenge of low volume of training data and lack of
manipulation variety in existing image difference captioning (IDC) datasets by
training on synthetically manipulated images from the recent InstructPix2Pix
dataset generated via prompt-to-prompt editing framework. We augment this
dataset with change summaries produced via GPT-3. We show that VIXEN produces
state-of-the-art, comprehensible difference captions for diverse image contents
and edit types, offering a potential mitigation against misinformation
disseminated via manipulated image content. Code and data are available at
http://github.com/alexblck/vixen
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-linguistically Consistent Semantic and Syntactic Annotation of
  Child-directed Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.10952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.10952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ida Szubert, Omri Abend, Nathan Schneider, Samuel Gibbon, Louis Mahon, Sharon Goldwater, Mark Steedman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a methodology for constructing such corpora of child
directed speech (CDS) paired with sentential logical forms, and uses this
method to create two such corpora, in English and Hebrew. The approach enforces
a cross-linguistically consistent representation, building on recent advances
in dependency representation and semantic parsing. Specifically, the approach
involves two steps. First, we annotate the corpora using the Universal
Dependencies (UD) scheme for syntactic annotation, which has been developed to
apply consistently to a wide variety of domains and typologically diverse
languages. Next, we further annotate these data by applying an automatic method
for transducing sentential logical forms (LFs) from UD structures. The UD and
LF representations have complementary strengths: UD structures are
language-neutral and support consistent and reliable annotation by multiple
annotators, whereas LFs are neutral as to their syntactic derivation and
transparently encode semantic relations.
  Using this approach, we provide syntactic and semantic annotation for two
corpora from CHILDES: Brown's Adam corpus (English; we annotate ~80% of its
child-directed utterances), all child-directed utterances from Berman's Hagar
corpus (Hebrew). We verify the quality of the UD annotation using an
inter-annotator agreement study, and manually evaluate the transduced meaning
representations. We then demonstrate the utility of the compiled corpora
through (1) a longitudinal corpus study of the prevalence of different
syntactic and semantic phenomena in the CDS, and (2) applying an existing
computational model of language acquisition to the two corpora and briefly
comparing the results across languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional preference models for aligning LMs <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. Through these simple steps, CPMs allow to control which properties
of the preference data are used to train the preference model and to build it
based on features that are believed to underlie the human preference judgment.
Our experiments show that CPMs not only improve generalization and are more
robust to overoptimization than standard PMs, but also that best-of-n samples
obtained using CPMs tend to be preferred over samples obtained using
conventional PMs. Overall, our approach demonstrates the benefits of endowing
PMs with priors about which features determine human preferences while relying
on LM capabilities to extract those features in a scalable and robust way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAM: Dynamic Adapter Merging for Continual Video QA Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a parameter-efficient method for continual video
question-answering (VidQA) learning. Our method, named DAM, uses the proposed
Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable
efficient adaptation to continually arriving datasets, (iii) handle inputs from
unknown datasets during inference, and (iv) enable knowledge sharing across
similar dataset domains. Given a set of continually streaming VidQA datasets,
we sequentially train dataset-specific adapters for each dataset while freezing
the parameters of a large pretrained video-language backbone. During inference,
given a video-question sample from an unknown domain, our method first uses the
proposed non-parametric router function to compute a probability for each
adapter, reflecting how relevant that adapter is to the current video-question
input instance. Subsequently, the proposed dynamic adapter merging scheme
aggregates all the adapter weights into a new adapter instance tailored for
that particular test sample to compute the final VidQA prediction, mitigating
the impact of inaccurate router predictions and facilitating knowledge sharing
across domains. Our DAM model outperforms prior state-of-the-art continual
learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA
datasets spanning various domains. We further extend DAM to continual image
classification and image QA and outperform prior methods by a large margin. The
code is publicly available at: https://github.com/klauscc/DAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can easily generate biased and discriminative
responses. As LLMs tap into consequential decision-making (e.g., hiring and
healthcare), it is of crucial importance to develop strategies to mitigate
these biases. This paper focuses on social bias, tackling the association
between demographic information and LLM outputs. We propose a causality-guided
debiasing framework that utilizes causal understandings of (1) the
data-generating process of the training corpus fed to LLMs, and (2) the
internal reasoning process of LLM inference, to guide the design of prompts for
debiasing LLM outputs through selection mechanisms. Our framework unifies
existing de-biasing prompting approaches such as inhibitive instructions and
in-context contrastive examples, and sheds light on new ways of debiasing by
encouraging bias-free reasoning. Our strong empirical performance on real-world
datasets demonstrates that our framework provides principled guidelines on
debiasing LLM outputs even with only the black-box access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Garden of Forking Paths: Observing Dynamic Parameters Distribution
  in Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Nicolini, Jacopo Staiano, Bruno Lepri, Raffaele Marino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A substantial gap persists in understanding the reasons behind the
exceptional performance of the Transformer architecture in NLP. A particularly
unexplored area involves the mechanistic description of how the distribution of
parameters evolves over time during training. In this work we suggest that
looking at the time evolution of the statistic distribution of model
parameters, and specifically at bifurcation effects, can help understanding the
model quality, potentially reducing training costs and evaluation efforts and
empirically showing the reasons behind the effectiveness of weights
sparsification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Acoustic Word Embeddings through Correspondence Training of
  <span class="highlight-title">Self-supervised</span> Speech Representations <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Meghanani, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic word embeddings (AWEs) are vector representations of spoken words.
An effective method for obtaining AWEs is the Correspondence Auto-Encoder
(CAE). In the past, the CAE method has been associated with traditional MFCC
features. Representations obtained from self-supervised learning (SSL)-based
speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many
downstream tasks. However, they have not been well studied in the context of
learning AWEs. This work explores the effectiveness of CAE with SSL-based
speech representations to obtain improved AWEs. Additionally, the capabilities
of SSL-based speech models are explored in cross-lingual scenarios for
obtaining AWEs. Experiments are conducted on five languages: Polish,
Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the
best results for word discrimination in all languages, despite Hu-BERT being
pre-trained on English only. Also, the HuBERT-based CAE model works well in
cross-lingual settings. It outperforms MFCC-based CAE models trained on the
target languages when trained on one source language and tested on target
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024 Main Conference, Long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayar Ghosh Roy, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Machine Learning approaches for local citation recommendation
directly map or translate a query, which is typically a claim or an entity
mention, to citation-worthy research papers. Within such a formulation, it is
challenging to pinpoint why one should cite a specific research paper for a
particular query, leading to limited recommendation interpretability. To
alleviate this, we introduce the evidence-grounded local citation
recommendation task, where the target latent space comprises evidence spans for
recommending specific papers. Using a distantly-supervised evidence retrieval
and multi-step re-ranking framework, our proposed system, ILCiteR, recommends
papers to cite for a query grounded on similar evidence spans extracted from
the existing research literature. Unlike past formulations that simply output
recommendations, ILCiteR retrieves ranked lists of evidence span and
recommended paper pairs. Secondly, previously proposed neural models for
citation recommendation require expensive training on massive labeled data,
ideally after every significant update to the pool of candidate papers. In
contrast, ILCiteR relies solely on distant supervision from a dynamic evidence
database and pre-trained Transformer-based Language Models without any model
training. We contribute a novel dataset for the evidence-grounded local
citation recommendation task and demonstrate the efficacy of our proposed
conditional neural rank-ensembling approach for re-ranking evidence spans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strengthening Multimodal Large Language Model with Bootstrapped
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) excel in generating responses based
on visual inputs. However, they often suffer from a bias towards generating
responses similar to their pretraining corpus, overshadowing the importance of
visual information. We treat this bias as a "preference" for pretraining
statistics, which hinders the model's grounding in visual input. To mitigate
this issue, we propose Bootstrapped Preference Optimization (BPO), which
conducts preference learning with datasets containing negative responses
bootstrapped from the model itself. Specifically, we propose the following two
strategies: 1) using distorted image inputs to the MLLM for eliciting responses
that contain signified pretraining bias; 2) leveraging text-based LLM to
explicitly inject erroneous but common elements into the original response.
Those undesirable responses are paired with original annotated responses from
the datasets to construct the preference dataset, which is subsequently
utilized to perform preference learning. Our approach effectively suppresses
pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive
experimentation demonstrates significant performance improvements across
multiple benchmarks, advancing the state-of-the-art in multimodal
conversational systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, <span class="highlight-author">Graham Neubig</span>, Yonatan Bisk, Hao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeaMs-RL: Teaching LLMs to Teach Themselves Better <span class="highlight-title">Instruct</span>ions via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangding Gu, Alois Knoll, Ming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
$5.73\%$ of WizardLM's total), along with enhanced capabilities of LLMs in
crafting and comprehending complex instructions compared to strong baselines,
and substantially improved model privacy protection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">Language Models</span> Care About Text Quality? Evaluating Web-Crawled
  Corpora Across 11 Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rik van Noord, Taja Kuzman, Peter Rupnik, Nikola Ljubešić, Miquel Esplà-Gomis, Gema Ramírez-Sánchez, Antonio Toral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, curated, web-crawled corpora play a vital role in training language
models (LMs). They form the lion's share of the training data in virtually all
recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,
despite this importance, relatively little attention has been given to the
quality of these corpora. In this paper, we compare four of the currently most
relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across
eleven lower-resourced European languages. Our approach is two-fold: first, we
perform an intrinsic evaluation by performing a human evaluation of the quality
of samples taken from different corpora; then, we assess the practical impact
of the qualitative differences by training specific LMs on each of the corpora
and evaluating their performance on downstream tasks. We find that there are
clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining
the best results. However, during the extrinsic evaluation, we actually find
that the CC100 corpus achieves the highest scores. We conclude that, in our
experiments, the quality of the web-crawled corpora does not seem to play a
significant role when training LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Alignment via Character Matching for Subword Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Athiwaratkun, Shiqi Wang, Mingyue Shang, Yuchen Tian, Zijian Wang, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Rob Kwiatowski, Ramesh Nallapati, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, widely utilized in various applications, can often
struggle with prompts corresponding to partial tokens. This struggle stems from
tokenization, where partial tokens fall out of distribution during inference,
leading to incorrect or nonsensical outputs. This paper examines a technique to
alleviate the tokenization artifact on text completion in generative models,
maintaining performance even in regular non-subword cases. The method, termed
token alignment, involves backtracking to the last complete tokens and ensuring
the model's generation aligns with the prompt. This approach showcases marked
improvement across many partial token scenarios, including nuanced cases like
space-prefix and partial indentation, with only a minor time increase. The
technique and analysis detailed in this paper contribute to the continuous
advancement of generative models in handling partial inputs, bearing relevance
for applications like code completion and text autocompletion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot and Few-shot <span class="highlight-title">Generation</span> Strategies for Artificial Clinical
  Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlend Frayling, Jake Lever, Graham McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedInsight: A Multi-Source Context Augmentation Framework for Generating
  Patient-Centric Medical Responses using Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subash Neupane, Shaswata Mitra, Sudip Mittal, Noorbakhsh Amiri Golilarz, Shahram Rahimi, Amin Amirlatifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive capabilities in generating
human-like responses. However, their lack of domain-specific knowledge limits
their applicability in healthcare settings, where contextual and comprehensive
responses are vital. To address this challenge and enable the generation of
patient-centric responses that are contextually relevant and comprehensive, we
propose MedInsight:a novel retrieval augmented framework that augments LLM
inputs (prompts) with relevant background information from multiple sources.
MedInsight extracts pertinent details from the patient's medical record or
consultation transcript. It then integrates information from authoritative
medical textbooks and curated web resources based on the patient's health
history and condition. By constructing an augmented context combining the
patient's record with relevant medical knowledge, MedInsight generates
enriched, patient-specific responses tailored for healthcare applications such
as diagnosis, treatment recommendations, or patient education. Experiments on
the MTSamples dataset validate MedInsight's effectiveness in generating
contextually appropriate medical responses. Quantitative evaluation using the
Ragas metric and TruLens for answer similarity and answer correctness
demonstrates the model's efficacy. Furthermore, human evaluation studies
involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with
moderate inter-rater agreement on the relevance and correctness of the
generated responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DevBench: A Comprehensive Benchmark for Software Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, <span class="highlight-author">Binyuan Hui</span>, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their coding capabilities. However, existing benchmarks predominantly
focused on simplified or isolated aspects of programming, such as single-file
code generation or repository issue debugging, falling short of measuring the
full spectrum of challenges raised by real-world programming activities. To
this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs
across various stages of the software development lifecycle, including software
design, environment setup, implementation, acceptance testing, and unit
testing. DevBench features a wide range of programming languages and domains,
high-quality data collection, and carefully designed and verified metrics for
each task. Empirical studies show that current LLMs, including GPT-4-Turbo,
fail to solve the challenges presented within DevBench. Analyses reveal that
models struggle with understanding the complex structures in the repository,
managing the compilation process, and grasping advanced programming concepts.
Our findings offer actionable insights for the future development of LLMs
toward real-world programming applications. Our benchmark is available at
https://github.com/open-compass/DevBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our data and code are available at
  https://github.com/open-compass/DevBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over
  Structured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown potential in reasoning over
structured environments, e.g., knowledge graph and table. Such tasks typically
require multi-hop reasoning, i.e., match natural language utterance with
instances in the environment. Previous methods leverage LLMs to incrementally
build a reasoning path, where the LLMs either invoke tools or pick up schemas
by step-by-step interacting with the environment. We propose
Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently
and faithfully reason over structured environments. In Readi, LLMs initially
generate a reasoning path given a query, and edit the path only when necessary.
We instantiate the path on structured environments and provide feedback to edit
the path if anything goes wrong. Experimental results on three KGQA datasets
and two TableQA datasets show the effectiveness of Readi, significantly
surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%
on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and
74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).
Our code will be available upon publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-discrimination Criteria for Generative <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sterlie, Nina Weng, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within recent years, generative AI, such as large language models, has
undergone rapid development. As these models become increasingly available to
the public, concerns arise about perpetuating and amplifying harmful biases in
applications. Gender stereotypes can be harmful and limiting for the
individuals they target, whether they consist of misrepresentation or
discrimination. Recognizing gender bias as a pervasive societal construct, this
paper studies how to uncover and quantify the presence of gender biases in
generative language models. In particular, we derive generative AI analogues of
three well-known non-discrimination criteria from classification, namely
independence, separation and sufficiency. To demonstrate these criteria in
action, we design prompts for each of the criteria with a focus on occupational
gender stereotype, specifically utilizing the medical test to introduce the
ground truth in the generative AI context. Our results address the presence of
occupational gender bias within such conversational language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures. Submitted to ACM Conference on Fairness,
  Accountability, and Transparency (ACM FAccT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Language models</span> scale reliably with over-training and on downstream
  tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, Ludwig Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws are useful guides for developing language models, but there are
still gaps between current scaling studies and how language models are
ultimately trained and evaluated. For instance, scaling is usually studied in
the compute-optimal training regime (i.e., "Chinchilla optimal" regime);
however, in practice, models are often over-trained to reduce inference costs.
Moreover, scaling laws mostly predict loss on next-token prediction, but
ultimately models are compared based on downstream task performance. In this
paper, we address both shortcomings. To do so, we create a testbed of 104
models with 0.011B to 6.9B parameters trained with various numbers of tokens on
three data distributions. First, we investigate scaling in the over-trained
regime. We fit scaling laws that extrapolate in both the number of model
parameters and the ratio of training tokens to parameters. This enables us to
predict the validation loss of a 1.4B parameter, 900B token run (i.e.,
32$\times$ over-trained) and a 6.9B parameter, 138B token
run$\unicode{x2014}$each from experiments that take 300$\times$ less compute.
Second, we relate the perplexity of a language model to its downstream task
performance via a power law. We use this law to predict top-1 error averaged
over downstream tasks for the two aforementioned models using experiments that
take 20$\times$ less compute. Our experiments are available at
https://github.com/mlfoundations/scaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Interactive Evaluation for Large <span class="highlight-title">Language Models</span> with State
  Aware Patient Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable proficiency in
human interactions, yet their application within the medical field remains
insufficiently explored. Previous works mainly focus on the performance of
medical knowledge with examinations, which is far from the realistic scenarios,
falling short in assessing the abilities of LLMs on clinical tasks. In the
quest to enhance the application of Large Language Models (LLMs) in healthcare,
this paper introduces the Automated Interactive Evaluation (AIE) framework and
the State-Aware Patient Simulator (SAPS), targeting the gap between traditional
LLM evaluations and the nuanced demands of clinical practice. Unlike prior
methods that rely on static medical knowledge assessments, AIE and SAPS provide
a dynamic, realistic platform for assessing LLMs through multi-turn
doctor-patient simulations. This approach offers a closer approximation to real
clinical scenarios and allows for a detailed analysis of LLM behaviors in
response to complex patient interactions. Our extensive experimental validation
demonstrates the effectiveness of the AIE framework, with outcomes that align
well with human evaluations, underscoring its potential to revolutionize
medical LLM testing for improved healthcare delivery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rich Semantic Knowledge Enhanced Large <span class="highlight-title">Language Models</span> for Few-shot
  Chinese Spell Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Dong, Yujing Chen, Miao Zhang, Hao Sun, Tingting He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Spell Checking (CSC) is a widely used technology, which plays a vital
role in speech to text (STT) and optical character recognition (OCR). Most of
the existing CSC approaches relying on BERT architecture achieve excellent
performance. However, limited by the scale of the foundation model, BERT-based
method does not work well in few-shot scenarios, showing certain limitations in
practical applications. In this paper, we explore using an in-context learning
method named RS-LLM (Rich Semantic based LLMs) to introduce large language
models (LLMs) as the foundation model. Besides, we study the impact of
introducing various Chinese rich semantic information in our framework. We
found that by introducing a small number of specific Chinese rich semantic
structures, LLMs achieve better performance than the BERT-based model on
few-shot CSC task. Furthermore, we conduct experiments on multiple datasets,
and the experimental results verified the superiority of our proposed
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH
  Mask based Efficient Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Dong, Kang Xue, Bolong Zheng, Tingting He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In view of the huge number of parameters of Large language models (LLMs) ,
tuning all parameters is very costly, and accordingly fine-tuning specific
parameters is more sensible. Most of parameter efficient fine-tuning (PEFT)
concentrate on parameter selection strategies, such as additive method,
selective method and reparametrization-based method. However, there are few
methods that consider the impact of data samples on parameter selecting, such
as Fish Mask based method. Fish Mask randomly choose a part of data samples and
treat them equally during parameter selection, which is unable to dynamically
select optimal parameters for inconstant data distributions. In this work, we
adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline
I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline
D}$ecreasing) algorithm to search the best setting of sample-parameter pair for
FISH Mask. In each iteration, by searching the set of samples and parameters
with larger Fish information, IRD can find better sample-parameter pair in most
scale. We demonstrate the effectiveness and rationality of proposed strategy by
conducting experiments on GLUE benchmark. Experimental results show our
strategy optimizes the parameter selection and achieves preferable performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Authorship Verification based on the Likelihood Ratio of Grammar Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Nini, Oren Halvani, Lukas Graner, Valerio Gherardi, Shunichi Ishihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authorship Verification (AV) is the process of analyzing a set of documents
to determine whether they were written by a specific author. This problem often
arises in forensic scenarios, e.g., in cases where the documents in question
constitute evidence for a crime. Existing state-of-the-art AV methods use
computational solutions that are not supported by a plausible scientific
explanation for their functioning and that are often difficult for analysts to
interpret. To address this, we propose a method relying on calculating a
quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a
document given a model of the Grammar for the candidate author and the
likelihood of the same document given a model of the Grammar for a reference
population. These Grammar Models are estimated using $n$-gram language models
that are trained solely on grammatical features. Despite not needing large
amounts of data for training, LambdaG still outperforms other established AV
methods with higher computational complexity, including a fine-tuned Siamese
Transformer network. Our empirical evaluation based on four baseline methods
applied to twelve datasets shows that LambdaG leads to better results in terms
of both accuracy and AUC in eleven cases and in all twelve cases if considering
only topic-agnostic methods. The algorithm is also highly robust to important
variations in the genre of the reference population in many cross-genre
comparisons. In addition to these properties, we demonstrate how LambdaG is
easier to interpret than the current state-of-the-art. We argue that the
advantage of LambdaG over other methods is due to fact that it is compatible
with Cognitive Linguistic theories of language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tastle: Distract Large <span class="highlight-title">Language Models</span> for Automatic Jailbreak Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant advances in recent
days. Extensive efforts have been made before the public release of LLMs to
align their behaviors with human values. The primary goal of alignment is to
ensure their helpfulness, honesty and harmlessness. However, even meticulously
aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,
leading to unintended behaviors. The jailbreak is to intentionally develop a
malicious prompt that escapes from the LLM security restrictions to produce
uncensored detrimental contents. Previous works explore different jailbreak
methods for red teaming LLMs, yet they encounter challenges regarding to
effectiveness and scalability. In this work, we propose Tastle, a novel
black-box jailbreak framework for automated red teaming of LLMs. We designed
malicious content concealing and memory reframing with an iterative
optimization algorithm to jailbreak LLMs, motivated by the research about the
distractibility and over-confidence phenomenon of LLMs. Extensive experiments
of jailbreaking both open-source and proprietary LLMs demonstrate the
superiority of our framework in terms of effectiveness, scalability and
transferability. We also evaluate the effectiveness of existing jailbreak
defense methods against our attack and highlight the crucial need to develop
more effective and practical defense strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Misinformation is not about Bad Facts: An Analysis of the Production and
  Consumption of Fringe Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JooYoung Lee, Emily Booth, Hany Farid, Marian-Andrei Rizoiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What if misinformation is not an information problem at all? Our findings
suggest that online fringe ideologies spread through the use of content that is
consensus-based and "factually correct". We found that Australian news
publishers with both moderate and far-right political leanings contain
comparable levels of information completeness and quality; and furthermore,
that far-right Twitter users often share from moderate sources. However, a
stark difference emerges when we consider two additional factors: 1) the narrow
topic selection of articles by far-right users, suggesting that they cherrypick
only news articles that engage with specific topics of their concern, and 2)
the difference between moderate and far-right publishers when we examine the
writing style of their articles. Furthermore, we can even identify users prone
to sharing misinformation based on their communication style. These findings
have important implications for countering online misinformation, as they
highlight the powerful role that users' personal bias towards specific topics,
and publishers' writing styles, have in amplifying fringe ideologies online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Describe for Predicting Zero-shot Drug-Drug Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangqi Zhu, Yongqi Zhang, Lei Chen, Bing Qin, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of
concurrent drug administration, posing a significant challenge in healthcare.
As the development of new drugs continues, the potential for unknown adverse
effects resulting from DDIs becomes a growing concern. Traditional
computational methods for DDI prediction may fail to capture interactions for
new drugs due to the lack of knowledge. In this paper, we introduce a new
problem setup as zero-shot DDI prediction that deals with the case of new
drugs. Leveraging textual information from online databases like DrugBank and
PubChem, we propose an innovative approach TextDDI with a language model-based
DDI predictor and a reinforcement learning~(RL)-based information selector,
enabling the selection of concise and pertinent text for accurate DDI
prediction on new drugs. Empirical results show the benefits of the proposed
approach on several settings including zero-shot and few-shot DDI prediction,
and the selected texts are semantically relevant. Our code and data are
available at \url{https://github.com/zhufq00/DDIs-Prediction}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Translating between SQL Dialects for Cloud Migration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zmigrod, Salwa Alamir, Xiaomo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Migrations of systems from on-site premises to the cloud has been a
fundamental endeavor by many industrial institutions. A crucial component of
such cloud migrations is the transition of databases to be hosted online. In
this work, we consider the difficulties of this migration for SQL databases.
While SQL is one of the prominent methods for storing database procedures,
there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)
which can complicate migrations when the on-premise SQL dialect differs to the
dialect hosted on the cloud. Tools exist by common cloud provides such as AWS
and Azure to aid in translating between dialects in order to mitigate the
majority of the difficulties. However, these tools do not successfully
translate $100\%$ of the code. Consequently, software engineers must manually
convert the remainder of the untranslated database. For large organizations,
this task quickly becomes intractable and so more innovative solutions are
required. We consider this challenge a novel yet vital industrial research
problem for any large corporation that is considering cloud migrations.
Furthermore, we introduce potential avenues of research to tackle this
challenge that have yielded promising preliminary results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMART: Submodular Data Mixture Strategy for <span class="highlight-title">Instruct</span>ion Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H S V N S Kowndinya Renduchintala, Sumit Bhatia, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Tuning involves finetuning a language model on a collection of
instruction-formatted datasets in order to enhance the generalizability of the
model to unseen tasks. Studies have shown the importance of balancing different
task proportions during finetuning, but finding the right balance remains
challenging. Unfortunately, there's currently no systematic method beyond
manual tuning or relying on practitioners' intuition. In this paper, we
introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a
novel data mixture strategy which makes use of a submodular function to assign
importance scores to tasks which are then used to determine the mixture
weights. Given a fine-tuning budget, SMART redistributes the budget among tasks
and selects non-redundant samples from each task. Experimental results
demonstrate that SMART significantly outperforms traditional methods such as
examples proportional mixing and equal mixing. Furthermore, SMART facilitates
the creation of data mixtures based on a few representative subsets of tasks
alone and through task pruning analysis, we reveal that in a limited budget
setting, allocating budget among a subset of representative tasks yields
superior performance compared to distributing the budget among all tasks. The
code for reproducing our results is open-sourced at
https://github.com/kowndinya-renduchintala/SMART.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Log Summarisation for Defect Evolution Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Dolga, Ran Zmigrod, Rui Silva, Salwa Alamir, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Log analysis and monitoring are essential aspects in software maintenance and
identifying defects. In particular, the temporal nature and vast size of log
data leads to an interesting and important research question: How can logs be
summarised and monitored over time? While this has been a fundamental topic of
research in the software engineering community, work has typically focused on
heuristic-, syntax-, or static-based methods. In this work, we suggest an
online semantic-based clustering approach to error logs that dynamically
updates the log clusters to enable monitoring code error life-cycles. We also
introduce a novel metric to evaluate the performance of temporal log clusters.
We test our system and evaluation metric with an industrial dataset and find
that our solution outperforms similar systems. We hope that our work encourages
further temporal exploration in defect datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vamsi Krishna Kommineni, Birgitta König-Ries, Sheeba Samuel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autoregressive Score <span class="highlight-title">Generation</span> for Multi-trait Essay Scoring <span class="chip">EACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejin Do, Yunsu Kim, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, encoder-only pre-trained models such as BERT have been successfully
applied in automated essay scoring (AES) to predict a single overall score.
However, studies have yet to explore these models in multi-trait AES, possibly
due to the inefficiency of replicating BERT-based models for each trait.
Breaking away from the existing sole use of encoder, we propose an
autoregressive prediction of multi-trait scores (ArTS), incorporating a
decoding process by leveraging the pre-trained T5. Unlike prior regression or
classification methods, we redefine AES as a score-generation task, allowing a
single model to predict multiple scores. During decoding, the subsequent trait
prediction can benefit by conditioning on the preceding trait scores.
Experimental results proved the efficacy of ArTS, showing over 5% average
improvements in both prompts and traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EACL2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Conflicts for LLMs: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey provides an in-depth analysis of knowledge conflicts for large
language models (LLMs), highlighting the complex challenges they encounter when
blending contextual and parametric knowledge. Our focus is on three categories
of knowledge conflicts: context-memory, inter-context, and intra-memory
conflict. These conflicts can significantly impact the trustworthiness and
performance of LLMs, especially in real-world applications where noise and
misinformation are common. By categorizing these conflicts, exploring the
causes, examining the behaviors of LLMs under such conflicts, and reviewing
available solutions, this survey aims to shed light on strategies for improving
the robustness of LLMs, thereby serving as a valuable resource for advancing
research in this evolving area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Context Helpful for Chat Translation Evaluation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sweta Agrawal, Amin Farajian, Patrick Fernandes, Ricardo Rei, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent success of automatic metrics for assessing translation
quality, their application in evaluating the quality of machine-translated
chats has been limited. Unlike more structured texts like news, chat
conversations are often unstructured, short, and heavily reliant on contextual
information. This poses questions about the reliability of existing
sentence-level metrics in this domain as well as the role of context in
assessing the translation quality. Motivated by this, we conduct a
meta-evaluation of existing sentence-level automatic metrics, primarily
designed for structured domains such as news, to assess the quality of
machine-translated chats. We find that reference-free metrics lag behind
reference-based ones, especially when evaluating translation quality in
out-of-English settings. We then investigate how incorporating conversational
contextual information in these metrics affects their performance. Our findings
show that augmenting neural learned metrics with contextual information helps
improve correlation with human judgments in the reference-free scenario and
when evaluating translations in out-of-English settings. Finally, we propose a
new evaluation metric, Context-MQM, that utilizes bilingual context with a
large language model (LLM) and further validate that adding context helps even
for LLM-based evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> StreamingDialogue: Prolonged Dialogue Learning via Long Context
  Compression with Minimal Losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Nan Li, Quan Tu, Cunli Mao, Zheng<span class="highlight-author">tao Yu</span>, Ji-Rong Wen, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard Large Language Models (LLMs) struggle with handling dialogues with
long contexts due to efficiency and consistency issues. According to our
observation, dialogue contexts are highly structured, and the special token of
\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate
information. We refer to the EoU tokens as ``conversational attention sinks''
(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which
compresses long dialogue history into conv-attn sinks with minimal losses, and
thus reduces computational complexity quadratically with the number of sinks
(i.e., the number of utterances). Current LLMs already demonstrate the ability
to handle long context window, e.g., a window size of 200k or more. To this
end, by compressing utterances into EoUs, our method has the potential to
handle more than 200k of utterances, resulting in a prolonged dialogue
learning. In order to minimize information losses from reconstruction after
compression, we design two learning strategies of short-memory reconstruction
(SMR) and long-memory reactivation (LMR). Our method outperforms strong
baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing
memory usage by 18 $\times$ compared to dense attention recomputation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Personalized Evaluation of Large <span class="highlight-title">Language Models</span> with An
  Anonymous Crowd-Sourcing Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model evaluation plays a pivotal role in the enhancement of
its capacity. Previously, numerous methods for evaluating large language models
have been proposed in this area. Despite their effectiveness, these existing
works mainly focus on assessing objective questions, overlooking the capability
to evaluate subjective questions which is extremely common for large language
models. Additionally, these methods predominantly utilize centralized datasets
for evaluation, with question banks concentrated within the evaluation
platforms themselves. Moreover, the evaluation processes employed by these
platforms often overlook personalized factors, neglecting to consider the
individual characteristics of both the evaluators and the models being
evaluated. To address these limitations, we propose a novel anonymous
crowd-sourcing evaluation platform, BingJian, for large language models that
employs a competitive scoring mechanism where users participate in ranking
models based on their performance. This platform stands out not only for its
support of centralized evaluations to assess the general capabilities of models
but also for offering an open evaluation gateway. Through this gateway, users
have the opportunity to submit their questions, testing the models on a
personalized and potentially broader range of capabilities. Furthermore, our
platform introduces personalized evaluation scenarios, leveraging various forms
of human-computer interaction to assess large language models in a manner that
accounts for individual user preferences and contexts. The demonstration of
BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gemma: Open Models Based on Gemini Research and Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, Kathleen Kenealy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Gemma, a family of lightweight, state-of-the art open
models built from the research and technology used to create Gemini models.
Gemma models demonstrate strong performance across academic benchmarks for
language understanding, reasoning, and safety. We release two sizes of models
(2 billion and 7 billion parameters), and provide both pretrained and
fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out
of 18 text-based tasks, and we present comprehensive evaluations of safety and
responsibility aspects of the models, alongside a detailed description of model
development. We believe the responsible release of LLMs is critical for
improving the safety of frontier models, and for enabling the next wave of LLM
innovations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative <span class="highlight-title">Pretrain</span>ed Structured <span class="highlight-title">Transformer</span>s: Unsupervised Syntactic
  <span class="highlight-title">Language Models</span> at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A syntactic language model (SLM) incrementally generates a sentence with its
syntactic tree in a left-to-right manner. We present Generative Pretrained
Structured Transformers (GPST), an unsupervised SLM at scale capable of being
pre-trained from scratch on raw texts with high parallelism. GPST circumvents
the limitations of previous SLMs such as relying on gold trees and sequential
training. It consists of two components, a usual SLM supervised by a
uni-directional language modeling loss, and an additional composition model,
which induces syntactic parse trees and computes constituent representations,
supervised by a bi-directional language modeling loss. We propose a
representation surrogate to enable joint parallel training of the two models in
a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion
tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable
size in numerous tasks covering both language understanding and language
generation. Meanwhile, GPST also significantly outperforms existing
unsupervised SLMs on left-to-right grammar induction, while holding a
substantial acceleration on training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RECIPE4U: Student-<span class="highlight-title">ChatGPT</span> Interaction <span class="highlight-title">Dataset</span> in EFL Writing Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon Ahn, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of generative AI in education is expanding, yet empirical
analyses of large-scale and real-world interactions between students and AI
systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE
for University), a dataset sourced from a semester-long experiment with 212
college students in English as Foreign Language (EFL) writing courses. During
the study, students engaged in dialogues with ChatGPT to revise their essays.
RECIPE4U includes comprehensive records of these interactions, including
conversation logs, students' intent, students' self-rated satisfaction, and
students' essay edit histories. In particular, we annotate the students'
utterances in RECIPE4U with 13 intention labels based on our coding schemes. We
establish baseline results for two subtasks in task-oriented dialogue systems
within educational contexts: intent detection and satisfaction estimation. As a
foundational step, we explore student-ChatGPT interaction patterns through
RECIPE4U and analyze them by focusing on students' dialogue, essay data
statistics, and students' essay edits. We further illustrate potential
applications of RECIPE4U dataset for enhancing the incorporation of LLMs in
educational frameworks. RECIPE4U is publicly available at
https://zeunie.github.io/RECIPE4U/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2309.13243</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Zhu, Sining Sun, Changhao Shan, Peng Fan, Qing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformer-based attention models have become the de facto backbone model for
Automatic Speech Recognition tasks. A blank symbol is usually introduced to
align the input and output sequences for CTC or RNN-T models. Unfortunately,
the long input length overloads computational budget and memory consumption
quadratically by attention mechanism. In this work, we propose a
"Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze
sequence input length dynamically and inhomogeneously. Skipformer uses an
intermediate CTC output as criteria to split frames into three groups: crucial,
skipping and ignoring. The crucial group feeds into next conformer blocks and
its output joint with skipping group by original temporal order as the final
encoder output. Experiments show that our model reduces the input sequence
length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,
the model can achieve better recognition accuracy and faster inference speed
than recent baseline models. Our code is open-sourced and available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Disfluency Detection with Large Language Model as Disfluency
  Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrong Cheng, Jiayan Guo, Hao Sun, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current disfluency detection methods heavily rely on costly and scarce
human-annotated data. To tackle this issue, some approaches employ heuristic or
statistical features to generate disfluent sentences, partially improving
detection performance. However, these sentences often deviate from real-life
scenarios, constraining overall model enhancement. In this study, we propose a
lightweight data augmentation approach for disfluency detection, utilizing the
superior generative and semantic understanding capabilities of large language
model (LLM) to generate disfluent sentences as augmentation data. We leverage
LLM to generate diverse and more realistic sentences guided by specific
prompts, without the need for fine-tuning the LLM. Subsequently, we apply an
uncertainty-aware data filtering approach to improve the quality of the
generated sentences, utilized in training a small detection model for improved
performance. Experiments using enhanced data yielded state-of-the-art results.
The results showed that using a small amount of LLM-generated enhanced data can
significantly improve performance, thereby further enhancing
cost-effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on the Application of Deep Learning-based <span class="highlight-title">BERT</span> Model in
  Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichao Wu, Zhengyu Jin, Chenxi Shi, Penghao Liang, Tong Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of deep learning techniques, particularly
focusing on BERT models, in sentiment analysis. It begins by introducing the
fundamental concept of sentiment analysis and how deep learning methods are
utilized in this domain. Subsequently, it delves into the architecture and
characteristics of BERT models. Through detailed explanation, it elucidates the
application effects and optimization strategies of BERT models in sentiment
analysis, supported by experimental validation. The experimental findings
indicate that BERT models exhibit robust performance in sentiment analysis
tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes
by summarizing the potential applications of BERT models in sentiment analysis
and suggests directions for future research and practical implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large <span class="highlight-title">Language Models</span> Identify Authorship? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixiang Huang, Canyu Chen, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately identify authorship is crucial for verifying
content authenticity and mitigating misinformation. Large Language Models
(LLMs) have demonstrated exceptional capacity for reasoning and
problem-solving. However, their potential in authorship analysis, encompassing
authorship verification and attribution, remains underexplored. This paper
conducts a comprehensive evaluation of LLMs in these critical tasks.
Traditional studies have depended on hand-crafted stylistic features, whereas
state-of-the-art approaches leverage text embeddings from pre-trained language
models. These methods, which typically require fine-tuning on labeled data,
often suffer from performance degradation in cross-domain applications and
provide limited explainability. This work seeks to address three research
questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification
effectively? (2) Are LLMs capable of accurately attributing authorship among
multiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide
explainability in authorship analysis, particularly through the role of
linguistic features? Moreover, we investigate the integration of explicit
linguistic features to guide LLMs in their reasoning processes. Our extensive
assessment demonstrates LLMs' proficiency in both tasks without the need for
domain-specific fine-tuning, providing insights into their decision-making via
a detailed analysis of linguistic features. This establishes a new benchmark
for future research on LLM-based authorship analysis. The code and data are
available at https://github.com/baixianghuang/authorship-llm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large <span class="highlight-title">Language Models</span> are Contrastive Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting methods play a crucial role in enhancing the capabilities of
pre-trained large language models (LLMs). We explore how contrastive prompting
(CP) significantly improves the ability of large language models to perform
complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by
simply adding "Let's give a correct and a wrong answer." before LLMs provide
answers. Experiments on two large language models show that zero-shot
contrastive prompting improves performance on a range of arithmetic,
commonsense, and symbolic reasoning tasks without any hand-crafted few-shot
examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and
AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method
not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and
commonsense reasoning tasks but also can seamlessly integrate with existing
prompting methods, resulting in improved or comparable results when compared to
state-of-the-art methods. Our code is available at
https://github.com/yao8839836/cp
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Validating and Exploring Large Geographic Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Dunn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the impact of corpus creation decisions on large
multi-lingual geographic web corpora. Beginning with a 427 billion word corpus
derived from the Common Crawl, three methods are used to improve the quality of
sub-corpora representing specific language-country pairs like New Zealand
English: (i) the agreement of independent language identification systems, (ii)
hash-based deduplication, and (iii) location-specific outlier detection. The
impact of each of these steps is then evaluated at the language level and the
country level by using corpus similarity measures to compare each resulting
corpus with baseline data sets. The goal is to understand the impact of
upstream data cleaning decisions on downstream corpora with a specific focus on
under-represented languages and populations. The evaluation shows that the
validity of sub-corpora is improved with each stage of cleaning but that this
improvement is unevenly distributed across languages and populations. This
result shows how standard corpus creation techniques can accidentally exclude
under-represented populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech
  Recognition Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Du, Jinpeng Li, Guoguo Chen, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wake of the surging tide of deep learning over the past decade,
Automatic Speech Recognition (ASR) has garnered substantial attention, leading
to the emergence of numerous publicly accessible ASR systems that are actively
being integrated into our daily lives. Nonetheless, the impartial and
replicable evaluation of these ASR systems encounters challenges due to various
crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a
general-purpose, open-source platform designed for ASR evaluation. With this
platform: (i) We report a comprehensive benchmark, unveiling the current
state-of-the-art panorama for ASR systems, covering both open-source models and
industrial commercial services. (ii) We quantize how distinct nuances in the
scoring pipeline influence the final benchmark outcomes. These include nuances
related to capitalization, punctuation, interjection, contraction, synonym
usage, compound words, etc. These issues have gained prominence in the context
of the transition towards an End-to-End future. (iii) We propose a practical
modification to the conventional Token-Error-Rate (TER) evaluation metric, with
inspirations from Kolmogorov complexity and Normalized Information Distance
(NID). This adaptation, called modified-TER (mTER), achieves proper
normalization and symmetrical treatment of reference and hypothesis. By
leveraging this platform as a large-scale testing ground, this study
demonstrates the robustness and backward compatibility of mTER when compared to
TER. The SpeechColab Leaderboard is accessible at
https://github.com/SpeechColab/Leaderboard
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoleculeQA: A <span class="highlight-title">Dataset</span> to Evaluate Factual Accuracy in Molecular
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are playing an increasingly significant role in
molecular research, yet existing models often generate erroneous information,
posing challenges to accurate molecular comprehension. Traditional evaluation
metrics for generated content fail to assess a model's accuracy in molecular
understanding. To rectify the absence of factual evaluation, we present
MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA
pairs over 23K molecules. Each QA pair, composed of a manual question, a
positive option and three negative options, has consistent semantics with a
molecular description from authoritative molecular corpus. MoleculeQA is not
only the first benchmark for molecular factual bias evaluation but also the
largest QA dataset for molecular research. A comprehensive evaluation on
MoleculeQA for existing molecular LLMs exposes their deficiencies in specific
areas and pinpoints several particularly crucial factors for molecular
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded Translations for Low-resource Automated Glossing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changbing Yang, Garrett Nicolai, Miikka Silfverberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate automatic interlinear glossing in low-resource settings. We
augment a hard-attentional neural model with embedded translation information
extracted from interlinear glossed text. After encoding these translations
using large language models, specifically BERT and T5, we introduce a
character-level decoder for generating glossed output. Aided by these
enhancements, our model demonstrates an average improvement of 3.97\%-points
over the previous state of the art on datasets from the SIGMORPHON 2023 Shared
Task on Interlinear Glossing. In a simulated ultra low-resource setting,
trained on as few as 100 sentences, our system achieves an average 9.78\%-point
improvement over the plain hard-attentional baseline. These results highlight
the critical role of translation information in boosting the system's
performance, especially in processing and interpreting modest data sources. Our
findings suggest a promising avenue for the documentation and preservation of
languages, with our experiments on shared task datasets indicating significant
advancements over the existing state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of
  Speech Sound Disorders in Korean children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Ahn, Yeonjung Hong, Younggon Im, Do Hyung Kim, Dayoung Kang, Joo Won Jeong, Jae Won Kim, Min Jung Kim, Ah-ra Cho, Dae-Hyun Jang, Hosung Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a model of automatic speech recognition (ASR) designed to
diagnose pronunciation issues in children with speech sound disorders (SSDs) to
replace manual transcriptions in clinical procedures. Since ASR models trained
for general purposes primarily predict input speech into real words, employing
a well-known high-performance ASR model for evaluating pronunciation in
children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to
recognize speech as pronounced rather than as existing words. The model was
fine-tuned with a speech dataset from 137 children with inadequate speech
production pronouncing 73 Korean words selected for actual clinical diagnosis.
The model's predictions of the pronunciations of the words matched the human
annotations with about 90% accuracy. While the model still requires improvement
in recognizing unclear pronunciation, this study demonstrates that ASR models
can streamline complex pronunciation error diagnostic procedures in clinical
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Loss Functions for Fact Verification <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Mukobara, Yutaro Shigeto, Masashi Shimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore loss functions for fact verification in the FEVER shared task.
While the cross-entropy loss is a standard objective for training verdict
predictors, it fails to capture the heterogeneity among the FEVER verdict
classes. In this paper, we develop two task-specific objectives tailored to
FEVER. Experimental results confirm that the proposed objective functions
outperform the standard cross-entropy. Performance is further improved when
these objectives are combined with simple class weighting, which effectively
overcomes the imbalance in the training data. The souce code is available at
https://github.com/yuta-mukobara/RLF-KGAT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EACL 2024 (short paper). The souce code is available at
  https://github.com/yuta-mukobara/RLF-KGAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MolBind: Multimodal Alignment of Language, Molecules, and Proteins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Xiao, Chao Cui, Huaisheng Zhu, Vasant G. Honavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in biology and chemistry have leveraged multi-modal
learning, integrating molecules and their natural language descriptions to
enhance drug discovery. However, current pre-training frameworks are limited to
two modalities, and designing a unified network to process different modalities
(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and
3D proteins) remains challenging due to inherent gaps among them. In this work,
we propose MolBind, a framework that trains encoders for multiple modalities
through contrastive learning, mapping all modalities to a shared feature space
for multi-modal semantic alignment. To facilitate effective pre-training of
MolBind on multiple modalities, we also build and collect a high-quality
dataset with four modalities, MolBind-M4, including graph-language,
conformation-language, graph-conformation, and conformation-protein paired
data. MolBind shows superior zero-shot learning performance across a wide range
of tasks, demonstrating its strong capability of capturing the underlying
semantics of multiple modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethos: Rectifying <span class="highlight-title">Language Models</span> in Orthogonal Parameter Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, Murali Annavaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have greatly propelled the research on natural language
processing. However, LMs also raise concerns regarding the generation of biased
or toxic content and the potential disclosure of private information from the
training dataset. In this work, we present a new efficient approach, Ethos,
that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy
leakage. Ethos is built on task arithmetic. However, unlike current task
arithmetic algorithms, Ethos distinguishes general beneficial and undesired
knowledge when reconstructing task vectors. Specifically, Ethos first obtains a
set of principal components from the pre-trained models using singular value
decomposition. Then, by projecting the task vector onto principal components,
Ethos identifies the principal components that encode general or undesired
knowledge. Ethos performs negating using the task vector with undesired
knowledge only, thereby minimizing collateral damage on general model utility.
We demonstrate the efficacy of our approach on three different tasks:
debiasing, detoxification, and memorization unlearning. Evaluations show Ethos
is more effective in removing undesired knowledge and maintaining the overall
model performance compared to current task arithmetic methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoGuide: Automated <span class="highlight-title">Generation</span> and Selection of State-Aware Guidelines
  for Large Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary limitation of large language models (LLMs) is their restricted
understanding of the world. This poses significant difficulties for LLM-based
agents, particularly in domains where pre-trained LLMs lack sufficient
knowledge. In this paper, we introduce a novel framework, called AutoGuide,
that bridges the knowledge gap in pre-trained LLMs by leveraging implicit
knowledge in offline experiences. Specifically, AutoGuide effectively extracts
knowledge embedded in offline data by extracting a set of state-aware
guidelines. Importantly, each state-aware guideline is expressed in concise
natural language and follows a conditional structure, clearly describing the
state where it is applicable. As such, the resulting guidelines enable a
principled way to provide helpful knowledge pertinent to an agent's current
decision-making process. We show that our approach outperforms competitive
LLM-based baselines by a large margin in sequential decision-making benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM
  Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) refers to techniques that provide human-understandable
insights into the workings of AI models. Recently, the focus of XAI is being
extended towards Large Language Models (LLMs) which are often criticized for
their lack of transparency. This extension calls for a significant
transformation in XAI methodologies because of two reasons. First, many
existing XAI methods cannot be directly applied to LLMs due to their complexity
advanced capabilities. Second, as LLMs are increasingly deployed across diverse
industry applications, the role of XAI shifts from merely opening the "black
box" to actively enhancing the productivity and applicability of LLMs in
real-world settings. Meanwhile, unlike traditional machine learning models that
are passive recipients of XAI insights, the distinct abilities of LLMs can
reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in
the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,
and (2) how LLMs can contribute to the advancement of XAI. We introduce 10
strategies, introducing the key techniques for each and discussing their
associated challenges. We also provide case studies to demonstrate how to
obtain and leverage explanations. The code used in this paper can be found at:
https://github.com/JacksonWuxs/UsableXAI_LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the breakthrough of ChatGPT, large language models (LLMs) have garnered
significant attention in the research community. With the development of LLMs,
the question of text style transfer for conversational models has emerged as a
natural extension, where chatbots may possess their own styles or even
characters. However, standard evaluation metrics have not yet been established
for this new settings. This paper aims to address this issue by proposing the
LMStyle Benchmark, a novel evaluation framework applicable to chat-style text
style transfer (C-TST), that can measure the quality of style transfer for LLMs
in an automated and scalable manner. In addition to conventional style strength
metrics, LMStyle Benchmark further considers a novel aspect of metrics called
appropriateness, a high-level metrics take account of coherence, fluency and
other implicit factors without the aid of reference samples. Our experiments
demonstrate that the new evaluation methods introduced by LMStyle Benchmark
have a higher correlation with human judgments in terms of appropriateness.
Based on LMStyle Benchmark, we present a comprehensive list of evaluation
results for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their
stylistic properties, such as formality and sentiment strength, along with
their appropriateness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Hallucination and Coverage Errors in Retrieval Augmented
  <span class="highlight-title">Generation</span> for Controversial Topics <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler A. Chang, Katrin Tomanek, Jessica Hoffmann, Nithum Thain, Erin van Liemt, Kathleen Meier-Hellstern, Lucas Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a strategy to handle controversial topics in LLM-based chatbots
based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the
absence of a single true answer and surface multiple perspectives. We frame
this as retrieval augmented generation, where perspectives are retrieved from a
knowledge base and the LLM is tasked with generating a fluent and faithful
response from the given perspectives. As a starting point, we use a
deterministic retrieval system and then focus on common LLM failure modes that
arise during this approach to text generation, namely hallucination and
coverage errors. We propose and evaluate three methods to detect such errors
based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our
results demonstrate that LLM-based classifiers, even when trained only on
synthetic errors, achieve high error detection performance, with ROC AUC scores
of 95.3% for hallucination and 90.5% for coverage error detection on
unambiguous error cases. We show that when no training data is available, our
other methods still yield good results on hallucination (84.0%) and coverage
error (85.2%) detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From "um" to "yeah": Producing, predicting, and regulating information
  flow in human conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Augusta Bergey, Simon DeDeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversation demands attention. Speakers must call words to mind, listeners
must make sense of them, and both together must negotiate this flow of
information, all in fractions of a second. We used large language models to
study how this works in a large-scale dataset of English-language conversation,
the CANDOR corpus. We provide a new estimate of the information density of
unstructured conversation, of approximately 13 bits/second, and find
significant effects associated with the cognitive load of both retrieving, and
presenting, that information. We also reveal a role for backchannels -- the
brief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the
production of novelty: the lead-up to a backchannel is associated with
declining information rate, while speech downstream rebounds to previous rates.
Our results provide new insights into long-standing theories of how we respond
to fluctuating demands on cognitive resources, and how we negotiate those
demands in partnership with others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAPERCLIP: Associating Astronomical Observations and Natural Language
  with Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Mishra-Sharma, Yiding Song, Jesse Thaler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation
for Contrastive Language-Image Pre-training), a method which associates
astronomical observations imaged by telescopes with natural language using a
neural network model. The model is fine-tuned from a pre-trained Contrastive
Language-Image Pre-training (CLIP) model using successful observing proposal
abstracts and corresponding downstream observations, with the abstracts
optionally summarized via guided generation using large language models (LLMs).
Using observations from the Hubble Space Telescope (HST) as an example, we show
that the fine-tuned model embodies a meaningful joint representation between
observations and natural language through tests targeting image retrieval
(i.e., finding the most relevant observations using natural language queries)
and description retrieval (i.e., querying for astrophysical object classes and
use cases most relevant to a given observation). Our study demonstrates the
potential for using generalist foundation models rather than task-specific
models for interacting with astronomical data by leveraging text as an
interface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17+6 pages, 3+1 figures, 5+2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Gender Bias in Chemical Named Entity
  Recognition Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingmeng Zhao, Ali Niazi, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical named entity recognition (NER) models are used in many downstream
tasks, from adverse drug reaction identification to pharmacoepidemiology.
However, it is unknown whether these models work the same for everyone.
Performance disparities can potentially cause harm rather than the intended
good. This paper assesses gender-related performance disparities in chemical
NER systems. We develop a framework for measuring gender bias in chemical NER
models using synthetic data and a newly annotated corpus of over 92,405 words
with self-identified gender information from Reddit. Our evaluation of multiple
biomedical NER models reveals evident biases. For instance, synthetic data
suggests female-related names are frequently misclassified as chemicals,
especially for brand name mentions. Additionally, we observe performance
disparities between female- and male-associated data in both datasets. Many
systems fail to detect contraceptives such as birth control. Our findings
emphasize the biases in chemical NER models, urging practitioners to account
for these biases in downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Embedding Spaces using Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings have become a pivotal means to represent complex, multi-faceted
information about entities, concepts, and relationships in a condensed and
useful format. Nevertheless, they often preclude direct interpretation. While
downstream tasks make use of these compressed representations, meaningful
interpretation usually requires visualization using dimensionality reduction or
specialized machine learning interpretability methods. This paper addresses the
challenge of making such embeddings more interpretable and broadly useful, by
employing Large Language Models (LLMs) to directly interact with embeddings --
transforming abstract vectors into understandable narratives. By injecting
embeddings into LLMs, we enable querying and exploration of complex embedding
data. We demonstrate our approach on a variety of diverse tasks, including:
enhancing concept activation vectors (CAVs), communicating novel embedded
entities, and decoding user preferences in recommender systems. Our work
couples the immense information potential of embeddings with the interpretative
power of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speculative Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models~(LLMs) exhibit exceptional performance in language
tasks, yet their auto-regressive inference is limited due to high computational
requirements and is sub-optimal due to the exposure bias. Inspired by
speculative decoding and contrastive decoding, we introduce Speculative
Contrastive Decoding~(SCD), a straightforward yet powerful decoding approach
that leverages predictions from smaller language models~(LMs) to achieve both
decoding acceleration and quality improvement. Extensive evaluations and
analyses on four diverse language tasks demonstrate the effectiveness of SCD,
showing that decoding efficiency and quality can compatibly benefit from one
smaller LM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenTKG: Generative Forecasting on Temporal Knowledge Graph <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07793v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07793v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUQL: Conversational Search over Structured and Unstructured Data with
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Monica S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most conversational agents are grounded on either free-text or
structured knowledge, many knowledge corpora consist of hybrid sources. This
paper presents the first conversational agent that supports the full generality
of hybrid data access for large knowledge corpora, through a language we
developed called SUQL (Structured and Unstructured Query Language).
Specifically, SUQL extends SQL with free-text primitives (summary and answer),
so information retrieval can be composed with structured data accesses
arbitrarily in a formal, succinct, precise, and interpretable notation. With
SUQL, we propose the first semantic parser, an LLM with in-context learning,
that can handle hybrid data sources.
  Our in-context learning-based approach, when applied to the HybridQA dataset,
comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K
data samples. More significantly, unlike previous approaches, our technique is
applicable to large databases and free-text corpora. We introduce a dataset
consisting of crowdsourced questions and conversations on Yelp, a large, real
restaurant knowledge base with structured and unstructured data. We show that
our few-shot conversational agent based on SUQL finds an entity satisfying all
user requirements 90.3% of the time, compared to 63.4% for a baseline based on
linearization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large <span class="highlight-title">Language Models</span> on Answering and Explaining
  Challenging Medical Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18060v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18060v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have demonstrated impressive performance in answering medical questions,
such as passing scores on medical licensing examinations. However, medical
board exam questions or general clinical questions do not capture the
complexity of realistic clinical cases. Moreover, the lack of reference
explanations means we cannot easily evaluate the reasoning of model decisions,
a crucial component of supporting doctors in making complex medical decisions.
To address these challenges, we construct two new datasets: JAMA Clinical
Challenge and Medbullets. JAMA Clinical Challenge consists of questions based
on challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style
clinical questions. Both datasets are structured as multiple-choice
question-answering tasks, where each question is accompanied by an
expert-written explanation. We evaluate four LLMs on the two datasets using
various prompts. Experiments demonstrate that our datasets are harder than
previous benchmarks. The inconsistency between automatic and human evaluations
of model-generated explanations highlights the need to develop new metrics to
support future research on explainable medical QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOOLVERIFIER: Generalization to New Tools via Self-Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, Jane Dwivedi-Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching language models to use tools is an important milestone towards
building general assistants, but remains an open problem. While there has been
significant progress on learning to use specific tools via fine-tuning,
language models still struggle with learning how to robustly use new tools from
only a few demonstrations. In this work we introduce a self-verification method
which distinguishes between close candidates by self-asking contrastive
questions during (1) tool selection; and (2) parameter generation. We construct
synthetic, high-quality, self-generated data for this goal using Llama-2 70B,
which we intend to release publicly. Extensive experiments on 4 tasks from the
ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average
improvement of 22% over few-shot baselines, even in scenarios where the
distinctions between candidate tools are finely nuanced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseGraph: Graph-oriented <span class="highlight-title">Instruct</span>ion Tuning of Large <span class="highlight-title">Language Models</span>
  for Generic Graph Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs with abundant attributes are essential in modeling interconnected
entities and improving predictions in various real-world applications.
Traditional Graph Neural Networks (GNNs), which are commonly used for modeling
attributed graphs, need to be re-trained every time when applied to different
graph tasks and datasets. Although the emergence of Large Language Models
(LLMs) has introduced a new paradigm in natural language processing, the
generative potential of LLMs in graph mining remains largely under-explored. To
this end, we propose a novel framework MuseGraph, which seamlessly integrates
the strengths of GNNs and LLMs and facilitates a more effective and generic
approach for graph mining across different tasks and datasets. Specifically, we
first introduce a compact graph description via the proposed adaptive input
generation to encapsulate key information from the graph under the constraints
of language token limitations. Then, we propose a diverse instruction
generation mechanism, which distills the reasoning capabilities from LLMs
(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction
packages for different graph tasks. Finally, we propose a graph-aware
instruction tuning with a dynamic instruction package allocation strategy
across tasks and datasets, ensuring the effectiveness and generalization of the
training process. Our experimental results demonstrate significant improvements
in different graph tasks, showcasing the potential of our MuseGraph in
enhancing the accuracy of graph-oriented downstream tasks while keeping the
generation powers of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Learning Learns Label Relationships but Is Not Conventional
  Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12375v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12375v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Kossen, Yarin Gal, Tom Rainforth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predictions of Large Language Models (LLMs) on downstream tasks often
improve significantly when including examples of the input--label relationship
in the context. However, there is currently no consensus about how this
in-context learning (ICL) ability of LLMs works. For example, while Xie et al.
(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)
argue ICL does not even learn label relationships from in-context examples. In
this paper, we provide novel insights into how ICL leverages label information,
revealing both capabilities and limitations. To ensure we obtain a
comprehensive picture of ICL behavior, we study probabilistic aspects of ICL
predictions and thoroughly examine the dynamics of ICL as more examples are
provided. Our experiments show that ICL predictions almost always depend on
in-context labels and that ICL can learn truly novel tasks in-context. However,
we also find that ICL struggles to fully overcome prediction preferences
acquired from pre-training data and, further, that ICL does not consider all
in-context information equally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Quantum CNN Model for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.11155v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.11155v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        X. Q. Zhao, T. L. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum density matrix represents all the information of the entire quantum
system, and novel models of meaning employing density matrices naturally model
linguistic phenomena such as hyponymy and linguistic ambiguity, among others in
quantum question answering tasks. Naturally, we argue that the quantum density
matrix can enhance the image feature information and the relationship between
the features for the classical image classification. Specifically, we (i)
combine density matrices and CNN to design a new mechanism; (ii) apply the new
mechanism to some representative classical image classification tasks. A series
of experiments show that the application of quantum density matrix in image
classification has the generalization and high efficiency on different
datasets. The application of quantum density matrix both in classical question
answering tasks and classical image classification tasks show more effective
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StableToolBench: Towards Stable Large-Scale Benchmarking on Tool
  Learning of Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have witnessed remarkable advancements in recent
years, prompting the exploration of tool learning, which integrates LLMs with
external tools to address diverse real-world challenges. Assessing the
capability of LLMs to utilise tools necessitates large-scale and stable
benchmarks. However, previous works relied on either hand-crafted online tools
with limited scale, or large-scale real online APIs suffering from instability
of API status. To address this problem, we introduce StableToolBench, a
benchmark evolving from ToolBench, proposing a virtual API server and stable
evaluation system. The virtual API server contains a caching system and API
simulators which are complementary to alleviate the change in API status.
Meanwhile, the stable evaluation system designs solvable pass and win rates
using GPT-4 as the automatic evaluator to eliminate the randomness during
evaluation. Experimental results demonstrate the stability of StableToolBench,
and further discuss the effectiveness of API simulators, the caching system,
and the evaluator system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jellyfish: A Large Language Model for Data Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01678v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01678v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the utilization of LLMs for data preprocessing (DP), a
crucial step in the data mining pipeline that transforms raw data into a clean
format conducive to easy processing. Whereas the use of LLMs has sparked
interest in devising universal solutions to DP, recent initiatives in this
domain typically rely on GPT APIs, raising inevitable data breach concerns.
Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B
models) as universal DP ask solver. We select a collection of datasets across
four representative DP tasks and construct instruction-tuning data using
serialization and knowledge injection techniques tailored to DP. As such, the
instruction-tuned LLMs empower users to manually craft instructions for DP.
Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring
data security and enabling further tuning. Our experiments show that our
dataset constructed for DP instruction tuning, namely Jellyfish, effectively
enhances LLMs' DP performances and barely compromises their abilities in NLP
tasks. By tuning Mistral-7B and OpenOrca-Platypus2-13B with Jellyfish, the
models deliver competitiveness compared to state-of-the-art DP methods and
strong generalizability to unseen tasks. The models' performance rivals that of
GPT series models, and the interpretation offers enhanced reasoning
capabilities compared to GPT-3.5. The 7B and 13B Jellyfish models are available
at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish-7B
https://huggingface.co/NECOUDBFM/Jellyfish-13B
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a.k.a. "Jellyfish: Instruction-Tuning Local Large Language Models for
  Data Preprocessing''</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Beyond Memorization: The Challenge of Random Memory Access in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyao Zhu, <span class="highlight-author">Qian Liu</span>, Liang Pang, Zhengbao Jiang, <span class="highlight-author">Min-Yen Kan</span>, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.com/sail-sg/lm-random-memory-access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures; fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism
  of <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Cao, Qiaoyu Tang, Hongyu Lin, Shanshan Jiang, Bin Dong, Xianpei Han, Jiawei Chen, Tianshu Wang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory is one of the most essential cognitive functions serving as a
repository of world knowledge and episodes of activities. In recent years,
large-scale pre-trained language models have shown remarkable memorizing
ability. On the contrary, vanilla neural networks without pre-training have
been long observed suffering from the catastrophic forgetting problem. To
investigate such a retentive-forgetful contradiction and understand the memory
mechanism of language models, we conduct thorough experiments by controlling
the target knowledge types, the learning strategies and the learning schedules.
We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads
to retentive language models; 3) Knowledge relevance and diversification
significantly influence the memory formation. These conclusions are useful for
understanding the abilities of pre-trained language models and shed light on
designing and evaluating new learning and inference algorithms of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated Large Language Models (LLMs) can extend
their zero-shot generalization capabilities to multimodal learning through
instruction tuning. As more modalities and downstream tasks are introduced,
negative conflicts and interference may have a worse impact on performance.
While this phenomenon has been overlooked in previous work, we propose a novel
and extensible framework, called Octavius, for comprehensive studies and
experimentation on multimodal learning with Multimodal Large Language Models
(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and
one of the representative PEFT techniques, i.e., LoRA, designing a novel
LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our
knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to
address this problem. The experimental results (about 20% improvement) have
shown the effectiveness and versatility of our design in various 2D and 3D
downstream tasks. Code and datasets are available at
https://openlamm.github.io/paper_list/Octavius.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 12 figures. Accepted in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer
  Inputs of <span class="highlight-title">Language Models</span> in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Li, Sheng Liu, Qi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models trained via federated learning (FL) demonstrate impressive
capabilities in handling complex tasks while protecting user privacy. Recent
studies indicate that leveraging gradient information and prior knowledge can
potentially reveal training samples within FL setting. However, these
investigations have overlooked the potential privacy risks tied to the
intrinsic architecture of the models. This paper presents a two-stage privacy
attack strategy that targets the vulnerabilities in the architecture of
contemporary language models, significantly enhancing attack performance by
initially recovering certain feature directions as additional supervisory
signals. Our comparative experiments demonstrate superior attack performance
across various datasets and scenarios, highlighting the privacy leakage risk
associated with the increasingly complex architectures of language models. We
call for the community to recognize and address these potential privacy risks
in designing large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent Lumos: Unified and Modular Training for Open-Source Language
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, Bill Yuchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closed-source agents suffer from several issues such as a lack of
affordability, transparency, and reproducibility, particularly on complex
interactive tasks. This motivates the development of open-source alternatives.
We introduce LUMOS, one of the first frameworks for training open-source
LLM-based agents. LUMOS features a learnable, unified, and modular architecture
with a planning module that learns high-level subgoal generation, and a
grounding module trained to translate these into actions using various tools in
the execution module. The design allows for modular upgrades and wider
applicability to diverse interactive tasks. To foster generalizable agent
learning, we collect large-scale, unified, and high-quality training
annotations derived from diverse ground-truth reasoning rationales across
various complex interactive tasks. On 9 datasets, LUMOS exhibits several key
advantages: (1) LUMOS excels multiple larger open-source agents on the held-out
datasets (unused for training) for each task type. LUMOS even surpasses GPT
agents on QA and web tasks; (2) LUMOS outperforms open-source agents produced
by chain-of-thoughts and unmodularized integrated training; and (3) LUMOS
effectively generalizes to unseen tasks, outperforming 33B-scale agents and
domain-specific agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://allenai.github.io/lumos/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-verbal information in spontaneous speech -- towards a new framework
  of analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-verbal signals in speech are encoded by prosody and carry information
that ranges from conversation action to attitude and emotion. Despite its
importance, the principles that govern prosodic structure are not yet
adequately understood. This paper offers an analytical schema and a
technological proof-of-concept for the categorization of prosodic signals and
their association with meaning. The schema interprets surface-representations
of multi-layered prosodic events. As a first step towards implementation, we
present a classification process that disentangles prosodic phenomena of three
orders. It relies on fine-tuning a pre-trained speech recognition model,
enabling the simultaneous multi-class/multi-label detection. It generalizes
over a large variety of spontaneous data, performing on a par with, or superior
to, human annotation. In addition to a standardized formalization of prosody,
disentangling prosodic patterns can direct a theory of communication and speech
organization. A welcome by-product is an interpretation of prosody that will
enhance speech- and language-related technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToPro: Token-Level <span class="highlight-title">Prompt</span> Decomposition for Cross-Lingual Sequence
  Labeling Tasks <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolei Ma, Ercong Nie, Shuzhou Yuan, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based methods have been successfully applied to multilingual
pretrained language models for zero-shot cross-lingual understanding. However,
most previous studies primarily focused on sentence-level classification tasks,
and only a few considered token-level labeling tasks such as Named Entity
Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose
Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based
method for token-level sequence labeling tasks. The ToPro method decomposes an
input sentence into single tokens and applies one prompt template to each
token. Our experiments on multilingual NER and POS tagging datasets demonstrate
that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning
in zero-shot cross-lingual transfer, especially for languages that are
typologically different from the source language English. Our method also
attains state-of-the-art performance when employed with the mT5 model. Besides,
our exploratory study in multilingual large language models shows that ToPro
performs much better than the current in-context learning method. Overall, the
performance improvements show that ToPro could potentially serve as a novel and
simple benchmarking method for sequence labeling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Naming, Describing, and Quantifying Visual Objects in Humans and LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Testoni, Juell Sprott, Sandro Pezzelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While human speakers use a variety of different expressions when describing
the same object in an image, giving rise to a distribution of plausible labels
driven by pragmatic constraints, the extent to which current Vision \& Language
Large Language Models (VLLMs) can mimic this crucial feature of language use is
an open question. This applies to common, everyday objects, but it is
particularly interesting for uncommon or novel objects for which a category
label may be lacking or fuzzy. Furthermore, humans show clear production
preferences for highly context-sensitive expressions, such as the quantifiers
`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on
three categories (nouns, attributes, and quantifiers) where humans show great
subjective variability concerning the distribution over plausible labels, using
datasets and resources mostly under-explored in previous work. Our results
reveal mixed evidence on the ability of VLLMs to capture human naming
preferences, with all models failing in tasks that require high-level reasoning
such as assigning quantifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go
  without Hallucination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent advancement in large language models (LLMs) and their high
performances across numerous benchmarks, recent research has unveiled that LLMs
suffer from hallucinations and unfaithful reasoning. This work studies a
specific type of hallucination induced by semantic associations. Specifically,
we investigate to what extent LLMs take shortcuts from certain keyword/entity
biases in the prompt instead of following the correct reasoning path. To
quantify this phenomenon, we propose a novel probing method and benchmark
called EureQA. We start from questions that LLMs will answer correctly with
utmost certainty, and mask the important entity with evidence sentence
recursively, asking models to find masked entities according to a chain of
evidence before answering the question.
  During the construction of the evidence, we purposefully replace semantic
clues (entities) that may lead to the correct answer with distractor clues
(evidence) that will not directly lead to the correct answer but require a
chain-like reasoning process. We evaluate if models can follow the correct
reasoning chain instead of short-cutting through distractor clues. We find that
existing LLMs lack the necessary capabilities to follow correct reasoning paths
and resist the attempt of greedy shortcuts. We show that the distractor
semantic associations often lead to model hallucination, which is strong
evidence that questions the validity of current LLM reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Owen, Biddwan Ahmed, Abhay Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method leveraging bi-encoder-based detectors
along with a comprehensive study comparing different out-of-distribution (OOD)
detection methods in NLP using different feature extractors. The feature
extraction stage employs popular methods such as Universal Sentence Encoder
(USE), BERT, MPNET, and GLOVE to extract informative representations from
textual data. The evaluation is conducted on several datasets, including
CLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using
metrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental
results demonstrate that the proposed bi-encoder-based detectors outperform
other methods, both those that require OOD labels in training and those that do
not, across all datasets, showing great potential for OOD detection in NLP. The
simplicity of the training process and the superior detection performance make
them applicable to real-world scenarios. The presented methods and benchmarking
metrics serve as a valuable resource for future research in OOD detection,
enabling further advancements in this field. The code and implementation
details can be found on our GitHub repository:
https://github.com/yellowmessenger/ood-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE: https://ieeexplore.ieee.org/document/10389907</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLLMs-Augmented Visual-Language Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18765v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18765v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanqing Liu, Kai Wang, Wenqi Shao, Ping Luo, Yu Qiao, Mike Zheng Shou, Kaipeng Zhang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language pre-training has achieved remarkable success in many
multi-modal tasks, largely attributed to the availability of large-scale
image-text datasets. In this work, we demonstrate that Multi-modal Large
Language Models (MLLMs) can enhance visual-language representation learning by
establishing richer image-text associations for image-text datasets. Our
approach is simple, utilizing MLLMs to extend multiple diverse captions for
each image. To prevent the bias introduced by MLLMs' hallucinations and
monotonous language styles, we propose "text shearing" to maintain the quality
and availability of extended captions. In image-text retrieval, without
introducing additional training cost, our method consistently obtains 5.6 ~
35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and
zero-shot settings, respectively. Notably, we obtain zero-shot results that are
comparable to fine-tuning on target datasets, which encourages more exploration
of the versatile use of MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Plausibility of Context Reliance in Neural Machine
  Translation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Sarti, Grzegorz Chrupała, Malvina Nissim, Arianna Bisazza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing whether language models can use contextual information in a
human-plausible way is important to ensure their trustworthiness in real-world
settings. However, the questions of when and which parts of the context affect
model generations are typically tackled separately, with current plausibility
evaluations being practically limited to a handful of artificial benchmarks. To
address this, we introduce Plausibility Evaluation of Context Reliance
(PECoRe), an end-to-end interpretability framework designed to quantify context
usage in language models' generations. Our approach leverages model internals
to (i) contrastively identify context-sensitive target tokens in generated
texts and (ii) link them to contextual cues justifying their prediction. We use
\pecore to quantify the plausibility of context-aware machine translation
models, comparing model rationales with human annotations across several
discourse-level phenomena. Finally, we apply our method to unannotated model
translations to identify context-mediated predictions and highlight instances
of (im)plausible context usage throughout generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Camera Ready. Code: https://github.com/gsarti/pecore.
  Artifacts:
  https://huggingface.co/collections/gsarti/pecore-iclr-2024-65edab42e28439e21b612c2e</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTabE: A Universal <span class="highlight-title">Pretrain</span>ing Protocol for Tabular Foundation Model
  in Data Science <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in NLP have witnessed the groundbreaking impact of
pretrained models, yielding impressive outcomes across various tasks. This
study seeks to extend the power of pretraining methodologies to facilitating
the prediction over tables in data science, a domain traditionally overlooked,
yet inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the establishment of a universal pretraining protocol for tables with
varied structures, the generalizability and transferability of learned
knowledge across tasks, the adaptation to diverse downstream applications, and
the incorporation of incremental columns over time. In response to these
challenges, we introduce UniTabE, a straightforward yet effective method
designed to process tables in a uniform manner, devoid of constraints imposed
by specific table structures. UniTabE's core concept relies on representing
each basic table element with a module, termed TabUnit. This is subsequently
followed by a Transformer encoder to refine the representation. Moreover, our
model is designed to facilitate pretraining and finetuning through the
utilization of free-form prompts. In order to implement the pretraining phase,
we curated an expansive tabular dataset comprising approximately 13B samples,
meticulously gathered from the Kaggle platform. This research primarily centers
on classification and regression tasks involving tabular data, and conducts
rigorous experimental testing and analyses to validate the effectiveness of our
methodology. The experimental results demonstrate UniTabE's superior
performance against several baselines across massive benchmarks. This,
therefore, underscores UniTabE's potential to significantly enhance the
semantic representation of tabular data, thereby marking a significant stride
for tabular data analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws of RoPE-based Extrapolation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extrapolation capability of Large Language Models (LLMs) based on Rotary
Position Embedding is currently a topic of considerable interest. The
mainstream approach to addressing extrapolation with LLMs involves modifying
RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the
original RoPE, with a larger value and providing longer fine-tuning text. In
this work, we first observe that fine-tuning a RoPE-based LLM with either a
smaller or larger base in pre-training context length could significantly
enhance its extrapolation performance. After that, we propose
\textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework
from the periodic perspective, to describe the relationship between the
extrapolation performance and base value as well as tuning context length. In
this process, we also explain the origin of the RoPE-based extrapolation issue
by \textbf{\textit{critical dimension for extrapolation}}. Besides these
observations and analyses, we achieve extrapolation up to 1 million context
length within only 16K training length on LLaMA2 7B and 13B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know<span class="highlight-title">GPT</span>: Knowledge Injection for Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06185v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06185v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs), such as ChatGPT, offer interactive
APIs that can answer common questions at a human-expert level. However, these
models often give inaccurate or incorrect responses when faced with questions
requiring domain-specific or professional-specific knowledge not covered in
their training corpus. Furthermore, many state-of-the-art LLMs are not
open-source, making it challenging to inject knowledge with model APIs only. In
this work, we introduce KnowGPT, a black-box knowledge injection framework for
LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)
to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed
Bandit (MAB) to construct the most suitable prompt for each question. Our
extensive experiments on three benchmark datasets showcase that KnowGPT
significantly enhances the existing methods. Notably, KnowGPT achieves an
average improvement of 23.7% over ChatGPT and an average improvement of 2.9%
over GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA
official leaderboard, which is comparable to human-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PROGrasp: Pragmatic Human-Robot Communication for Object Grasping <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive Object Grasping (IOG) is the task of identifying and grasping the
desired object via human-robot natural language interaction. Current IOG
systems assume that a human user initially specifies the target object's
category (e.g., bottle). Inspired by pragmatics, where humans often convey
their intentions by relying on context to achieve goals, we introduce a new IOG
task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented
Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an
intention-oriented utterance (e.g., "I am thirsty") is initially given to the
robot. The robot should then identify the target object by interacting with a
human user. Based on the task setup, we propose a new robotic system that can
interpret the user's intention and pick up the target object, Pragmatic Object
Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules
for visual grounding, question asking, object grasping, and most importantly,
answer interpretation for pragmatic inference. Experimental results show that
PROGrasp is effective in offline (i.e., target object discovery) and online
(i.e., IOG with a physical robot arm) settings. Code and data are available at
https://github.com/gicheonkang/prograsp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongAgent: Scaling <span class="highlight-title">Language Models</span> to 128k Context through Multi-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive performance in
understanding language and executing complex reasoning tasks. However, LLMs
with long context windows have been notorious for their expensive training
costs and high inference latency. Even the most advanced models such as GPT-4
and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a
phenomenon also known as \textit{lost in the middle}. In this paper, we propose
\textsc{LongAgent}, a method based on multi-agent collaboration, which scales
LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority
in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is
responsible for understanding user intent and directing team members to acquire
information from documents. Due to members' hallucinations, it is non-trivial
for a leader to obtain accurate information from the responses of dozens to
hundreds of members. To address this, we develop an \textit{inter-member
communication} mechanism to resolve response conflicts caused by hallucinations
through information sharing. Our experimental results indicate that
\textsc{LongAgent} offers a promising alternative for long-text processing. The
agent team instantiated with LLaMA-7B achieves significant improvements in
tasks such as 128k-long text retrieval, multi-hop question answering, compared
to GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electrocardiogram <span class="highlight-title">Instruct</span>ion Tuning for Report <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool
for cardiac conditions monitoring, are crucial in assisting clinicians. Recent
studies have concentrated on classifying cardiac conditions using ECG data but
have overlooked ECG report generation, which is not only time-consuming but
also requires clinical expertise. To automate ECG report generation and ensure
its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)
framework, the \textit{first} attempt to tackle ECG report generation with LLMs
and multimodal instructions. To facilitate future research, we establish a
benchmark to evaluate MEIT with various LLMs backbones across two large-scale
ECG datasets. Our approach uniquely aligns the representations of the ECG
signal and the report, and we conduct extensive experiments to benchmark MEIT
with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results
underscore the superior performance of instruction-tuned LLMs, showcasing their
proficiency in quality report generation, zero-shot capabilities, and
resilience to signal perturbation. These findings emphasize the efficacy of our
MEIT framework and its potential for real-world clinical application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CRAFT: Customizing LLMs by Creating and Retrieving from Specialized
  Toolsets <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are often augmented with tools to solve complex
tasks. By generating code snippets and executing them through task-specific
Application Programming Interfaces (APIs), they can offload certain functions
to dedicated external modules, such as image encoding and performing
calculations. However, most existing approaches to augment LLMs with tools are
constrained by general-purpose APIs and lack the flexibility for tailoring them
to specific tasks. In this work, we present CRAFT, a general tool creation and
retrieval framework for LLMs. It creates toolsets specifically curated for the
tasks and equips LLMs with a component that retrieves tools from these sets to
enhance their capability to solve complex tasks. For each task, we collect
specific code solutions by prompting GPT-4 to solve the training examples.
Following a validation step ensuring the correctness, these solutions are
abstracted into code snippets to enhance reusability, and deduplicated for
higher quality. At inference time, the language model retrieves snippets from
the toolsets and then executes them or generates the output conditioning on the
retrieved snippets. Our method is designed to be flexible and offers a
plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and
modalities, without any finetuning. Experiments on vision-language, tabular
processing, and mathematical reasoning tasks show that our approach achieves
substantial improvements compared to strong baselines. In addition, our
in-depth analysis reveals that: (1) consistent performance improvement can be
achieved by scaling up the number of tools and the capability of the backbone
models; (2) each component of our approach contributes to the performance
gains; (3) the created tools are well-structured and reliable with low
complexity and atomicity. The code is available at
https://github.com/lifan-yuan/CRAFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024. Code is available at
  https://github.com/lifan-yuan/CRAFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LitCab: Lightweight Language Model Calibration over Short- and Long-form
  Responses <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Liu, Muhammad Khalifa, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A model is considered well-calibrated when its probability estimate aligns
with the actual likelihood of the output being correct. Calibrating language
models (LMs) is crucial, as it plays a vital role in detecting and mitigating
hallucinations of LMs as well as building more trustworthy models. However,
standard calibration techniques may not be suited for LM calibration. For
instance, post-processing methods such as temperature scaling do not reorder
the candidate generations. On the other hand, training-based methods require
fine-tuning the entire model, which is impractical for LMs of large scale. We
present LitCab, a lightweight calibration mechanism consisting of a single
linear layer that takes the input text representation and predicts a bias term,
which is then added to the LM output logits. LitCab improves model calibration
by only adding < 2% of the original model parameters. For evaluation, we
construct CaT, a benchmark consisting of eight text generation tasks, covering
responses ranging from short phrases to paragraphs. We test LitCab with
Llama2-7B, where it improves calibration across all tasks, reducing the average
ECE score by as large as 30%. We further conduct a comprehensive evaluation
with multiple popular open-sourced LMs from GPT and LLaMA families, yielding
the following key findings: (i) Larger models within the same family exhibit
better calibration on tasks with short generation tasks, but not necessarily
for longer ones. (ii) GPT-family models show superior calibration compared to
LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii)
Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose
(e.g., conversations) may lead to worse calibration, highlighting the
importance of fine-tuning setups for calibrating LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Secrets of Engaging Conversations: Factors that Keep Users
  Hooked on Role-Playing Dialog Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhang, Yu Lu, Junwen Liu, Jia Yu, Huachuan Qiu, Yuming Yan, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing humanlike nature of dialog agents, people are now engaging
in extended conversations that can stretch from brief moments to substantial
periods of time. Understanding the factors that contribute to sustaining these
interactions is crucial, yet existing studies primarily focusing on short-term
simulations that rarely explore such prolonged and real conversations.
  In this paper, we investigate the factors influencing retention rates in real
interactions with roleplaying models. By analyzing a large dataset of
interactions between real users and thousands of characters, we systematically
examine multiple factors and assess their impact on user retention rate.
Surprisingly, we find that the degree to which the bot embodies the roles it
plays has limited influence on retention rates, while the length of each turn
it speaks significantly affects retention rates. This study sheds light on the
critical aspects of user engagement with role-playing models and provides
valuable insights for future improvements in the development of large language
models for role-playing purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WsiCaption: Multiple Instance <span class="highlight-title">Generation</span> of Pathology Reports for
  Gigapixel Whole-Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16480v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16480v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide images are the foundation of digital pathology for the diagnosis
and treatment of carcinomas. Writing pathology reports is laborious and
error-prone for inexperienced pathologists. To reduce the workload and improve
clinical automation, we investigate how to generate pathology reports given
whole slide images. On the data end, we curated the largest WSI-text dataset
(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text
pairs for visual-language models by recognizing and cleaning pathology reports
which narrate diagnostic slides in TCGA. On the model end, we propose the
multiple instance generative model (MI-Gen) which can produce pathology reports
for gigapixel WSIs. We benchmark our model on the largest subset of
TCGA-PathoText. Experimental results show our model can generate pathology
reports which contain multiple clinical clues and achieve competitive
performance on certain slide-level tasks. We observe that simple semantic
extraction from the pathology reports can achieve the best performance (0.838
of F1 score) on BRCA subtyping surpassing previous state-of-the-art approaches.
Our collected dataset and related code are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Turn-taking Prediction Using Voice Activity Projection <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge sharing about emerging threats is crucial in the rapidly advancing
field of cybersecurity and forms the foundation of Cyber Threat Intelligence
(CTI). In this context, Large Language Models are becoming increasingly
significant in the field of cybersecurity, presenting a wide range of
opportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,
Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary
classification and Named Entity Recognition (NER) tasks performed using Open
Source INTelligence (OSINT). We utilize well-established data collected in
previous research from Twitter to assess the competitiveness of these chatbots
when compared to specialized models trained for those tasks. In binary
classification experiments, Chatbot GPT-4 as a commercial model achieved an
acceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1
score of 0.90. However, concerning cybersecurity entity recognition, all
evaluated chatbots have limitations and are less effective. This study
demonstrates the capability of chatbots for OSINT binary classification and
shows that they require further improvement in NER to effectively replace
specially trained models. Our results shed light on the limitations of the LLM
chatbots when compared to specialized models, and can help researchers improve
chatbots technology with the objective to reduce the required effort to
integrate machine learning in OSINT-based CTI tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Large Language Model (KG-LLM) for Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which Modality should I use -- Text, Motif, or Image? : Understanding
  Graphs with Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research integrates graph data with Large Language Models (LLMs), which,
despite their advancements in various fields using large text corpora, face
limitations in encoding entire graphs due to context size constraints. This
paper introduces a new approach to encoding a graph with diverse modalities,
such as text, image, and motif, coupled with prompts to approximate a graph's
global connectivity, thereby enhancing LLMs' efficiency in processing complex
graph structures. The study also presents GraphTMI, a novel benchmark for
evaluating LLMs in graph structure analysis, focusing on homophily, motif
presence, and graph difficulty. Key findings indicate that the image modality,
especially with vision-language models like GPT-4V, is superior to text in
balancing token limits and preserving essential information and outperforms
prior graph neural net (GNN) encoders. Furthermore, the research assesses how
various factors affect the performance of each encoding modality and outlines
the existing challenges and potential future developments for LLMs in graph
understanding and reasoning tasks. All data will be publicly available upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amortizing intractable inference in large <span class="highlight-title">language models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive large language models (LLMs) compress knowledge from their
training data through next-token conditional distributions. This limits
tractable querying of this knowledge to start-to-end autoregressive sampling.
However, many tasks of interest -- including sequence continuation, infilling,
and other forms of constrained generation -- involve sampling from intractable
posterior distributions. We address this limitation by using amortized Bayesian
inference to sample from these intractable posteriors. Such amortization is
algorithmically achieved by fine-tuning LLMs via diversity-seeking
reinforcement learning algorithms: generative flow networks (GFlowNets). We
empirically demonstrate that this distribution-matching paradigm of LLM
fine-tuning can serve as an effective alternative to maximum-likelihood
training and reward-maximizing policy optimization. As an important
application, we interpret chain-of-thought reasoning as a latent variable
modeling problem and demonstrate that our approach enables data-efficient
adaptation of LLMs to tasks that require multi-step rationalization and tool
use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; 23 pages; code: https://github.com/GFNOrg/gfn-lm-tuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think before you speak: Training <span class="highlight-title">Language Models</span> With Pause Tokens <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models generate responses by producing a series of tokens in
immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$
hidden vectors per layer, one vector per preceding token. What if instead we
were to let the model manipulate say, $K+10$ hidden vectors, before it outputs
the $(K+1)^{th}$ token? We operationalize this idea by performing training and
inference on language models with a (learnable) $\textit{pause}$ token, a
sequence of which is appended to the input prefix. We then delay extracting the
model's outputs until the last pause token is seen, thereby allowing the model
to process extra computation before committing to an answer. We empirically
evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M
parameters with causal pretraining on C4, and on downstream tasks covering
reasoning, question-answering, general understanding and fact recall. Our main
finding is that inference-time delays show gains when the model is both
pre-trained and finetuned with delays. For the 1B model, we witness gains on 8
of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of
SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of
GSM8k. Our work raises a range of conceptual and practical future research
questions on making delayed next-token prediction a widely applicable new
paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Davidsonian Scene Graph: Improving Reliability in Fine-grained
  Evaluation for Text-to-Image <span class="highlight-title">Generation</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18235v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18235v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating text-to-image models is notoriously difficult. A strong recent
approach for assessing text-image faithfulness is based on QG/A (question
generation and answering), which uses pre-trained foundational models to
automatically generate a set of questions and answers from the prompt, and
output images are scored based on whether these answers extracted with a visual
question answering model are consistent with the prompt-based answers. This
kind of evaluation is naturally dependent on the quality of the underlying QG
and VQA models. We identify and address several reliability challenges in
existing QG/A work: (a) QG questions should respect the prompt (avoiding
hallucinations, duplications, and omissions) and (b) VQA answers should be
consistent (not asserting that there is no motorcycle in an image while also
claiming the motorcycle is blue). We address these issues with Davidsonian
Scene Graph (DSG), an empirically grounded evaluation framework inspired by
formal semantics, which is adaptable to any QG/A frameworks. DSG produces
atomic and unique questions organized in dependency graphs, which (i) ensure
appropriate semantic coverage and (ii) sidestep inconsistent answers. With
extensive experimentation and human evaluation on a range of model
configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG
addresses the challenges noted above. Finally, we present DSG-1k, an
open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide
range of fine-grained semantic categories with a balanced distribution. We
release the DSG-1k prompts and the corresponding DSG questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; Project website: https://google.github.io/dsg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Feldman. James R. Foulds, Shimei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 1 Figure, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval
  Augmentation to <span class="highlight-title">Language Models</span> <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LMs) demonstrate remarkable performance, they
encounter challenges in providing accurate responses when queried for
information beyond their pre-trained memorization. Although augmenting them
with relevant external information can mitigate these issues, failure to
consider the necessity of retrieval may adversely affect overall performance.
Previous research has primarily focused on examining how entities influence
retrieval models and knowledge recall in LMs, leaving other aspects relatively
unexplored. In this work, our goal is to offer a more detailed, fact-centric
analysis by exploring the effects of combinations of entities and relations. To
facilitate this, we construct a new question answering (QA) dataset called
WiTQA (Wikipedia Triple Question Answers). This dataset includes questions
about entities and relations of various popularity levels, each accompanied by
a supporting passage. Our extensive experiments with diverse LMs and retrievers
reveal when retrieval does not consistently enhance LMs from the viewpoints of
fact-centric popularity.Confirming earlier findings, we observe that larger LMs
excel in recalling popular facts. However, they notably encounter difficulty
with infrequent entity-relation pairs compared to retrievers. Interestingly,
they can effectively retain popular relations of less common entities. We
demonstrate the efficacy of our finer-grained metric and insights through an
adaptive retrieval system that selectively employs retrieval and recall based
on the frequencies of entities and relations in the question.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL2024 (main)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-12T00:00:00Z">2024-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Generative Large Language Model Evaluation for Semantic
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyun Wei, Xi Chen, Lin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their sophisticated capabilities, large language models (LLMs)
encounter a major hurdle in effective assessment. This paper first revisits the
prevalent evaluation method-multiple choice question answering (MCQA), which
allows for straightforward accuracy measurement. Through a comprehensive
evaluation of 24 models across 11 benchmarks, we highlight several potential
drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation
and the generation of open-ended responses in practical scenarios. In response,
we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,
Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with
GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This
system is designed to mirror real-world usage, and for this purpose, we have
compiled a new benchmark called ``Real-world questions'' (RWQ), comprising
20,772 authentic user inquiries. Additionally, we thoroughly analyze the
characteristics of our system and compare it with prior leaderboards like
AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo
system, the feasibility of registering new models, and its potential to reshape
LLM leaderboards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Safety Generalization Challenges of Large <span class="highlight-title">Language Models</span> via
  Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80\% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage
  Brought By Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have revolutionized numerous tasks with their
remarkable efficacy.However, the editing of these models, crucial for
rectifying outdated or erroneous information, often leads to a complex issue
known as the ripple effect in the hidden space. This effect, while difficult to
detect, can significantly impede the efficacy of model editing tasks and
deteriorate model performance.This paper addresses this scientific challenge by
proposing a novel evaluation methodology, Graphical Outlier Relation based
Assessment(GORA), which quantitatively evaluates the adaptations of the model
and the subsequent impact of editing. Furthermore, we introduce the Selective
Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate
this ripple effect. Our comprehensive evaluations reveal that the ripple effect
in the hidden space is a significant issue in all current model editing
methods. However, our proposed methods, GORA and SORA, effectively identify and
alleviate this issue, respectively, contributing to the advancement of LLM
editing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, <span class="highlight-author">Xi Victoria Lin</span>, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, Xian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate efficient methods for training Large Language Models (LLMs) to
possess capabilities in multiple specialized domains, such as coding, math
reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts
from a seed model, which is branched to train experts in embarrassingly
parallel fashion with high throughput and reduced communication cost. After
individual experts are asynchronously trained, BTX brings together their
feedforward parameters as experts in Mixture-of-Expert (MoE) layers and
averages the remaining parameters, followed by an MoE-finetuning stage to learn
token-level routing. BTX generalizes two special cases, the Branch-Train-Merge
method, which does not have the MoE finetuning stage to learn routing, and
sparse upcycling, which omits the stage of training experts asynchronously.
Compared to alternative approaches, BTX achieves the best accuracy-efficiency
tradeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pyvene: A Library for Understanding and Improving PyTorch Models via
  Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interventions on model-internal states are fundamental operations in many
areas of AI, including model editing, steering, robustness, and
interpretability. To facilitate such research, we introduce $\textbf{pyvene}$,
an open-source Python library that supports customizable interventions on a
range of different PyTorch modules. $\textbf{pyvene}$ supports complex
intervention schemes with an intuitive configuration format, and its
interventions can be static or include trainable parameters. We show how
$\textbf{pyvene}$ provides a unified and extensible framework for performing
interventions on neural models and sharing the intervened upon models with
others. We illustrate the power of the library via interpretability analyses
using causal abstraction and knowledge localization. We publish our library
through Python Package Index (PyPI) and provide code, documentation, and
tutorials at https://github.com/stanfordnlp/pyvene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Beyond Memorization: The Challenge of Random Memory Access in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyao Zhu, <span class="highlight-author">Qian Liu</span>, Liang Pang, Zhengbao Jiang, <span class="highlight-author">Min-Yen Kan</span>, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.
com/sail-sg/lm-random-memory-access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning Large <span class="highlight-title">Language Models</span> with Sequential <span class="highlight-title">Instruct</span>ions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxu Hu, Pinzhen Chen, Edoardo M. Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) struggle to follow a sequence of instructions in
a single query as they may ignore or misinterpret part of it. This impairs
their performance in complex problems whose solution requires multiple
intermediate steps, such as multilingual (translate then answer) and multimodal
(caption then answer) tasks. We empirically verify this with open-source LLMs
as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential
instructions in present-day data, we propose sequential instruction tuning, a
simple yet effective strategy to automatically augment instruction tuning data
and equip LLMs with the ability to execute multiple sequential instructions.
After exploring interleaving instructions in existing datasets, such as Alpaca,
with a wide range of intermediate tasks, we find that sequential
instruction-tuned models consistently outperform the conventional
instruction-tuned baselines in downstream tasks involving reasoning,
multilingual, and multimodal abilities. To shed further light on our technique,
we analyse how adversarial intermediate texts, unseen tasks, prompt
verbalization, number of tasks, and prompt length affect SIT. We hope that this
method will open new research avenues on instruction tuning for complex tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and <span class="highlight-title">Language Models</span> in Modern Organizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Jose Xavier Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese
  Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To thoroughly assess the mathematical reasoning abilities of Large Language
Models (LLMs), we need to carefully curate evaluation datasets covering diverse
mathematical concepts and mathematical problems at different difficulty levels.
In pursuit of this objective, we propose FineMath in this paper, a fine-grained
mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath
is created to cover the major key mathematical concepts taught in elementary
school math, which are further divided into 17 categories of math word
problems, enabling in-depth analysis of mathematical reasoning abilities of
LLMs. All the 17 categories of math word problems are manually annotated with
their difficulty levels according to the number of reasoning steps required to
solve these problems. We conduct extensive experiments on a wide range of LLMs
on FineMath and find that there is still considerable room for improvements in
terms of mathematical reasoning capability of Chinese LLMs. We also carry out
an in-depth analysis on the evaluation process and methods that have been
overlooked previously. These two factors significantly influence the model
results and our understanding of their mathematical reasoning capabilities. The
dataset will be publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Over<span class="highlight-title">generation</span> Mistakes <span class="chip">SemEval 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the results of the SHROOM, a shared task focused on
detecting hallucinations: outputs from natural language generation (NLG)
systems that are fluent, yet inaccurate. Such cases of overgeneration put in
jeopardy many NLG applications, where correctness is often mission-critical.
The shared task was conducted with a newly constructed dataset of 4000 model
outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine
translation, paraphrase generation and definition modeling.
  The shared task was tackled by a total of 58 different users grouped in 42
teams, out of which 27 elected to write a system description paper;
collectively, they submitted over 300 prediction sets on both tracks of the
shared task. We observe a number of key trends in how this approach was tackled
-- many participants rely on a handful of model, and often rely either on
synthetic data for fine-tuning or zero-shot prompting strategies. While a
majority of the teams did outperform our proposed baseline system, the
performances of top-scoring systems are still consistent with a random handling
of the more challenging items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SemEval 2024 shared task. Pre-review version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StableToolBench: Towards Stable Large-Scale Benchmarking on Tool
  Learning of Large <span class="highlight-title">Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have witnessed remarkable advancements in recent
years, prompting the exploration of tool learning, which integrates LLMs with
external tools to address diverse real-world challenges. Assessing the
capability of LLMs to utilise tools necessitates large-scale and stable
benchmarks. However, previous works relied on either hand-crafted online tools
with limited scale, or large-scale real online APIs suffering from instability
of API status. To address this problem, we introduce StableToolBench, a
benchmark evolving from ToolBench, proposing a virtual API server and stable
evaluation system. The virtual API server contains a caching system and API
simulators which are complementary to alleviate the change in API status.
Meanwhile, the stable evaluation system designs solvable pass and win rates
using GPT-4 as the automatic evaluator to eliminate the randomness during
evaluation. Experimental results demonstrate the stability of StableToolBench,
and further discuss the effectiveness of API simulators, the caching system,
and the evaluator system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Reinforcement Learning from <span class="highlight-title">Human Feedback</span> Using Contrastive
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large, Small or Both: A Novel Data Augmentation Framework Based on
  <span class="highlight-title">Language Models</span> for Debiasing Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyue Zhang, Pengfei Li, Yilong Lai, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As more than 70$\%$ of reviews in the existing opinion summary data set are
positive, current opinion summarization approaches are reluctant to generate
negative summaries given the input of negative texts. To address such sentiment
bias, a direct approach without the over-reliance on a specific framework is to
generate additional data based on large language models to balance the
emotional distribution of the dataset. However, data augmentation based on
large language models faces two disadvantages: 1) the potential issues or
toxicity in the augmented data; 2) the expensive costs. Therefore, in this
paper, we propose a novel data augmentation framework based on both large and
small language models for debiasing opinion summarization. In specific, a small
size of synthesized negative reviews is obtained by rewriting the positive text
via a large language model. Then, a disentangle reconstruction model is trained
based on the generated data. After training, a large amount of synthetic data
can be obtained by decoding the new representation obtained from the
combination of different sample representations and filtering based on
confusion degree and sentiment classification. Experiments have proved that our
framework can effectively alleviate emotional bias same as using only large
models, but more economically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference-free Monolithic Preference Optimization with Odds Ratio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Hong, Noah Lee, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent preference alignment algorithms for language models have
demonstrated promising results, supervised fine-tuning (SFT) remains imperative
for achieving successful convergence. In this paper, we study the crucial role
of SFT within the context of preference alignment, emphasizing that a minor
penalty for the disfavored generation style is sufficient for
preference-aligned SFT. Building on this foundation, we introduce a
straightforward and innovative reference model-free monolithic odds ratio
preference optimization algorithm, ORPO, eliminating the necessity for an
additional preference alignment phase. We demonstrate, both empirically and
theoretically, that the odds ratio is a sensible choice for contrasting favored
and disfavored styles during SFT across the diverse sizes from 125M to 7B.
Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with
ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art
language models with more than 7B and 13B parameters: achieving up to 12.20% on
$\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.
We release code and model checkpoints for Mistral-ORPO-$\alpha$ (7B) and
Mistral-ORPO-$\beta$ (7B).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SATDAUG -- A Balanced and Augmented <span class="highlight-title">Dataset</span> for Detecting Self-Admitted
  Technical Debt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edi Sutoyo, Andrea Capiluppi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-admitted technical debt (SATD) refers to a form of technical debt in
which developers explicitly acknowledge and document the existence of technical
shortcuts, workarounds, or temporary solutions within the codebase. Over recent
years, researchers have manually labeled datasets derived from various software
development artifacts: source code comments, messages from the issue tracker
and pull request sections, and commit messages. These datasets are designed for
training, evaluation, performance validation, and improvement of machine
learning and deep learning models to accurately identify SATD instances.
However, class imbalance poses a serious challenge across all the existing
datasets, particularly when researchers are interested in categorizing the
specific types of SATD. In order to address the scarcity of labeled data for
SATD \textit{identification} (i.e., whether an instance is SATD or not) and
\textit{categorization} (i.e., which type of SATD is being classified) in
existing datasets, we share the \textit{SATDAUG} dataset, an augmented version
of existing SATD datasets, including source code comments, issue tracker, pull
requests, and commit messages. These augmented datasets have been balanced in
relation to the available artifacts and provide a much richer source of labeled
data for training machine learning or deep learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published at the 21st IEEE/ACM International
  Conference on Mining Software Repositories (MSR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model
  Performance and Annotation Cost <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current foundation models have shown impressive performance across various
tasks. However, several studies have revealed that these models are not
effective for everyone due to the imbalanced geographical and economic
representation of the data used in the training process. Most of this data
comes from Western countries, leading to poor results for underrepresented
countries. To address this issue, more data needs to be collected from these
countries, but the cost of annotation can be a significant bottleneck. In this
paper, we propose methods to identify the data to be annotated to balance model
performance and annotation costs. Our approach first involves finding the
countries with images of topics (objects and actions) most visually distinct
from those already in the training datasets used by current large
vision-language foundation models. Next, we identify countries with higher
visual similarity for these topics and show that using data from these
countries to supplement the training data improves model performance and
reduces annotation costs. The resulting lists of countries and corresponding
topics are made available at
https://github.com/MichiganNLP/visual_diversity_budget.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moral<span class="highlight-title">BERT</span>: Detecting Moral Values in Social Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vjosa Preniqi, Iacopo Ghinassi, Kyriaki Kalimeri, Charalampos Saitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Morality plays a fundamental role in how we perceive information while
greatly influencing our decisions and judgements. Controversial topics,
including vaccination, abortion, racism, and sexuality, often elicit opinions
and attitudes that are not solely based on evidence but rather reflect moral
worldviews. Recent advances in natural language processing have demonstrated
that moral values can be gauged in human-generated textual content. Here, we
design a range of language representation models fine-tuned to capture exactly
the moral nuances in text, called MoralBERT. We leverage annotated moral data
from three distinct sources: Twitter, Reddit, and Facebook user-generated
content covering various socially relevant topics. This approach broadens
linguistic diversity and potentially enhances the models' ability to comprehend
morality in various contexts. We also explore a domain adaptation technique and
compare it to the standard fine-tuned BERT model, using two different
frameworks for moral prediction: single-label and multi-label. We compare
in-domain approaches with conventional models relying on lexicon-based
techniques, as well as a Machine Learning classifier with Word2Vec
representation. Our results showed that in-domain prediction models
significantly outperformed traditional models. While the single-label setting
reaches a higher accuracy than previously achieved for the task when using BERT
pretrained models. Experiments in an out-of-domain setting, instead, suggest
that further work is needed for existing domain adaptation techniques to
generalise between different social media platforms, especially for the
multi-label task. The investigations and outcomes from this study pave the way
for further exploration, enabling a more profound comprehension of moral
narratives about controversial social issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harder Tasks Need More Experts: Dynamic Routing in MoE Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel dynamic expert selection framework for
Mixture of Experts (MoE) models, aiming to enhance computational efficiency and
model performance by adjusting the number of activated experts based on input
difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,
which activates a predetermined number of experts regardless of the input's
complexity, our method dynamically selects experts based on the confidence
level in expert selection for each input. This allows for a more efficient
utilization of computational resources, activating more experts for complex
tasks requiring advanced reasoning and fewer for simpler tasks. Through
extensive evaluations, our dynamic routing method demonstrates substantial
improvements over conventional Top-2 routing across various benchmarks,
achieving an average improvement of 0.7% with less than 90% activated
parameters. Further analysis shows our model dispatches more experts to tasks
requiring complex reasoning skills, like BBH, confirming its ability to
dynamically allocate computational resources in alignment with the input's
complexity. Our findings also highlight a variation in the number of experts
needed across different layers of the transformer model, offering insights into
the potential for designing heterogeneous MoE frameworks. The code and models
are available at https://github.com/ZhenweiAn/Dynamic_MoE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced
  Personality Detection Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality detection aims to detect one's personality traits underlying in
social media posts. One challenge of this task is the scarcity of ground-truth
personality traits which are collected from self-report questionnaires. Most
existing methods learn post features directly by fine-tuning the pre-trained
language models under the supervision of limited personality labels. This leads
to inferior quality of post features and consequently affects the performance.
In addition, they treat personality traits as one-hot classification labels,
overlooking the semantic information within them. In this paper, we propose a
large language model (LLM) based text augmentation enhanced personality
detection model, which distills the LLM's knowledge to enhance the small model
for personality detection, even when the LLM fails in this task. Specifically,
we enable LLM to generate post analyses (augmentations) from the aspects of
semantic, sentiment, and linguistic, which are critical for personality
detection. By using contrastive learning to pull them together in the embedding
space, the post encoder can better capture the psycho-linguistic information
within the post representations, thus improving personality detection.
Furthermore, we utilize the LLM to enrich the information of personality labels
for enhancing the detection performance. Experimental results on the benchmark
datasets demonstrate that our model outperforms the state-of-the-art methods on
personality detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource
  Agglutinative Data-to-Text <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francois Meyer, Jan Buys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most data-to-text datasets are for English, so the difficulties of modelling
data-to-text for low-resource languages are largely unexplored. In this paper
we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.
We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of
WebNLG, which presents a new linguistic context that shifts modelling demands
to subword-driven techniques. We also develop an evaluation framework for T2X
that measures how accurately generated text describes the data. This enables
future users of T2X to go beyond surface-level metrics in evaluation. On the
modelling side we explore two classes of methods - dedicated data-to-text
models trained from scratch and pretrained language models (PLMs). We propose a
new dedicated architecture aimed at agglutinative data-to-text, the Subword
Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy
entities, and outperforms existing dedicated models for 2 agglutinative
languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,
which reveals that standard PLMs come up short. Fine-tuning machine translation
models emerges as the best method overall. These findings underscore the
distinct challenge presented by T2X: neither well-established data-to-text
architectures nor customary pretrained methodologies prove optimal. We conclude
with a qualitative analysis of generation errors and an ablation study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIFiD: Reassess Summary Factual Inconsistency Detection with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuding Yang, Hui Liu, Weidong Guo, Zhuwei Rao, Yu Xu, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring factual consistency between the summary and the original document is
paramount in summarization tasks. Consequently, considerable effort has been
dedicated to detecting inconsistencies. With the advent of Large Language
Models (LLMs), recent studies have begun to leverage their advanced language
understanding capabilities for inconsistency detection. However, early attempts
have shown that LLMs underperform traditional models due to their limited
ability to follow instructions and the absence of an effective detection
methodology. In this study, we reassess summary inconsistency detection with
LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in
LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency
Detection with Filtered Document) that identify key sentences within documents
by either employing natural language inference or measuring semantic similarity
between summaries and documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Truth-Aware Context Selection: Mitigating the Hallucinations of Large
  <span class="highlight-title">Language Models</span> Being Misled by Untruthful Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yu, Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have demonstrated impressive text
generation capabilities, they are easily misled by the untruthful context
provided by users or knowledge argumentation tools, thereby producing
hallucinations. To alleviate the LLMs from being misled by untruthful
information and take advantage of knowledge argumentation, we propose
Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful
context from the inputs. TACS begins by performing truth detection on the input
context, leveraging the parameterized knowledge within the LLM. Subsequently,
it constructs a corresponding attention mask based on the truthfulness of each
position, selecting the truthful context and discarding the untruthful context.
Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate,
to further study the LLMs' ability to accept truthful information and resist
untruthful information. Experimental results show that TACS can effectively
filter information in context and significantly improve the overall quality of
LLMs' responses when presented with misleading information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at: https://github.com/ictnlp/TACS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothee Mickus, Stig-Arne Grönroos, Joseph Attieh, Michele Boggia, Ona De Gibert, Shaoxiong Ji, Niki Andreas Lopi, Alessandro Raganato, Raúl Vázquez, Jörg Tiedemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NLP in the age of monolithic large language models is approaching its limits
in terms of size and information that can be handled. The trend goes to
modularization, a necessary step into the direction of designing smaller
sub-networks and components with specialized functionality. In this paper, we
present the MAMMOTH toolkit: a framework designed for training massively
multilingual modular machine translation systems at scale, initially derived
from OpenNMT-py and then adapted to ensure efficient training across
computation clusters. We showcase its efficiency across clusters of A100 and
V100 NVIDIA GPUs, and discuss our design philosophy and plans for future
information. The toolkit is publicly available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented as a demo at EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liang, Yuwei Wang, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and
WebNLG tasks, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large
  <span class="highlight-title">Language Models</span> by Summarizing Training Trajectories of Small Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the effectiveness of data selection for large language models (LLMs)
during pretraining and instruction fine-tuning phases, improving data
efficiency in supervised fine-tuning (SFT) for specialized domains poses
significant challenges due to the complexity of fine-tuning data. To bridge
this gap, we introduce an effective and scalable data selection method for SFT,
SmallToLarge (S2L), which leverages training trajectories from small models to
guide the data selection for larger models. We demonstrate through extensive
experiments that S2L significantly improves data efficiency in SFT for
mathematical problem-solving, reducing the training data to just 11% of the
original MathInstruct dataset (Yue et al., 2023) to match full dataset
performance while outperforming state-of-the-art data selection algorithms by
an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,
selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most
challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et
al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset
(Johnson et al., 2016), S2L again outperforms training on the full dataset
using only 50% of the data. Notably, S2L can perform data selection using a
reference model 40x smaller than the target model, proportionally reducing the
cost of data selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The
  Lengths, Bends, and Dead Ends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a fresh take on understanding the mechanisms of neural networks by
analyzing the rich structure of parameters contained within their optimization
trajectories. Towards this end, we introduce some natural notions of the
complexity of optimization trajectories, both qualitative and quantitative,
which reveal the inherent nuance and interplay involved between various
optimization choices, such as momentum, weight decay, and batch size. We use
them to provide key hallmarks about the nature of optimization in deep neural
networks: when it goes right, and when it finds itself in a dead end. Further,
thanks to our trajectory perspective, we uncover an intertwined behaviour of
momentum and weight decay that promotes directional exploration, as well as a
directional regularization behaviour of some others. We perform experiments
over large-scale vision and language settings, including large language models
(LLMs) with up to 12 billion parameters, to demonstrate the value of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancements in Large Language Models (LLMs) have been hindered by their
substantial sizes, which necessitate LLM compression methods for practical
deployment. Singular Value Decomposition (SVD) offers a promising solution for
LLM compression. However, state-of-the-art SVD-based LLM compression methods
have two key limitations: truncating smaller singular values may lead to higher
compression loss, and the lack of update on the remaining model parameters
after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM
compression method that addresses the limitations of existing methods. SVD-LLM
incorporates a truncation-aware data whitening strategy to ensure a direct
mapping between singular values and compression loss. Moreover, SVD-LLM adopts
a layer-wise closed-form model parameter update strategy to compensate for
accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total
of 11 datasets and seven models from three different LLM families at four
different scales. Our results demonstrate the superiority of SVD-LLM over
state-of-the-arts, especially at high model compression ratios. The source code
is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning
  Disentangled Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN), as a crucial research problem of
Embodied AI, requires an embodied agent to navigate through complex 3D
environments following natural language instructions. Recent research has
highlighted the promising capacity of large language models (LLMs) in VLN by
improving navigational reasoning accuracy and interpretability. However, their
predominant use in an offline manner usually suffers from substantial domain
gap between the VLN task and the LLM training corpus. This paper introduces a
novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill
parameter-efficient in-domain training to enable self-guided navigational
decision, leading to a significant mitigation of the domain gap in a
cost-effective manner. Specifically, at each timestep, the LLM is prompted to
forecast the navigational chain-of-thought by: 1) acting as a world model to
imagine the next observation according to the instruction, 2) selecting the
candidate observation that best aligns with the imagination, and 3) determining
the action based on the reasoning from the prior steps. Through constructing
formalized labels for training, the LLM can learn to generate desired and
reasonable chain-of-thought outputs for improving the action decision.
Experimental results across various training settings and popular VLN
benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room
(R4R)) show the significant superiority of NavCoT over the direct action
prediction variants. Through simple parameter-efficient finetuning, our NavCoT
outperforms a recent GPT4-based approach with ~7% relative improvement on the
R2R dataset. We believe that NavCoT will help unlock more task-adaptive and
scalable LLM-based embodied agents, which are helpful for developing real-world
robotics applications. Code is available at
https://github.com/expectorlin/NavCoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KEBench: A Benchmark on Knowledge Editing for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Huang, Haitian Zhong, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, little research has been done on knowledge editing for Large
Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of
effectively integrating diverse modalities (image and text) while ensuring
coherent and contextually relevant modifications. An existing benchmark has
three metrics (Reliability, Locality and Generality) to measure knowledge
editing for LVLMs. However, the benchmark falls short in the quality of
generated images used in evaluation and cannot assess whether models
effectively utilize edited knowledge in relation to the associated content. We
adopt different data collection methods to construct a new benchmark,
$\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive
evaluation. Leveraging a multimodal knowledge graph, our image data exhibits
clear directionality towards entities. This directional aspect can be further
utilized to extract entity-related knowledge and form editing data. We
conducted experiments of different editing methods on five LVLMs, and
thoroughly analyze how these methods impact the models. The results reveal
strengths and deficiencies of these methods and, hopefully, provide insights
into potential avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of
fine-grained sentiment analysis, aiming to extract structured sentiment
triplets from unstructured textual data. Existing approaches to ASTE often
complicate the task with additional structures or external data. In this
research, we propose a novel tagging scheme and employ a contrastive learning
approach to mitigate these challenges. The proposed approach demonstrates
comparable or superior performance in comparison to state-of-the-art
techniques, while featuring a more compact design and reduced computational
overhead. Notably, even in the era of Large Language Models (LLMs), our method
exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning
scenarios. This study also provides valuable insights for the advancement of
ASTE techniques within the paradigm of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IM-Unpack: Training and Inference with Arbitrarily Low Precision
  Integers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Zeng, Karthikeyan Sankaralingam, Vikas Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and
corresponds to the largest chunk of the compute footprint. Therefore, improving
its efficiency is an active topic of ongoing research. A popular strategy is
the use of low bit-width integers to approximate the original entries in a
matrix. This allows efficiency gains, but often requires sophisticated
techniques to control the rounding error incurred. In this work, we first
verify/check that when the low bit-width restriction is removed, for a variety
of Transformer-based models, whether integers are sufficient for all GEMMs need
-- for {\em both} training and inference stages, and can achieve parity with
floating point counterparts. No sophisticated techniques are needed. We find
that while a large majority of entries in matrices (encountered in such models)
can be easily represented by {\em low} bit-width integers, the existence of a
few heavy hitter entries make it difficult to achieve efficiency gains via the
exclusive use of low bit-width GEMMs alone. To address this issue, we develop a
simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a
matrix with large integer entries into a larger matrix whose entries all lie
within the representable range of arbitrarily low bit-width integers. This
allows {\em equivalence} with the original GEMM, i.e., the exact result can be
obtained using purely low bit-width integer GEMMs. This comes at the cost of
additional operations -- we show that for many popular models, this overhead is
quite small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-generated Text Detection: Benchmark <span class="highlight-title">Dataset</span> and Tensor-based
  Detection Method <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zubair Qazi, William Shiao, Evangelos E. Papalexakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural language models like ChatGPT become increasingly prevalent in
applications and services, the need for robust and accurate methods to detect
their output is of paramount importance. In this paper, we present GPT Reddit
Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text
detection dataset designed to assess the performance of detection models in
identifying generated responses from ChatGPT. The dataset consists of a diverse
collection of context-prompt pairs based on Reddit, with human-generated and
ChatGPT-generated responses. We provide an analysis of the dataset's
characteristics, including linguistic diversity, context complexity, and
response quality. To showcase the dataset's utility, we benchmark several
detection methods on it, demonstrating their efficacy in distinguishing between
human and ChatGPT-generated responses. This dataset serves as a resource for
evaluating and advancing detection techniques in the context of ChatGPT and
contributes to the ongoing efforts to ensure responsible and trustworthy
AI-driven communication on the internet. Finally, we propose GpTen, a novel
tensor-based GPT text detection method that is semi-supervised in nature since
it only has access to human-generated text and performs on par with
fully-supervised baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, published in the WWW 2024 Short Papers Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph Large Language Model (KG-LLM) for Link Prediction <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, submit to ECML PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming <span class="highlight-title">Pre-train</span>ed LLMs for Generalised Time Series Forecasting via
  Cross-modal Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series forecasting has recently gained great success with
the rapid growth of deep learning models. However, existing approaches usually
train models from scratch using limited temporal data, preventing their
generalization. Recently, with the surge of the Large Language Models (LLMs),
several works have attempted to introduce LLMs into time series forecasting.
Despite promising results, these methods directly take time series as the input
to LLMs, ignoring the inherent modality gap between temporal and text data. In
this work, we propose a novel Large Language Models and time series alignment
framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time
series forecasting challenge. Based on cross-modal knowledge distillation, the
proposed method exploits both input-agnostic static knowledge and
input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers
the forecasting model with favorable performance as well as strong
generalization abilities. Extensive experiments demonstrate the proposed method
establishes a new state of the art for both long- and short-term forecasting.
Code is available at \url{https://github.com/Hank0626/LLaTA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Cost-Effective and Self-Adaptive LLM Shaking and
  Recovery Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao, Dianhai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) gain great success in real-world
applications, an increasing number of users are seeking to develop and deploy
their customized LLMs through cloud services. Nonetheless, in some specific
domains, there are still concerns regarding cost and trade-offs between privacy
issues and accuracy. In this study, we introduce a cost-effective and
self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With
carefully designed horizontal and vertical shaking operators, we can achieve
comparable accuracy results with SOTA privacy-preserving LLM schemes using
Cryptography-based or Differential Privacy-based methods. Experiments also show
that with the CypherTalk framework, users can achieve reliable accuracy when
using optimized shaking operator settings. To our best knowledge, this is the
first work that considers cost, and trade-off between model utility and privacy
in LLM scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Explainable Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhong Bai, Jiabao Zhao, Tingjiang Wei, Qing Cai, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the long term accumulation of high quality educational data, artificial
intelligence has shown excellent performance in knowledge tracing. However, due
to the lack of interpretability and transparency of some algorithms, this
approach will result in reduced stakeholder trust and a decreased acceptance of
intelligent decisions. Therefore, algorithms need to achieve high accuracy, and
users need to understand the internal operating mechanism and provide reliable
explanations for decisions. This paper thoroughly analyzes the interpretability
of KT algorithms. First, the concepts and common methods of explainable
artificial intelligence and knowledge tracing are introduced. Next, explainable
knowledge tracing models are classified into two categories: transparent models
and black box models. Then, the interpretable methods used are reviewed from
three stages: ante hoc interpretable methods, post hoc interpretable methods,
and other dimensions. It is worth noting that current evaluation methods for
explainable knowledge tracing are lacking. Hence, contrast and deletion
experiments are conducted to explain the prediction results of the deep
knowledge tracing model on the ASSISTment2009 by using three XAI methods.
Moreover, this paper offers some insights into evaluation methods from the
perspective of educational stakeholders. This paper provides a detailed and
comprehensive review of the research on explainable knowledge tracing, aiming
to offer some basis and inspiration for researchers interested in the
interpretability of knowledge tracing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CKERC : Joint Large <span class="highlight-title">Language Models</span> with Commonsense Knowledge for
  Emotion Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition in conversation (ERC) is a task which predicts the
emotion of an utterance in the context of a conversation. It tightly depends on
dialogue context, speaker identity information, multiparty dialogue scenario
and so on. However, the state-of-the-art method (instructERC) solely
identifying speaker, and ignores commonsense knowledge(i.e., reaction of the
listeners and intention of the speaker, etc.) behind speakers during a
conversation, which can deeply mine speaker information. To this end, we
propose a novel joint large language models with commonsense knowledge
framework for emotion recognition in conversation, namely CKERC.We design
prompts to generate interlocutors' commonsense based on historical utterances
with large language model. And we use the interlocutor commonsense
identification task for LLM pre-training to fine-tune speaker implicit clues
information.By solving above challenge, our method achieve state-of-the-art.We
extensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,
EmoryNLP, demonstrate our method superiority. Also, we conduct in-depth
analysis and further demonstrate the effectiveness of commonsense knowledge in
ERC task in large language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BAGEL: Bootstrapping Agents by Guiding Exploration with Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, Kenton Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following natural language instructions by executing actions in digital
environments (e.g. web-browsers and REST APIs) is a challenging task for
language model (LM) agents. Unfortunately, LM agents often fail to generalize
to new environments without human demonstrations. This work presents BAGEL, a
method for bootstrapping LM agents without human supervision. BAGEL converts a
seed set of randomly explored trajectories or synthetic instructions, into
demonstrations, via round-trips between two noisy LM components: an LM labeler
which converts a trajectory into a synthetic instruction, and a zero-shot LM
agent which maps the synthetic instruction into a refined trajectory. By
performing these round-trips iteratively, BAGEL quickly converts the initial
distribution of trajectories towards those that are well-described by natural
language. We use BAGEL demonstrations to adapt a zero shot LM agent at test
time via in-context learning over retrieved demonstrations, and find
improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x
reduction in execution failures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Paper to Card: Transforming Design Implications with Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Shin, Lucy Lu Wang, Gary Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communicating design implications is common within the HCI community when
publishing academic papers, yet these papers are rarely read and used by
designers. One solution is to use design cards as a form of translational
resource that communicates valuable insights from papers in a more digestible
and accessible format to assist in design processes. However, creating design
cards can be time-consuming, and authors may lack the resources/know-how to
produce cards. Through an iterative design process, we built a system that
helps create design cards from academic papers using an LLM and text-to-image
model. Our evaluation with designers (N=21) and authors of selected papers
(N=12) revealed that designers perceived the design implications from our
design cards as more inspiring and generative, compared to reading original
paper texts, and the authors viewed our system as an effective way of
communicating their design implications. We also propose future enhancements
for AI-generated design cards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Legally Binding but Unfair? Towards Assessing Fairness of Privacy
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy policies are expected to inform data subjects about their data
protection rights. They should explain the data controller's data management
practices, and make facts such as retention periods or data transfers to third
parties transparent. Privacy policies only fulfill their purpose, if they are
correctly perceived, interpreted, understood, and trusted by the data subject.
Amongst others, this requires that a privacy policy is written in a fair way,
e.g., it does not use polarizing terms, does not require a certain education,
or does not assume a particular social background. In this work-in-progress
paper, we outline our approach to assessing fairness in privacy policies. To
this end, we identify from fundamental legal sources and fairness research, how
the dimensions informational fairness, representational fairness and
ethics/morality are related to privacy policies. We propose options to
automatically assess policies in these fairness dimensions, based on text
statistics, linguistic methods and artificial intelligence. Finally, we conduct
initial experiments with German privacy policies to provide evidence that our
approach is applicable. Our experiments indicate that there are indeed issues
in all three dimensions of fairness. For example, our approach finds out if a
policy discriminates against individuals with impaired reading skills or
certain demographics, and identifies questionable ethics. This is important, as
future privacy policies may be used in a corpus for legal artificial
intelligence models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IWSPA 2024 and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Assisted Causal Pathway Diagram for Human-Centered Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruican Zhong, Donghoon Shin, Rosemary Meza, Predrag Klasnja, Lucas Colusso, Gary Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of causal pathway diagrams (CPD) into
human-centered design (HCD), investigating how these diagrams can enhance the
early stages of the design process. A dedicated CPD plugin for the online
collaborative whiteboard platform Miro was developed to streamline diagram
creation and offer real-time AI-driven guidance. Through a user study with
designers (N=20), we found that CPD's branching and its emphasis on causal
connections supported both divergent and convergent processes during design.
CPD can also facilitate communication among stakeholders. Additionally, we
found our plugin significantly reduces designers' cognitive workload and
increases their creativity during brainstorming, highlighting the implications
of AI-assisted tools in supporting creative work and evidence-based designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Clarity: Generating Sentences with <span class="highlight-title">Transformer</span> Models using
  Context-Reverso Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Musaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of information abundance, the ability to provide users with
contextually relevant and concise information is crucial. Keyword in Context
(KIC) generation is a task that plays a vital role in and generation
applications, such as search engines, personal assistants, and content
summarization. In this paper, we present a novel approach to generating
unambiguous and brief sentence-contexts for given keywords using the T5
transformer model, leveraging data obtained from the Context-Reverso API. The
code is available at https://github.com/Rusamus/word2context/tree/main .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanics of Next Token Prediction with Self-Attention <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, Yixiao Huang, M. Emrullah Ildiz, Ankit Singh Rawat, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models are trained on large datasets to predict
the next token given an input sequence. Despite this simple training objective,
they have led to revolutionary advances in natural language processing.
Underlying this success is the self-attention mechanism. In this work, we ask:
$\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$
$\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$
$\textit{next-token}$ $\textit{prediction?}$ We show that training
self-attention with gradient descent learns an automaton which generates the
next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$
$\textbf{retrieval:}$ Given input sequence, self-attention precisely selects
the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with
the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It
then creates a convex combination of the high-priority tokens from which the
next token can be sampled. Under suitable conditions, we rigorously
characterize these mechanics through a directed graph over tokens extracted
from the training data. We prove that gradient descent implicitly discovers the
strongly-connected components (SCC) of this graph and self-attention learns to
retrieve the tokens that belong to the highest-priority SCC available in the
context window. Our theory relies on decomposing the model weights into a
directional component and a finite component that correspond to hard retrieval
and soft composition steps respectively. This also formalizes a related
implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that
these findings shed light on how self-attention processes sequential data and
pave the path toward demystifying more complex architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FluoroSAM: A Language-aligned Foundation Model for X-ray Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin D. Killeen, Liam J. Wang, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated X-ray image segmentation would accelerate research and development
in diagnostic and interventional precision medicine. Prior efforts have
contributed task-specific models capable of solving specific image analysis
problems, but the utility of these models is restricted to their particular
task domain, and expanding to broader use requires additional data, labels, and
retraining efforts. Recently, foundation models (FMs) -- machine learning
models trained on large amounts of highly variable data thus enabling broad
applicability -- have emerged as promising tools for automated image analysis.
Existing FMs for medical image analysis focus on scenarios and modalities where
objects are clearly defined by visually apparent boundaries, such as surgical
tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally
offer such clearly delineated boundaries or structure priors. During X-ray
image formation, complex 3D structures are projected in transmission onto the
imaging plane, resulting in overlapping features of varying opacity and shape.
To pave the way toward an FM for comprehensive and automated analysis of
arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned
variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic
X-ray images. FluoroSAM is trained on data including masks for 128 organ types
and 464 non-anatomical objects, such as tools and implants. In real X-ray
images of cadaveric specimens, FluoroSAM is able to segment bony anatomical
structures based on text-only prompting with 0.51 and 0.79 DICE with
point-based refinement, outperforming competing SAM variants for all
structures. FluoroSAM is also capable of zero-shot generalization to segmenting
classes beyond the training set thanks to its language alignment, which we
demonstrate for full lung segmentation on real chest X-rays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHAI: Clustered Head Attention for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) with hundreds of billions of parameters have
transformed the field of machine learning. However, serving these models at
inference time is both compute and memory intensive, where a single request can
require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is
one of the key components of LLMs, which can account for over 50% of LLMs
memory and compute requirement. We observe that there is a high amount of
redundancy across heads on which tokens they pay attention to. Based on this
insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a
high amount of correlation for self-attention at runtime, thus reducing both
memory and compute. In our experiments, we show that CHAI is able to reduce the
memory requirements for storing K,V cache by up to 21.4% and inference time
latency by up to 1.73x without any fine-tuning required. CHAI achieves this
with a maximum 3.2% deviation in accuracy across 3 different models (i.e.
OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Clarification Questions for Disambiguating Contracts <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Singhal, Chirag Jain, Preethu Rose Anish, Arkajyoti Chakraborty, Smita Ghaisas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprises frequently enter into commercial contracts that can serve as
vital sources of project-specific requirements. Contractual clauses are
obligatory, and the requirements derived from contracts can detail the
downstream implementation activities that non-legal stakeholders, including
requirement analysts, engineers, and delivery personnel, need to conduct.
However, comprehending contracts is cognitively demanding and error-prone for
such stakeholders due to the extensive use of Legalese and the inherent
complexity of contract language. Furthermore, contracts often contain
ambiguously worded clauses to ensure comprehensive coverage. In contrast,
non-legal stakeholders require a detailed and unambiguous comprehension of
contractual clauses to craft actionable requirements. In this work, we
introduce a novel legal NLP task that involves generating clarification
questions for contracts. These questions aim to identify contract ambiguities
on a document level, thereby assisting non-legal stakeholders in obtaining the
necessary details for eliciting requirements. This task is challenged by three
core issues: (1) data availability, (2) the length and unstructured nature of
contracts, and (3) the complexity of legal text. To address these issues, we
propose ConRAP, a retrieval-augmented prompting framework for generating
clarification questions to disambiguate contractual text. Experiments conducted
on contracts sourced from the publicly available CUAD dataset show that ConRAP
with ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the
generated clarification questions are deemed useful by human evaluators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big City Bias: Evaluating the Impact of Metropolitan Size on
  Computational Job Market Abilities of <span class="highlight-title">Language Models</span> <span class="chip">EACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Campanella, Rob van der Goot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have emerged as a useful technology for job
matching, for both candidates and employers. Job matching is often based on a
particular geographic location, such as a city or region. However, LLMs have
known biases, commonly derived from their training data. In this work, we aim
to quantify the metropolitan size bias encoded within large language models,
evaluating zero-shot salary, employer presence, and commute duration
predictions in 384 of the United States' metropolitan regions. Across all
benchmarks, we observe negative correlations between the metropolitan size and
the performance of the LLMS, indicating that smaller regions are indeed
underrepresented. More concretely, the smallest 10 metropolitan regions show
upwards of 300% worse benchmark performance than the largest 10.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables, NLP4HR Workshop @ EACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Authorship Style Transfer with Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Liu, Shantanu Agarwal, Jonathan May
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authorship style transfer aims to rewrite a given text into a specified
target while preserving the original meaning in the source. Existing approaches
rely on the availability of a large number of target style exemplars for model
training. However, these overlook cases where a limited number of target style
examples are available. The development of parameter-efficient transfer
learning techniques and policy optimization (PO) approaches suggest lightweight
PO is a feasible approach to low-resource style transfer. In this work, we
propose a simple two step tune-and-optimize technique for low-resource textual
style transfer. We apply our technique to authorship transfer as well as a
larger-data native language style task and in both cases find it outperforms
state-of-the-art baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Artificial Intelligence to Combat Online Hate: Exploring the
  Challenges and Opportunities of Large <span class="highlight-title">Language Models</span> in Hate Speech
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tharindu Kumarage, Amrita Bhattacharjee, Joshua Garland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in many diverse applications beyond
language generation, e.g., translation, summarization, and sentiment analysis.
One intriguing application is in text classification. This becomes pertinent in
the realm of identifying hateful or toxic speech -- a domain fraught with
challenges and ethical dilemmas. In our study, we have two objectives: firstly,
to offer a literature review revolving around LLMs as classifiers, emphasizing
their role in detecting and classifying hateful or toxic content. Subsequently,
we explore the efficacy of several LLMs in classifying hate speech: identifying
which LLMs excel in this task as well as their underlying attributes and
training. Providing insight into the factors that contribute to an LLM
proficiency (or lack thereof) in discerning hateful content. By combining a
comprehensive literature review with an empirical analysis, our paper strives
to shed light on the capabilities and constraints of LLMs in the crucial domain
of hate speech detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gujarati-English Code-Switching Speech Recognition using ensemble
  prediction of spoken language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Sharma, Basil Abraham, Preethi Jyothi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important and difficult task in code-switched speech recognition is to
recognize the language, as lots of words in two languages can sound similar,
especially in some accents. We focus on improving performance of end-to-end
Automatic Speech Recognition models by conditioning transformer layers on
language ID of words and character in the output in an per layer supervised
manner. To this end, we propose two methods of introducing language specific
parameters and explainability in the multi-head attention mechanism, and
implement a Temporal Loss that helps maintain continuity in input alignment.
Despite being unable to reduce WER significantly, our method shows promise in
predicting the correct language from just spoken data. We introduce
regularization in the language prediction by dropping LID in the sequence,
which helps align long repeated output sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Bachelor's thesis, 28 pages, includes appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological
  Analysis Based on LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we construct an automated debate judge to evaluate an extensive,
vibrant, multi-turn debate? This task is challenging, as judging a debate
involves grappling with lengthy texts, intricate argument relationships, and
multi-dimensional assessments. At the same time, current research mainly
focuses on short dialogues, rarely touching upon the evaluation of an entire
debate. In this paper, by leveraging Large Language Models (LLMs), we propose
Debatrix, which makes the analysis and assessment of multi-turn debates more
aligned with majority preferences. Specifically, Debatrix features a vertical,
iterative chronological analysis and a horizontal, multi-dimensional evaluation
collaboration. To align with real-world debate scenarios, we introduced the
PanelBench benchmark, comparing our system's performance to actual debate
outcomes. The findings indicate a notable enhancement over directly using LLMs
for debate evaluation. Source code and benchmark data are available online at
https://github.com/ljcleo/Debatrix .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pix2Pix-OnTheFly: Leveraging LLMs for <span class="highlight-title">Instruct</span>ion-Guided Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Santos, João Silva, António Branco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of language processing and image processing keeps attracting
increased interest given recent impressive advances that leverage the combined
strengths of both domains of research. Among these advances, the task of
editing an image on the basis solely of a natural language instruction stands
out as a most challenging endeavour. While recent approaches for this task
resort, in one way or other, to some form of preliminary preparation, training
or fine-tuning, this paper explores a novel approach: We propose a
preparation-free method that permits instruction-guided image editing on the
fly. This approach is organized along three steps properly orchestrated that
resort to image captioning and DDIM inversion, followed by obtaining the edit
direction embedding, followed by image editing proper. While dispensing with
preliminary preparation, our approach demonstrates to be effective and
competitive, outperforming recent, state of the art models for this task when
evaluated on the MAGICBRUSH dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Small Multimodal Models to Bridge Biomedical Competency Gap: A
  Case Study in Radiology Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling laws and extraordinary performance of large foundation models
motivate the development and utilization of such large models in biomedicine.
However, despite early promising results on some biomedical benchmarks, there
are still major challenges that need to be addressed before these models can be
used in real-world applications. Frontier models such as GPT-4V still have
major competency gaps in multimodal capabilities for biomedical applications.
Moreover, pragmatic issues such as access, cost, latency, and compliance make
it hard for clinicians to use privately-hosted state-of-the-art large models
directly on private patient data. In this paper, we explore training
open-source small multimodal models (SMMs) to bridge biomedical competency gaps
for unmet clinical needs. To maximize data efficiency, we adopt a modular
approach by incorporating state-of-the-art pre-trained models for image and
text modalities, and focusing on training a lightweight adapter to ground each
modality to the text embedding space. We conduct a comprehensive study of this
approach on radiology imaging. For training, we assemble a large dataset with
over 1 million image-text pairs. For evaluation, we propose a clinically driven
novel approach using GPT-4 and demonstrate its parity with expert evaluation.
We also study grounding qualitatively using attention. For best practice, we
conduct a systematic ablation study on various choices in data engineering and
multimodal training. The resulting LLaVA-Rad (7B) model attains
state-of-the-art results on radiology tasks such as report generation and
cross-modal retrieval, even outperforming much larger models such as GPT-4V and
Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in
private settings, offering a promising state-of-the-art tool for real-world
clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiveCodeBench: Holistic and Contamination Free Evaluation of Large
  <span class="highlight-title">Language Models</span> for Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) applied to code-related applications have
emerged as a prominent field, attracting significant interest from both
academia and industry. However, as new and improved LLMs are developed,
existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient
for assessing their capabilities. In this work, we propose LiveCodeBench, a
comprehensive and contamination-free evaluation of LLMs for code, which
continuously collects new problems over time from contests across three
competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our
benchmark also focuses on a broader range of code related capabilities, such as
self-repair, code execution, and test output prediction, beyond just code
generation. Currently, LiveCodeBench hosts four hundred high-quality coding
problems that were published between May 2023 and February 2024. We have
evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We
present empirical findings on contamination, holistic performance comparisons,
potential overfitting in existing benchmarks as well as individual model
comparisons. We will release all prompts and model completions for further
community analysis, along with a general toolkit for adding new scenarios and
model
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website - https://livecodebench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two critical
under-explored problems: the neglect of the physically grounded reasoning
(counting and position understanding) and the potential of using highly capable
text and image generation models for semantic counterfactual fine-tuning. Our
work pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
grounded image generation model GLIGEN to generate fine-tuning data, resulting
in significant performance improvements: +33% and +37% for CLIP and LLaVA,
respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we
exploit the capabilities of high-performing text generation and image
generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 8 tables, Project Page:
  https://countercurate.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Misgendering and Assuming Gender in Machine Translation when Working
  with Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourojit Ghosh, Srishti Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter focuses on gender-related errors in machine translation (MT) in
the context of low-resource languages. We begin by explaining what low-resource
languages are, examining the inseparable social and computational factors that
create such linguistic hierarchies. We demonstrate through a case study of our
mother tongue Bengali, a global language spoken by almost 300 million people
but still classified as low-resource, how gender is assumed and inferred in
translations to and from the high(est)-resource English when no such
information is provided in source texts. We discuss the postcolonial and
societal impacts of such errors leading to linguistic erasure and
representational harms, and conclude by discussing potential solutions towards
uplifting languages by providing them more agency in MT conversations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Upcoming Publication, Gendered Technology in Translation and
  Interpreting Centering Rights in the Development of Language Technology,
  Routledge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BizBench: A Quantitative Reasoning Benchmark for Business and Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions within business and finance requires reasoning,
precision, and a wide-breadth of technical knowledge. Together, these
requirements make this domain difficult for large language models (LLMs). We
introduce BizBench, a benchmark for evaluating models' ability to reason about
realistic financial problems. BizBench comprises eight quantitative reasoning
tasks, focusing on question-answering (QA) over financial data via program
synthesis. We include three financially-themed code-generation tasks from newly
collected and augmented QA data. Additionally, we isolate the reasoning
capabilities required for financial QA: reading comprehension of financial text
and tables for extracting intermediate values, and understanding financial
concepts and formulas needed to calculate complex solutions. Collectively,
these tasks evaluate a model's financial background knowledge, ability to parse
financial documents, and capacity to solve problems with code. We conduct an
in-depth evaluation of open-source and commercial LLMs, comparing and
contrasting the behavior of code-focused and language-focused models. We
demonstrate that the current bottleneck in performance is due to LLMs' limited
business and financial understanding, highlighting the value of a challenging
benchmark for quantitative reasoning within this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMSR: Symbolic Regression is a Multimodal Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan Hao, Su Wei, Yusong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical formulas are the crystallization of human wisdom in exploring
the laws of nature for thousands of years. Describing the complex laws of
nature with a concise mathematical formula is a constant pursuit of scientists
and a great challenge for artificial intelligence. This field is called
symbolic regression. Symbolic regression was originally formulated as a
combinatorial optimization problem, and GP and reinforcement learning
algorithms were used to solve it. However, GP is sensitive to hyperparameters,
and these two types of algorithms are inefficient. To solve this problem,
researchers treat the mapping from data to expressions as a translation
problem. And the corresponding large-scale pre-trained model is introduced.
However, the data and expression skeletons do not have very clear word
correspondences as the two languages do. Instead, they are more like two
modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.
The SR problem is solved as a pure multimodal problem, and contrastive learning
is also introduced in the training process for modal alignment to facilitate
later modal feature fusion. It is worth noting that in order to better promote
the modal feature fusion, we adopt the strategy of training contrastive
learning loss and other losses at the same time, which only needs one-step
training, instead of training contrastive learning loss first and then training
other losses. Because our experiments prove training together can make the
feature extraction module and feature fusion module running-in better.
Experimental results show that compared with multiple large-scale pre-training
baselines, MMSR achieves the most advanced results on multiple mainstream
datasets including SRBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Distillation of Large <span class="highlight-title">Language Models</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge of white-box LLMs into small models is still
under-explored, which becomes more important with the prosperity of open-source
LLMs. In this work, we propose a KD approach that distills LLMs into smaller
language models. We first replace the forward Kullback-Leibler divergence (KLD)
objective in the standard KD approaches with reverse KLD, which is more
suitable for KD on generative language models, to prevent the student model
from overestimating the low-probability regions of the teacher distribution.
Then, we derive an effective optimization approach to learn this objective. The
student models are named MiniLLM. Extensive experiments in the
instruction-following setting show that MiniLLM generates more precise
responses with higher overall quality, lower exposure bias, better calibration,
and higher long-text generation performance than the baselines. Our method is
scalable for different model families with 120M to 13B parameters. Our code,
data, and model checkpoints can be found in
https://github.com/microsoft/LMOps/tree/main/minillm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12177v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12177v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language
  Feedback <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10691v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10691v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve complex tasks, large language models (LLMs) often require multiple
rounds of interactions with the user, sometimes assisted by external tools.
However, current evaluation protocols often emphasize benchmark performance
with single-turn exchanges, neglecting the nuanced interactions among the user,
LLMs, and external tools, while also underestimating the importance of natural
language feedback from users. These oversights contribute to discrepancies
between research benchmark evaluations and real-world use cases. We introduce
MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn
interactions by (1) using tools and (2) leveraging natural language feedback.
To ensure reproducibility, we provide an evaluation framework where LLMs can
access tools by executing Python code and receive users' natural language
feedback simulated by GPT-4. We repurpose a diverse set of established
evaluation datasets focusing on reasoning, coding, and decision-making and
carefully curate them into a compact subset for efficient evaluation. Our
analysis of 20 open- and closed-source LLMs offers intriguing findings. (a)
LLMs generally benefit from tools and language feedback, with performance gains
(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural
language feedback. (b) Better single-turn performance does not guarantee better
multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised
instruction-finetuning (SIFT) and reinforcement learning from human feedback
(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure
progress and incentivize research in improving LLMs' capabilities in multi-turn
interactions, especially for open-source communities where multi-turn human
evaluation can be less accessible compared to commercial LLMs with a larger
user base.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024. Code is available on our project website:
  https://xingyaoww.github.io/mint-bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEND: Meta dEmonstratioN Distillation for Efficient and Effective
  In-Context Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models (LLMs) have demonstrated impressive in-context learning
(ICL) capabilities, where a LLM makes predictions for a given test input
together with a few input-output pairs (demonstrations). Nevertheless, the
inclusion of demonstrations leads to a quadratic increase in the computational
overhead of the self-attention mechanism. Existing solutions attempt to distill
lengthy demonstrations into compact vectors. However, they often require
task-specific retraining or compromise LLM's in-context learning performance.
To mitigate these challenges, we present Meta dEmonstratioN Distillation
(MEND), where a language model learns to distill any lengthy demonstrations
into vectors without retraining for a new downstream task. We exploit the
knowledge distillation to enhance alignment between MEND and LLM, achieving
both efficiency and effectiveness simultaneously. MEND is endowed with the
meta-knowledge of distilling demonstrations through a two-stage training
process, which includes meta-distillation pretraining and fine-tuning.
Comprehensive evaluations across seven diverse ICL task partitions using
decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not
only matches but often outperforms the Vanilla ICL as well as other
state-of-the-art distillation models, while significantly reducing the
computational demands. This innovation promises enhanced scalability and
efficiency for the practical deployment of large language models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language
  Conversion for <span class="highlight-title">Language Models</span> <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbols (or more broadly, non-natural language textual representations) such
as numerical sequences, molecular formulas, and table delimiters widely exist,
playing important roles in various tasks such as abstract reasoning, chemical
property prediction, and table question answering. Despite the impressive
natural language comprehension capabilities of large language models (LLMs),
their reasoning abilities for symbols remain inadequate, which could attributed
to the difference between symbol representations and general natural languages.
We propose symbol-to-language (S2L), a tuning-free method that enables large
language models to solve symbol-related problems with information expressed in
natural language. Specifically, S2L first converts the symbols involved to
language-based representations, which can be implemented by prompting LLMs or
leveraging external tools, then these language-based representations are
integrated into the original problem via direct substitution or concatenation,
serving as useful input information for LLMs. We evaluate the S2L method using
both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight
symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment
analysis in social media. Experimental results show that S2L consistently leads
to superior performance. For example, by employing S2L for GPT-4, there can be
average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and
Dyck language, respectively. Codes and data are available at
https://github.com/THUNLP-MT/symbol2language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR AGI Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Specific Representation of Emotion-Concept Knowledge Causally
  Supports Emotion Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09582v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09582v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan Liu, Dan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans no doubt use language to communicate about their emotional
experiences, but does language in turn help humans understand emotions, or is
language just a vehicle of communication? This study used a form of artificial
intelligence (AI) known as large language models (LLMs) to assess whether
language-based representations of emotion causally contribute to the AI's
ability to generate inferences about the emotional meaning of novel situations.
Fourteen attributes of human emotion concept representation were found to be
represented by the LLM's distinct artificial neuron populations. By
manipulating these attribute-related neurons, we in turn demonstrated the role
of emotion concept knowledge in generative emotion inference. The
attribute-specific performance deterioration was related to the importance of
different attributes in human mental space. Our findings provide a
proof-in-concept that even a LLM can learn about emotions in the absence of
sensory-motor representations and highlight the contribution of
language-derived emotion-concept knowledge for emotion inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech
  Models via Language-Specific Experts <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina Nikoulina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still underperforms on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effects of Noise in <span class="highlight-title">Text-to-SQL</span>: An Examination of the
  BIRD-Bench Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi, Oskar Holmström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL, which involves translating natural language into Structured
Query Language (SQL), is crucial for enabling broad access to structured
databases without expert knowledge. However, designing models for such tasks is
challenging due to numerous factors, including the presence of 'noise,' such as
ambiguous questions and syntactical errors. This study provides an in-depth
analysis of the distribution and types of noise in the widely used BIRD-Bench
benchmark and the impact of noise on models. While BIRD-Bench was created to
model dirty and noisy database values, it was not created to contain noise and
errors in the questions and gold queries. We found that noise in questions and
gold queries are prevalent in the dataset, with varying amounts across domains,
and with an uneven distribution between noise types. The presence of incorrect
gold SQL queries, which then generate incorrect gold answers, has a significant
impact on the benchmark's reliability. Surprisingly, when evaluating models on
corrected SQL queries, zero-shot baselines surpassed the performance of
state-of-the-art prompting methods. We conclude that informative noise labels
and reliable benchmarks are crucial to developing new Text-to-SQL methods that
can handle varying types of noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRAM: Bridging Trust Regions and Sharpness Aware Minimization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness-aware minimization (SAM) reports improving domain generalization by
reducing the loss surface curvature in the parameter space. However,
generalization during fine-tuning is often more dependent on the
transferability of representations in the function space. Trust-region methods
(TR) target this goal by regularizing representation curvature to reduce
catastrophic forgetting of pre-trained task-agnostic information while adopting
task-specific skills. We consider unifying these strategies for low curvature
in both parameter space and function space to improve out-of-domain (OOD)
generalization. We propose Trust Region Aware Minimization (TRAM), a SAM
algorithm fine-tuning for low parameter sharpness and smooth, informative
representations preserving pre-trained structure. TRAM uses a trust region
bound to inform the SAM adversarial neighborhood, introducing an awareness of
function curvature within optimization for flatter minima. We empirically
validate TRAM in vision (cross-dataset adaptation) and text (OOD language
modeling, zero-shot cross-lingual transfer) tasks where robust domain transfer
and representation generality are critical. TRAM outperforms SAM- and TR-based
optimization across all tasks, notably surpassing competing methods for hard
transfer between anticorrelated domains. TRAM establishes a novel standard in
fine-tuning for domain-generalizable models with minimal additional computation
over previous sharpness-aware methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14
  tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Hang Zhang, Chen Lin, Xiangdong Su, Yeyun Gong, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form numerical reasoning in financial analysis aims to generate a
reasoning program to calculate the correct answer for a given question.
Previous work followed a retriever-generator framework, where the retriever
selects key facts from a long-form document, and the generator generates a
reasoning program based on retrieved facts. However, they treated all facts
equally without considering the different contributions of facts with and
without numbers. Meanwhile, the program consistency were ignored under
supervised training, resulting in lower training accuracy and diversity. To
solve these problems, we proposed APOLLO to improve the long-form numerical
reasoning framework. For the retriever, we adopt a number-aware negative
sampling strategy to enable the retriever to be more discriminative on key
numerical facts. For the generator, we design consistency-based reinforcement
learning and target program augmentation strategy based on the consistency of
program execution results. Experimental results on the FinQA and ConvFinQA
leaderboard verify the effectiveness of our proposed method, achieving the new
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Instruct</span>ERC: Reforming Emotion Recognition in Conversation with a
  Retrieval Multi-task LLMs Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of emotion recognition of conversation (ERC) has been focusing on
separating sentence feature encoding and context modeling, lacking exploration
in generative paradigms based on unified designs. In this study, we propose a
novel approach,
  \textbf{InstructERC}, to reformulate the ERC task from a discriminative
framework to a generative framework based on Large Language Models (LLMs).
  InstructERC makes three significant contributions: (1) it introduces a simple
yet effective retrieval template module, which helps the model explicitly
integrate multi-granularity dialogue supervision information. (2) We introduce
two additional emotion alignment tasks, namely speaker identification and
emotion prediction tasks, to implicitly model the dialogue role relationships
and future emotional tendencies in conversations. (3) Pioneeringly, we unify
emotion labels across benchmarks through the feeling wheel to fit real
application scenarios. InstructERC still perform impressively on this unified
dataset. Our LLM-based plugin framework significantly outperforms all previous
models and achieves comprehensive SOTA on three commonly used ERC datasets.
Extensive analysis of parameter-efficient and data-scaling experiments provides
empirical guidance for applying it in practical scenarios. Our code and aligned
unified dataset (UIME) can be found in the Github link.\footnote{You can find
the offical realization in the Github link:
https://github.com/LIN-SHANG/InstructERC}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19282v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19282v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Zhenxiang Li, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang Xu, Wei Li, Zhongying Tu, Hang Yan, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents WanJuan-CC, a safe and high-quality open-sourced English
webtext dataset derived from Common Crawl data. The study addresses the
challenges of constructing large-scale pre-training datasets for language
models, which require vast amounts of high-quality data. A comprehensive
process was designed to handle Common Crawl data, including extraction,
heuristic rule filtering, fuzzy deduplication, content safety filtering, and
data quality filtering. From approximately 68 billion original English
documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of
high-quality data as part of WanJuan-CC. We have open-sourced 100B Tokens from
this dataset. The paper also provides statistical information related to data
quality, enabling users to select appropriate data according to their needs. To
evaluate the quality and utility of the dataset, we trained 1B-parameter and
3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results
show that WanJuan-CC performs better on validation datasets and downstream
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Liu, Jingjing Wang, Jiamin Luo, Tao Zeng, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,
Question-Answering and Dialogue) has attracted ever-more interest in recent
years and achieved important progresses. However, existing studies on
interactive ASU largely ignore the coreference issue for opinion targets (i.e.,
aspects), while this phenomenon is ubiquitous in interactive scenarios
especially dialogues, limiting the ASU performance. Recently, large language
models (LLMs) shows the powerful ability to integrate various NLP tasks with
the chat paradigm. In this way, this paper proposes a new Chat-based Aspect
Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in
understanding aspect sentiments in dialogue scenarios. Particularly, this
ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to
address the aspect coreference issue. On this basis, we propose a Trusted
Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.
Specifically, this TSA treats the ACR task as an auxiliary task to boost the
performance of the primary ASU task, and further integrates trusted learning
into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination
problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to
evaluate TSA, and extensive experiments show that our proposed TSA can
significantly outperform several state-of-the-art baselines, justifying the
effectiveness of TSA to ChatASU and the importance of considering the
coreference and hallucination issues in ChatASU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WsiCaption: Multiple Instance <span class="highlight-title">Generation</span> of Pathology Reports for
  Gigapixel Whole-Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide images are the foundation of digital pathology for the diagnosis
and treatment of carcinomas. Writing pathology reports is laborious and
error-prone for inexperienced pathologists. To reduce the workload and improve
clinical automation, we investigate how to generate pathology reports given
whole slide images. On the data end, we curated the largest WSI-text dataset
(TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text
pairs for visual-language models by recognizing and cleaning pathology reports
which narrate diagnostic slides in TCGA. On the model end, we propose the
multiple instance generative model (MI-Gen) which can produce pathology reports
for gigapixel WSIs. We benchmark our model on the largest subset of
TCGA-PathoText. Experimental results show our model can generate pathology
reports which contain multiple clinical clues. Furthermore, WSI-text prediction
can be seen as an approach of visual-language pre-training, which enables our
model to be transferred to downstream diagnostic tasks like carcinoma grading
and phenotyping. We observe that simple semantic extraction from the pathology
reports can achieve the best performance (0.838 of F1 score) on BRCA subtyping
without adding extra parameters or tricky fine-tuning. Our collected dataset
and related code are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Audio-textual Diffusion Model For Converting Speech Signals Into
  Ultrasound Tongue Imaging Data <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, Lan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator
movements, such as ultrasound tongue imaging (UTI) data. An issue of existing
AAI methods is only using the personalized acoustic information to derive the
general patterns of tongue motions, and thus the quality of generated UTI data
is limited. To address this issue, this paper proposes an audio-textual
diffusion model for the UTI data generation task. In this model, the inherent
acoustic characteristics of individuals related to the tongue motion details
are encoded by using wav2vec 2.0, while the ASR transcriptions related to the
universality of tongue motions are encoded by using BERT. UTI data are then
generated by using a diffusion module. Experimental results showed that the
proposed diffusion model could generate high-quality UTI data with clear tongue
contour that is crucial for the linguistic analysis and clinical assessment.
The project can be found on the
website\footnote{https://yangyudong2020.github.io/wav2uti/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP2024 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIcK: A Benchmark <span class="highlight-title">Dataset</span> of Cultural and Linguistic Intelligence in
  Korean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of large language models (LLMs) for the Korean
language, there remains an obvious lack of benchmark datasets that test the
requisite Korean cultural and linguistic knowledge. Because many existing
Korean benchmark datasets are derived from the English counterparts through
translation, they often overlook the different cultural contexts. For the few
benchmark datasets that are sourced from Korean data capturing cultural
knowledge, only narrow tasks such as bias and hate speech detection are
offered. To address this gap, we introduce a benchmark of Cultural and
Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.
CLIcK sources its data from official Korean exams and textbooks, partitioning
the questions into eleven categories under the two main categories of language
and culture. For each instance in CLIcK, we provide fine-grained annotation of
which cultural and linguistic knowledge is required to answer the question
correctly. Using CLIcK, we test 13 language models to assess their performance.
Our evaluation uncovers insights into their performances across the categories,
as well as the diverse factors affecting their comprehension. CLIcK offers the
first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in
Korean culture and language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model based Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Sharpness as Alerts: An Inner Representation Perspective for
  Hallucination Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) frequently hallucinate and produce factual
errors, yet our understanding of why they make these errors remains limited. In
this study, we delve into the underlying mechanisms of LLM hallucinations from
the perspective of inner representations, and discover a salient pattern
associated with hallucinations: correct generations tend to have sharper
context activations in the hidden states of the in-context tokens, compared to
the incorrect ones. Leveraging this insight, we propose an entropy-based metric
to quantify the ``sharpness'' among the in-context hidden states and
incorporate it into the decoding process to formulate a constrained decoding
approach. Experiments on various knowledge-seeking and hallucination benchmarks
demonstrate our approach's consistent effectiveness, for example, achieving up
to an 8.6 point improvement on TruthfulQA. We believe this study can improve
our understanding of hallucinations and serve as a practical solution for
hallucination mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code repo is available at:
  https://github.com/hkust-nlp/Activation_decoding.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented
  Dialogue Agents <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13040v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13040v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) models have made significant progress in recent
years. However, previous studies primarily focus on datasets written by
annotators, which has resulted in a gap between academic research and
real-world spoken conversation scenarios. While several small-scale spoken TOD
datasets are proposed to address robustness issues such as ASR errors, they
ignore the unique challenges in spoken conversation. To tackle the limitations,
we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,
containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from
human-to-human spoken conversations. SpokenWOZ further incorporates common
spoken characteristics such as word-by-word processing and reasoning in spoken
language. Based on these characteristics, we present cross-turn slot and
reasoning slot detection as new challenges. We conduct experiments on various
baselines, including text-modal models, newly proposed dual-modal models, and
LLMs, e.g., ChatGPT. The results show that the current models still have
substantial room for improvement in spoken conversation, where the most
advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and
the SOTA end-to-end model only correctly completes the user request in 52.1% of
dialogues. The dataset, code, and leaderboard are available:
https://spokenwoz.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cons<span class="highlight-title">Prompt</span>: Exploiting Contrastive Samples for Fewshot <span class="highlight-title">Prompt</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinta Weng, Yifan Deng, d Donghao Li, Hao You, Yue Hu, Heyan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prompt has become an effective linguistic tool for utilizing pre-trained
language models. However, in few-shot scenarios, subtle changes in the prompt
design always make the result widely different, and the prompt learning methods
also make it easy to overfit the limited samples. To alleviate this, we explore
utilizing suitable contrastive samples and multi-degree contrastive learning
methods to improve the robustness of the prompt representation. Therefore, the
proposed Consprompt combined with the prompt encoding network, contrastive
sampling modules, and contrastive scoring modules, is introduced to realize
differential contrastive learning. Our results exhibit state-of-the-art
performance in different few-shot settings, and the ablation experiments also
certify the effectiveness of utilizing multi-degree contrastive learning in the
prompt-based fine-tuning process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Language Models</span> Great on Tabular Prediction <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Z. Chen, Jimeng Sun, Jian Wu, Jintai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transferability of deep neural networks (DNNs) has made significant
progress in image and language processing. However, due to the heterogeneity
among tables, such DNN bonus is still far from being well exploited on tabular
data prediction (e.g., regression or classification tasks). Condensing
knowledge from diverse domains, language models (LMs) possess the capability to
comprehend feature names from various tables, potentially serving as versatile
learners in transferring knowledge across distinct tables and diverse
prediction tasks, but their discrete text representation space is inherently
incompatible with numerical feature values in tables. In this paper, we present
TP-BERTa, a specifically pre-trained LM for tabular data prediction.
Concretely, a novel relative magnitude tokenization converts scalar numerical
feature values to finely discrete, high-dimensional tokens, and an
intra-feature attention approach integrates feature values with the
corresponding feature names. Comprehensive experiments demonstrate that our
pre-trained TP-BERTa leads the performance among tabular DNNs and is
competitive with Gradient Boosted Decision Tree models in typical tabular data
regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).
  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be
  available at https://github.com/jyansir/tp-berta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overthinking the Truth: Understanding how <span class="highlight-title">Language Models</span> Process False
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models can imitate complex patterns through few-shot
learning, enabling them to complete challenging tasks without fine-tuning.
However, imitation can also lead models to reproduce inaccuracies or harmful
content if present in the context. We study harmful imitation through the lens
of a model's internal representations, and identify two related phenomena:
"overthinking" and "false induction heads". The first phenomenon, overthinking,
appears when we decode predictions from intermediate layers, given correct vs.
incorrect few-shot demonstrations. At early layers, both demonstrations induce
similar model behavior, but the behavior diverges sharply at some "critical
layer", after which the accuracy given incorrect demonstrations progressively
decreases. The second phenomenon, false induction heads, are a possible
mechanistic cause of overthinking: these are heads in late layers that attend
to and copy false information from previous demonstrations, and whose ablation
reduces overthinking. Beyond scientific understanding, our results suggest that
studying intermediate model computations could be a promising avenue for
understanding and guarding against harmful model behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> "In Dialogues We Learn": Towards Personalized Dialogue Without
  Pre-defined Profiles through In-Dialogue Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zheng<span class="highlight-author">tao Yu</span>, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized dialogue systems have gained significant attention in recent
years for their ability to generate responses in alignment with different
personas. However, most existing approaches rely on pre-defined personal
profiles, which are not only time-consuming and labor-intensive to create but
also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning
framework that enhances the ability of pre-trained large language models to
leverage dialogue history to characterize persona for completing personalized
dialogue generation tasks without pre-defined profiles. Our experiments on
three datasets demonstrate that IDL brings substantial improvements, with BLEU
and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,
the results of human evaluations further validate the efficacy of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Alignment with <span class="highlight-title">Instruct</span>ion Backtranslation <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06259v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06259v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, Mike Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a scalable method to build a high quality instruction following
language model by automatically labelling human-written text with corresponding
instructions. Our approach, named instruction backtranslation, starts with a
language model finetuned on a small amount of seed data, and a given web
corpus. The seed model is used to construct training examples by generating
instruction prompts for web documents (self-augmentation), and then selecting
high quality examples from among these candidates (self-curation). This data is
then used to finetune a stronger model. Finetuning LLaMa on two iterations of
our approach yields a model that outperforms all other LLaMa-based models on
the Alpaca leaderboard not relying on distillation data, demonstrating highly
effective self-alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Learning with Retrieved Demonstrations for <span class="highlight-title">Language Models</span>: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11624v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11624v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Take a Step Back: Evoking Reasoning via Abstraction in Large Language
  Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaixiu Steven Zheng, Swaroop Mishra, <span class="highlight-author">Xinyun Chen</span>, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Back Prompting, a simple prompting technique that enables
LLMs to do abstractions to derive high-level concepts and first principles from
instances containing specific details. Using the concepts and principles to
guide reasoning, LLMs significantly improve their abilities in following a
correct reasoning path towards the solution. We conduct experiments of
Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe
substantial performance gains on various challenging reasoning-intensive tasks
including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back
Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%
and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Opinion Dynamics with Networks of LLM-based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately simulating human opinion dynamics is crucial for understanding a
variety of societal phenomena, including polarization and the spread of
misinformation. However, the agent-based models (ABMs) commonly used for such
simulations often over-simplify human behavior. We propose a new approach to
simulating opinion dynamics based on populations of Large Language Models
(LLMs). Our findings reveal a strong inherent bias in LLM agents towards
producing accurate information, leading simulated agents to consensus in line
with scientific reality. This bias limits their utility for understanding
resistance to consensus views on issues like climate change. After inducing
confirmation bias through prompt engineering, however, we observed opinion
fragmentation in line with existing agent-based modeling and opinion dynamics
research. These insights highlight the promise and limitations of LLM agents in
this domain and suggest a path forward: refining LLMs with real-world discourse
to better simulate the evolution of human beliefs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting <span class="highlight-title">Transformer</span>s with Recursively Composed Multi-grained
  Representations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ReCAT, a recursive composition augmented Transformer that is able
to explicitly model hierarchical syntactic structures of raw texts without
relying on gold trees during both learning and inference. Existing research
along this line restricts data to follow a hierarchical tree structure and thus
lacks inter-span communications. To overcome the problem, we propose a novel
contextual inside-outside (CIO) layer that learns contextualized
representations of spans through bottom-up and top-down passes, where a
bottom-up pass forms representations of high-level spans by composing low-level
spans, while a top-down pass combines information inside and outside a span. By
stacking several CIO layers between the embedding layer and the attention
layers in Transformer, the ReCAT model can perform both deep intra-span and
deep inter-span interactions, and thus generate multi-grained representations
fully contextualized with other spans. Moreover, the CIO layers can be jointly
pre-trained with Transformers, making ReCAT enjoy scaling ability, strong
performance, and interpretability at the same time. We conduct experiments on
various sentence-level and span-level tasks. Evaluation results indicate that
ReCAT can significantly outperform vanilla Transformer models on all span-level
tasks and baselines that combine recursive networks with Transformers on
natural language inference tasks. More interestingly, the hierarchical
structures induced by ReCAT exhibit strong consistency with human-annotated
syntactic trees, indicating good interpretability brought by the CIO layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">ChatGPT</span> is a Knowledgeable but Inexperienced Solver: An Investigation of
  Commonsense Problem in Large <span class="highlight-title">Language Models</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He, Shanshan Jiang, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have made significant progress in NLP. However,
their ability to memorize, represent, and leverage commonsense knowledge has
been a well-known pain point. In this paper, we specifically focus on ChatGPT,
a widely used and easily accessible LLM, and ask the following questions: (1)
Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of
the underlying commonsense knowledge for answering a specific question? (3) Is
ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage
commonsense for answering questions? We conduct a series of experiments on 11
datasets to evaluate ChatGPT's commonsense abilities, including answering
commonsense questions, identifying necessary knowledge, generating knowledge
descriptions, and using knowledge descriptions to answer questions again.
Experimental results show that: (1) ChatGPT can achieve good QA accuracies in
commonsense tasks, while still struggling with certain domains of datasets. (2)
ChatGPT is knowledgeable, and can accurately generate most of the commonsense
knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an
inexperienced commonsense problem solver, which cannot precisely identify the
needed commonsense for answering a specific question. These findings raise the
need to explore improved mechanisms for effectively incorporating commonsense
into LLMs like ChatGPT, such as better instruction following and commonsense
guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias and Fairness in Large <span class="highlight-title">Language Models</span>: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements of large language models (LLMs) have enabled the
processing, understanding, and generation of human-like text, with increasing
integration into systems that touch our social sphere. Despite this success,
these models can learn, perpetuate, and amplify harmful social biases. In this
paper, we present a comprehensive survey of bias evaluation and mitigation
techniques for LLMs. We first consolidate, formalize, and expand notions of
social bias and fairness in natural language processing, defining distinct
facets of harm and introducing several desiderata to operationalize fairness
for LLMs. We then unify the literature by proposing three intuitive taxonomies,
two for bias evaluation, namely metrics and datasets, and one for mitigation.
Our first taxonomy of metrics for bias evaluation disambiguates the
relationship between metrics and evaluation datasets, and organizes metrics by
the different levels at which they operate in a model: embeddings,
probabilities, and generated text. Our second taxonomy of datasets for bias
evaluation categorizes datasets by their structure as counterfactual inputs or
prompts, and identifies the targeted harms and social groups; we also release a
consolidation of publicly-available datasets for improved access. Our third
taxonomy of techniques for bias mitigation classifies methods by their
intervention during pre-processing, in-training, intra-processing, and
post-processing, with granular subcategories that elucidate research trends.
Finally, we identify open problems and challenges for future work. Synthesizing
a wide range of recent research, we aim to provide a clear guide of the
existing literature that empowers researchers and practitioners to better
understand and prevent the propagation of bias in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health-LLM: Personalized Retrieval-Augmented Disease Prediction System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00746v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00746v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Suiyuan Zhu, Mengnan Du, Yanda Meng, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) in healthcare has significantly advanced
intelligent medical treatment. However, traditional intelligent healthcare is
limited by static data and unified standards, preventing full integration with
individual situations and other challenges. Hence, a more professional and
detailed intelligent healthcare method is needed for development. To this end,
we propose an innovative framework named Heath-LLM, which combines large-scale
feature extraction and medical knowledge trade-off scoring. Compared to
traditional health management methods, our system has three main advantages.
First, our system integrates health reports into a large model to provide
detailed task information. Second, professional medical expertise is used to
adjust the weighted scores of health characteristics. Third, we use a
semi-automated feature extraction framework to enhance the analytical power of
language models and incorporate expert insights to improve the accuracy of
disease prediction. We have conducted disease prediction experiments on a large
number of health reports to assess the effectiveness of Health-LLM. The results
of the experiments indicate that the proposed system surpasses traditional
methods and has the potential to revolutionize disease prediction and
personalized health management. The code is available at
https://github.com/jmyissb/HealthLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents -- it
can successfully complete 51.1 of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out to be not effective for web agents, and the best grounding strategy
we develop in this paper leverages both the HTML structure and visuals. Yet,
there is still a substantial gap with oracle grounding, leaving ample room for
further improvement. All code, data, and evaluation tools are available at
https://github.com/OSU-NLP-Group/SeeAct.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Taxonomy-Guided <span class="highlight-title">Instruct</span>ion Tuning Framework for Entity Set
  Expansion and Taxonomy Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy
Construction are three representative tasks that can be used to automatically
populate an existing taxonomy with new entities. However, previous approaches
often address these tasks separately with heterogeneous techniques, lacking a
unified perspective. To tackle this issue, in this paper, we identify the
common key skills needed for these tasks from the view of taxonomy structures
-- finding 'siblings' and finding 'parents' -- and propose a unified
taxonomy-guided instruction tuning framework to jointly solve the three tasks.
To be specific, by leveraging the existing taxonomy as a rich source of entity
relationships, we utilize instruction tuning to fine-tune a large language
model to generate parent and sibling entities. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct,
which outperforms task-specific baselines across all three tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-stage LLM Fine-tuning with Less Specialization and More
  Generalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit S Dhillon, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained large language models (LLMs) are general purpose problem solvers
applicable to a diverse set of tasks with prompts. They can be further improved
towards a specific task by fine-tuning on a specialized dataset. However,
fine-tuning usually makes the model narrowly specialized on this dataset with
reduced general in-context learning performances, which is undesirable whenever
the fine-tuned model needs to handle additional tasks where no fine-tuning data
is available. In this work, we first demonstrate that fine-tuning on a single
task indeed decreases LLMs' general in-context learning performance. We
discover one important cause of such forgetting, format specialization, where
the model overfits to the format of the fine-tuned task.We further show that
format specialization happens at the very beginning of fine-tuning. To solve
this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet
effective two-stage fine-tuning framework that reduces format specialization
and improves generalization.ProMoT offloads task-specific format learning into
additional and removable parameters by first doing prompt tuning and then
fine-tuning the model itself with this soft prompt attached. With experiments
on several fine-tuning tasks and 8 in-context evaluation tasks, we show that
ProMoT achieves comparable performance on fine-tuned tasks to standard
fine-tuning, but with much less loss of in-context learning performances across
a board range of out-of-domain evaluation tasks. More importantly, ProMoT can
even enhance generalization on in-context learning tasks that are semantically
related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly
improves performance on other language pairs, and ProMoT on NLI improves
performance on summarization. Experiments also show that ProMoT can improve the
generalization performance of multi-task training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection of <span class="highlight-title">ChatGPT</span> Fake Science with the xFakeSci Learning Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeen Hamed, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT and generative AI tools are becoming the new reality. This work is
motivated by the premise that ``ChatGPT content may exhibit a distinctive
behavior that can be separated from scientific articles''. In this study, we
demonstrate how we tested this premise in two phases and prove its validity.
Subsequently, we introduce xFakeSci, a novel learning algorithm, that is
capable of distinguishing ChatGPT-generated articles from publications produced
by scientists. The algorithm is trained using network models driven from
multiple types of data sources, such as ChatGPT-generated documents achieved by
means of prompt-engineering, and PubMed articles. To mitigate over-fitting
issues, we incorporate a calibration step that is built upon data-driven
heuristics, including ratios. We evaluate the algorithm across multiple
datasets covering publication periods and diseases (cancer, depression, and
Alzheimer's). Further, we show how the algorithm is benchmarked against the
state-of-the-art (SOTA) algorithms. While the xFakeSci algorithm achieve F1
score ranging from 80% - 94%, SOTA algorithms score F1 values between 38% -
52%. We attribute the noticeable difference to the introduction of calibration
and a proximity distance heuristic, which we underscore this promising
performance. Indeed, the prediction of fake science generated by ChatGPT
presents a considerable challenge. Nonetheless, the introduction of xFakeSci
algorithm is a significant step on the way to combating fake science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 6 tables, 5 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Network Formation and Dynamics Among Multi-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Papachristou, Yuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social networks shape opinions, behaviors, and information dissemination in
human societies. As large language models (LLMs) increasingly integrate into
social and professional environments, understanding their behavior within the
context of social interactions and networks becomes essential. Our study
analyzes LLMs' network formation behavior to examine whether the dynamics of
multiple LLMs are similar to or different from human social dynamics. We
observe that LLMs exhibit key social network principles, including preferential
attachment, triadic closure, homophily, community structure, and the
small-world phenomenon, when asked about their preferences in network
formation. We also investigate LLMs' decision-making based on real-world
networks, revealing that triadic closure and homophily have a stronger
influence than preferential attachment and that LLMs perform well in network
formation predictions. Overall, our study opens up new possibilities for using
LLMs in network science research and helps develop socially aware LLMs by
shedding light on their network formation behaviors and exploring their impacts
on social dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Schema-Driven Information Extraction from Heterogeneous Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the question of whether large language models can
support cost-efficient information extraction from tables. We introduce
schema-driven information extraction, a new task that transforms tabular data
into structured records following a human-authored schema. To assess various
LLM's capabilities on this task, we present a benchmark comprised of tables
from four diverse domains: machine learning papers, chemistry literature,
material science journals, and webpages. We use this collection of annotated
tables to evaluate the ability of open-source and API-based language models to
extract information from tables covering diverse domains and data formats. Our
experiments demonstrate that surprisingly competitive performance can be
achieved without requiring task-specific pipelines or labels, achieving F1
scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,
through detailed ablation studies and analyses, we investigate the factors
contributing to model success and validate the practicality of distilling
compact models to reduce API reliance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciGLM: Training Scientific <span class="highlight-title">Language Models</span> with Self-Reflective
  <span class="highlight-title">Instruct</span>ion Annotation and Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promise in assisting scientific
discovery. However, such applications are currently limited by LLMs'
deficiencies in understanding intricate scientific concepts, deriving symbolic
equations, and solving advanced numerical calculations. To bridge these gaps,
we introduce SciGLM, a suite of scientific language models able to conduct
college-level scientific reasoning. Central to our approach is a novel
self-reflective instruction annotation framework to address the data scarcity
challenge in the science domain. This framework leverages existing LLMs to
generate step-by-step reasoning for unlabelled scientific questions, followed
by a process of self-reflective critic-and-revise. Applying this framework, we
curated SciInstruct, a diverse and high-quality dataset encompassing physics,
chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of
language models with SciInstruct, enhancing their scientific and mathematical
reasoning capabilities. Remarkably, the SciGLM consistently improves both the
base model (ChatGLM3-6B-Base) by 4.87% and larger-scale models (32B) by 2.67%,
without sacrificing the language understanding capabilities of the base model.
This makes SciGLM a suitable foundational model to facilitate diverse
scientific discovery tasks. For the benefit of the wider research community, we
release SciInstruct, and SciGLM, alongside a self-reflective framework and
fine-tuning code at https://github.com/THUDM/SciGLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:25:04.261077611Z">
            2024-03-28 05:25:04 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
